diff -Naur linux-2.6.9.src/afterburn/afterburn.c linux-2.6.9/afterburn/afterburn.c
--- linux-2.6.9.src/afterburn/afterburn.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/afterburn.c	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,161 @@
+
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <asm/current.h>
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+
+// Exports to the afterburn wedge.
+
+#if defined(CONFIG_AFTERBURN_THREAD_HOOKS)
+void * afterburn_thread_get_handle( void )
+{
+    return current->afterburn_handle;
+}
+
+void afterburn_thread_assign_handle( void * handle )
+{
+    current->afterburn_handle = handle;
+}
+
+
+// Imports from the afterburn wedge.  If these hooks are in the bss,
+// Linux will zero them after the wedge has hooked them.
+
+void afterburn_exit_thread( void *handle )
+{
+    // Dummy
+}
+
+int afterburn_signal_thread( void *handle )
+{
+    // Dummy
+    return 0;
+}
+
+void (*afterburn_exit_hook)( void *handle ) = afterburn_exit_thread;
+int (*afterburn_signal_hook)( void *handle ) = afterburn_signal_thread;
+#endif	/* CONFIG_AFTERBURN_THREAD_HOOKS */
+
+
+unsigned int afterburn_cpu_get_startup_ip( unsigned int apic_id  )
+{
+#if defined CONFIG_SMP
+	extern unsigned char startup_32_smp[];	
+	return (unsigned int) startup_32_smp;
+#else
+	return 0;
+#endif
+}
+
+
+#if defined(CONFIG_AFTERBURN_MODULE_HOOKS)
+__attribute__((section(".data"))) int (*afterburn_rewrite_module_hook)( unsigned long ) = NULL;
+#endif
+
+#if defined(CONFIG_AFTERBURN_XEN_HOOKS)
+__attribute__((section(".data"))) void (*afterburn_sync_esp0)(void) = NULL;
+#endif
+
+__attribute__((section("__ex_table"))) struct exception_table_entry 
+  afterburn_uaccess_fault_handler = {
+      (long)&afterburn_uaccess_fault_handler, 
+      (long)&afterburn_uaccess_fault_handler
+  };
+
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+__attribute__((section(".data"))) unsigned long (*afterburn_get_user_hook)(void *to, const void *from, unsigned long n)  = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_put_user_hook)(void *to, const void *from, unsigned long n) = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_copy_from_user_hook)(void *to, const void *from, unsigned long n)  = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_copy_to_user_hook)(void *to, const void *from, unsigned long n) = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_clear_user_hook)(void *to, unsigned long n) = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_strnlen_user_hook)(const char *s, unsigned long n) = NULL;
+__attribute__((section(".data"))) unsigned long (*afterburn_strncpy_from_user_hook)(char *dst, const char *src, unsigned long n, unsigned long *success) = NULL;
+#endif
+
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+__attribute__((section(".data"))) unsigned long (*afterburn_dma_coherent_check)(unsigned long phys, unsigned long size);
+__attribute__((section(".data"))) unsigned long (*afterburn_phys_to_dma_hook)(unsigned long phys, unsigned long size);
+__attribute__((section(".data"))) unsigned long (*afterburn_dma_to_phys_hook)(unsigned long dma);
+#endif
+__attribute__((section(".data"))) unsigned long (*afterburn_free_pgd_hook)(unsigned long pgd);
+
+#define EXPORT_THREAD_GET_HANDLE	0
+#define EXPORT_THREAD_SET_HANDLE	1
+#define EXPORT_UACCESS_FAULT_DATA	2
+#define EXPORT_BURN_PROF_COUNTERS_START	3
+#define EXPORT_BURN_PROF_COUNTERS_END	4
+#define EXPORT_CPU_GET_STARTUP_IP       5
+
+
+#define IMPORT_EXIT_HOOK		0
+#define IMPORT_SET_PTE_HOOK		1
+#define IMPORT_GET_USER_HOOK		2
+#define IMPORT_PUT_USER_HOOK		3
+#define IMPORT_COPY_FROM_USER_HOOK	4
+#define IMPORT_COPY_TO_USER_HOOK	5
+#define IMPORT_CLEAR_USER_HOOK		6
+#define IMPORT_STRNLEN_USER_HOOK	7
+#define IMPORT_STRNCPY_FROM_USER_HOOK	8
+#define IMPORT_READ_PTE_HOOK		9
+#define IMPORT_PTE_TEST_AND_CLEAR_HOOK	10
+#define IMPORT_PTE_GET_AND_CLEAR_HOOK	11
+#define IMPORT_PHYS_TO_DMA_HOOK		12
+#define IMPORT_DMA_TO_PHYS_HOOK		13
+#define IMPORT_FREE_PGD_HOOK		14
+#define IMPORT_REWRITE_MODULE_HOOK	15
+#define IMPORT_SIGNAL_HOOK		16
+#define IMPORT_ESP0_SYNC		17
+#define IMPORT_DMA_COHERENT_CHECK       18
+#define IMPORT_EXIT_PGD			19
+
+// These macros install the info into an unallocated ELF section.
+#define AFTERBURN_EXPORT( id, func ) \
+    asm ( ".pushsection .afterburn.exports;" \
+	    ".long " MKSTR(id) ";" \
+	    ".long " MKSTR(func) ";" \
+	    ".popsection;" )
+#define AFTERBURN_IMPORT( id, ptr ) \
+    EXPORT_SYMBOL( ptr ); \
+    asm ( ".pushsection .afterburn.imports;" \
+	    ".long " MKSTR(id) ";" \
+	    ".long " MKSTR(ptr) ";" \
+	    ".popsection;" )
+
+#define MKSTR(sym)	MKSTR2(sym)
+#define MKSTR2(sym)	#sym
+
+#if defined(CONFIG_AFTERBURN_THREAD_HOOKS)
+AFTERBURN_EXPORT( EXPORT_THREAD_GET_HANDLE, afterburn_thread_get_handle );
+AFTERBURN_EXPORT( EXPORT_THREAD_SET_HANDLE, afterburn_thread_assign_handle );
+AFTERBURN_IMPORT( IMPORT_EXIT_HOOK, afterburn_exit_hook );
+AFTERBURN_IMPORT( IMPORT_SIGNAL_HOOK, afterburn_signal_hook );
+#endif
+AFTERBURN_EXPORT( EXPORT_UACCESS_FAULT_DATA, afterburn_uaccess_fault_handler );
+AFTERBURN_EXPORT( EXPORT_BURN_PROF_COUNTERS_START, __burn_prof_counters_start );
+AFTERBURN_EXPORT( EXPORT_BURN_PROF_COUNTERS_END, __burn_prof_counters_end );
+AFTERBURN_EXPORT( EXPORT_CPU_GET_STARTUP_IP, afterburn_cpu_get_startup_ip );
+
+
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+AFTERBURN_IMPORT( IMPORT_GET_USER_HOOK, afterburn_get_user_hook );
+AFTERBURN_IMPORT( IMPORT_COPY_FROM_USER_HOOK, afterburn_copy_from_user_hook );
+AFTERBURN_IMPORT( IMPORT_PUT_USER_HOOK, afterburn_put_user_hook );
+AFTERBURN_IMPORT( IMPORT_COPY_TO_USER_HOOK, afterburn_copy_to_user_hook );
+AFTERBURN_IMPORT( IMPORT_CLEAR_USER_HOOK, afterburn_clear_user_hook );
+AFTERBURN_IMPORT( IMPORT_STRNLEN_USER_HOOK, afterburn_strnlen_user_hook );
+AFTERBURN_IMPORT( IMPORT_STRNCPY_FROM_USER_HOOK, afterburn_strncpy_from_user_hook );
+#endif
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+AFTERBURN_IMPORT( IMPORT_DMA_COHERENT_CHECK, afterburn_dma_coherent_check );
+AFTERBURN_IMPORT( IMPORT_PHYS_TO_DMA_HOOK, afterburn_phys_to_dma_hook );
+AFTERBURN_IMPORT( IMPORT_DMA_TO_PHYS_HOOK, afterburn_dma_to_phys_hook );
+AFTERBURN_IMPORT( IMPORT_FREE_PGD_HOOK, afterburn_free_pgd_hook );
+#endif
+#if defined(CONFIG_AFTERBURN_MODULE_HOOKS)
+AFTERBURN_IMPORT( IMPORT_REWRITE_MODULE_HOOK, afterburn_rewrite_module_hook );
+#endif
+#if defined(CONFIG_AFTERBURN_XEN_HOOKS)
+AFTERBURN_IMPORT( IMPORT_ESP0_SYNC, afterburn_sync_esp0 );
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/annotate.h linux-2.6.9/afterburn/annotate.h
--- linux-2.6.9.src/afterburn/annotate.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/annotate.h	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,442 @@
+#ifndef __AFTERBURN__ANNOTATE_H__
+#define __AFTERBURN__ANNOTATE_H__
+
+#include <asm/bitops.h>
+
+static inline void 
+dp83820_write_annotate(unsigned int b, volatile void __iomem *addr)
+{
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %0, %1\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.dp83820\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : : "r"(b), "m"(*(unsigned long *)addr) : "memory" );
+}
+
+static inline unsigned int 
+dp83820_read_annotate(const volatile void __iomem *addr)
+{
+    unsigned int temp;
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.dp83820\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=r"(temp) : "m"(*(unsigned long *)addr) );
+    return temp;
+}
+
+#define pgd_read_annotate( pgd ) ({ \
+    unsigned long temp; \
+    __asm__ ( \
+	    "7777:" \
+	    "mov %1, %0\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "7778:" \
+	    ".pushsection .afterburn.pgd_read\n" \
+	    ".align 4\n" \
+	    ".long 7777b\n" \
+	    ".long 7778b\n" \
+	    ".popsection\n" \
+	    : "=r"(temp) : "m"(pgd) ); \
+    temp; })
+
+
+#define pte_read_annotate( pte ) ({ \
+    unsigned long temp; \
+    __asm__ ( \
+	    "7777:" \
+	    "mov %1, %0\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "nop\n" \
+	    "7778:" \
+	    ".pushsection .afterburn.pte_read\n" \
+	    ".align 4\n" \
+	    ".long 7777b\n" \
+	    ".long 7778b\n" \
+	    ".popsection\n" \
+	    : "=r"(temp) : "m"(pte) ); \
+    temp; })
+
+static inline void 
+pte_set_annotate(pte_t *addr, unsigned long val )
+{
+    __asm__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.pte_set\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=m"(*addr) : "r"(val), "m"(*addr) );
+}
+
+static inline void 
+pgd_set_annotate(pgd_t *addr, unsigned long val )
+{
+    __asm__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.pgd_set\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=m"(*addr) : "r"(val) );
+}
+
+static inline void 
+pmd_set_annotate(pmd_t *addr, unsigned long val )
+{
+    __asm__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.pmd_set\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=m"(*addr) : "r"(val) );
+}
+
+static inline int pte_test_and_clear_bit_annotate( int bit, volatile pte_t *ptep )
+{
+    int oldbit;
+
+    __asm__ (
+	    "7777:"
+	    LOCK_PREFIX
+	    "btrl %2, %1\n"
+	    "sbbl %0, %0\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.pte_test_clear\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=a"(oldbit), "=m"(*(volatile long *)ptep)
+	    : "0"(bit) );
+    return oldbit;
+}
+
+static inline pte_t
+pte_read_clear_annotate( pte_t *pteptr )
+{
+    pte_t x = {0};
+
+    __asm__ (
+	    "7777:"
+	    "xchgl %0, %1\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.pte_read\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=r"(x) 
+	    : "m"(*(volatile unsigned long *)pteptr), "0"(x)
+	    : "memory" );
+
+    return x;
+}
+
+
+static inline void 
+ioapic_write_annotate(unsigned int b, volatile void __iomem *addr)
+{
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %0, %1\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.ioapic\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : : "r"(b), "m"(*(unsigned long *)addr) : "memory" );
+}
+
+static inline unsigned int 
+ioapic_read_annotate(const volatile void __iomem *addr)
+{
+    unsigned int temp;
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.ioapic\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=r"(temp) : "m"(*(unsigned long *)addr) );
+    return temp;
+}
+
+
+static inline void 
+lapic_write_annotate(unsigned int b, volatile void __iomem *addr)
+{
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %0, %1\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.lapic\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : : "r"(b), "m"(*(unsigned long *)addr) : "memory" );
+}
+
+static inline void 
+lapic_xchg_annotate(unsigned int b, volatile void __iomem *addr)
+{
+    __asm__ __volatile__ (
+	    "7777:"
+	    "xchgl %0, %1\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.lapic\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : : "r"(b), "m"(*(unsigned long *)addr) : "memory" );
+}
+
+
+static inline unsigned int 
+lapic_read_annotate(const volatile void __iomem *addr)
+{
+    unsigned int temp;
+    __asm__ __volatile__ (
+	    "7777:"
+	    "mov %1, %0\n" 
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "nop\n"
+	    "7778:"
+	    ".pushsection .afterburn.lapic\n"
+	    ".align 4\n"
+	    ".long 7777b\n"
+	    ".long 7778b\n"
+	    ".popsection\n"
+	    : "=r"(temp) : "m"(*(unsigned long *)addr) );
+    return temp;
+}
+
+
+
+
+
+#endif /* __AFTERBURN__ANNOTATE_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/block/block.h linux-2.6.9/afterburn/drivers/block/block.h
--- linux-2.6.9.src/afterburn/drivers/block/block.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/block.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,67 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock.h
+ * Description:	Common declarations for the server and client of the
+ * 		Linux block driver.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: block.h,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+#ifndef __linuxblock__L4VMblock_h__
+#define __linuxblock__L4VMblock_h__
+
+#include <linux/version.h>
+#include <l4/kdebug.h>
+
+/*
+#if IDL4_HEADER_REVISION < 20031207
+# error "Your version of IDL4 is too old.  Please upgrade to the latest."
+#endif
+*/
+
+#define TRUE	1
+#define FALSE	0
+
+#define L4_TAG_IRQ      0x100
+
+#define RAW(a)	((void *)((a).raw))
+
+#if defined(CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE)
+
+#define L4VMblock_debug_level	2
+
+#define PARANOID(a)		// Don't execute paranoid stuff.
+#define ASSERT(a)		// Don't execute asserts.
+
+#else
+
+extern int L4VMblock_debug_level;
+
+#define PARANOID(a)		a
+#define ASSERT(a)		do { if(!(a)) { printk( PREFIX "assert failure %s:%s:%d\n", __FILE__, __FUNCTION__, __LINE__); L4_KDB_Enter("assert"); } } while(0)
+
+#endif	/* CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE */
+
+#define dprintk(n,a...) do { if(L4VMblock_debug_level >= (n)) printk(a); } while(0)
+
+typedef struct 
+{
+    L4_Word16_t cnt;
+    L4_Word16_t start_free;
+    L4_Word16_t start_dirty;
+} L4VMblock_ring_t;
+
+extern inline L4_Word16_t
+L4VMblock_ring_available( L4VMblock_ring_t *ring )
+{
+    return (ring->start_dirty + ring->cnt - (ring->start_free + 1)) % ring->cnt;
+}
+
+#define L4VMBLOCK_SERVER_IRQ_DISPATCH	(1)
+#define L4VMBLOCK_CLIENT_IRQ_CLEAN	(1)
+
+#endif	/* __linuxblock__L4VMblock_h__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/block/client26.c linux-2.6.9/afterburn/drivers/block/client26.c
--- linux-2.6.9.src/afterburn/drivers/block/client26.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/client26.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,777 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/block/client26.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+/*
+ * The logic for handling the request queue is modeled from 
+ * linux/drivers/block/nbd.c.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/blkdev.h>
+#include <linux/interrupt.h>
+
+#include <asm/io.h>
+
+#include "client26.h"
+
+
+#define PREFIX "L4VMblock client: "
+
+static int L4VMblock_major;
+
+
+int L4VMblock_probe_devices[16] = { 0, 0 };
+MODULE_PARM( L4VMblock_probe_devices, "0-16i" );
+#define L4VMBLOCK_MAX_PROBE_PARAMS	\
+	(sizeof(L4VMblock_probe_devices)/sizeof(L4VMblock_probe_devices[0]))
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE)
+int L4VMblock_debug_level = 2;
+MODULE_PARM( L4VMblock_debug_level, "i" );
+#endif
+
+static L4_Word_t L4VMblock_irq_no = 9;
+static L4VMblock_client_t L4VMblock_client;
+
+#define L4VMBLOCK_MAX_DEVS	16
+
+static L4VMblock_descriptor_t L4VMblock_descriptors[ L4VMBLOCK_MAX_DEVS ];
+
+static int L4VMblock_probe_device( L4VMblock_client_t *, kdev_t, int minor );
+static int L4VMblock_attach_device( L4VMblock_client_t *, L4VMblock_descriptor_t *);
+static int L4VMblock_detach_device( L4VMblock_client_t *, L4VMblock_descriptor_t *);
+
+static void L4VMblock_deliver_server_irq( L4VMblock_client_t *client );
+
+/****************************************************************************
+ *
+ * Functions for providing the device driver's block interface.
+ *
+ ****************************************************************************/
+
+static int L4VMblock_open( struct inode *inode, struct file *file )
+{
+    int err;
+    L4VMblock_client_t *client = &L4VMblock_client;
+    L4VMblock_descriptor_t *descriptor = 
+	(L4VMblock_descriptor_t *)inode->i_bdev->bd_disk->private_data;
+    struct block_device *bdev = inode->i_bdev;
+
+    if( descriptor->refcnt == 0 )
+    {
+	err = L4VMblock_attach_device( client, descriptor );
+	if( err == 0 )
+    	    descriptor->refcnt++;
+    }
+    else
+    {
+	err = 0;
+	descriptor->refcnt++;
+    }
+
+    set_blocksize( bdev, PAGE_SIZE );
+
+    return err;
+}
+
+static int L4VMblock_release( struct inode *inode, struct file *file )
+{
+    L4VMblock_client_t *client = &L4VMblock_client;
+    L4VMblock_descriptor_t *descriptor = 
+	(L4VMblock_descriptor_t *)inode->i_bdev->bd_disk->private_data;
+
+    if( descriptor->refcnt == 0 )
+	return 0;
+    descriptor->refcnt--;
+    if( descriptor->refcnt )
+	return 0;
+
+    return L4VMblock_detach_device( client, descriptor );
+}
+
+
+static struct block_device_operations L4VMblock_device_operations = {
+    .owner = THIS_MODULE,
+    .open = L4VMblock_open,
+    .release = L4VMblock_release,
+};
+
+
+static void 
+L4VMblock_process_request( 
+	L4VMblock_client_t *client, 
+	request_queue_t *req_q, 
+	struct request *req )
+{
+    struct bio *bio;
+    struct bio_vec *bv;
+    int i;
+    L4VMblock_descriptor_t *descriptor = 
+	(L4VMblock_descriptor_t *)req->rq_disk->private_data;
+    volatile IVMblock_ring_descriptor_t *rdesc;
+    L4VMblock_ring_t *ring_info = &client->desc_ring_info;
+    sector_t sector;
+
+    // Sanity check for connection to the block server.
+    if( !descriptor->refcnt ) {
+	dprintk( 2, KERN_ERR PREFIX "device not attached.\n" );
+	req->errors++;
+	return;
+    }
+
+    // The request must include at least one command.
+    if( req->nr_hw_segments == 0 ) {
+	req->errors++;
+	return;
+    }
+    dprintk( 4, KERN_ERR PREFIX "request has %d segments\n", 
+	    req->nr_hw_segments );
+
+    rq_for_each_bio( bio, req )
+    {
+	sector = bio->bi_sector;
+
+	bio_for_each_segment( bv, bio, i )
+	{
+	    L4VMblock_shadow_t *shadow;
+	    L4_Word_t nsect;
+
+	    // Wait for an available descriptor.
+	    while( L4VMblock_ring_available(ring_info) == 0 )
+	    {
+		dprintk( 4, PREFIX "queue full, sleeping.\n" );
+		spin_unlock_irq( req_q->queue_lock );
+		L4VMblock_deliver_server_irq( client );
+		wait_event( client->ring_wait,
+			(L4VMblock_ring_available(ring_info) > 0) );
+		spin_lock_irq( req_q->queue_lock );
+		dprintk( 4, PREFIX "queue resumed.\n" );
+	    }
+
+	    dprintk( 4, PREFIX "block request %lx, size %d, "
+		    "sector %llx, segments %d/%d/%d\n", 
+		    bvec_to_phys(bv), bv->bv_len, sector,
+		    bio->bi_hw_segments, bio->bi_phys_segments, bio->bi_vcnt );
+
+	    // Get the next descriptor.
+	    rdesc = &client->client_shared->desc_ring[ ring_info->start_free ];
+	    ASSERT( !rdesc->status.X.server_owned );
+
+	    // Get the shadow.
+	    shadow = (L4VMblock_shadow_t *)rdesc->client_data;
+	    shadow->bio = bio;
+	    shadow->req = req;
+
+	    // Fill-in the descriptor.
+	    rdesc->handle = descriptor->handle;
+	    rdesc->size = bv->bv_len;
+	    rdesc->offset = sector;
+	    rdesc->page = (void *)bvec_to_phys(bv);
+	    rdesc->status.raw = 0;
+	    rdesc->status.X.do_write = (rq_data_dir(req) == WRITE);
+	    rdesc->status.X.speculative = (rq_data_dir(req) == READA);
+
+	    // Update the request.
+	    nsect = bv->bv_len >> 9;
+	    sector += nsect;
+	    process_that_request_first( req, nsect );
+
+	    // Transfer the descriptor to the server.
+	    rdesc->status.X.server_owned = 1;
+
+	    // Next ...
+	    ring_info->start_free = (ring_info->start_free+1) % ring_info->cnt;
+	}
+    }
+
+    blkdev_dequeue_request( req );
+}
+
+static void
+L4VMblock_end_request( L4VMblock_client_t *client, struct request *req )
+    // Must be called while the queue lock is held.
+{
+    int uptodate = (req->errors == 0);
+
+    if( !end_that_request_first(req, uptodate, req->nr_sectors) )
+    {
+	blkdev_dequeue_request( req );
+	end_that_request_last(req);
+    }
+}
+
+static void
+L4VMblock_do_request( request_queue_t *q )
+{
+    struct request *req;
+    L4VMblock_client_t *client = &L4VMblock_client;
+
+    // Walk the requests.
+    while( (req = elv_next_request(q)) != NULL )
+    {
+	req->errors = 0;
+	L4VMblock_process_request( client, q, req );
+	if( req->errors )
+	    L4VMblock_end_request( client, req );
+    }
+
+    // Wake the block server.
+    spin_unlock_irq( q->queue_lock );
+    L4VMblock_deliver_server_irq( client );
+    spin_lock_irq( q->queue_lock );
+}
+
+
+/****************************************************************************
+ *
+ * Functions for handling the server.
+ *
+ ****************************************************************************/
+
+static void
+L4VMblock_deliver_server_irq( L4VMblock_client_t *client )
+{
+    unsigned long flags;
+
+    client->server_shared->irq_status |= L4VMBLOCK_SERVER_IRQ_DISPATCH;
+    if( client->server_shared->irq_pending )
+        return;
+
+    client->server_shared->irq_pending = TRUE;
+
+    local_irq_save( flags );
+    {
+      	L4_ThreadId_t from_tid;
+	L4_MsgTag_t msg_tag, result_tag;
+	msg_tag.raw = 0; msg_tag.X.label = L4_TAG_IRQ; msg_tag.X.u = 1;
+	L4_Set_MsgTag( msg_tag );
+	L4_LoadMR( 1, client->client_shared->server_irq_no );
+	result_tag = L4_Reply( client->client_shared->server_main_tid );
+	if( L4_IpcFailed(result_tag) )
+	{
+	    L4_Set_MsgTag( msg_tag );
+	    L4_Ipc( client->client_shared->server_irq_tid, L4_nilthread,
+		    L4_Timeouts(L4_Never, L4_Never), &from_tid );
+	}
+    }
+    local_irq_restore( flags );
+}
+
+static int L4VMblock_probe_device( 
+	L4VMblock_client_t *client,
+	kdev_t kdev,
+	int minor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    IVMblock_devprobe_t probe_data;
+    IVMblock_devid_t devid = { major: MAJOR(kdev), minor: MINOR(kdev) };
+    struct gendisk *disk;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_probe( client->server_tid, &devid, &probe_data, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to probe device %d:%d\n",
+		MAJOR(kdev), MINOR(kdev) );
+	return -ENODEV;
+    }
+
+    dprintk( 1, KERN_INFO PREFIX "probed remote device %d:%d\n", 
+	    MAJOR(kdev), MINOR(kdev) );
+    dprintk( 2, KERN_INFO PREFIX "device size: %lu KB\n"
+	    "\tblock size: %lu\n"
+	    "\thardware sector size: %lu\n"
+	    "\tread ahead: %lu\n"
+	    "\tmax read ahead: %lu\n"
+	    "\trequest max sectors: %lu\n",
+	    probe_data.device_size, probe_data.block_size,
+	    probe_data.hardsect_size, probe_data.read_ahead,
+	    probe_data.max_read_ahead, probe_data.req_max_sectors );
+
+    L4VMblock_descriptors[ minor ].remote_kdev = kdev;
+    L4VMblock_descriptors[ minor ].refcnt = 0;
+    L4VMblock_descriptors[ minor ].lnx_disk = NULL;
+    spin_lock_init( &L4VMblock_descriptors[ minor ].lnx_req_lock );
+
+    disk = alloc_disk(1);
+    if( disk == NULL )
+	return -ENOMEM;
+
+    disk->queue = blk_init_queue( L4VMblock_do_request, 
+	    &L4VMblock_descriptors[minor].lnx_req_lock );
+    if( NULL == disk->queue )
+    {
+	put_disk(disk);
+	return -ENOMEM;
+    }
+
+    L4VMblock_descriptors[ minor ].lnx_disk = disk;
+
+    disk->major = L4VMblock_major;
+    disk->first_minor = minor;
+    disk->fops = &L4VMblock_device_operations;
+    disk->flags |= GENHD_FL_SUPPRESS_PARTITION_INFO; // TODO: necessary?
+    disk->private_data = (void *)&L4VMblock_descriptors[minor];
+    sprintf( disk->disk_name, "vmblock%d", minor );
+    sprintf( disk->devfs_name, "vmblock/%d", minor );
+    // Convert from 1024-byte blocks to 512-byte blocks.
+    set_capacity( disk, probe_data.device_size * 2 );
+    add_disk( disk );
+
+    return 0;
+}
+
+static int L4VMblock_attach_device( 
+	L4VMblock_client_t *client,
+	L4VMblock_descriptor_t *descriptor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    IVMblock_devid_t devid;
+
+    devid.major = MAJOR( descriptor->remote_kdev );
+    devid.minor = MINOR( descriptor->remote_kdev );
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_attach( client->server_tid, client->handle, &devid, 3, 
+	    &descriptor->handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to open device %d:%d\n",
+	    	MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev));
+	return -EBUSY;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "opened %d:%d\n", 
+	    MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev) );
+
+    return 0;
+}
+
+static int L4VMblock_detach_device( 
+	L4VMblock_client_t *client,
+	L4VMblock_descriptor_t *descriptor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_detach( client->server_tid, descriptor->handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to close device %d:%d\n",
+	    	MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev));
+	return -EBUSY;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "closed %d:%d\n", 
+	    MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev) );
+
+    return 0;
+}
+
+static int L4VMblock_dspace_handler( 
+	L4_Word_t fault_addr,
+	L4_Word_t *ip,
+	L4_Fpage_t *map_fp,
+	void *data )
+{
+    // NOTE!!!!  This runs in the context of the L4 pager thread.  Do
+    // not invoke Linux functions!!
+
+    L4VMblock_client_t *client = &L4VMblock_client;
+    L4_Word_t start = L4_Address(client->shared_window.fpage);
+    L4_Word_t end = L4_Address(client->shared_window.fpage) + L4_Size(client->shared_window.fpage);
+    idl4_fpage_t client_mapping, server_mapping;
+    CORBA_Environment ipc_env;
+
+    if( (fault_addr < start) || (fault_addr > end) )
+	return FALSE;
+
+    ipc_env = idl4_default_environment;
+    idl4_set_rcv_window( &ipc_env, client->shared_window.fpage );
+    IVMblock_Control_reattach( client->server_tid, client->handle,
+	    &client_mapping, &server_mapping, &ipc_env );
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	while( 1 )
+	    L4_KDB_Enter("dspace panic");
+    }
+
+    *map_fp = client->shared_window.fpage;
+    L4_Set_Rights( map_fp, L4_FullyAccessible );
+
+    return TRUE;
+}
+
+/****************************************************************************
+ *
+ * Interrupt handling functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMblock_finish_transfers( L4VMblock_client_t *client )
+{
+    L4VMblock_ring_t *ring_info = &client->desc_ring_info;
+    volatile IVMblock_ring_descriptor_t *rdesc;
+    unsigned long flags = 0;
+    unsigned cleaned = 0;
+    L4VMblock_shadow_t *shadow;
+    struct request *req;
+    struct bio *bio;
+    L4_Word_t nsect;
+
+    while( ring_info->start_dirty != ring_info->start_free )
+    {
+	// Get the next transaction.
+	rdesc = &client->client_shared->desc_ring[ ring_info->start_dirty ];
+	if( rdesc->status.X.server_owned )
+	    break;
+
+	cleaned++;
+
+	// Get the shadow info.
+	shadow = (L4VMblock_shadow_t *)rdesc->client_data;
+	ASSERT( shadow );
+	req = shadow->req;
+	bio = shadow->bio;
+
+	// Lock the request's queue.
+	spin_lock_irqsave( req->q->queue_lock, flags );
+	
+	// End the bio.
+	if( rdesc->status.X.server_err ) {
+	    dprintk( 2, PREFIX "server block I/O error.\n" );
+	    req->errors++;
+	    bio_io_error( bio, rdesc->size );
+	}
+	else
+	{
+	    dprintk( 4, PREFIX "finished a bio.\n" );
+	    bio_endio( bio, rdesc->size, 0 );
+	}
+
+	// End the request if it is finished.
+	nsect = rdesc->size >> 9;
+	if( req->hard_nr_sectors <= nsect )
+	{
+	    dprintk( 4, PREFIX "finished a request.\n" );
+	    req->hard_nr_sectors = 0;
+	    ASSERT( list_empty(&req->queuelist) );
+	    end_that_request_last( req );
+	}
+	else
+	    req->hard_nr_sectors -= nsect;
+
+	// Unlock the request's queue.
+	spin_unlock_irqrestore( req->q->queue_lock, flags );
+
+	shadow->req = NULL;
+	shadow->bio = NULL;
+	// Move to the next.
+	ring_info->start_dirty = (ring_info->start_dirty + 1) % ring_info->cnt;
+    }
+
+    if( cleaned )
+	wake_up( &client->ring_wait );
+}
+
+static irqreturn_t
+L4VMblock_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMblock_client_t *client = (L4VMblock_client_t *)data;
+    int cnt = 0;
+    L4_Word_t events;
+
+    do
+    {
+	client->client_shared->client_irq_pending = TRUE;
+	events = L4VM_irq_status_reset( &client->client_shared->client_irq_status );
+
+	if( events )
+	{
+	    dprintk( 4, PREFIX "irq handler: 0x%lx\n", events );
+	    L4VMblock_finish_transfers( client );
+	}
+
+	client->client_shared->client_irq_pending = FALSE;
+	cnt++;
+	if( cnt > 100 )
+	    dprintk( 1, PREFIX "too many interrupts.\n" );
+    }
+    while( events );
+
+    return IRQ_HANDLED;
+}
+
+/****************************************************************************
+ *
+ * Module-level functions.
+ *
+ ****************************************************************************/
+
+static int 
+L4VMblock_client_register( L4VMblock_client_t *client )
+{
+    int err;
+    L4_Word_t log2size, size, size2, window_base;
+    idl4_fpage_t client_mapping, server_mapping;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    L4_Word_t index;
+
+    // Try to allocate a virtual memory area for the shared windows.
+    size  = L4_Size( L4_Fpage(0,sizeof(IVMblock_client_shared_t)) );
+    size2 = L4_Size( L4_Fpage(0,sizeof(IVMblock_server_shared_t)) );
+    log2size = L4_SizeLog2( L4_Fpage(0, size+size2) );
+    err = L4VM_fpage_vmarea_get( log2size, &client->shared_window );
+    if( err < 0 )
+	return err;
+
+    ASSERT( L4_Address(client->shared_window.fpage) >= 
+	    (L4_Word_t)client->shared_window.ioremap_addr );
+
+    dprintk( 2, KERN_INFO PREFIX "receive window at %p, size %ld\n",
+	    (void *)L4_Address(client->shared_window.fpage),
+	    L4_Size(client->shared_window.fpage) );
+
+    // Register with the server.
+    ipc_env = idl4_default_environment;
+    idl4_set_rcv_window( &ipc_env, client->shared_window.fpage );
+    local_irq_save(irq_flags);
+    IVMblock_Control_register( client->server_tid, &client->handle,
+	    &client_mapping, &server_mapping, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	L4VM_fpage_vmarea_release( &client->shared_window );
+	return -EIO;
+    }
+
+    // Initialize shared structures.
+    dprintk( 2, KERN_INFO PREFIX "client shared region at %p, offset %p, "
+	    "size %ld\n",
+	    (void *)idl4_fpage_get_base(client_mapping),
+	    (void *)L4_Address(idl4_fpage_get_page(client_mapping)),
+	    L4_Size(idl4_fpage_get_page(client_mapping)) );
+    dprintk( 2, KERN_INFO PREFIX "server shared region at %p, offset %p, "
+	    "size %ld\n",
+	    (void *)idl4_fpage_get_base(server_mapping),
+	    (void *)L4_Address(idl4_fpage_get_page(server_mapping)),
+	    L4_Size(idl4_fpage_get_page(server_mapping)) );
+
+    window_base = L4_Address( client->shared_window.fpage );
+    client->client_shared = (IVMblock_client_shared_t *)
+	(window_base + idl4_fpage_get_base(client_mapping));
+    client->server_shared = (IVMblock_server_shared_t *)
+	(window_base + idl4_fpage_get_base(server_mapping));
+
+    dprintk( 2, KERN_INFO PREFIX "server irq tid: %p, server irq no: %lu\n",
+	    RAW(client->client_shared->server_irq_tid),
+	    client->client_shared->server_irq_no );
+
+    // Initialize stuff related to the shared structures.
+    client->desc_ring_info.cnt = IVMblock_descriptor_ring_size;
+    client->desc_ring_info.start_free = 0;
+    client->desc_ring_info.start_dirty = 0;
+    init_waitqueue_head( &client->ring_wait );
+
+    // Associate shadow info with each descriptor ring entry's client_data.
+    // We learn about command reordering by changes in the client_data.
+    for( index = 0; index < IVMblock_descriptor_ring_size; index++ )
+	client->client_shared->desc_ring[ index ].client_data = 
+	    (void *)&client->shadow_ring[ index ];
+
+    return 0;
+}
+
+static int __init
+L4VMblock_client_init_module( void )
+{
+    int i, err, minor;
+    L4VMblock_client_t *client = &L4VMblock_client;
+
+    if( L4VMblock_probe_devices[0] == 0 ) {
+	// No block device requested; abort.
+	return 0;
+    }
+
+    // Locate the L4VMblock server.
+    err = L4VM_server_locate( UUID_IVMblock, &client->server_tid );
+    if( err == -ENODEV )
+    {
+	printk( KERN_ERR PREFIX "unable to locate a block service.\n" );
+	goto err_no_server;
+    }
+    else if( err )
+    {
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+	goto err_no_server;
+    }
+
+    // Register with the server.
+    err = L4VMblock_client_register( client );
+    if( err < 0 )
+	goto err_register;
+
+    // Allocate a virtual interrupt.
+    if( L4VMblock_irq_no > NR_IRQS )
+    {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	err = -ENOMEM;
+	goto err_vmpic_reserve;
+    }
+    l4ka_wedge_add_virtual_irq( L4VMblock_irq_no );
+    err = request_irq( L4VMblock_irq_no, L4VMblock_irq_handler, 0, 
+	    "vmblock", client );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+    client->client_shared->client_irq_no = L4VMblock_irq_no;
+    client->client_shared->client_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    client->client_shared->client_main_tid = L4VM_linux_main_thread( smp_processor_id() );
+
+    // Register block handlers with Linux, and allocate a major number.
+    err = register_blkdev( 0, "vmblock" );
+    if( err < 0 )
+	goto err_blkdev_register;
+    else
+	L4VMblock_major = err;
+
+    // Finish the block driver init.
+
+    // Probe for some devices within the server.  These probes are based
+    // on the module parameters L4VMblock_probe_devices.
+    for( i = 0, minor = 0;
+	    (i < L4VMBLOCK_MAX_PROBE_PARAMS) && (minor < L4VMBLOCK_MAX_DEVS);
+	    i += 2 )
+    {
+	L4_Word_t target_major = L4VMblock_probe_devices[i];
+	L4_Word_t target_minor = L4VMblock_probe_devices[i+1];
+	if( target_major == 0 )
+	    break;
+	if( L4VMblock_probe_device(client, MKDEV(target_major,target_minor), minor) == 0 )
+	                minor++;
+    }
+
+    l4ka_wedge_add_dspace_handler( L4VMblock_dspace_handler, NULL );
+
+    return 0;
+
+err_blkdev_register:
+    // TODO: free irq resources
+err_request_irq:
+err_vmpic_reserve:
+    // TODO: disconnect from the block server
+err_register:
+err_no_server:
+    return err;
+}
+
+static void __exit
+L4VMblock_client_exit_module( void )
+{
+    // TODO: disconnect from the block server
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@irq.uka.de>" );
+MODULE_DESCRIPTION( "L4Linux client stub block driver" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM block" );
+MODULE_VERSION("slink");
+
+module_init( L4VMblock_client_init_module );
+module_exit( L4VMblock_client_exit_module ); 
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+
+#else
+
+static int __init L4VMblock_setup_probe_param( char *param )
+    // Parses a kernel command line parameter, consisting of a series
+    // of major,minor pairs.  An example such parameter:
+    //   bootprobe=3,1:3,2:3,3
+{
+    int idx = 0;
+    int major, minor;
+
+    while( idx < L4VMBLOCK_MAX_PROBE_PARAMS-2 )
+    {
+	if( !*param )
+	    break;
+
+	major = simple_strtoul(param, &param, 10);
+	if( !*param || !*(++param) ) // Skip the separator.
+	    break;
+	minor = simple_strtoul(param, &param, 10);
+
+	L4VMblock_probe_devices[idx++] = major;
+	L4VMblock_probe_devices[idx++] = minor;
+
+	if( *param )	// Skip the separator.
+	    param++;
+    }
+    if( *param )
+	printk( KERN_WARNING PREFIX "too many block probes.\n" );
+    L4VMblock_probe_devices[idx] = 0;
+    return 1;
+}
+__setup( "blockprobe=", L4VMblock_setup_probe_param );
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/block/client26.h linux-2.6.9/afterburn/drivers/block/client26.h
--- linux-2.6.9.src/afterburn/drivers/block/client26.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/client26.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,60 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_client.h
+ * Description:	Declarations specific to the block driver client.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: client26.h,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+#ifndef __LINUXBLOCK__L4VMBLOCK_CLIENT_H__
+#define __LINUXBLOCK__L4VMBLOCK_CLIENT_H__
+
+#include <linux/version.h>
+#include <linux/genhd.h>
+
+#include "L4VMblock_idl_client.h"
+#include "block.h"
+#include <glue/vmirq.h>
+#include <glue/vmserver.h>
+#include <glue/vmmemory.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+typedef dev_t kdev_t;
+#endif
+
+typedef struct
+{
+    kdev_t remote_kdev;
+    IVMblock_handle_t handle;
+    L4_Word_t refcnt;
+    struct gendisk *lnx_disk;
+    spinlock_t lnx_req_lock;
+} L4VMblock_descriptor_t;
+
+
+typedef struct
+{
+    struct bio *bio;
+    struct request *req;
+} L4VMblock_shadow_t;
+
+typedef struct
+{
+    L4_ThreadId_t server_tid;
+    IVMblock_handle_t handle;
+
+    L4VM_alligned_vmarea_t shared_window;
+    IVMblock_client_shared_t *client_shared;
+    IVMblock_server_shared_t *server_shared;
+
+    L4VMblock_ring_t desc_ring_info;
+    wait_queue_head_t ring_wait;
+    L4VMblock_shadow_t shadow_ring[IVMblock_descriptor_ring_size];
+
+} L4VMblock_client_t;
+
+#endif	/* __LINUXBLOCK__L4VMBLOCK_CLIENT_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/block/client.c linux-2.6.9/afterburn/drivers/block/client.c
--- linux-2.6.9.src/afterburn/drivers/block/client.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/client.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,878 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_client.c
+ * Description:	Implements the client stub portion of a virtual 
+ * 		block device for L4Linux.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: client.c,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+
+/*
+ * The logic for handling the request queue is modeled from 
+ * linux/drivers/block/nbd.c.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/blkpg.h>
+#include <linux/devfs_fs_kernel.h>
+
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/l4lxapi/thread.h>
+#include <asm/l4linux/debug.h>
+
+/*********
+ * begin blk.h stuff
+ */
+#define MAJOR_NR	L4VMblock_major
+static int L4VMblock_major;	/* Must be declared before including blk.h. */
+#define DEVICE_NR(d)	MINOR(d)	/* No partition bits. */
+#define DEVICE_NAME	"vmblock"
+#define DEVICE_NO_RANDOM
+#define DEVICE_OFF(d)	/* do nothing */
+#define DEVICE_REQUEST	L4VMblock_client_request
+#define LOCAL_END_REQUEST
+
+#include <linux/blk.h>
+/*
+ * end blk.h stuff
+ *********/
+
+#define PREFIX "L4VMblock client: "
+
+#include "client.h"
+
+
+
+/*
+ * How do we virtualize devices?  The client requests a connection to a
+ * block device in the server.  The client identifies a server block device
+ * via a major and minor tuple.  If the server accepts the connection,
+ * then the client creates a new block request queue within its Linux.
+ *
+ * How does the client Linux know the available major and minor numbers?
+ * Since the client Linux has no real device drivers, it can claim all the
+ * official block device numbers, and just forward requests to the server.
+ * But the client must reserve the appropriate major/minor tuples.  It can
+ * do this by asking the server for all valid tuples, and then replicating
+ * them in the client space.  The server need not provide access to all of
+ * its devices; it especially must not if the devices are already in use.
+ * What is the definition of in-use?  For r/w disks, a particular partition.
+ *
+ * Block device sequencing:
+ * 1.  Probe(major,minor): client requests information about a static device.
+ *     The client can't perform operations, but it can obtain the information
+ *     necessary to create a virtual Linux device, such as the block size,
+ *     read-ahead, etc.  The server can permit a device probe even
+ *     if it won't permit the client to open the device.
+ * 2.  Open(major,minor): the client obtains permission to operate on the
+ *     device.  The server puts the device into a state where it is
+ *     functional.
+ * 3.  Request(major,minor): the client requests a block read/write operation.
+ * 4.  Close(major,minor): the client terminates its device session.
+ */
+
+int L4VMblock_probe_devices[16] = { 3, 1, 3, 2, 3, 3 };
+MODULE_PARM( L4VMblock_probe_devices, "0-16i" );
+#define L4VMBLOCK_MAX_PROBE_PARAMS	\
+	(sizeof(L4VMblock_probe_devices)/sizeof(L4VMblock_probe_devices[0]))
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE)
+int L4VMblock_debug_level = 2;
+MODULE_PARM( L4VMblock_debug_level, "i" );
+#endif
+
+static L4_Word_t L4VMblock_irq_no;
+static L4VMblock_client_t L4VMblock_client;
+
+#define L4VMBLOCK_MAX_DEVS	16
+
+static int L4VMblock_dev_length[ L4VMBLOCK_MAX_DEVS ];
+static int L4VMblock_hardsect_sizes[ L4VMBLOCK_MAX_DEVS ];
+static int L4VMblock_block_sizes[ L4VMBLOCK_MAX_DEVS ];
+static int L4VMblock_max_request_sectors[ L4VMBLOCK_MAX_DEVS ];
+
+static devfs_handle_t devfs_handle;
+
+static L4VMblock_descriptor_t L4VMblock_descriptors[ L4VMBLOCK_MAX_DEVS ];
+
+static int L4VMblock_probe_device( L4VMblock_client_t *, kdev_t, int minor );
+static int L4VMblock_attach_device( L4VMblock_client_t *, L4VMblock_descriptor_t *);
+static int L4VMblock_detach_device( L4VMblock_client_t *, L4VMblock_descriptor_t *);
+static void L4VMblock_deliver_server_irq( L4VMblock_client_t *client );
+
+/****************************************************************************
+ *
+ * Functions for providing the device driver's block interface.
+ *
+ ****************************************************************************/
+
+static int L4VMblock_open( struct inode *inode, struct file *file )
+{
+    int err;
+
+    L4VMblock_client_t *client = &L4VMblock_client;
+    int minor = DEVICE_NR( inode->i_rdev );
+    L4VMblock_descriptor_t *descriptor;
+
+    if( minor >= L4VMBLOCK_MAX_DEVS )
+	return -ENXIO;
+
+    descriptor = &L4VMblock_descriptors[minor];
+    if( descriptor->refcnt == 0 )
+    {
+	err = L4VMblock_attach_device( client, descriptor );
+	if( err == 0 )
+    	    descriptor->refcnt++;
+    }
+    else
+    {
+	err = 0;
+	descriptor->refcnt++;
+    }
+
+    return err;
+}
+
+static int L4VMblock_release( struct inode *inode, struct file *file )
+{
+    L4VMblock_client_t *client = &L4VMblock_client;
+    int minor = DEVICE_NR( inode->i_rdev );
+    L4VMblock_descriptor_t *descriptor;
+
+    if( minor >= L4VMBLOCK_MAX_DEVS )
+	return -ENXIO;
+
+    descriptor = &L4VMblock_descriptors[minor];
+
+    if( descriptor->refcnt == 0 )
+	return 0;
+    descriptor->refcnt--;
+    if( descriptor->refcnt )
+	return 0;
+
+    return L4VMblock_detach_device( client, descriptor );
+}
+
+static int L4VMblock_ioctl( struct inode *inode, struct file * file,
+	unsigned cmd, unsigned long opt )
+{
+    unsigned long size;
+    int err;
+
+    switch( cmd )
+    {
+	case BLKGETSIZE:
+	    // Be careful, don't lose bits!
+	    size = L4VMblock_dev_length[MINOR(inode->i_rdev)];
+	    size *= 
+		(BLOCK_SIZE / L4VMblock_hardsect_sizes[MINOR(inode->i_rdev)]);
+	    err = put_user( size, (unsigned long *)opt );
+	    dprintk( 2, KERN_INFO PREFIX "ioctl(BLKGETSIZE) --> %lu\n", size );
+	    return err;
+
+	case BLKGETSIZE64:
+	    size = 
+		(L4VMblock_dev_length[MINOR(inode->i_rdev)] << BLOCK_SIZE_BITS);
+	    err = put_user( size, (unsigned long *)opt );
+	    dprintk( 2, KERN_INFO PREFIX "ioctl(BLKGETSIZE64) --> %lu\n", size);
+	    return err;
+
+	default:
+	    err = blk_ioctl( inode->i_rdev, cmd, opt );
+	    dprintk( 2, KERN_INFO PREFIX "ioctl(%d) returns %d\n",
+		    cmd, err );
+	    return err;
+    }
+}
+
+static int L4VMblock_check_media_change( kdev_t kdev )
+{
+    // Returns 1 if the media changed, 0 otherwise.
+    return 0;
+}
+
+static int L4VMblock_revalidate( kdev_t kdev )
+{
+    return 0;
+}
+
+static struct block_device_operations L4VMblock_device_operations = {
+    open: L4VMblock_open,
+    release: L4VMblock_release,
+    ioctl: L4VMblock_ioctl,
+    check_media_change: L4VMblock_check_media_change,
+    revalidate: L4VMblock_revalidate
+};
+
+static L4VMblock_request_shadow_t *
+L4VMblock_new_request_shadow( L4VMblock_client_t *client, struct request *req )
+{
+    L4VMblock_request_shadow_t *req_shadow;
+
+    req_shadow = kmalloc( sizeof(L4VMblock_request_shadow_t), GFP_ATOMIC );
+    if( req_shadow == NULL )
+	return NULL;
+
+    req_shadow->request = req;
+    req_shadow->bh_count = 0;
+    INIT_LIST_HEAD( &req_shadow->bh_list );
+
+    // The io_request_lock must be held when manipulating the linked list.
+    list_add_tail( &req_shadow->shadow_list, &client->shadow_list );
+
+    return req_shadow;
+}
+
+static void 
+L4VMblock_walk_request( L4VMblock_client_t *client, struct request *req )
+    // The io_request_lock must *not* be held, since this function
+    // may block.
+{
+    struct buffer_head *bh;
+    L4VMblock_descriptor_t *descriptor = 
+	&L4VMblock_descriptors[MINOR(req->rq_dev)];
+    volatile IVMblock_ring_descriptor_t *rdesc;
+    L4VMblock_ring_t *ring_info = &client->desc_ring_info;
+    int nsect;
+    L4VMblock_request_shadow_t *req_shadow;
+    L4VMblock_bh_shadow_t *bh_shadow;
+
+    if( !descriptor->refcnt )
+    {
+	dprintk( 2, KERN_INFO PREFIX "device not attached: %x:%x.\n",
+		MAJOR(req->rq_dev), MINOR(req->rq_dev) );
+	req->errors++;
+	return;
+    }
+
+    ASSERT(req->bh); // The request must have at least one buffer head.
+
+    // Init the shadow data structures.
+    req_shadow = L4VMblock_new_request_shadow( client, req );
+    if( req_shadow == NULL )
+    {
+	req->errors++;
+	return;
+    }
+
+    // Tentatively queue the buffer heads.
+    while( req->bh )
+    {
+	// Wait for an available descriptor.
+	while( L4VMblock_ring_available(ring_info) == 0 )
+	{
+	    dprintk( 4, PREFIX "queue full, sleeping.\n" );
+	    spin_unlock_irq(&io_request_lock);
+	    L4VMblock_deliver_server_irq( client );
+	    wait_event( client->ring_wait, 
+		    (L4VMblock_ring_available(ring_info) > 0) );
+	    spin_lock_irq(&io_request_lock);
+	}
+
+	// Get the next descriptor.
+	rdesc = &client->client_shared->desc_ring[ ring_info->start_free ];
+	ASSERT( !rdesc->status.X.server_owned );
+
+	// Create a bh shadow.
+	bh_shadow = kmalloc( sizeof(L4VMblock_bh_shadow_t), GFP_ATOMIC );
+	ASSERT( bh_shadow );
+	if( bh_shadow == NULL )
+	{
+	    req->errors++;
+	    return;
+	}
+
+	// Remove the buffer head from the request.
+	bh = req->bh;
+	req->bh = bh->b_reqnext;
+	bh->b_reqnext = NULL;
+
+	nsect = bh->b_size >> 9;
+    	req->hard_sector += nsect;
+	req->sector += nsect;
+	req->hard_nr_sectors -= nsect;
+	req->nr_sectors -= nsect;
+
+	if( req->bh )
+	{
+	    req->current_nr_sectors = req->bh->b_size >> 9;
+	    req->hard_cur_sectors = req->current_nr_sectors;
+	    if( req->nr_sectors < req->current_nr_sectors )
+	    {
+		req->nr_sectors = req->current_nr_sectors;
+		printk( KERN_ERR PREFIX "end_request: buffer-list destroyed\n");
+	    }
+	    req->buffer = req->bh->b_data;
+	}
+	else
+	    blkdev_dequeue_request( req );
+
+	// Init the bh shadow.
+	bh_shadow->bh = bh;
+	bh_shadow->owner = req_shadow;
+	// The io_request_lock must be held while manipulating the linked list
+	// and the bh_count.
+	req_shadow->bh_count++;
+	list_add_tail( &bh_shadow->bh_list, &req_shadow->bh_list );
+
+	// Fill-in the descriptor.
+	rdesc->handle = descriptor->handle;
+	rdesc->size = bh->b_size;
+	rdesc->offset = bh->b_rsector;
+	rdesc->page = (void *)(virt_to_bus(bh->b_data));
+	rdesc->status.raw = 0;
+	rdesc->status.X.do_write = (req->cmd == WRITE);
+	rdesc->status.X.speculative = (req->cmd == READA);
+	rdesc->client_data = (void *)bh_shadow;
+
+	dprintk( 4, KERN_INFO PREFIX "offset %p, size %p\n",
+		(void *)rdesc->offset, (void *)rdesc->size );
+
+	// Transfer the buffers to the server.
+	rdesc->status.X.server_owned = 1;
+
+	// Next ...
+	ring_info->start_free = (ring_info->start_free + 1) % ring_info->cnt;
+    }
+}
+
+static void 
+L4VMblock_end_request( L4VMblock_client_t *client, struct request *req )
+    // Must be called while the io_request_lock is held.
+{
+    int uptodate = (req->errors == 0);
+
+    dprintk( 2, KERN_INFO PREFIX "prematurely ending block request.\n" );
+
+    if( !end_that_request_first(req, uptodate, "vmblock") )
+    {
+	blkdev_dequeue_request( req );
+	end_that_request_last( req );
+    }
+}
+
+static void
+L4VMblock_client_request( request_queue_t *queue )
+{
+    L4VMblock_client_t *client = &L4VMblock_client;
+    struct request *req;
+
+    if( client->rings_busy )
+    {
+	dprintk( 2, PREFIX "reentrance denied.\n" );
+	return;
+    }
+    client->rings_busy = 1;
+
+    dprintk( 4, PREFIX "request submitted.\n" );
+
+    while( !QUEUE_EMPTY )
+    {
+	req = CURRENT;
+	ASSERT(req);
+	req->errors = 0;
+	if( req->bh == NULL )
+	    L4VMblock_end_request( client, req );
+	else
+	{
+	    L4VMblock_walk_request( client, req );
+	    if( req->errors )
+	       	L4VMblock_end_request( client, req );
+	}
+    }
+
+    client->rings_busy = 0;
+
+    spin_unlock_irq(&io_request_lock);
+    L4VMblock_deliver_server_irq( client );
+    spin_lock_irq(&io_request_lock);
+}
+
+/****************************************************************************
+ *
+ * Functions for handling the server.
+ *
+ ****************************************************************************/
+
+static int L4VMblock_probe_device( 
+	L4VMblock_client_t *client,
+	kdev_t kdev,
+	int minor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    IVMblock_devprobe_t probe_data;
+    IVMblock_devid_t devid = { major: MAJOR(kdev), minor: MINOR(kdev) };
+    char name[10];
+
+    ASSERT( devfs_handle );
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_probe( client->server_tid, &devid, &probe_data, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to probe device %d:%d\n",
+		MAJOR(kdev), MINOR(kdev) );
+	return -ENODEV;
+    }
+
+    dprintk( 1, KERN_INFO PREFIX "probed remote device %d:%d\n", 
+	    MAJOR(kdev), MINOR(kdev) );
+    dprintk( 2, KERN_INFO PREFIX "device size: %lu KB\n"
+	    "\tblock size: %lu\n"
+	    "\thardware sector size: %lu\n"
+	    "\tread ahead: %lu\n"
+	    "\tmax read ahead: %lu\n"
+	    "\trequest max sectors: %lu\n",
+	    probe_data.device_size, probe_data.block_size,
+	    probe_data.hardsect_size, probe_data.read_ahead,
+	    probe_data.max_read_ahead, probe_data.req_max_sectors );
+
+    L4VMblock_dev_length[ minor ] = probe_data.device_size;
+    L4VMblock_hardsect_sizes[ minor ] = probe_data.hardsect_size;
+    L4VMblock_block_sizes[ minor ] = PAGE_SIZE;
+    L4VMblock_max_request_sectors[ minor ] = probe_data.req_max_sectors;
+
+    L4VMblock_descriptors[ minor ].remote_kdev = kdev;
+    L4VMblock_descriptors[ minor ].refcnt = 0;
+
+    snprintf( name, sizeof(name), "%u", minor );
+    devfs_register( devfs_handle, name, DEVFS_FL_DEFAULT, MAJOR_NR, minor, 
+	    S_IFBLK | S_IRUSR | S_IWUSR, &L4VMblock_device_operations, client );
+
+    return 0;
+}
+
+static int L4VMblock_attach_device( 
+	L4VMblock_client_t *client,
+	L4VMblock_descriptor_t *descriptor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    IVMblock_devid_t devid;
+
+    devid.major = MAJOR( descriptor->remote_kdev );
+    devid.minor = MINOR( descriptor->remote_kdev );
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_attach( client->server_tid, client->handle, &devid, 3, 
+	    &descriptor->handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to open device %d:%d\n",
+	    	MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev));
+	return -EBUSY;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "opened %d:%d\n", 
+	    MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev) );
+
+    return 0;
+}
+
+static int L4VMblock_detach_device( 
+	L4VMblock_client_t *client,
+	L4VMblock_descriptor_t *descriptor )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMblock_Control_detach( client->server_tid, descriptor->handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_WARNING PREFIX "failed to close device %d:%d\n",
+	    	MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev));
+	return -EBUSY;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "closed %d:%d\n", 
+	    MAJOR(descriptor->remote_kdev), MINOR(descriptor->remote_kdev) );
+
+    return 0;
+}
+
+static void
+L4VMblock_deliver_server_irq( L4VMblock_client_t *client )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+
+    client->server_shared->irq_status |= L4VMBLOCK_SERVER_IRQ_DISPATCH;
+    if( client->server_shared->irq_pending )
+	return;
+
+    client->server_shared->irq_pending = TRUE;
+
+    // Send a zero-timeout IPC.
+    ipc_env._timeout = L4_Timeouts( L4_ZeroTime, L4_Never );
+
+    ILinux_raise_irq( client->client_shared->server_irq_tid, 
+	    client->client_shared->server_irq_no, &ipc_env );
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+	CORBA_exception_free( &ipc_env );
+}
+
+/****************************************************************************
+ *
+ * Interrupt handling functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMblock_finish_transfers( L4VMblock_client_t *client )
+{
+    L4VMblock_ring_t *ring_info = &client->desc_ring_info;
+    volatile IVMblock_ring_descriptor_t *rdesc;
+    struct buffer_head *bh;
+    struct request *req;
+    unsigned long flags;
+    unsigned cleaned = 0;
+    L4VMblock_bh_shadow_t *bh_shadow;
+    L4VMblock_request_shadow_t *req_shadow;
+
+    spin_lock_irqsave( &io_request_lock, flags );
+
+    while( ring_info->start_dirty != ring_info->start_free )
+    {
+	// Get the next transaction.
+	rdesc = &client->client_shared->desc_ring[ ring_info->start_dirty ];
+	if( rdesc->status.X.server_owned )
+	    break;
+
+	cleaned++;
+
+	// Get the shadow information.
+	bh_shadow = (L4VMblock_bh_shadow_t *)rdesc->client_data;
+	ASSERT( bh_shadow );
+	bh = bh_shadow->bh;
+	req_shadow = bh_shadow->owner;
+	req = req_shadow->request;
+
+	// Finish the transaction.
+	blk_finished_io( bh->b_size >> 9 );
+	blk_finished_sectors( req, bh->b_size >> 9 );
+	bh->b_reqnext = NULL;
+
+	if( rdesc->status.X.server_err )
+	{
+	    dprintk( 2, PREFIX "server block I/O error.\n" );
+	    bh->b_end_io( bh, 0 );
+	}
+	else
+	{
+	    if( rdesc->status.X.do_write )
+		mark_buffer_uptodate( bh, 1 );
+	    bh->b_end_io( bh, 1 );
+	}
+
+	// Clean-up the bh shadow.
+	list_del( &bh_shadow->bh_list );
+	kfree( bh_shadow );
+
+	// Update the request.
+	ASSERT( req_shadow->bh_count );
+	req_shadow->bh_count--;
+	if( req_shadow->bh_count == 0 )
+	{
+	    // It was the last buffer head for this request, so 
+	    // release the request.
+	    end_that_request_last( req );
+	    list_del( &req_shadow->shadow_list );
+	    kfree( req_shadow );
+	}
+
+	// Move to the next.
+	ring_info->start_dirty = (ring_info->start_dirty + 1) % ring_info->cnt;
+    }
+
+    spin_unlock_irqrestore( &io_request_lock, flags );
+
+    if( cleaned )
+	wake_up( &client->ring_wait );
+}
+
+static void
+L4VMblock_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMblock_client_t *client = (L4VMblock_client_t *)data;
+    int cnt = 0;
+    L4_Word_t events;
+
+    do
+    {
+	client->client_shared->client_irq_pending = TRUE;
+	events = L4VM_irq_status_reset( &client->client_shared->client_irq_status );
+
+	if( events )
+	{
+	    dprintk( 3, PREFIX "irq handler: 0x%lx\n", events );
+	    L4VMblock_finish_transfers( client );
+	}
+
+	client->client_shared->client_irq_pending = FALSE;
+	cnt++;
+	if( cnt > 100 )
+	    dprintk( 1, PREFIX "too many interrupts.\n" );
+    }
+    while( events );
+}
+
+static int
+L4VMblock_irq_pending( void *data )
+{
+    L4VMblock_client_t *client = (L4VMblock_client_t *)data;
+
+    return client->client_shared->client_irq_status;
+}
+
+/****************************************************************************
+ *
+ * Module-level functions.
+ *
+ ****************************************************************************/
+
+static int 
+L4VMblock_client_register( L4VMblock_client_t *client )
+{
+    int err;
+    L4_Word_t log2size, size, size2, window_base;
+    idl4_fpage_t client_mapping, server_mapping;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    // Try to allocate a virtual memory area for the shared windows.
+    size  = L4_Size( L4_Fpage(0,sizeof(IVMblock_client_shared_t)) );
+    size2 = L4_Size( L4_Fpage(0,sizeof(IVMblock_server_shared_t)) );
+    log2size = L4_SizeLog2( L4_Fpage(0, size+size2) );
+    err = L4VM_fpage_vmarea_get( log2size, &client->shared_window );
+    if( err < 0 )
+	return err;
+
+    ASSERT( L4_Address(client->shared_window.fpage) >= 
+	    (L4_Word_t)client->shared_window.vmarea->addr );
+
+    dprintk( 2, KERN_INFO PREFIX "receive window at %p, size %ld\n",
+	    (void *)L4_Address(client->shared_window.fpage),
+	    L4_Size(client->shared_window.fpage) );
+
+    // Register with the server.
+    ipc_env = idl4_default_environment;
+    idl4_set_rcv_window( &ipc_env, client->shared_window.fpage );
+    local_irq_save(irq_flags);
+    IVMblock_Control_register( client->server_tid, &client->handle,
+	    &client_mapping, &server_mapping, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	L4VM_fpage_vmarea_release( &client->shared_window );
+	return -EIO;
+    }
+
+    // Initialize shared structures.
+    dprintk( 2, KERN_INFO PREFIX "client shared region at %p, offset %p, "
+	    "size %ld\n",
+	    (void *)idl4_fpage_get_base(client_mapping),
+	    (void *)L4_Address(idl4_fpage_get_page(client_mapping)),
+	    L4_Size(idl4_fpage_get_page(client_mapping)) );
+    dprintk( 2, KERN_INFO PREFIX "server shared region at %p, offset %p, "
+	    "size %ld\n",
+	    (void *)idl4_fpage_get_base(server_mapping),
+	    (void *)L4_Address(idl4_fpage_get_page(server_mapping)),
+	    L4_Size(idl4_fpage_get_page(server_mapping)) );
+
+    window_base = L4_Address( client->shared_window.fpage );
+    client->client_shared = (IVMblock_client_shared_t *)
+	(window_base + idl4_fpage_get_base(client_mapping));
+    client->server_shared = (IVMblock_server_shared_t *)
+	(window_base + idl4_fpage_get_base(server_mapping));
+
+    dprintk( 2, KERN_INFO PREFIX "server irq tid: %p, server irq no: %lu\n",
+	    RAW(client->client_shared->server_irq_tid),
+	    client->client_shared->server_irq_no );
+
+    // Initialize stuff related to the shared structures.
+    client->desc_ring_info.cnt = IVMblock_descriptor_ring_size;
+    client->desc_ring_info.start_free = 0;
+    client->desc_ring_info.start_dirty = 0;
+    client->rings_busy = FALSE;
+    init_waitqueue_head( &client->ring_wait );
+    INIT_LIST_HEAD( &client->shadow_list );
+
+    return 0;
+}
+
+static int __init
+L4VMblock_client_init_module( void )
+{
+    int i, err, minor;
+    L4VMblock_client_t *client = &L4VMblock_client;
+
+    // Locate the L4VMblock server.
+    err = L4VM_server_locate( UUID_IVMblock, &client->server_tid );
+    if( err == -ENODEV )
+    {
+	printk( KERN_ERR PREFIX "unable to locate a block service.\n" );
+	goto err_no_server;
+    }
+    else if( err )
+    {
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+	goto err_no_server;
+    }
+
+    // Register with the server.
+    err = L4VMblock_client_register( client );
+    if( err < 0 )
+	goto err_register;
+
+    // Register an IRQ handler with Linux.
+    err = request_mlx_irq( L4_nilthread, L4VMblock_irq_handler,
+	    SA_SHIRQ, "L4VMblock", client, L4VMblock_irq_pending,
+	    client, &client->client_shared->client_irq_tid );
+    if( err >= 0 )
+    {
+	L4VMblock_irq_no = err;
+	client->client_shared->client_irq_no = L4VMblock_irq_no;
+    }
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Register block handlers with Linux, and allocate a major number.
+    err = register_blkdev( MAJOR_NR, DEVICE_NAME, &L4VMblock_device_operations);
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to a block device: %d.\n", err );
+	goto err_invalid_major;
+    }
+    else if( err == 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate a major number.\n" );
+	goto err_invalid_major;
+    }
+
+    MAJOR_NR = err;
+    dprintk( 1, KERN_INFO PREFIX "major number %d.\n", MAJOR_NR );
+
+    // Finish the block driver init.
+    hardsect_size[MAJOR_NR] = L4VMblock_hardsect_sizes;
+    blksize_size[MAJOR_NR] = L4VMblock_block_sizes;
+    blk_size[MAJOR_NR] = L4VMblock_dev_length;
+    max_sectors[MAJOR_NR] = L4VMblock_max_request_sectors;
+
+    blk_init_queue( BLK_DEFAULT_QUEUE(MAJOR_NR), DEVICE_REQUEST );
+    blk_queue_headactive( BLK_DEFAULT_QUEUE(MAJOR_NR), 0 );
+
+    // Create a device tree in the devfs.
+    devfs_handle = devfs_mk_dir( NULL, "vmblock", NULL );
+
+    // Probe for some devices within the server.  These probes are based
+    // on the module parameters L4VMblock_probe_devices.
+    for( i = 0, minor = 0; 
+	    (i < L4VMBLOCK_MAX_PROBE_PARAMS) && (minor < L4VMBLOCK_MAX_DEVS);
+	    i += 2 )
+    {
+	L4_Word_t target_major = L4VMblock_probe_devices[i];
+	L4_Word_t target_minor = L4VMblock_probe_devices[i+1];
+	if( target_major == 0 )
+	    break;
+	if( L4VMblock_probe_device(client, MKDEV(target_major,target_minor), minor) == 0 )
+	    minor++;
+    }
+
+    return 0;
+
+err_invalid_major:
+err_request_irq:
+err_register:
+err_no_server:
+    return err;
+}
+
+static void __exit
+L4VMblock_client_exit_module( void )
+{
+    // TODO: unregister the irq handler.
+
+    if( devfs_handle )
+	devfs_unregister( devfs_handle );
+    devfs_handle = NULL;
+
+    hardsect_size[MAJOR_NR] = NULL;
+    blksize_size[MAJOR_NR] = NULL;
+    blk_size[MAJOR_NR] = NULL;
+
+    MAJOR_NR = 0;
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@bothan.net>" );
+MODULE_DESCRIPTION( "L4Linux client stub block driver" );
+MODULE_LICENSE( "Proprietary, owned by Joshua LeVasseur" );
+MODULE_SUPPORTED_DEVICE( "L4 VM block" );
+
+#if defined(MODULE)
+module_init( L4VMblock_client_init_module );
+module_exit( L4VMblock_client_exit_module ); 
+#else
+void __init l4vm_block_init( void ) { L4VMblock_client_init_module(); }
+
+static int __init L4VMblock_setup_probe_param( char *param )
+    // Parses a kernel command line parameter, consisting of a series
+    // of major,minor pairs.  An example such parameter:
+    //   bootprobe=3,1:3,2:3,3
+{
+    int idx = 0;
+    int major, minor;
+
+    while( idx < L4VMBLOCK_MAX_PROBE_PARAMS-2 )
+    {
+	if( !*param )
+	    break;
+
+	major = simple_strtoul(param, &param, 10);
+	if( !*param || !*(++param) ) // Skip the separator.
+	    break;
+	minor = simple_strtoul(param, &param, 10);
+
+	L4VMblock_probe_devices[idx++] = major;
+	L4VMblock_probe_devices[idx++] = minor;
+
+	if( *param )	// Skip the separator.
+	    param++;
+    }
+    if( *param )
+	printk( KERN_WARNING PREFIX "too many block probes.\n" );
+    L4VMblock_probe_devices[idx] = 0;
+    return 1;
+}
+__setup( "blockprobe=", L4VMblock_setup_probe_param );
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/block/client.h linux-2.6.9/afterburn/drivers/block/client.h
--- linux-2.6.9.src/afterburn/drivers/block/client.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/client.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,69 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_client.h
+ * Description:	Declarations specific to the block driver client.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: client.h,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+#ifndef __LINUXBLOCK__L4VMBLOCK_CLIENT_H__
+#define __LINUXBLOCK__L4VMBLOCK_CLIENT_H__
+
+#include <linux/version.h>
+
+#include "L4VMblock_idl_client.h"
+#include "block.h"
+#include <linuxglue/vmirq.h>
+#include <linuxglue/vmserver.h>
+#include <linuxglue/vmmemory.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+typedef dev_t kdev_t;
+#endif
+
+typedef struct
+{
+    kdev_t remote_kdev;
+    IVMblock_handle_t handle;
+    L4_Word_t refcnt;
+} L4VMblock_descriptor_t;
+
+struct L4VMblock_bh_shadow;
+struct L4VMblock_reqest_shadow;
+
+typedef struct L4VMblock_bh_shadow
+{
+    struct buffer_head *bh;
+    struct L4VMblock_request_shadow *owner;
+    struct list_head bh_list;
+} L4VMblock_bh_shadow_t;
+
+typedef struct L4VMblock_request_shadow
+{
+    struct request *request;
+    unsigned bh_count;
+    struct list_head bh_list;
+    struct list_head shadow_list;
+} L4VMblock_request_shadow_t;
+
+typedef struct
+{
+    L4_ThreadId_t server_tid;
+    IVMblock_handle_t handle;
+
+    L4VM_alligned_vmarea_t shared_window;
+    IVMblock_client_shared_t *client_shared;
+    IVMblock_server_shared_t *server_shared;
+
+    int rings_busy;
+    L4VMblock_ring_t desc_ring_info;
+    struct list_head shadow_list;
+    wait_queue_head_t ring_wait;
+
+} L4VMblock_client_t;
+
+#endif	/* __LINUXBLOCK__L4VMBLOCK_CLIENT_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/block/L4VMblock_idl_reply.h linux-2.6.9/afterburn/drivers/block/L4VMblock_idl_reply.h
--- linux-2.6.9.src/afterburn/drivers/block/L4VMblock_idl_reply.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/L4VMblock_idl_reply.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,183 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_idl_reply.h
+ * Description:	Implementation of idl4 compatible inlined functions 
+ * 		to send propagated IPC.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: L4VMblock_idl_reply.h,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+#ifndef __LINUXBLOCK__L4VMBLOCK_IDL_REPLY_H__
+#define __LINUXBLOCK__L4VMBLOCK_IDL_REPLY_H__
+
+static inline void IVMblock_Control_probe_propagate_reply(CORBA_Object _client, IVMblock_devprobe_t *probe_data, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      IVMblock_devprobe_t probe_data;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0 << 6)+8;
+      _par->_out.probe_data = *probe_data;
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMblock_Control_attach_propagate_reply(CORBA_Object _client, IVMblock_handle_t *handle, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      IVMblock_handle_t handle;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0 << 6)+1;
+      _par->_out.handle = *handle;
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMblock_Control_detach_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+      msgtag = (1 << 12)+(0 << 6)+0;
+ 
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+static inline void IVMblock_Control_register_propagate_reply(CORBA_Object _client, IVMblock_handle_t *client_handle, idl4_fpage_t *client_config, idl4_fpage_t *server_config, idl4_server_environment *_env)
+
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      IVMblock_handle_t client_handle;
+      idl4_mapitem client_config;
+      idl4_mapitem server_config;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(_env->_action << 16)+(4 << 6)+1;
+      _par->_out.client_handle = *client_handle;
+      _par->_out.client_config = *client_config;
+      _par->_out.server_config = *server_config;
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+static inline void IVMblock_Control_reattach_propagate_reply(CORBA_Object _client, idl4_fpage_t *client_config, idl4_fpage_t *server_config, idl4_server_environment *_env)
+
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      idl4_mapitem client_config;
+      idl4_mapitem server_config;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(_env->_action << 16)+(4 << 6);
+      _par->_out.client_config = *client_config;
+      _par->_out.server_config = *server_config;
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+
+#endif	/* __LINUXBLOCK__L4VMBLOCK_IDL_REPLY_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/block/Makefile linux-2.6.9/afterburn/drivers/block/Makefile
--- linux-2.6.9.src/afterburn/drivers/block/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/Makefile	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,8 @@
+include $(srctree)/afterburn/drivers/Makesupport
+
+obj-$(CONFIG_AFTERBURN_DRIVERS_BLOCK_SERVER) += l4ka_block_server.o
+obj-$(CONFIG_AFTERBURN_DRIVERS_BLOCK_CLIENT) += l4ka_block_client.o
+l4ka_block_server-y := server26.o
+l4ka_block_client-y := client26.o
+
+$(addprefix $(obj)/,$(obj-m)): $(wedge_symbols)
diff -Naur linux-2.6.9.src/afterburn/drivers/block/server26.c linux-2.6.9/afterburn/drivers/block/server26.c
--- linux-2.6.9.src/afterburn/drivers/block/server26.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/server26.c	2007-07-13 10:02:01.000000000 +0200
@@ -0,0 +1,1115 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/block/server26.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#define DO_L4VMBLOCK_REORDER
+//#define L4VMBLOCK_READ_ONLY // Read-only can cause I/O errors (fs journals).
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/interrupt.h>
+#include <linux/mm.h>
+
+#include <asm/io.h>
+
+#include <linux/blkdev.h>
+#include <linux/buffer_head.h>
+#include <linux/kdev_t.h>
+typedef dev_t kdev_t;
+
+#define PREFIX "L4VMblock server: "
+
+#include "server.h"
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE)
+int L4VMblock_debug_level = 2;
+MODULE_PARM( L4VMblock_debug_level, "i" );
+#endif
+
+static L4_Word_t L4VMblock_irq_no = 7;
+static L4VMblock_server_t L4VMblock_server;
+
+static void L4VMblock_notify_tasklet_handler( unsigned long unused );
+DECLARE_TASKLET( L4VMblock_notify_tasklet, L4VMblock_notify_tasklet_handler, 0);
+
+/***************************************************************************
+ *
+ * Linux block functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_deliver_client_irq( L4VMblock_client_info_t *client )
+{
+    unsigned long flags;
+
+    client->client_shared->client_irq_status |= L4VMBLOCK_CLIENT_IRQ_CLEAN;
+    if( client->client_shared->client_irq_pending )
+	return;
+
+    client->client_shared->client_irq_pending = TRUE;
+
+    local_irq_save(flags);
+    {
+	L4_ThreadId_t from_tid;
+	L4_MsgTag_t msg_tag, result_tag;
+	msg_tag.raw = 0; msg_tag.X.label = L4_TAG_IRQ; msg_tag.X.u = 1;
+	L4_Set_MsgTag( msg_tag );
+	L4_LoadMR( 1, client->client_shared->client_irq_no );
+	result_tag = L4_Reply( client->client_shared->client_main_tid );
+	if( L4_IpcFailed(result_tag) )
+	{
+	    L4_Set_MsgTag( msg_tag );
+	    L4_Ipc( client->client_shared->client_irq_tid, L4_nilthread,
+		    L4_Timeouts(L4_Never, L4_Never), &from_tid );
+	}
+    }
+    local_irq_restore(flags);
+}
+
+static void
+L4VMblock_notify_tasklet_handler( unsigned long unused )
+{
+    IVMblock_handle_t handle;
+    L4VMblock_client_info_t *client;
+    L4VMblock_server_t *server = &L4VMblock_server;
+
+    for( handle = 0; handle < L4VMBLOCK_MAX_CLIENTS; handle++ )
+    {
+	client = L4VMblock_client_lookup( server, handle );
+	if( client )
+	    L4VMblock_deliver_client_irq( client );
+    }
+}
+
+static int L4VMblock_end_io(
+	struct bio *bio,
+	unsigned int bytes_done,
+	int err
+	)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    IVMblock_ring_descriptor_t *desc;
+    L4VMblock_conn_info_t *conn;
+    L4_Word_t ring_index;
+
+    spin_lock( &server->ring_lock );
+
+    ASSERT(bio);
+    desc = (IVMblock_ring_descriptor_t *)bio->bi_private;
+    ASSERT(desc);
+
+    if( err ) {
+	desc->status.X.server_err = 1;
+	dprintk( 2, KERN_INFO PREFIX "bio error\n" );
+    }
+
+    if( bio->bi_size ) {
+	spin_unlock( &server->ring_lock );
+	return 1;  // Not finished.
+    }
+
+    dprintk( 4, KERN_INFO PREFIX "io completed %lx/%p\n",
+	    (L4_Word_t)bio->bi_sector, bio->bi_io_vec[0].bv_page );
+
+    conn = L4VMblock_conn_lookup( server, desc->handle );
+    if( !conn )
+	dprintk( 1, KERN_INFO PREFIX "io completed, but connection is gone.\n");
+    else
+    {
+#if defined(DO_L4VMBLOCK_REORDER)
+ 	L4VMblock_client_info_t *client = conn->client;
+	L4VMblock_ring_t *ring_info = &client->ring_info;
+	IVMblock_ring_descriptor_t *first_desc;
+#endif
+
+	ring_index = (L4_Word_t)(desc - conn->client->client_shared->desc_ring);
+
+#if defined(DO_L4VMBLOCK_REORDER)
+	first_desc = &client->client_shared->desc_ring[ ring_info->start_dirty];
+	ASSERT( first_desc->status.X.server_owned );
+
+	// Is it out-of-order?
+	if( first_desc != desc )
+	{
+	    IVMblock_ring_descriptor_t tmp_desc;
+
+	    // Swap the descriptor contents.
+	    tmp_desc = *desc;
+	    *desc = *first_desc;
+	    *first_desc = tmp_desc;
+
+	    // Update the relocated bio.
+	    client->bio_ring[ ring_index ] = client->bio_ring[ ring_info->start_dirty ];
+	    client->bio_ring[ ring_index ]->bi_private = desc;
+
+	    // Update our desc pointer.
+	    desc = first_desc;
+	    ring_index = ring_info->start_dirty;
+
+	    dprintk( 3, KERN_INFO PREFIX "out-of-order\n" );
+	}
+
+	ring_info->start_dirty = (ring_info->start_dirty + 1) % ring_info->cnt;
+#endif
+
+	// Release to the client.
+	conn->client->bio_ring[ ring_index ] = NULL;
+	desc->status.X.server_owned = 0;
+    }
+
+    spin_unlock( &server->ring_lock );
+
+    bio->bi_private = NULL;
+    bio_put( bio );
+
+#if 1
+    tasklet_schedule( &L4VMblock_notify_tasklet );
+#else
+    L4VMblock_deliver_client_irq( conn->client );
+#endif
+
+    return 0;
+}
+
+static int L4VMblock_initiate_io(
+	L4VMblock_conn_info_t *conn,
+	IVMblock_ring_descriptor_t *desc,
+	L4_Word_t ring_index
+	)
+{
+    // Inspired by submit_bh() in fs/buffer.c.
+    struct bio *bio = bio_alloc( GFP_NOIO, 1 );
+    L4_Word_t paddr = l4ka_wedge_bus_to_phys( (L4_Word_t)desc->page + conn->client->client_space->bus_start );
+    int rw;
+
+    ASSERT( paddr < num_physpages * PAGE_SIZE );
+    ASSERT(bio);
+
+    conn->client->bio_ring[ ring_index ] = bio;
+
+    bio->bi_sector              = desc->offset;
+    bio->bi_bdev                = conn->blkdev;
+    bio->bi_io_vec[0].bv_page   = mem_map + (paddr >> PAGE_SHIFT);
+    bio->bi_io_vec[0].bv_len    = desc->size;
+    bio->bi_io_vec[0].bv_offset = paddr & ~(PAGE_MASK);
+
+    bio->bi_vcnt = 1;
+    bio->bi_idx  = 0;
+    bio->bi_size = desc->size;
+
+    bio->bi_end_io  = L4VMblock_end_io;
+    bio->bi_private = desc;
+
+#ifndef L4VMBLOCK_READ_ONLY
+    if( desc->status.X.do_write )
+	rw = WRITE;
+    else if( desc->status.X.speculative )
+	rw = READA;
+    else
+#endif
+	rw = READ;
+
+    dprintk( 4, KERN_INFO PREFIX "io submit %lx/%p size %u\n",
+	    (L4_Word_t)bio->bi_sector, bio->bi_io_vec[0].bv_page,
+	    bio->bi_io_vec[0].bv_len );
+
+    submit_bio( rw, bio );
+
+    return 1;
+}
+
+static void L4VMblock_process_io_queue(
+	L4VMblock_server_t *server,
+	L4VMblock_client_info_t *client )
+{
+    L4VMblock_ring_t *ring_info = &client->ring_info;
+    IVMblock_ring_descriptor_t *desc;
+    L4VMblock_conn_info_t *conn;
+    int errors = 0;
+    L4_Word_t ring_index;
+
+    while( 1 )
+    {
+	// Manage the rings.
+	spin_lock( &server->ring_lock );
+	ring_index = ring_info->start_free;
+
+	desc = &client->client_shared->desc_ring[ ring_index ];
+	if( !desc->status.X.server_owned || client->bio_ring[ring_index] )
+	{
+	    spin_unlock( &server->ring_lock );
+	    break;
+	}
+
+	ring_info->start_free = (ring_info->start_free + 1) % ring_info->cnt;
+	spin_unlock( &server->ring_lock );
+
+	// Start the I/O (if possible).
+	conn = L4VMblock_conn_lookup( server, desc->handle );
+	if( !conn || (conn->client != client) || 
+		!L4VMblock_initiate_io(conn, desc, ring_index) )
+	{
+	    desc->status.X.server_err = 1;
+	    desc->status.X.server_owned = 0;
+	    errors++;
+	}
+    }
+
+    if( errors )
+	tasklet_schedule( &L4VMblock_notify_tasklet );
+}
+
+static void
+L4VMblock_process_client_io( L4VMblock_server_t *server )
+{
+    int i;
+    L4VMblock_client_info_t *client;
+
+    for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+    {
+	client = L4VMblock_client_lookup( server, i );
+	if( client )
+	    L4VMblock_process_io_queue( server, client );
+    }
+}
+
+/***************************************************************************
+ *
+ * Server handlers.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_probe_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    IVMblock_devprobe_t probe_data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    unsigned long flags;
+
+    kdev_t kdev;
+    struct block_device *bdev;
+    L4_Word_t minor = params->probe.devid.minor;
+    L4_Word_t major = params->probe.devid.major;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+    kdev = MKDEV( major, minor );
+
+    dprintk( 2, KERN_INFO PREFIX "probe request from %p for device %u:%u\n", RAW(cmd->reply_to_tid), MAJOR(kdev), MINOR(kdev) );
+
+    bdev = open_by_devnum( kdev, FMODE_READ );
+    if( IS_ERR(bdev) || !bdev->bd_disk || !bdev->bd_disk->queue )
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+    else
+    {
+	probe_data.devid = params->probe.devid;
+	probe_data.block_size = block_size(bdev);
+	probe_data.hardsect_size = queue_hardsect_size(bdev->bd_disk->queue);
+	probe_data.req_max_sectors = bdev->bd_disk->queue->max_sectors;
+	// Convert the partition size from 512-byte blocks to 1024-byte blocks.
+	// probe_data.device_size = bdev->bd_disk->part[minor-1]->nr_sects / 2 ;
+	if(minor)
+	    probe_data.device_size = bdev->bd_disk->part[minor-1]->nr_sects;
+	else
+	    probe_data.device_size = bdev->bd_disk->capacity;
+
+	// Fudge this stuff.
+	probe_data.read_ahead = 8;
+	probe_data.max_read_ahead = 31;
+    }
+
+    if( !IS_ERR(bdev) )
+	blkdev_put( bdev );
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_probe_propagate_reply( cmd->reply_to_tid,
+	    &probe_data, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static int L4VMblock_allocate_conn_handle( 
+	L4VMblock_server_t *server,
+	IVMblock_handle_t *handle )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    spin_lock( &lock );
+    {
+	int i;
+	for( i = 0; i < L4VMBLOCK_MAX_DEVICES; i++ )
+	    if( server->connections[i].handle == L4VMBLOCK_INVALID_HANDLE )
+	    {
+		*handle = i;
+		server->connections[i].handle = *handle;
+		spin_unlock( &lock );
+		return TRUE;
+	    }
+    }
+    spin_unlock( &lock );
+
+    return FALSE;
+}
+
+static int L4VMblock_allocate_client_handle(
+	L4VMblock_server_t *server,
+	IVMblock_handle_t *handle )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    spin_lock( &lock );
+    {
+	int i;
+	for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+	    if( server->clients[i].handle == L4VMBLOCK_INVALID_HANDLE )
+	    {
+		*handle = i;
+		server->clients[i].handle = *handle;
+		spin_unlock( &lock );
+		return TRUE;
+	    }
+    }
+    spin_unlock( &lock );
+
+    return FALSE;
+}
+
+static void
+L4VMblock_attach_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    // NOTE: see linux/mm/swapfile.c:sys_swapon
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    L4VMblock_client_info_t *client;
+    idl4_server_environment ipc_env;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    unsigned long flags;
+
+    kdev_t kdev;
+    struct block_device *bdev;
+    int err;
+    IVMblock_handle_t handle;
+    L4VMblock_conn_info_t *conn;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+    kdev = MKDEV( params->attach.devid.major, params->attach.devid.minor );
+
+    // Lookup the client.
+    client = L4VMblock_client_lookup( server, params->attach.client_handle );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_client, NULL );
+	goto err_client;
+    }
+
+    bdev = open_by_devnum( kdev, FMODE_READ | FMODE_WRITE );
+    if( IS_ERR(bdev) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_no_device;
+    }
+
+    err = bd_claim( bdev, L4VMblock_attach_handler );
+    if( err < 0 )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_is_mounted;
+    }
+
+    err = set_blocksize( bdev, PAGE_SIZE );
+    if( err < 0 )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_blocksize;
+    }
+
+    if( !L4VMblock_allocate_conn_handle(server, &handle) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_handle;
+    }
+    conn = L4VMblock_conn_lookup( server, handle );
+    conn->blkdev = bdev;
+    conn->kdev = kdev;
+    conn->client = client;
+
+    dprintk( 1, KERN_INFO PREFIX "opened device.\n" );
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_attach_propagate_reply( cmd->reply_to_tid,
+	    &handle, &ipc_env );
+    local_irq_restore(flags);
+    return;
+
+err_handle:
+err_blocksize:
+    bd_release( bdev );
+err_is_mounted:
+    blkdev_put( bdev );
+err_no_device:
+err_client:
+    printk( KERN_INFO PREFIX "failed to open a device.\n" );
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_attach_propagate_reply( cmd->reply_to_tid, NULL, &ipc_env);
+    local_irq_restore(flags);
+}
+
+static void
+L4VMblock_detach_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    L4VMblock_conn_info_t *conn;
+    unsigned long flags;
+ 
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+
+    conn = L4VMblock_conn_lookup( server, params->detach.handle );
+    if( conn == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_client, NULL );
+	goto out;
+    }
+
+    if( conn->blkdev )
+    {
+	bd_release( conn->blkdev );
+	blkdev_put( conn->blkdev );
+    }
+    conn->blkdev = NULL;
+    L4VMblock_conn_release( conn );
+
+out:
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_detach_propagate_reply( cmd->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMblock_register_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+
+    IVMblock_handle_t handle;
+    L4VMblock_client_info_t *client;
+    L4_Word_t log2size;
+    int err, i;
+    idl4_fpage_t idl4_fp_client, idl4_fp_server;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+
+    // Allocate a client structure.
+    if( !L4VMblock_allocate_client_handle(server, &handle) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_handle;
+    }
+    client = L4VMblock_client_lookup( server, handle );
+
+    // Get access to the client's pages.  This is a HACK.
+    client->client_space = L4VM_get_client_space_info( cmd->reply_to_tid );
+    if( client->client_space == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_client_space;
+    }
+
+    // Allocate a client shared info region.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMblock_client_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &client->client_alloc_info );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate client shared pages "
+		"(size %u).\n", 1 << log2size );
+	goto err_shared_alloc;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "client shared region at %p, size %ld (%d)\n",
+	    (void *)L4_Address(client->client_alloc_info.fpage),
+	    L4_Size(client->client_alloc_info.fpage), 
+	    sizeof(IVMblock_client_shared_t) );
+
+    // Init the client shared page.
+    client->client_shared = (IVMblock_client_shared_t *)
+	L4_Address( client->client_alloc_info.fpage );
+    client->client_shared->server_irq_no = L4VMblock_irq_no;
+    client->client_shared->server_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    client->client_shared->server_main_tid = L4VM_linux_main_thread( smp_processor_id() );
+    client->client_shared->client_irq_status = 0;
+    client->client_shared->client_irq_pending = 0;
+    for( i = 0; i < IVMblock_descriptor_ring_size; i++ )
+    {
+	client->client_shared->desc_ring[i].page = NULL;
+	client->client_shared->desc_ring[i].status.raw = 0;
+    }
+
+    client->ring_info.start_free = 0;
+    client->ring_info.start_dirty = 0;
+    client->ring_info.cnt = IVMblock_descriptor_ring_size;
+    for( i = 0; i < client->ring_info.cnt; i++ )
+	client->bio_ring[i] = NULL;
+
+    // Build the reply fpages.
+    idl4_fpage_set_base( &idl4_fp_client, 0 );
+    idl4_fpage_set_mode( &idl4_fp_client, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_client, client->client_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_client, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // The server page is mapped after the client page.
+    idl4_fpage_set_base( &idl4_fp_server, 
+	    L4_Size(client->client_alloc_info.fpage) );
+    idl4_fpage_set_mode( &idl4_fp_server, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_server, server->server_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_server, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // Reply to the client.
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_register_propagate_reply( cmd->reply_to_tid, 
+	    &handle, &idl4_fp_client, &idl4_fp_server, &ipc_env );
+    local_irq_restore(flags);
+    return;
+
+err_shared_alloc:
+    L4VMblock_client_release( client );
+err_client_space:
+err_handle:
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_register_propagate_reply( cmd->reply_to_tid, 
+	    0, NULL, NULL, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMblock_reattach_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    L4VMblock_client_info_t *client;
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+    idl4_fpage_t idl4_fp_client, idl4_fp_server;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+
+    // Lookup the client.
+    client = L4VMblock_client_lookup( server, params->attach.client_handle );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_client, NULL );
+	goto err_client;
+    }
+
+    L4VM_page_in( client->client_alloc_info.fpage, PAGE_SIZE );
+    L4VM_page_in( server->server_alloc_info.fpage, PAGE_SIZE );
+
+    // Build the reply fpages.
+    idl4_fpage_set_base( &idl4_fp_client, 0 );
+    idl4_fpage_set_mode( &idl4_fp_client, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_client, client->client_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_client, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // The server page is mapped after the client page.
+    idl4_fpage_set_base( &idl4_fp_server, 
+	    L4_Size(client->client_alloc_info.fpage) );
+    idl4_fpage_set_mode( &idl4_fp_server, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_server, server->server_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_server, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // Reply to the client.
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_reattach_propagate_reply( cmd->reply_to_tid, 
+	    &idl4_fp_client, &idl4_fp_server, &ipc_env );
+    local_irq_restore(flags);
+    return;
+
+err_client:
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_reattach_propagate_reply( cmd->reply_to_tid, 
+	    NULL, NULL, &ipc_env );
+    local_irq_restore(flags);
+}
+ 
+/***************************************************************************
+ *
+ * Linux interrupt functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_bottom_half_handler( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    ASSERT( !in_interrupt() );
+
+    L4VM_server_cmd_dispatcher( &server->bottom_half_cmds, server,
+	    server->server_tid );
+}
+
+#if !defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+static void
+L4VMblock_dispatch_handler( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    L4VMblock_process_client_io( server );
+}
+#endif
+
+static irqreturn_t
+L4VMblock_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    while( 1 )
+    {
+	// Outstanding events? Read them and reset without losing events.
+	L4_Word_t events = L4VM_irq_status_reset( &server->irq.status );
+	L4_Word_t client_events = L4VM_irq_status_reset( &server->server_info->irq_status);
+	if( !events && !client_events )
+	    break;
+
+	dprintk( 4, KERN_INFO PREFIX "irq handler: 0x%lx/0x%lx\n", 
+		events, client_events );
+
+	server->irq.pending = TRUE;
+	server->server_info->irq_pending = TRUE;
+
+	if( client_events )
+	{
+#if defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+	    L4VMblock_process_client_io( server );
+#else
+	    schedule_task( &server->dispatch_task );
+#endif
+	}
+
+	if( (events & L4VMBLOCK_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VM_server_cmd_dispatcher( &server->top_half_cmds, server,
+		    server->server_tid );
+
+	if( (events & L4VMBLOCK_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &server->bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	server->irq.pending = FALSE;
+	server->server_info->irq_pending = FALSE;
+    }
+
+    return IRQ_HANDLED;
+}
+
+#if 0
+static int
+L4VMblock_irq_pending( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    return (server->irq.status & server->irq.mask) ||
+	server->server_info->irq_status;
+}
+#endif
+
+/***************************************************************************
+ *
+ * Server thread dispatch.
+ *
+ ***************************************************************************/
+
+IDL4_INLINE void IVMblock_Control_probe_implementation(
+	CORBA_Object _caller,
+	const IVMblock_devid_t *devid,
+	IVMblock_devprobe_t *probe_data,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client probe request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->probe.devid = *devid;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_probe_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMBLOCK_CONTROL_PROBE(IVMblock_Control_probe_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_attach_implementation(
+	CORBA_Object _caller,
+	const IVMblock_handle_t client_handle,
+	const IVMblock_devid_t *devid,
+	const unsigned int rw,
+	IVMblock_handle_t *handle,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client attach request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->attach.client_handle = client_handle;
+    params->attach.devid = *devid;
+    params->attach.rw = rw;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_attach_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMBLOCK_CONTROL_ATTACH(IVMblock_Control_attach_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_detach_implementation(
+	CORBA_Object _caller,
+
+	const IVMblock_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client detach request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->detach.handle = handle;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_detach_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMBLOCK_CONTROL_DETACH(IVMblock_Control_detach_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_register_implementation(
+	CORBA_Object _caller,
+	IVMblock_handle_t *client_handle,
+	idl4_fpage_t *client_config,
+	idl4_fpage_t *server_config,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client register request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_register_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMBLOCK_CONTROL_REGISTER(IVMblock_Control_register_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_reattach_implementation(
+	CORBA_Object _caller,
+	const IVMblock_handle_t client_handle,
+	idl4_fpage_t *client_config,
+	idl4_fpage_t *server_config,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client reattach request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->reattach.handle = client_handle;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_reattach_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMBLOCK_CONTROL_REATTACH(IVMblock_Control_reattach_implementation);
+
+
+
+static void
+L4VMblock_server_thread( void *data )
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    static void *IVMblock_Control_vtable[IVMBLOCK_CONTROL_DEFAULT_VTABLE_SIZE] = IVMBLOCK_CONTROL_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init(&msgbuf);
+
+    while (1)
+    {
+    	partner = L4_nilthread;
+      	msgtag.raw = 0;
+	cnt = 0;
+
+	while (1)
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt, IVMblock_Control_vtable[idl4_get_function_id(&msgtag) & IVMBLOCK_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMblock_Control_discard(void)
+{
+    // Invoked in response to invalid IPC requests.
+}
+
+/***************************************************************************
+ *
+ * Linux module functions.
+ *
+ ***************************************************************************/
+
+static int __init
+L4VMblock_server_init_module( void )
+{
+    int err, i;
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word_t log2size;
+
+    spin_lock_init( &server->ring_lock );
+    L4VM_irq_init( &server->irq );
+    L4VM_server_cmd_init( &server->top_half_cmds );
+    L4VM_server_cmd_init( &server->bottom_half_cmds );
+
+    for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+	server->clients[i].handle = L4VMBLOCK_INVALID_HANDLE;
+    for( i = 0; i < L4VMBLOCK_MAX_DEVICES; i++ )
+	server->connections[i].handle = L4VMBLOCK_INVALID_HANDLE;
+
+    INIT_TQUEUE( &server->bottom_half_task, 
+	    L4VMblock_bottom_half_handler, server );
+#if !defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+    INIT_TQUEUE( &server->dispatch_task,
+	    L4VMblock_dispatch_handler, server );
+#endif
+
+    // Allocate a shared memory region for access by all clients.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMblock_server_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &server->server_alloc_info );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate server shared pages "
+		"(size %u).\n", 1 << log2size );
+	goto err_shared_alloc;
+    }
+    server->server_info = (IVMblock_server_shared_t *)
+	L4_Address( server->server_alloc_info.fpage );
+    server->server_info->irq_status = 0;
+    server->server_info->irq_pending = 0;
+
+    // Allocate a virtual interrupt.
+    server->irq.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    if( L4VMblock_irq_no > NR_IRQS )
+    {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	err = -ENOMEM;
+	goto err_vmpic_reserve;
+    }
+    l4ka_wedge_add_virtual_irq( L4VMblock_irq_no );
+    err = request_irq( L4VMblock_irq_no, L4VMblock_irq_handler, 0, 
+	    "L4VMblock", server );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Start the server thread.
+    server->server_tid = L4VM_thread_create( GFP_KERNEL,
+	    L4VMblock_server_thread, l4ka_wedge_get_irq_prio(), 
+	    smp_processor_id(), &server, sizeof(server) );
+    if( L4_IsNilThread(server->server_tid) )
+    {
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	err = -ENOMEM;
+	goto err_thread_start;
+    }
+
+    // Register with the locator.
+    err = L4VM_server_register_location( UUID_IVMblock, server->server_tid );
+    if( err == -ENODEV )
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+    else if( err )
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+
+    printk( KERN_INFO PREFIX "L4VMblock driver initialized.\n" );
+
+    return 0;
+
+err_thread_start:
+    // TODO: free the interrupt handler
+err_request_irq:
+err_vmpic_reserve:
+    L4VM_fpage_dealloc( &server->server_alloc_info );
+err_shared_alloc:
+    return err;
+}
+
+static void __exit
+L4VMblock_server_exit_module( void )
+{
+#if 0
+    L4VMblock_server_t *server = &L4VMblock_server;
+    if( !L4_IsNilThread(server->server_tid) )
+    {
+	// TODO: free the interrupt handler
+	l4lx_thread_shutdown( server->server_tid );
+	server->server_tid = L4_nilthread;
+    }
+#endif
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4 block driver server" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM block" );
+MODULE_VERSION( "alpha" );
+
+module_init( L4VMblock_server_init_module );
+module_exit( L4VMblock_server_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
+
diff -Naur linux-2.6.9.src/afterburn/drivers/block/server.c linux-2.6.9/afterburn/drivers/block/server.c
--- linux-2.6.9.src/afterburn/drivers/block/server.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/server.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,958 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_server.c
+ * Description:	
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: server.c,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+
+#define DO_L4VMBLOCK_REORDER
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/interrupt.h>
+
+#include <asm/io.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+#include <linux/blk.h>
+#include <asm/irq.h>
+#include <asm/arch/hardirq.h>
+#include <asm/l4lxapi/thread.h>
+#include <asm/l4linux/debug.h>
+#else
+#include <linux/blkdev.h>
+#include <linux/buffer_head.h>
+#include <linux/kdev_t.h>
+typedef dev_t kdev_t;
+#define herc_printf L4_printk
+#endif
+
+#define PREFIX "L4VMblock server: "
+
+#include "server.h"
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_BLOCK_OPTIMIZE)
+int L4VMblock_debug_level = 2;
+MODULE_PARM( L4VMblock_debug_level, "i" );
+#endif
+
+static L4_Word_t L4VMblock_irq_no;
+static L4VMblock_server_t L4VMblock_server;
+
+static void L4VMblock_notify_tasklet_handler( unsigned long unused );
+DECLARE_TASKLET( L4VMblock_notify_tasklet, L4VMblock_notify_tasklet_handler, 0);
+
+/***************************************************************************
+ *
+ * Linux block functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_deliver_client_irq( L4VMblock_client_info_t *client )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+
+    client->client_shared->client_irq_status |= L4VMBLOCK_CLIENT_IRQ_CLEAN;
+#if 0
+    if( client->client_shared->client_irq_pending )
+	return;
+#endif
+
+    client->client_shared->client_irq_pending = TRUE;
+
+#if 0
+    // Send a zero-timeout IPC.
+    ipc_env._timeout = L4_Timeouts( L4_ZeroTime, L4_Never );
+#endif
+
+    ILinux_raise_irq( client->client_shared->client_irq_tid,
+	    client->client_shared->client_irq_no, &ipc_env );
+
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+	CORBA_exception_free( &ipc_env );
+}
+
+static void
+L4VMblock_notify_tasklet_handler( unsigned long unused )
+{
+    IVMblock_handle_t handle;
+    L4VMblock_client_info_t *client;
+    L4VMblock_server_t *server = &L4VMblock_server;
+
+    for( handle = 0; handle < L4VMBLOCK_MAX_CLIENTS; handle++ )
+    {
+	client = L4VMblock_client_lookup( server, handle );
+	if( client )
+	    L4VMblock_deliver_client_irq( client );
+    }
+}
+
+static void
+L4VMblock_end_io( struct buffer_head *bh, int uptodate )
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    IVMblock_ring_descriptor_t *desc = (IVMblock_ring_descriptor_t *)
+	bh->b_private;
+    L4VMblock_conn_info_t *conn;
+
+    dprintk( 4, KERN_INFO PREFIX "io completed, up to date? %d\n", uptodate );
+
+    conn = L4VMblock_conn_lookup( server, desc->handle );
+    if( !conn )
+	dprintk( 1, KERN_INFO PREFIX "io completed, but connection is gone.\n");
+    else
+    {
+#if defined(DO_L4VMBLOCK_REORDER)
+	L4VMblock_client_info_t *client = conn->client;
+	L4VMblock_ring_t *ring_info = &client->ring_info;
+	L4_Word16_t later_idx, earlier_idx;
+
+	// Find location of the descriptor in the ring.
+	later_idx = ((L4_Word_t)desc - (L4_Word_t)client->client_shared->desc_ring) / sizeof(IVMblock_ring_descriptor_t);
+	earlier_idx = ring_info->start_dirty;
+
+	ASSERT( client->client_shared->desc_ring[earlier_idx].status.X.server_owned );
+
+	// Is it out-of-order?
+	if( later_idx != earlier_idx )
+	{
+	    IVMblock_ring_descriptor_t tmp_desc;
+	    struct buffer_head *tmp_bh;
+
+	    // Make temporary copies of the later info.
+	    tmp_desc = client->client_shared->desc_ring[later_idx];
+	    tmp_bh = client->bh_ring[later_idx];
+
+	    ASSERT( client->client_shared->desc_ring[earlier_idx].status.X.server_owned );
+	    ASSERT( desc == &client->client_shared->desc_ring[later_idx] );
+	    ASSERT( bh == tmp_bh );
+
+	    // Move the earlier info to the later slots.
+	    client->bh_ring[later_idx] = client->bh_ring[earlier_idx];
+	    client->client_shared->desc_ring[later_idx] = client->client_shared->desc_ring[earlier_idx];
+	    // Update the bh's private data.
+	    client->bh_ring[later_idx]->b_private = (void *)
+		&client->client_shared->desc_ring[later_idx];
+
+	    // Move the temporary copies into the earlier slots.
+	    client->bh_ring[earlier_idx] = tmp_bh;
+	    client->client_shared->desc_ring[earlier_idx] = tmp_desc;
+
+	    // Update our desc pointer.
+	    desc = &client->client_shared->desc_ring[earlier_idx];
+
+	    dprintk( 4, KERN_INFO PREFIX "out-of-order, current: %d, "
+		    "done: %d\n",
+		    earlier_idx, later_idx );
+	}
+	ring_info->start_dirty = (ring_info->start_dirty + 1) % ring_info->cnt;
+#endif
+
+	if( !uptodate )
+    	    desc->status.X.server_err = 1;
+	desc->status.X.server_owned = 0;
+
+//	L4VMblock_deliver_client_irq( conn->client );
+	tasklet_schedule( &L4VMblock_notify_tasklet );
+    }
+}
+
+static void L4VMblock_initiate_io( 
+	L4VMblock_conn_info_t *conn,
+	IVMblock_ring_descriptor_t *desc,
+	struct buffer_head *bh )
+{
+    // Modeled from brw_kiovec() and generic_direct_IO() in linux/fs/buffer.c
+    int rw;
+    if( desc->status.X.do_write )
+	rw = WRITE;
+    else if( desc->status.X.speculative )
+	rw = READA;
+    else
+	rw = READ;
+
+    if( desc->size > conn->block_size )
+    {
+	printk( KERN_ERR PREFIX "client requested block size %lu, but the "
+		"device wants %lu\n", desc->size, conn->block_size );
+	desc->status.X.server_err = 1;
+	desc->status.X.server_owned = 0;
+	return;
+    }
+
+    /* From init_buffer_head() in fs/dcache.c. */
+    memset( bh, 0, sizeof(*bh) );
+    init_waitqueue_head( &bh->b_wait );
+
+    init_buffer( bh, L4VMblock_end_io, /* private */ desc );
+    bh->b_dev = conn->kdev;
+    bh->b_rdev = conn->kdev;
+    bh->b_size = desc->size;
+    bh->b_rsector = desc->offset;
+    bh->b_blocknr = bh->b_rsector / (bh->b_size >> 9);
+    bh->b_this_page = bh;
+    bh->b_state = (1 << BH_Uptodate) | (1 << BH_Lock) | (1 << BH_Mapped);
+    if( desc->status.X.do_write )
+	clear_bit( BH_Dirty, &bh->b_state );
+
+    bh->b_page = NULL;
+    bh->b_data = bus_to_virt( (L4_Word_t)desc->page );
+    if( unlikely(bh->b_data == NULL) )
+    {
+	// TODO: reorder!!
+	desc->status.X.server_err = 1;
+	desc->status.X.server_owned = 0;
+	return;
+    }
+    atomic_inc( &bh->b_count );
+ 
+    dprintk( 4, KERN_INFO PREFIX "i/o request, size %lu, page %p, offset %lu, "
+	    "write? %d, bus addr %p, vaddr %p (%p)\n",
+	    desc->size, desc->page, desc->offset, desc->status.X.do_write,
+	    (void *)virt_to_bus(bh->b_data), 
+	    bh->b_data, bus_to_virt(virt_to_bus(bh->b_data)) );
+
+    submit_bh( rw, bh );
+
+}
+
+static void L4VMblock_process_io_queue(
+	L4VMblock_server_t *server,
+	L4VMblock_client_info_t *client )
+{
+    L4VMblock_ring_t *ring_info = &client->ring_info;
+    IVMblock_ring_descriptor_t *desc;
+    struct buffer_head *bh;
+    L4VMblock_conn_info_t *conn;
+    L4_Word_t start_free = ring_info->start_free;
+
+    while( 1 )
+    {
+	// Get stuff from the appropriate rings.
+	desc = &client->client_shared->desc_ring[ ring_info->start_free ];
+	if( !desc->status.X.server_owned )
+	    break;
+	bh = client->bh_ring[ ring_info->start_free ];
+
+	// Start the I/O (if possible).
+	conn = L4VMblock_conn_lookup( server, desc->handle );
+	if( conn && (conn->client == client) )
+	    L4VMblock_initiate_io( conn, desc, bh );
+	else
+	{
+	    // TODO: reorder!!
+	    printk( KERN_ERR PREFIX "client request accesses invalid connection.\n" );
+	    desc->status.X.server_err = 1;
+	    desc->status.X.server_owned = 0;
+	}
+
+	// Move to the next descriptor.
+	ring_info->start_free = (ring_info->start_free + 1) % ring_info->cnt;
+	if( ring_info->start_free == start_free )
+	{
+	    dprintk( 2, KERN_INFO PREFIX "client caused a wrap-around.\n" );
+	    break;	// Wrapped around.
+	}
+    }
+}
+
+static void
+L4VMblock_process_client_io( L4VMblock_server_t *server )
+{
+    int i;
+    L4VMblock_client_info_t *client;
+
+    for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+    {
+	client = L4VMblock_client_lookup( server, i );
+	if( client )
+	    L4VMblock_process_io_queue( server, client );
+    }
+
+    // Start execution of our queued block requests.
+    run_task_queue( &tq_disk );
+}
+
+/***************************************************************************
+ *
+ * Server handlers.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_probe_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    IVMblock_devprobe_t probe_data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    kdev_t kdev;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+    kdev = MKDEV( params->probe.devid.major, params->probe.devid.minor );
+
+    dprintk( 2, KERN_INFO PREFIX "probe request from %p for device %u:%u\n", RAW(cmd->reply_to_tid), MAJOR(kdev), MINOR(kdev) );
+
+    if( !bdget(kdev_t_to_nr(kdev)) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+    }
+    else
+    {
+	// TODO: acquire some lock, for in case the device disappears
+	// during our lookups of minor devices.
+	probe_data.devid = params->probe.devid;
+	probe_data.device_size = blk_size[MAJOR(kdev)] ? blk_size[MAJOR(kdev)][MINOR(kdev)] : 0;
+	probe_data.block_size = block_size(kdev);
+	probe_data.hardsect_size = get_hardsect_size(kdev);
+	probe_data.read_ahead = read_ahead[MAJOR(kdev)];
+	probe_data.max_read_ahead = max_readahead[MAJOR(kdev)] ? max_readahead[MAJOR(kdev)][MINOR(kdev)] : 31;
+	probe_data.req_max_sectors = max_sectors[MAJOR(kdev)] ? max_sectors[MAJOR(kdev)][MINOR(kdev)] : MAX_SECTORS;
+    }
+
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_probe_propagate_reply( cmd->reply_to_tid,
+	    &probe_data, &ipc_env );
+}
+
+static int L4VMblock_allocate_conn_handle( 
+	L4VMblock_server_t *server,
+	IVMblock_handle_t *handle )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    spin_lock( lock );
+    {
+	int i;
+	for( i = 0; i < L4VMBLOCK_MAX_DEVICES; i++ )
+	    if( server->connections[i].handle == L4VMBLOCK_INVALID_HANDLE )
+	    {
+		*handle = i;
+		server->connections[i].handle = *handle;
+		spin_unlock( lock );
+		return TRUE;
+	    }
+    }
+    spin_unlock( lock );
+
+    return FALSE;
+}
+
+static int L4VMblock_allocate_client_handle(
+	L4VMblock_server_t *server,
+	IVMblock_handle_t *handle )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    spin_lock( lock );
+    {
+	int i;
+	for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+	    if( server->clients[i].handle == L4VMBLOCK_INVALID_HANDLE )
+	    {
+		*handle = i;
+		server->clients[i].handle = *handle;
+		spin_unlock( lock );
+		return TRUE;
+	    }
+    }
+    spin_unlock( lock );
+
+    return FALSE;
+}
+
+static void
+L4VMblock_attach_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    // NOTE: see linux/mm/swapfile.c:sys_swapon
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    L4VMblock_client_info_t *client;
+    idl4_server_environment ipc_env;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    kdev_t kdev;
+    struct block_device *blkdev;
+    int err;
+    IVMblock_handle_t handle;
+    L4VMblock_conn_info_t *conn;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+    kdev = MKDEV( params->attach.devid.major, params->attach.devid.minor );
+
+    // Lookup the client.
+    client = L4VMblock_client_lookup( server, params->attach.client_handle );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_client, NULL );
+	goto err_client;
+    }
+
+    lock_kernel();
+
+    if( is_mounted(kdev))
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_is_mounted;
+    }
+
+    blkdev = bdget( kdev_t_to_nr(kdev) );
+    if( blkdev == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_no_device;
+    }
+
+    set_blocksize( kdev, PAGE_SIZE );
+    err = blkdev_get( blkdev, 
+	    ((params->attach.rw & 2) ? FMODE_READ:0) | 
+	    ((params->attach.rw & 1) ? FMODE_WRITE:0), 0, 0 );
+    if( err )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_device, NULL );
+	goto err_open;
+    }
+    set_blocksize( kdev, PAGE_SIZE );
+
+    unlock_kernel();
+
+    if( !L4VMblock_allocate_conn_handle(server, &handle) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_handle;
+    }
+    conn = L4VMblock_conn_lookup( server, handle );
+    conn->blkdev = blkdev;
+    conn->block_size = block_size( kdev );
+    conn->kdev = kdev;
+    conn->client = client;
+
+    dprintk( 1, KERN_INFO PREFIX "opened device.\n" );
+
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_attach_propagate_reply( cmd->reply_to_tid,
+	    &handle, &ipc_env );
+    return;
+
+err_handle:
+    blkdev_put( blkdev, 0 );
+err_open:
+err_no_device:
+err_is_mounted:
+    unlock_kernel();
+err_client:
+    printk( KERN_INFO PREFIX "failed to open a device.\n" );
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_attach_propagate_reply( cmd->reply_to_tid, NULL, &ipc_env);
+}
+
+static void
+L4VMblock_detach_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    L4VMblock_conn_info_t *conn;
+ 
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+
+    conn = L4VMblock_conn_lookup( server, params->detach.handle );
+    if( conn == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_invalid_client, NULL );
+	goto out;
+    }
+
+    if( conn->blkdev )
+	blkdev_put( conn->blkdev, 0 );
+    conn->blkdev = NULL;
+    L4VMblock_conn_release( conn );
+
+out:
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_detach_propagate_reply( cmd->reply_to_tid, &ipc_env );
+}
+
+static void
+L4VMblock_register_handler( L4VM_server_cmd_t *cmd, void *data )
+{ 
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+    idl4_server_environment ipc_env;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    IVMblock_handle_t handle;
+    L4VMblock_client_info_t *client;
+    L4_Word_t log2size;
+    int err, i;
+    idl4_fpage_t idl4_fp_client, idl4_fp_server;
+
+    ASSERT( cmd->reply_to_tid.raw );
+    ASSERT( params );
+    ASSERT( server->server_tid.raw );
+    ipc_env._action = 0;
+
+    // Allocate a client structure.
+    if( !L4VMblock_allocate_client_handle(server, &handle) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_handle;
+    }
+    client = L4VMblock_client_lookup( server, handle );
+
+    // Get access to the client's pages.  This is a HACK.
+    client->client_space = L4VM_get_client_space_info( cmd->reply_to_tid );
+    if( client->client_space == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMblock_no_memory, NULL );
+	goto err_client_space;
+    }
+
+    // Allocate a client shared info region.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMblock_client_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &client->client_alloc_info );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate client shared pages "
+		"(size %u).\n", 1 << log2size );
+	goto err_shared_alloc;
+    }
+
+    dprintk( 2, KERN_INFO PREFIX "client shared region at %p, size %ld (%d)\n",
+	    (void *)L4_Address(client->client_alloc_info.fpage),
+	    L4_Size(client->client_alloc_info.fpage), 
+	    sizeof(IVMblock_client_shared_t) );
+
+    // Init the client shared page.
+    client->client_shared = (IVMblock_client_shared_t *)
+	L4_Address( client->client_alloc_info.fpage );
+    client->client_shared->server_irq_no = L4VMblock_irq_no;
+    client->client_shared->server_irq_tid = server->irq.my_irq_tid;
+    client->client_shared->client_irq_status = 0;
+    client->client_shared->client_irq_pending = 0;
+    for( i = 0; i < IVMblock_descriptor_ring_size; i++ )
+    {
+	client->client_shared->desc_ring[i].page = NULL;
+	client->client_shared->desc_ring[i].status.raw = 0;
+    }
+
+    client->ring_info.start_free = 0;
+    client->ring_info.start_dirty = 0;
+    client->ring_info.cnt = IVMblock_descriptor_ring_size;
+    for( i = 0; i < client->ring_info.cnt; i++ )
+	client->bh_ring[i] = &client->bh_ring_storage[i];
+
+    // Build the reply fpages.
+    idl4_fpage_set_base( &idl4_fp_client, 0 );
+    idl4_fpage_set_mode( &idl4_fp_client, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_client, client->client_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_client, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // The server page is mapped after the client page.
+    idl4_fpage_set_base( &idl4_fp_server, 
+	    L4_Size(client->client_alloc_info.fpage) );
+    idl4_fpage_set_mode( &idl4_fp_server, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp_server, server->server_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp_server, 
+	    IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    // Reply to the client.
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_register_propagate_reply( cmd->reply_to_tid, 
+	    &handle, &idl4_fp_client, &idl4_fp_server, &ipc_env );
+    return;
+
+err_shared_alloc:
+    L4VMblock_client_release( client );
+err_client_space:
+err_handle:
+    L4_Set_VirtualSender( server->server_tid );
+    IVMblock_Control_register_propagate_reply( cmd->reply_to_tid, 
+	    0, NULL, NULL, &ipc_env );
+}
+
+/***************************************************************************
+ *
+ * Linux interrupt functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMblock_bottom_half_handler( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    ASSERT( !in_interrupt() );
+
+    L4VM_server_cmd_dispatcher( &server->bottom_half_cmds, server,
+	    server->server_tid );
+}
+
+#if !defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+static void
+L4VMblock_dispatch_handler( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    L4VMblock_process_client_io( server );
+}
+#endif
+
+static void
+L4VMblock_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    while( 1 )
+    {
+	// Outstanding events? Read them and reset without losing events.
+	L4_Word_t events = L4VM_irq_status_reset( &server->irq.status );
+	L4_Word_t client_events = L4VM_irq_status_reset( &server->server_info->irq_status);
+	if( !events && !client_events )
+	    return;
+
+	dprintk( 3, KERN_INFO PREFIX "irq handler: 0x%lx/0x%lx\n", 
+		events, client_events );
+
+	server->irq.pending = TRUE;
+	server->server_info->irq_pending = TRUE;
+
+	if( client_events )
+	{
+#if defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+	    L4VMblock_process_client_io( server );
+#else
+	    schedule_task( &server->dispatch_task );
+#endif
+	}
+
+	if( (events & L4VMBLOCK_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VM_server_cmd_dispatcher( &server->top_half_cmds, server,
+		    server->server_tid );
+
+	if( (events & L4VMBLOCK_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &server->bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	server->irq.pending = FALSE;
+	server->server_info->irq_pending = FALSE;
+    }
+}
+
+static int
+L4VMblock_irq_pending( void *data )
+{
+    L4VMblock_server_t *server = (L4VMblock_server_t *)data;
+
+    return (server->irq.status & server->irq.mask) ||
+	server->server_info->irq_status;
+}
+
+/***************************************************************************
+ *
+ * Server thread dispatch.
+ *
+ ***************************************************************************/
+
+IDL4_INLINE void IVMblock_Control_probe_implementation(
+	CORBA_Object _caller,
+	const IVMblock_devid_t *devid,
+	IVMblock_devprobe_t *probe_data,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client probe request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->probe.devid = *devid;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_probe_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_IVMBLOCK_CONTROL_PROBE(IVMblock_Control_probe_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_attach_implementation(
+	CORBA_Object _caller,
+	const IVMblock_handle_t client_handle,
+	const IVMblock_devid_t *devid,
+	const unsigned int rw,
+	IVMblock_handle_t *handle,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client attach request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->attach.client_handle = client_handle;
+    params->attach.devid = *devid;
+    params->attach.rw = rw;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_attach_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_IVMBLOCK_CONTROL_ATTACH(IVMblock_Control_attach_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_detach_implementation(
+	CORBA_Object _caller,
+
+	const IVMblock_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client detach request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    params->detach.handle = handle;
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_detach_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_IVMBLOCK_CONTROL_DETACH(IVMblock_Control_detach_implementation);
+
+
+IDL4_INLINE void IVMblock_Control_register_implementation(
+	CORBA_Object _caller,
+	IVMblock_handle_t *client_handle,
+	idl4_fpage_t *client_config,
+	idl4_fpage_t *server_config,
+	idl4_server_environment *_env)
+{
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client register request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD, &server->irq,
+	    L4VMblock_irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMblock_register_handler;
+
+    L4VM_irq_deliver( &server->irq, L4VMblock_irq_no, 
+	    L4VMBLOCK_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_IVMBLOCK_CONTROL_REGISTER(IVMblock_Control_register_implementation);
+
+
+static void
+L4VMblock_server_thread( void *data )
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    static void *IVMblock_Control_vtable[IVMBLOCK_CONTROL_DEFAULT_VTABLE_SIZE] = IVMBLOCK_CONTROL_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init(&msgbuf);
+
+    while (1)
+    {
+    	partner = L4_nilthread;
+      	msgtag.raw = 0;
+	cnt = 0;
+
+	while (1)
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt, IVMblock_Control_vtable[idl4_get_function_id(&msgtag) & IVMBLOCK_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMblock_Control_discard(void)
+{
+    // Invoked in response to invalid IPC requests.
+}
+
+/***************************************************************************
+ *
+ * Linux module functions.
+ *
+ ***************************************************************************/
+
+static int __init
+L4VMblock_server_init_module( void )
+{
+    int err, i;
+    L4VMblock_server_t *server = &L4VMblock_server;
+    L4_Word_t log2size;
+
+    L4VM_irq_init( &server->irq );
+    L4VM_server_cmd_init( &server->top_half_cmds );
+    L4VM_server_cmd_init( &server->bottom_half_cmds );
+
+    for( i = 0; i < L4VMBLOCK_MAX_CLIENTS; i++ )
+	server->clients[i].handle = L4VMBLOCK_INVALID_HANDLE;
+    for( i = 0; i < L4VMBLOCK_MAX_DEVICES; i++ )
+	server->connections[i].handle = L4VMBLOCK_INVALID_HANDLE;
+
+    INIT_TQUEUE( &server->bottom_half_task, 
+	    L4VMblock_bottom_half_handler, server );
+#if !defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+    INIT_TQUEUE( &server->dispatch_task,
+	    L4VMblock_dispatch_handler, server );
+#endif
+
+    // Allocate a shared memory region for access by all clients.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMblock_server_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &server->server_alloc_info );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate server shared pages "
+		"(size %u).\n", 1 << log2size );
+	goto err_shared_alloc;
+    }
+    server->server_info = (IVMblock_server_shared_t *)
+	L4_Address( server->server_alloc_info.fpage );
+    server->server_info->irq_status = 0;
+    server->server_info->irq_pending = 0;
+
+    // Register an IRQ handler with Linux.
+    err = request_mlx_irq( L4_nilthread, L4VMblock_irq_handler,
+	    SA_SHIRQ, "L4VMblock", server, L4VMblock_irq_pending,
+	    server, &server->irq.my_irq_tid );
+    if( err >= 0 )
+	L4VMblock_irq_no = err;
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Start the server thread.
+    server->server_tid = l4lx_thread_create( L4VMblock_server_thread,
+	    NULL, server, sizeof(server), PRIO_IRQ(0), 
+	    "L4VMblock server thread" );
+    if( L4_IsNilThread(server->server_tid) )
+    {
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	err = -ENOMEM;
+	goto err_thread_start;
+    }
+
+    // Register with the locator.
+    err = L4VM_server_register_location( UUID_IVMblock, server->server_tid );
+    if( err == -ENODEV )
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+    else if( err )
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+
+    printk( KERN_INFO PREFIX "L4VMblock driver initialized.\n" );
+
+    return 0;
+
+err_thread_start:
+    // TODO: free the interrupt handler
+err_request_irq:
+    L4VM_fpage_dealloc( &server->server_alloc_info );
+err_shared_alloc:
+    return err;
+}
+
+static void __exit
+L4VMblock_server_exit_module( void )
+{
+#if 0
+    L4VMblock_server_t *server = &L4VMblock_server;
+    if( !L4_IsNilThread(server->server_tid) )
+    {
+	// TODO: free the interrupt handler
+	l4lx_thread_shutdown( server->server_tid );
+	server->server_tid = L4_nilthread;
+    }
+#endif
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@bothan.net>" );
+MODULE_DESCRIPTION( "L4Linux block driver server" );
+MODULE_LICENSE( "Proprietary, owned by Joshua LeVasseur" );
+MODULE_SUPPORTED_DEVICE( "L4 VM block" );
+
+#if defined(MODULE)
+module_init( L4VMblock_server_init_module );
+module_exit( L4VMblock_server_exit_module );
+#else
+void __init l4vm_block_init( void ) { L4VMblock_server_init_module(); }
+#endif
diff -Naur linux-2.6.9.src/afterburn/drivers/block/server.h linux-2.6.9/afterburn/drivers/block/server.h
--- linux-2.6.9.src/afterburn/drivers/block/server.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/block/server.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,145 @@
+/*********************************************************************
+ *                
+ * Copyright (C) 2004 Joshua LeVasseur
+ *
+ * File path:	linuxblock/L4VMblock_server.h
+ * Description:	Declarations for the Linux block driver server.
+ *
+ * Proprietary!  DO NOT DISTRIBUTE!
+ *
+ * $Id: server.h,v 1.1 2006/09/21 09:28:35 joshua Exp $
+ *                
+ ********************************************************************/
+#ifndef __linuxblock__L4VMblock_server_h__
+#define __linuxblock__L4VMblock_server_h__
+
+#include <glue/thread.h>
+#include <glue/bottomhalf.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+#include <glue/vmserver.h>
+#include <glue/wedge.h>
+
+#include "L4VMblock_idl_server.h"
+#include "L4VMblock_idl_reply.h"
+#include "block.h"
+
+#define L4VMBLOCK_IRQ_BOTTOM_HALF_CMD	(1)
+#define L4VMBLOCK_IRQ_TOP_HALF_CMD	(2)
+#define L4VMBLOCK_IRQ_DISPATCH		(4)
+
+#define L4VMBLOCK_MAX_DEVICES		(16)
+#define L4VMBLOCK_MAX_CLIENTS		(8)
+
+#define L4VMBLOCK_INVALID_HANDLE	(~0UL)
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+#define L4VMBLOCK_DO_IRQ_DISPATCH
+#endif
+
+typedef union
+{
+    struct {
+	IVMblock_devid_t devid;
+    } probe;
+
+    struct {
+	IVMblock_handle_t client_handle;
+	IVMblock_devid_t devid;
+	unsigned rw;
+    } attach;
+
+    struct {
+	IVMblock_handle_t handle;
+    } detach;
+
+    struct {
+	IVMblock_handle_t handle;
+    } reattach;
+
+} L4VM_server_cmd_params_t;
+
+
+typedef struct
+{
+    IVMblock_handle_t handle;
+    IVMblock_client_shared_t *client_shared;
+    L4VM_alligned_alloc_t client_alloc_info;
+
+    L4VMblock_ring_t ring_info;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+    struct buffer_head bh_ring_storage[ IVMblock_descriptor_ring_size ];
+    struct buffer_head *bh_ring[ IVMblock_descriptor_ring_size ];
+#else
+    struct bio *bio_ring[ IVMblock_descriptor_ring_size ];
+#endif
+
+    L4VM_client_space_info_t *client_space;
+} L4VMblock_client_info_t;
+
+
+typedef struct
+{
+    IVMblock_handle_t handle;
+    L4VMblock_client_info_t *client;
+    struct block_device *blkdev;
+    L4_Word_t block_size;
+    kdev_t kdev;
+} L4VMblock_conn_info_t;
+
+
+typedef struct L4VMblock_server
+{
+    L4_ThreadId_t server_tid;
+
+    L4VM_irq_t irq;
+    struct tq_struct bottom_half_task;
+#if !defined(L4VMBLOCK_DO_IRQ_DISPATCH)
+    struct tq_struct dispatch_task;
+#endif
+
+    IVMblock_server_shared_t *server_info;
+    L4VM_alligned_alloc_t server_alloc_info;
+
+    L4VM_server_cmd_ring_t top_half_cmds;
+    L4VM_server_cmd_params_t top_half_params[L4VM_SERVER_CMD_RING_LEN];
+    L4VM_server_cmd_ring_t bottom_half_cmds;
+    L4VM_server_cmd_params_t bottom_half_params[L4VM_SERVER_CMD_RING_LEN];
+
+    L4VMblock_conn_info_t connections[L4VMBLOCK_MAX_DEVICES];
+    L4VMblock_client_info_t clients[L4VMBLOCK_MAX_CLIENTS];
+
+    spinlock_t ring_lock;
+
+} L4VMblock_server_t;
+
+
+extern inline L4VMblock_conn_info_t * L4VMblock_conn_lookup( 
+	L4VMblock_server_t *server, IVMblock_handle_t handle )
+{
+    if( (handle < L4VMBLOCK_MAX_DEVICES) && 
+	    (server->connections[handle].handle == handle) )
+	return &server->connections[handle];
+    return NULL;
+}
+
+extern inline void L4VMblock_conn_release( L4VMblock_conn_info_t *conn )
+{
+    conn->handle = L4VMBLOCK_INVALID_HANDLE;
+}
+
+extern inline L4VMblock_client_info_t * L4VMblock_client_lookup(
+	L4VMblock_server_t *server, IVMblock_handle_t handle )
+{
+    if( (handle < L4VMBLOCK_MAX_CLIENTS) && 
+	    (server->clients[handle].handle == handle) )
+	return &server->clients[handle];
+    return NULL;
+}
+
+extern inline void L4VMblock_client_release( L4VMblock_client_info_t *client )
+{
+    client->handle = L4VMBLOCK_INVALID_HANDLE;
+}
+
+#endif	/* __linuxblock__L4VMblock_h__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/bottomhalf.h linux-2.6.9/afterburn/drivers/glue/bottomhalf.h
--- linux-2.6.9.src/afterburn/drivers/glue/bottomhalf.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/bottomhalf.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,13 @@
+#ifndef __LINUXGLUE__BOTTOMHALF_H__
+#define __LINUXGLUE__BOTTOMHALF_H__
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,5,41)
+# include <linux/tqueue.h>
+#else
+# include <linux/workqueue.h>
+# define INIT_TQUEUE	INIT_WORK
+# define schedule_task	schedule_work
+# define tq_struct	work_struct
+#endif
+
+#endif	/* __LINUXGLUE__BOTTOMHALF_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/glue.c linux-2.6.9/afterburn/drivers/glue/glue.c
--- linux-2.6.9.src/afterburn/drivers/glue/glue.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/glue.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,89 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/glue.c
+ * Description:	Module declarations.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+
+#include "wedge.h"
+
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Joshua LeVasseur <jtl@ira.uka.de>");
+MODULE_DESCRIPTION("Generic support for modules that wish to interact with the L4Ka wedge.");
+MODULE_VERSION("yoda");
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate sybmol resolution for
+// this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module"))) 
+burn_wedge_header_t burn_wedge_header = { 
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
+static int __init glue_init( void )
+{
+    printk( KERN_INFO "L4Ka glue module.\n" );
+
+    printk( KERN_INFO "Wedge configuration:\n" );
+    printk( KERN_INFO "  L4 priority : %lu\n", resourcemon_shared.prio );
+    printk( KERN_INFO "  TID space start: 0x%p\n", 
+	    (void *)resourcemon_shared.thread_space_start );
+    printk( KERN_INFO "  TID space len  : %lu\n", 
+	    resourcemon_shared.thread_space_len );
+    printk( KERN_INFO "  UTCB at 0x%p, size %lu\n",
+	    (void *)L4_Address(resourcemon_shared.utcb_fpage),
+	    L4_Size(resourcemon_shared.utcb_fpage) );
+    printk( KERN_INFO "  KIP at 0x%p, size %lu\n",
+	    (void *)L4_Address(resourcemon_shared.kip_fpage),
+	    L4_Size(resourcemon_shared.kip_fpage) );
+    printk( KERN_INFO "  Allocated memory: 0x%p\n", 
+	    (void *)resourcemon_shared.phys_size );
+    printk( KERN_INFO "  End of physical memory: 0x%p\n", 
+	    (void *)resourcemon_shared.phys_end );
+    printk( KERN_INFO "  Main TID    : 0x%p\n",
+	    (void *)get_vcpu()->main_gtid.raw );
+    printk( KERN_INFO "  IRQ TID     : 0x%p\n",
+	    (void *)get_vcpu()->irq_gtid.raw );
+    printk( KERN_INFO "  Monitor TID : 0x%p\n",
+	    (void *)get_vcpu()->monitor_gtid.raw);
+
+    l4ka_wedge_declare_pdir_master( init_mm.pgd );
+
+    return 0;
+}
+
+static void __exit glue_exit( void )
+{
+}
+
+module_init( glue_init );
+module_exit( glue_exit );
+
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/Makefile linux-2.6.9/afterburn/drivers/glue/Makefile
--- linux-2.6.9.src/afterburn/drivers/glue/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/Makefile	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,7 @@
+include $(srctree)/afterburn/drivers/Makesupport
+
+
+obj-$(CONFIG_AFTERBURN_DRIVERS) += l4ka_glue.o
+l4ka_glue-y := glue.o thread.o vmmemory.o vmserver.o
+
+$(addprefix $(obj)/,$(obj-m)): $(wedge_symbols)
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/thread.c linux-2.6.9/afterburn/drivers/glue/thread.c
--- linux-2.6.9.src/afterburn/drivers/glue/thread.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/thread.c	2007-07-16 09:58:09.000000000 +0200
@@ -0,0 +1,80 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/thread.c
+ * Description:	Wrappers for manipulating L4 threads within
+ * 		the L4Linux kernel.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/kdebug.h>
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+
+#include "thread.h"
+#include "wedge.h"
+
+#define STACK_SIZE_PAGE_SHIFT	2
+#define STACK_SIZE		(PAGE_SIZE << STACK_SIZE_PAGE_SHIFT)
+
+EXPORT_SYMBOL(L4VM_thread_create);
+EXPORT_SYMBOL(L4VM_thread_delete);
+
+L4_ThreadId_t 
+L4VM_thread_create( unsigned int gfp_mask, 
+	void (*thread_func)(void *), int prio, int cpu,
+	void *tlocal_data, unsigned tlocal_size )
+{
+    L4_ThreadId_t tid;
+    L4_Word_t stack;
+
+    if( !thread_func )
+	return L4_nilthread;
+
+    // Allocate a stack.
+    if( tlocal_size > STACK_SIZE/2 )
+	return L4_nilthread;
+    stack = __get_free_pages( gfp_mask, STACK_SIZE_PAGE_SHIFT );
+    if( stack == 0 )
+	return L4_nilthread;
+
+    tid = l4ka_wedge_thread_create( stack, STACK_SIZE, prio,
+	    thread_func, tlocal_data, tlocal_size );
+    if( L4_IsNilThread(tid) ) {
+	free_pages( stack, STACK_SIZE_PAGE_SHIFT );
+	L4_KDB_Enter("L4VM_thread_create: BUG");
+    }
+
+    return tid;
+}
+
+void
+L4VM_thread_delete( L4_ThreadId_t tid )
+{
+    l4ka_wedge_thread_delete( tid );
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/thread.c~ linux-2.6.9/afterburn/drivers/glue/thread.c~
--- linux-2.6.9.src/afterburn/drivers/glue/thread.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/thread.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,80 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/thread.c
+ * Description:	Wrappers for manipulating L4 threads within
+ * 		the L4Linux kernel.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/kdebug.h>
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+
+#include "thread.h"
+#include "wedge.h"
+
+#define STACK_SIZE_PAGE_SHIFT	2
+#define STACK_SIZE		(PAGE_SIZE << STACK_SIZE_PAGE_SHIFT)
+
+EXPORT_SYMBOL(L4VM_thread_create);
+EXPORT_SYMBOL(L4VM_thread_delete);
+
+L4_ThreadId_t 
+L4VM_thread_create( unsigned int gfp_mask, 
+	void (*thread_func)(void *), int prio, int cpu,
+	void *tlocal_data, unsigned tlocal_size )
+{
+    L4_ThreadId_t tid;
+    L4_Word_t stack;
+
+    if( !thread_func )
+	return L4_nilthread;
+
+    // Allocate a stack.
+    if( tlocal_size > STACK_SIZE/2 )
+	return L4_nilthread;
+    stack = __get_free_pages( gfp_mask, STACK_SIZE_PAGE_SHIFT );
+    if( stack == 0 )
+	return L4_nilthread;
+
+    tid = l4ka_wedge_thread_create( stack, STACK_SIZE, prio,
+	    thread_func, tlocal_data, tlocal_size );
+    if( L4_IsNilThread(tid) ) {
+	free_pages( stack, STACK_SIZE_PAGE_SHIFT );
+	L4_KDB_Enter("oops");
+    }
+
+    return tid;
+}
+
+void
+L4VM_thread_delete( L4_ThreadId_t tid )
+{
+    l4ka_wedge_thread_delete( tid );
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/thread.h linux-2.6.9/afterburn/drivers/glue/thread.h
--- linux-2.6.9.src/afterburn/drivers/glue/thread.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/thread.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,43 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/thread.h
+ * Description:	Wrappers for manipulating L4 threads within
+ * 		the L4Linux kernel.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__GLUE__THREAD_H__
+#define __L4KA_DRIVERS__GLUE__THREAD_H__
+
+#include <l4/types.h>
+#include <l4/thread.h>
+
+extern L4_ThreadId_t L4VM_thread_create( unsigned int gfp_mask, 
+	void (*thread_func)(void *), int prio, int cpu, 
+	void *tlocal_data, unsigned tlocal_size );
+
+extern void L4VM_thread_delete( L4_ThreadId_t tid );
+
+#endif	/* __L4KA_DRIVERS__GLUE__THREAD_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/vmirq.h linux-2.6.9/afterburn/drivers/glue/vmirq.h
--- linux-2.6.9.src/afterburn/drivers/glue/vmirq.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/vmirq.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,126 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/vmirq.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__GLUE__VMIRQ_H__
+#define __L4KA_DRIVERS__GLUE__VMIRQ_H__
+
+#include <l4/types.h>
+
+#include <linux/version.h>
+#include <asm/system.h>
+
+#include "l4linux_idl_client.h"
+#include "wedge.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+typedef void irqreturn_t;
+#define IRQ_HANDLED
+#endif
+
+extern inline L4_ThreadId_t L4VM_linux_irq_thread( L4_Word_t l4_cpu )
+{
+    return get_vcpu()->irq_gtid;
+}
+
+extern inline L4_ThreadId_t L4VM_linux_main_thread( L4_Word_t l4_cpu )
+{
+    return get_vcpu()->main_gtid;
+}
+
+typedef struct L4VM_irq
+{
+    L4_ThreadId_t my_irq_tid;
+    volatile int pending;
+    volatile L4_Word_t status;
+    volatile L4_Word_t mask;
+} L4VM_irq_t;
+
+extern inline void L4VM_irq_init( L4VM_irq_t *irq )
+{
+    irq->my_irq_tid = L4VM_linux_irq_thread( L4_ProcessorNo() );
+    irq->pending = 0;
+    irq->status = 0;
+    irq->mask = ~0UL;
+}
+
+extern inline int L4VM_irq_deliver_prepare( 
+	L4VM_irq_t *irq,
+	L4_Word_t linux_irq_no,
+	L4_Word_t irq_flags )
+{
+    irq->status |= irq_flags;
+
+    if( ((irq->status & irq->mask) != 0) && !irq->pending )
+    {
+	L4_MsgTag_t msgtag;
+
+	irq->pending = 1;
+
+	msgtag.raw = 0;
+	L4_Set_Label( &msgtag, linux_irq_no );
+	L4_Set_MsgTag( msgtag );
+
+	return 1;
+    }
+
+    return 0;
+}
+
+extern inline void L4VM_irq_deliver( 
+	L4VM_irq_t *irq,
+	L4_Word_t linux_irq_no,
+	L4_Word_t irq_flags )
+{
+    irq->status |= irq_flags;
+
+    if( ((irq->status & irq->mask) != 0) && !irq->pending )
+    {
+	CORBA_Environment ipc_env = idl4_default_environment;
+
+	irq->pending = 1;
+
+	ILinux_raise_irq( irq->my_irq_tid, linux_irq_no, &ipc_env );
+
+	if( ipc_env._major != CORBA_NO_EXCEPTION )
+	    CORBA_exception_free(&ipc_env);
+    }
+}
+
+extern inline L4_Word_t L4VM_irq_status_reset( volatile L4_Word_t *status )
+{
+    L4_Word_t events;
+
+    do {
+	events = *status;
+    } while( cmpxchg(status, events, 0) != events );
+
+    return events;
+}
+
+#endif	/* __L4KA_DRIVERS__GLUE__VMIRQ_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/vmmemory.c linux-2.6.9/afterburn/drivers/glue/vmmemory.c
--- linux-2.6.9.src/afterburn/drivers/glue/vmmemory.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/vmmemory.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,288 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/vmmemory.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/types.h>
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+
+#include <asm/io.h>
+
+#include "wedge.h"
+#include "vmmemory.h"
+#include "resourcemon_idl_client.h"
+
+EXPORT_SYMBOL(L4VM_fpage_alloc);
+EXPORT_SYMBOL(L4VM_fpage_vmarea_get);
+EXPORT_SYMBOL(L4VM_fpage_vmarea_release);
+EXPORT_SYMBOL(L4VM_tid_to_space_id);
+EXPORT_SYMBOL(L4VM_get_client_space_info);
+EXPORT_SYMBOL(L4VM_get_space_info);
+EXPORT_SYMBOL(L4VM_get_client_dma_info);
+EXPORT_SYMBOL(L4VM_get_space_dma_info);
+EXPORT_SYMBOL(L4VM_page_in);
+
+
+int
+L4VM_fpage_alloc( L4_Word_t req_log2size, L4VM_alligned_alloc_t *info )
+{
+    L4_Word_t order;
+    
+    if( req_log2size < PAGE_SHIFT )
+	req_log2size = PAGE_SHIFT;
+    order = req_log2size - PAGE_SHIFT;
+
+    info->actual_log2size = req_log2size;
+    info->start = __get_free_pages( GFP_KERNEL, order );
+    if( info->start == 0 )
+	return -ENOMEM;
+
+    info->fpage = L4_FpageLog2( info->start, req_log2size );
+    if( L4_Address(info->fpage) == info->start )
+	return 0;
+
+    // Double the allocation size, so that we can align it.
+    free_pages( info->start, order );
+    order += 1;
+    info->actual_log2size += 1;
+    info->start = __get_free_pages( GFP_KERNEL, order );
+    if( info->start == 0 )
+	return -ENOMEM;
+
+    info->fpage = L4_FpageLog2( info->start + (1 << req_log2size), 
+	    req_log2size );
+    return 0;
+}
+
+int
+L4VM_fpage_vmarea_get( L4_Word_t req_log2size, L4VM_alligned_vmarea_t *info )
+{
+    L4_Word_t actual_log2size, size;
+
+    if( req_log2size < PAGE_SHIFT )
+	req_log2size = PAGE_SHIFT;
+    actual_log2size = req_log2size;
+    size = 1 << actual_log2size;
+
+    // We'd really like to call get_vm_area(), but it isn't exposed to
+    // kernel modules.
+    info->ioremap_addr = ioremap( 0 - size, size );
+    if( info->ioremap_addr == NULL )
+	return -ENOMEM;
+
+    // Try to create the fpage.
+    info->fpage = L4_FpageLog2( (L4_Word_t)info->ioremap_addr, req_log2size );
+    if( L4_Address(info->fpage) == (L4_Word_t)info->ioremap_addr )
+	return 0;
+
+    // Realign somewhere within the oversized region.
+    L4VM_fpage_vmarea_release( info );
+    actual_log2size++; // Double the size, so that we can align.
+    size = 1 << actual_log2size;
+
+    info->ioremap_addr = ioremap( 0 - size, size );
+    if( info->ioremap_addr == NULL )
+	return -ENOMEM;
+
+    info->fpage = L4_FpageLog2( 
+	    (L4_Word_t)info->ioremap_addr + (1 << req_log2size),
+	    req_log2size );
+    return 0;
+}
+
+void L4VM_fpage_vmarea_release( L4VM_alligned_vmarea_t *info )
+{
+    if( info->ioremap_addr )
+	iounmap( info->ioremap_addr );
+    info->ioremap_addr = NULL;
+}
+
+int L4VM_tid_to_space_id( L4_ThreadId_t tid, L4_Word_t *space_id )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+               
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IResourcemon_tid_to_space_id( 
+	    resourcemon_shared.cpu[L4_ProcessorNo()].resourcemon_tid, 
+	    &tid, space_id, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -EIO;
+    }
+
+    return 0;
+}
+
+
+static L4VM_client_space_info_t *client_list = NULL;
+static spinlock_t client_space_lock = SPIN_LOCK_UNLOCKED;
+
+L4VM_client_space_info_t * L4VM_get_client_space_info( L4_ThreadId_t tid )
+{
+    L4_Word_t space_id;
+
+    if( L4VM_tid_to_space_id(tid, &space_id) )
+	return NULL;
+
+    return L4VM_get_space_info( space_id );
+}
+
+L4VM_client_space_info_t * L4VM_get_space_info( L4_Word_t space_id )
+{
+    /* This function mantains a central list of ioremaps for each
+     * client's address space.
+     */
+
+    L4VM_client_space_info_t *c;
+    L4_Word_t bus_start, bus_size;
+
+    // Search for a preexisting ioremap for this particular client.
+    spin_lock( &client_space_lock );
+    for( c = client_list; c; c = c->next )
+	if( c->space_id == space_id )
+	    break;
+    if( c )
+    {
+	c->refcnt++;
+	spin_unlock( &client_space_lock );
+	return c;
+    }
+
+    // Get info about the client's machine address space.
+    if( L4VM_get_space_dma_info(space_id, &bus_start, &bus_size) )
+    {
+	spin_unlock( &client_space_lock );
+	return NULL;
+    }
+
+
+    // Safe to proceed, so allocate a new structure and add to the list.
+    c = (L4VM_client_space_info_t *)
+	kmalloc( sizeof(L4VM_client_space_info_t), GFP_KERNEL );
+    if( c == NULL )
+    {
+	spin_unlock( &client_space_lock );
+	return NULL;
+    }
+
+    c->space_id = space_id;
+    c->bus_start = bus_start;
+    c->bus_size = bus_size;
+    c->refcnt = 1;
+    c->next = client_list;
+    client_list = c;
+
+    spin_unlock( &client_space_lock );
+
+    return c;
+}
+
+
+int L4VM_get_client_dma_info(
+	L4_ThreadId_t client_tid, 
+	L4_Word_t *phys_start,
+	L4_Word_t *phys_size )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IResourcemon_get_client_phys_range( 
+	    resourcemon_shared.cpu[L4_ProcessorNo()].resourcemon_tid,
+	    &client_tid, phys_start, phys_size, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free( &ipc_env );
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free( &ipc_env );
+	return -EIO;
+    }
+
+    return 0;
+}
+
+int L4VM_get_space_dma_info(
+	L4_Word_t space_id,
+	L4_Word_t *phys_start,
+	L4_Word_t *phys_size )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IResourcemon_get_space_phys_range( 
+	    resourcemon_shared.cpu[L4_ProcessorNo()].resourcemon_tid,
+	    space_id, phys_start, phys_size, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free( &ipc_env );
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free( &ipc_env );
+	return -EIO;
+    }
+
+    return 0;
+}
+
+
+void L4VM_page_in( L4_Fpage_t fp, L4_Word_t page_size )
+{
+    volatile L4_Word_t *page = (volatile L4_Word_t *)L4_Address(fp);
+    L4_Word_t *end = (L4_Word_t *)(L4_Address(fp) + L4_Size(fp));
+
+    while( page < end ) {
+	*page;
+	page += page_size;
+    }
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/vmmemory.h linux-2.6.9/afterburn/drivers/glue/vmmemory.h
--- linux-2.6.9.src/afterburn/drivers/glue/vmmemory.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/vmmemory.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,81 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/vmmemory.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__GLUE__VMMEMORY_H__
+#define __L4KA_DRIVERS__GLUE__VMMEMORY_H__
+
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+
+typedef struct 
+{
+    L4_Word_t start;
+    L4_Word_t actual_log2size;
+    L4_Fpage_t fpage;
+} L4VM_alligned_alloc_t;
+
+extern int L4VM_fpage_alloc( L4_Word_t req_log2size, L4VM_alligned_alloc_t *);
+
+extern inline void L4VM_fpage_dealloc( L4VM_alligned_alloc_t *info )
+{
+    L4_Word_t order = info->actual_log2size - PAGE_SHIFT;
+    free_pages( info->start, order );
+}
+
+typedef struct 
+{
+    void *ioremap_addr;
+    L4_Fpage_t fpage;
+} L4VM_alligned_vmarea_t;
+
+extern int L4VM_fpage_vmarea_get( L4_Word_t req_log2size, L4VM_alligned_vmarea_t * );
+extern void L4VM_fpage_vmarea_release( L4VM_alligned_vmarea_t * );
+
+typedef struct L4VM_client_space_info
+{
+    L4_Word_t space_id;
+    L4_Word_t bus_start, bus_size;
+    L4_Word_t refcnt;
+    struct L4VM_client_space_info *next;
+} L4VM_client_space_info_t;
+
+extern int L4VM_tid_to_space_id(
+	L4_ThreadId_t tid,
+	L4_Word_t *space_id );
+
+extern L4VM_client_space_info_t * L4VM_get_space_info( L4_Word_t space_id );
+extern L4VM_client_space_info_t * L4VM_get_client_space_info( L4_ThreadId_t );
+
+
+extern int L4VM_get_space_dma_info( L4_Word_t space_id, L4_Word_t *phys_start, L4_Word_t *phys_size );
+extern int L4VM_get_client_dma_info( L4_ThreadId_t client_tid, L4_Word_t *phys_start, L4_Word_t *phys_size );
+
+extern void L4VM_page_in( L4_Fpage_t, L4_Word_t page_size );
+
+#endif	/* __L4KA_DRIVERS__GLUE__VMMEMORY_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/vmserver.c linux-2.6.9/afterburn/drivers/glue/vmserver.c
--- linux-2.6.9.src/afterburn/drivers/glue/vmserver.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/vmserver.c	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,186 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/vmmemory.c
+ * Description:	Routines common for L4 server threads in the Linux kernel.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+
+#include "resourcemon_idl_client.h"
+#include "vmserver.h"
+#include "wedge.h"
+
+EXPORT_SYMBOL(L4VM_server_cmd_allocate);
+EXPORT_SYMBOL(L4VM_server_cmd_dispatcher);
+EXPORT_SYMBOL(L4VM_server_cmd_init);
+EXPORT_SYMBOL(L4VM_server_register_location);
+EXPORT_SYMBOL(L4VM_server_locate);
+
+
+L4_Word16_t L4VM_server_cmd_allocate(
+	L4VM_server_cmd_ring_t *ring,
+	L4_Word_t irq_flags,
+	L4VM_irq_t *irq,
+	L4_Word_t linux_irq_no )
+{
+    L4_Word16_t cmd_idx = ring->next_free;
+    L4VM_server_cmd_t *cmd = &ring->cmds[ cmd_idx ];
+
+    // Wait until the command is available for use.  We wait for an
+    // IPC from *any* thread, so confirm that we actually have
+    // a free cmd slot when we wake.
+    while( cmd->handler )
+    {
+	L4_ThreadId_t to_tid, from_tid;
+
+	// 1. Tell the handler that it needs to wake the server thread.
+	ring->wake_server = 1;
+
+	// 2. Prepare for IRQ delivery.
+	if( L4VM_irq_deliver_prepare(irq, linux_irq_no, irq_flags) )
+	    to_tid = irq->my_irq_tid;
+	else
+	    to_tid = L4_nilthread;
+
+	// 3. Deliver the IRQ, and wait for reactivation.
+	L4_ReplyWait_Timeout( to_tid, L4_Never, &from_tid );
+	// Ignore IPC errors and IPC from unknown clients.
+	// We restart the loop, which will correct for errors.
+    }
+
+    // Move to the next command.
+    ring->next_free = (ring->next_free + 1) % L4VM_SERVER_CMD_RING_LEN;
+
+    return cmd_idx;
+}
+
+void L4VM_server_cmd_dispatcher( 
+	L4VM_server_cmd_ring_t *cmd_ring,
+	void *data,
+	L4_ThreadId_t server_tid)
+{
+    while( 1 )
+    {
+	L4VM_server_cmd_t *cmd = &cmd_ring->cmds[ cmd_ring->next_cmd ];
+	if( !cmd->handler )
+	    break;
+
+	// Dispatch the command entry.
+	cmd->handler( cmd, data );
+
+	// Release the command entry and get the next command.
+	cmd->handler = NULL;
+	cmd_ring->next_cmd = (cmd_ring->next_cmd + 1) % L4VM_SERVER_CMD_RING_LEN;
+    }
+
+    if( cmd_ring->wake_server )
+    {
+	L4_MsgTag_t msgtag;
+	
+	cmd_ring->wake_server = 0;
+
+	msgtag.raw = 0;
+	L4_Set_MsgTag( msgtag );
+	L4_Reply( server_tid );
+    }
+}
+
+void
+L4VM_server_cmd_init( L4VM_server_cmd_ring_t *cmd_ring )
+{
+    L4_Word16_t idx;
+
+    cmd_ring->next_free = 0;
+    cmd_ring->next_cmd = 0;
+    cmd_ring->wake_server = 0;
+
+    for( idx = 0; idx < L4VM_SERVER_CMD_RING_LEN; idx++ )
+    {
+	cmd_ring->cmds[idx].handler = NULL;
+	cmd_ring->cmds[idx].reply_to_tid = L4_nilthread;
+    }
+}
+
+
+int L4VM_server_register_location( 
+	guid_t guid,
+	L4_ThreadId_t tid )
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IResourcemon_register_interface( 
+	    resourcemon_shared.cpu[L4_ProcessorNo()].locator_tid, 
+	    guid, &tid, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -EIO;
+    }
+
+    return 0;
+}
+
+int L4VM_server_locate( 
+	guid_t guid,
+	L4_ThreadId_t *server_tid)
+{
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+               
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IResourcemon_query_interface( 
+	    resourcemon_shared.cpu[L4_ProcessorNo()].locator_tid, 
+	    guid, server_tid, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -EIO;
+    }
+
+    return 0;
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/vmserver.h linux-2.6.9/afterburn/drivers/glue/vmserver.h
--- linux-2.6.9.src/afterburn/drivers/glue/vmserver.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/vmserver.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,88 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/glue/vmmemory.h
+ * Description:	Common data structures for L4 servers implemented in
+ * 		the Linux kernel.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__GLUE__VMSERVER_H__
+#define __L4KA_DRIVERS__GLUE__VMSERVER_H__
+
+#include <l4/types.h>
+
+#include "vmirq.h"
+#include "resourcemon_idl_client.h"
+
+/* this is required to get things compiled with gcc 3.3 */
+#define IDL4_PUBLISH_ATTR __attribute__ ((regparm (2))) __attribute__ ((noreturn))
+
+
+struct L4VM_server_cmd;
+typedef struct L4VM_server_cmd L4VM_server_cmd_t;
+
+typedef void (*L4VM_server_cmd_handler_t)( L4VM_server_cmd_t *, void *data );
+
+struct L4VM_server_cmd
+{
+    volatile L4VM_server_cmd_handler_t handler;
+    L4_ThreadId_t reply_to_tid;
+    void *data;
+};
+
+#define L4VM_SERVER_CMD_RING_LEN	8
+
+typedef struct L4VM_server_cmd_ring
+{
+    L4_Word16_t next_free;
+    L4_Word16_t next_cmd;
+    volatile int wake_server;
+    L4_ThreadId_t server_tid;
+    L4VM_server_cmd_t cmds[L4VM_SERVER_CMD_RING_LEN];
+} L4VM_server_cmd_ring_t;
+
+
+extern L4_Word16_t L4VM_server_cmd_allocate(
+	L4VM_server_cmd_ring_t *ring,
+	L4_Word_t irq_flags,
+	L4VM_irq_t *irq,
+	L4_Word_t linux_irq_no );
+
+extern void L4VM_server_cmd_dispatcher( 
+	L4VM_server_cmd_ring_t *cmd_ring,
+	void *data,
+	L4_ThreadId_t server_tid);
+
+extern void L4VM_server_cmd_init( L4VM_server_cmd_ring_t *cmd_ring );
+
+extern int L4VM_server_register_location( 
+	guid_t guid,
+	L4_ThreadId_t tid );
+
+extern int L4VM_server_locate(
+	guid_t guid,
+	L4_ThreadId_t *server_tid);
+
+#endif	/* __L4KA_DRIVERS__GLUE__VMSERVER_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/glue/wedge.h linux-2.6.9/afterburn/drivers/glue/wedge.h
--- linux-2.6.9.src/afterburn/drivers/glue/wedge.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/glue/wedge.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,104 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-driver-reuse/kernel/glue/wedge.h
+ * Description:	Declarations for the functions and data types that
+ * 		we dynamically link against in the L4Ka wedge.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVER_REUSE__KERNEL__GLUE__WEDGE_H__
+#define __L4KA_DRIVER_REUSE__KERNEL__GLUE__WEDGE_H__
+
+#include <l4/types.h>
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+
+#include "resourcemon_idl_client.h"
+
+#define BURN_WEDGE_MODULE_INTERFACE_VERSION	1
+
+
+typedef struct {
+    L4_Word_t version;
+} burn_wedge_header_t;
+
+typedef int (*dspace_pfault_handler_t)(L4_Word_t fault_addr, L4_Word_t *ip, L4_Fpage_t *map_fp, void *data);
+
+typedef struct
+{
+    L4_ThreadId_t monitor_ltid;		// 0
+    L4_ThreadId_t monitor_gtid;		// 4
+    L4_ThreadId_t irq_ltid;		// 8
+    L4_ThreadId_t irq_gtid;		// 12
+    L4_ThreadId_t main_ltid;		// 16
+    L4_ThreadId_t main_gtid;		// 20
+
+    volatile int dispatch_ipc;		// 24
+    volatile L4_Word_t dispatch_ipc_nr;	// 28
+
+    struct {
+	L4_Word_t flags;		// 32
+    } cpu;
+} vcpu_t;
+
+static inline __attribute__((const)) vcpu_t *get_vcpu( void )
+{
+#if 0
+    vcpu_t *vcpu = (vcpu_t *)L4_UserDefinedHandled();
+    return vcpu;
+#else
+    extern vcpu_t vcpu;
+    return &vcpu;
+#endif
+}
+
+extern volatile IResourcemon_shared_t resourcemon_shared;
+
+
+extern L4_ThreadId_t l4ka_wedge_thread_create( L4_Word_t stack_bottom,
+	L4_Word_t stack_size, L4_Word_t prio, 
+	void (*thread_func)(void *), void *tlocal_data, unsigned tlocal_size );
+extern void l4ka_wedge_thread_delete( L4_ThreadId_t tid );
+
+extern L4_Word_t l4ka_wedge_get_irq_prio( void );
+extern void l4ka_wedge_raise_irq( L4_Word_t irq );
+extern void l4ka_wedge_add_virtual_irq( L4_Word_t irq );
+
+extern L4_Word_t l4ka_wedge_phys_to_bus( L4_Word_t paddr );
+extern L4_Word_t l4ka_wedge_bus_to_phys( L4_Word_t bus_addr );
+
+
+static inline int vcpu_in_dispatch_ipc( void )
+	{ return get_vcpu()->dispatch_ipc; }
+
+static inline int vcpu_interrupts_enabled( void )
+	{ return (get_vcpu()->cpu.flags >> 9) & 1; }
+
+extern void l4ka_wedge_add_dspace_handler( dspace_pfault_handler_t handler, void * data );
+
+extern void l4ka_wedge_declare_pdir_master( pgd_t *pgd );
+
+#endif /* __L4KA_DRIVER_REUSE__KERNEL__GLUE__WEDGE_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/Kconfig linux-2.6.9/afterburn/drivers/Kconfig
--- linux-2.6.9.src/afterburn/drivers/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/Kconfig	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,54 @@
+
+menu "Afterburn Driver Reuse"
+
+config AFTERBURN_DRIVERS
+	tristate "Support Afterburn driver reuse?"
+	depends on AFTERBURN
+	default n
+
+config AFTERBURN_DRIVERS_NET_SERVER
+	tristate "Build the network server?"
+	depends on AFTERBURN_DRIVERS
+	default m
+
+config AFTERBURN_DRIVERS_NET_DP83820
+	bool "Use DP83820 interface, rather than the client interface?"
+	depends on AFTERBURN_DRIVERS_NET_SERVER
+	default y
+
+config AFTERBURN_DRIVERS_NET_CLIENT
+	tristate "Build the network client?"
+	depends on !AFTERBURN_DRIVERS_NET_SERVER && AFTERBURN_DRIVERS
+	default n
+
+config AFTERBURN_DRIVERS_NET_OPTIMIZE
+	bool "  Optimize (ommit networking debug code)?"
+	depends on AFTERBURN_DRIVERS_NET_SERVER || AFTERBURN_DRIVERS_NET_CLIENT
+	default n
+
+config AFTERBURN_DRIVERS_BLOCK_SERVER
+	tristate "Build the block server?"
+	depends on AFTERBURN_DRIVERS
+	default m
+
+config AFTERBURN_DRIVERS_BLOCK_CLIENT
+	tristate "Build the block client?"
+	depends on AFTERBURN_DRIVERS && !AFTERBURN_DRIVERS_BLOCK_SERVER
+	default n
+
+config AFTERBURN_DRIVERS_BLOCK_OPTIMIZE
+	bool "  Optimize (ommit block debug code)?"
+	depends on AFTERBURN_DRIVERS_BLOCK_SERVER || AFTERBURN_DRIVERS_BLOCK_CLIENT
+	default n
+
+config AFTERBURN_DRIVERS_PCI_SERVER
+	tristate "Build the PCI server?"
+	depends on AFTERBURN_DRIVERS
+	default n
+
+config AFTERBURN_WEDGE_NAME
+	string "Afterburn wedge name?"
+	depends on AFTERBURN_DRIVERS
+
+endmenu
+
diff -Naur linux-2.6.9.src/afterburn/drivers/lanaddress/lanaddress.c linux-2.6.9/afterburn/drivers/lanaddress/lanaddress.c
--- linux-2.6.9.src/afterburn/drivers/lanaddress/lanaddress.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/lanaddress/lanaddress.c	2007-07-13 13:39:30.000000000 +0200
@@ -0,0 +1,237 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/lanaddress/lanaddress.c
+ * Description:	Implements a simple L4 server that distributes virtual
+ * 		LAN addresses to other L4 clients.  It derives the virtual
+ * 		LAN address from the physical, thus ensuring that
+ * 		virtual machines on different hosts have unique virtual
+ * 		LAN addresses.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/types.h>
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/netdevice.h>
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+
+#include <glue/wedge.h>
+#include <glue/thread.h>
+#include <glue/vmserver.h>
+
+#include "lanaddress_idl_server.h"
+
+#define PREFIX "lanaddress: "
+
+/***************************************************************************
+ *
+ * Data structures.
+ *
+ ***************************************************************************/
+
+static L4_ThreadId_t lanaddress_tid = L4_nilthread;
+
+static lanaddress_t lanaddress_base;
+static L4_Word16_t lanaddress_handle = 0;
+static int lanaddress_valid = 0;
+
+extern inline void
+lanaddress_print_addr( lanaddress_t *lanaddr )
+{
+    int i;
+    for( i = 0; i < ETH_ALEN; i++ )
+    {
+	if(i) printk(":");
+	printk( "%02x", lanaddr->raw[i] );
+    }
+}
+
+
+/***************************************************************************
+ *
+ * Service functions.
+ *
+ ***************************************************************************/
+
+IDL4_INLINE void ILanAddress_generate_lan_address_implementation(
+	CORBA_Object _caller,
+	lanaddress_t *lanaddress,
+	idl4_server_environment *_env)
+{
+    if( !lanaddress_valid )
+    {
+	CORBA_exception_set( _env, ex_ILanAddress_insufficient_resources, NULL);
+	return;
+    }
+
+    // Create a new address, combined from the base, and the handle.
+    lanaddress_set_handle( &lanaddress_base, lanaddress_handle );
+    lanaddress->align4.lsb = lanaddress_base.align4.lsb;
+    lanaddress->align4.msb = lanaddress_base.align4.msb;
+
+    // Move to the next handle.
+    lanaddress_handle++;
+    if( lanaddress_handle == 0 )
+    {
+	// wrap around
+	lanaddress_valid = 0;
+    }
+}
+IDL4_PUBLISH_ILANADDRESS_GENERATE_LAN_ADDRESS(ILanAddress_generate_lan_address_implementation);
+
+
+void lanaddress_server_thread(void *p)
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    static void *ILanAddress_vtable[ILANADDRESS_DEFAULT_VTABLE_SIZE] = 
+	ILANADDRESS_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init( &msgbuf );
+
+    while( 1 )
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while( 1 )
+	{
+	    idl4_msgbuf_sync( &msgbuf );
+	    idl4_reply_and_wait( &partner, &msgtag, &msgbuf, &cnt );
+	    if( idl4_is_error( &msgtag ) )
+	     	break;
+
+	    idl4_process_request( &partner, &msgtag, &msgbuf, &cnt, 
+		    ILanAddress_vtable[
+		        idl4_get_function_id(&msgtag) & ILANADDRESS_FID_MASK] );
+	}
+    }
+}
+
+/***************************************************************************
+ *
+ * Module initialization and tear down.
+ *
+ ***************************************************************************/
+
+static void __init
+landaddress_generate_base_address( void )
+{
+    read_lock( &dev_base_lock );
+    {
+	struct net_device *dev;
+	for( dev = dev_base; dev; dev = dev->next )
+	{
+	    // See include/linux/if_arp.h for device type constants.  They
+	    // supposedly conform to ARP protocol hardware identifiers.
+	    if( (dev->type == ARPHRD_ETHER) && (dev->addr_len == ETH_ALEN) )
+	    {
+		int i;
+
+		// Reverse the hardware address, to make a slightly random
+		// LAN address, but one which we will always generate for
+		// this particular device/machine.
+		for( i = 0; i < ETH_ALEN; i++ )
+		    lanaddress_base.raw[i] = dev->dev_addr[ETH_ALEN-1-i];
+
+		lanaddress_set_handle( &lanaddress_base, 0 );
+		lanaddress_base.status.local = 1;
+		lanaddress_base.status.group = 0;
+
+		lanaddress_valid = 1;
+		break;
+	    }
+	}
+    }
+    read_unlock( &dev_base_lock );
+}
+
+static int __init
+lanaddress_init_module( void )
+{
+    int err;
+
+    landaddress_generate_base_address();
+
+    if( lanaddress_valid )
+    {
+	printk( KERN_INFO PREFIX "base address: " );
+	lanaddress_print_addr( &lanaddress_base );
+	printk( "\n" );
+    }
+    else
+	printk( KERN_ERR PREFIX "unable to generate a base address.\n" );
+
+    lanaddress_tid = L4VM_thread_create( GFP_KERNEL, lanaddress_server_thread,
+	    l4ka_wedge_get_irq_prio(), smp_processor_id(), NULL, 0 );
+    
+    err = L4VM_server_register_location( UUID_ILanAddress, lanaddress_tid );
+    if( err == -ENODEV )
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+    else if (err < 0 )
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+
+    printk( KERN_INFO PREFIX "initialized.\n" );
+
+    return 0;
+}
+
+static void __exit
+lanaddress_exit_module( void )
+{
+    if( !L4_IsNilThread(lanaddress_tid) )
+    {
+	L4VM_server_register_location( UUID_ILanAddress, L4_nilthread );
+	L4VM_thread_delete( lanaddress_tid );
+	lanaddress_tid = L4_nilthread;
+    }
+}
+
+module_init( lanaddress_init_module );
+module_exit( lanaddress_exit_module );
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "LAN address generator" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_VERSION( "yoda" );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate sybmol resolution for
+// this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/lanaddress/lanaddress.c~ linux-2.6.9/afterburn/drivers/lanaddress/lanaddress.c~
--- linux-2.6.9.src/afterburn/drivers/lanaddress/lanaddress.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/lanaddress/lanaddress.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,237 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/lanaddress/lanaddress.c
+ * Description:	Implements a simple L4 server that distributes virtual
+ * 		LAN addresses to other L4 clients.  It derives the virtual
+ * 		LAN address from the physical, thus ensuring that
+ * 		virtual machines on different hosts have unique virtual
+ * 		LAN addresses.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/types.h>
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/netdevice.h>
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+
+#include <glue/wedge.h>
+#include <glue/thread.h>
+#include <glue/vmserver.h>
+
+#include "lanaddress_idl_server.h"
+
+#define PREFIX "lanaddress: "
+
+/***************************************************************************
+ *
+ * Data structures.
+ *
+ ***************************************************************************/
+
+static L4_ThreadId_t lanaddress_tid = L4_nilthread;
+
+static lanaddress_t lanaddress_base;
+static L4_Word16_t lanaddress_handle = 0;
+static int lanaddress_valid = 0;
+
+extern inline void
+lanaddress_print_addr( lanaddress_t *lanaddr )
+{
+    int i;
+    for( i = 0; i < ETH_ALEN; i++ )
+    {
+	if(i) printk(":");
+	printk( "%02x", lanaddr->raw[i] );
+    }
+}
+
+
+/***************************************************************************
+ *
+ * Service functions.
+ *
+ ***************************************************************************/
+
+IDL4_INLINE void ILanAddress_generate_lan_address_implementation(
+	CORBA_Object _caller,
+	lanaddress_t *lanaddress,
+	idl4_server_environment *_env)
+{
+    if( !lanaddress_valid )
+    {
+	CORBA_exception_set( _env, ex_ILanAddress_insufficient_resources, NULL);
+	return;
+    }
+
+    // Create a new address, combined from the base, and the handle.
+    lanaddress_set_handle( &lanaddress_base, lanaddress_handle );
+    lanaddress->align4.lsb = lanaddress_base.align4.lsb;
+    lanaddress->align4.msb = lanaddress_base.align4.msb;
+
+    // Move to the next handle.
+    lanaddress_handle++;
+    if( lanaddress_handle == 0 )
+    {
+	// wrap around
+	lanaddress_valid = 0;
+    }
+}
+IDL4_PUBLISH_ILANADDRESS_GENERATE_LAN_ADDRESS(ILanAddress_generate_lan_address_implementation);
+
+
+void lanaddress_server_thread(void *p)
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    static void *ILanAddress_vtable[ILANADDRESS_DEFAULT_VTABLE_SIZE] = 
+	ILANADDRESS_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init( &msgbuf );
+
+    while( 1 )
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while( 1 )
+	{
+	    idl4_msgbuf_sync( &msgbuf );
+	    idl4_reply_and_wait( &partner, &msgtag, &msgbuf, &cnt );
+	    if( idl4_is_error( &msgtag ) )
+	     	break;
+
+	    idl4_process_request( &partner, &msgtag, &msgbuf, &cnt, 
+		    ILanAddress_vtable[
+		        idl4_get_function_id(&msgtag) & ILANADDRESS_FID_MASK] );
+	}
+    }
+}
+
+/***************************************************************************
+ *
+ * Module initialization and tear down.
+ *
+ ***************************************************************************/
+
+static void __init
+landaddress_generate_base_address( void )
+{
+    read_lock( &dev_base_lock );
+    {
+	struct net_device *dev;
+	for( dev = dev_base; dev; dev = dev->next )
+	{
+	    // See include/linux/if_arp.h for device type constants.  They
+	    // supposedly conform to ARP protocol hardware identifiers.
+	    if( (dev->type == ARPHRD_ETHER) && (dev->addr_len == ETH_ALEN) )
+	    {
+		int i;
+
+		// Reverse the hardware address, to make a slightly random
+		// LAN address, but one which we will always generate for
+		// this particular device/machine.
+		for( i = 0; i < ETH_ALEN; i++ )
+		    lanaddress_base.raw[i] = dev->dev_addr[ETH_ALEN-1-i];
+
+		lanaddress_set_handle( &lanaddress_base, 0 );
+		lanaddress_base.status.local = 1;
+		lanaddress_base.status.group = 0;
+
+		lanaddress_valid = 1;
+		break;
+	    }
+	}
+    }
+    read_unlock( &dev_base_lock );
+}
+
+static int __init
+lanaddress_init_module( void )
+{
+    int err;
+
+    landaddress_generate_base_address();
+
+    if( lanaddress_valid )
+    {
+	printk( KERN_INFO PREFIX "base address: " );
+	lanaddress_print_addr( &lanaddress_base );
+	printk( "\n" );
+    }
+    else
+	printk( KERN_ERR PREFIX "unable to generate a base address.\n" );
+
+    lanaddress_tid = L4VM_thread_create( GFP_KERNEL, lanaddress_server_thread,
+	    l4ka_wedge_get_irq_prio(), smp_processor_id(), NULL, 0 );
+
+    err = L4VM_server_register_location( UUID_ILanAddress, lanaddress_tid );
+    if( err == -ENODEV )
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+    else if (err < 0 )
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+
+    printk( KERN_INFO PREFIX "initialized.\n" );
+
+    return 0;
+}
+
+static void __exit
+lanaddress_exit_module( void )
+{
+    if( !L4_IsNilThread(lanaddress_tid) )
+    {
+	L4VM_server_register_location( UUID_ILanAddress, L4_nilthread );
+	L4VM_thread_delete( lanaddress_tid );
+	lanaddress_tid = L4_nilthread;
+    }
+}
+
+module_init( lanaddress_init_module );
+module_exit( lanaddress_exit_module );
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "LAN address generator" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_VERSION( "yoda" );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate sybmol resolution for
+// this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/lanaddress/Makefile linux-2.6.9/afterburn/drivers/lanaddress/Makefile
--- linux-2.6.9.src/afterburn/drivers/lanaddress/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/lanaddress/Makefile	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,6 @@
+include $(srctree)/afterburn/drivers/Makesupport
+
+obj-$(CONFIG_AFTERBURN_DRIVERS_NET_SERVER) += l4ka_lanaddress.o
+l4ka_lanaddress-y := lanaddress.o
+
+$(addprefix $(obj)/,$(obj-m)): $(wedge_symbols)
diff -Naur linux-2.6.9.src/afterburn/drivers/Makefile linux-2.6.9/afterburn/drivers/Makefile
--- linux-2.6.9.src/afterburn/drivers/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/Makefile	2007-07-03 16:22:40.000000000 +0200
@@ -0,0 +1,56 @@
+
+CONFIG_AFTERBURN_WEDGE_NAME := $(subst ",,$(CONFIG_AFTERBURN_WEDGE_NAME))
+
+##  cfg.Mk should define:
+##    cfg_pistachio_dir
+##    cfg_marzipan_dir
+##    cfg_wedge_build_prefix
+ifneq ($(CONFIG_AFTERBURN_DRIVERS),n)
+include $(obj)/cfg.Mk
+endif
+
+
+obj-$(CONFIG_AFTERBURN_DRIVERS) += force_idl.o
+obj-$(CONFIG_AFTERBURN_DRIVERS) += glue/
+obj-$(CONFIG_AFTERBURN_DRIVERS_NET_SERVER) += lanaddress/ net/
+obj-$(CONFIG_AFTERBURN_DRIVERS_NET_CLIENT) += net/
+obj-$(CONFIG_AFTERBURN_DRIVERS_BLOCK_SERVER) += block/
+obj-$(CONFIG_AFTERBURN_DRIVERS_BLOCK_CLIENT) += block/
+obj-$(CONFIG_AFTERBURN_DRIVERS_PCI_SERVER) += pci/
+
+ldflags_symbols := --just-symbols=$(cfg_wedge_build_prefix)$(CONFIG_AFTERBURN_WEDGE_NAME)/afterburn-wedge.symbols
+
+EXTRA_LDFLAGS += $(ldflags_symbols)
+LDFLAGS_MODULE += $(ldflags_symbols)
+
+
+IDL4 ?= idl4
+IDL4_CONFIG ?= idl4-config
+IDL4_FLAGS := -fctypes  -iV4 -Wall -fno-use-malloc -I$(cfg_pistachio_dir)/include
+
+quiet_cmd_idl4_client = IDL4c   $<
+quiet_cmd_idl4_server = IDL4s   $<
+cmd_idl4_client = $(IDL4) $(IDL4_FLAGS) -h $@ -c $<
+cmd_idl4_server = $(IDL4) $(IDL4_FLAGS) -h $@ -s $<
+
+##  NOTE: the IDL files will be rebuilt if the IDL command line arguments
+##  are changed.  Thus always add IDL4 output files to the extra-y list or 
+##  they'll always be built.
+
+$(obj)/%_idl_client.h: $(cfg_marzipan_dir)/%_idl.idl FORCE
+	$(call if_changed,idl4_client)
+
+$(obj)/%_idl_server.h: $(cfg_marzipan_dir)/%_idl.idl FORCE
+	$(call if_changed,idl4_server)
+
+idl4-interfaces = \
+    resourcemon_idl_client.h l4linux_idl_client.h \
+    lanaddress_idl_client.h lanaddress_idl_server.h \
+    L4VMnet_idl_client.h L4VMnet_idl_server.h \
+    L4VMblock_idl_client.h L4VMblock_idl_server.h \
+    L4VMpci_idl_server.h
+
+extra-y := $(idl4-interfaces)
+
+$(obj)/force_idl.o: $(addprefix $(obj)/,$(idl4-interfaces))
+$(obj)/force_idl.o: $(cfg_wedge_build_prefix)$(CONFIG_AFTERBURN_WEDGE_NAME)/afterburn-wedge.symbols
diff -Naur linux-2.6.9.src/afterburn/drivers/Makesupport linux-2.6.9/afterburn/drivers/Makesupport
--- linux-2.6.9.src/afterburn/drivers/Makesupport	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/Makesupport	2007-07-03 16:22:40.000000000 +0200
@@ -0,0 +1,20 @@
+CONFIG_AFTERBURN_WEDGE_NAME := $(subst ",,$(CONFIG_AFTERBURN_WEDGE_NAME))
+
+##  cfg.Mk should define:
+##    cfg_pistachio_dir
+##    cfg_marzipan_dir
+##    cfg_wedge_build_prefix
+ifneq ($(CONFIG_AFTERBURN_DRIVERS),n)
+include $(dir $(obj))/cfg.Mk
+endif
+IDL4 ?= idl4
+IDL4_CONFIG ?= idl4-config
+IDL4_FLAGS := -fctypes -iV4 -Wall -fno-use-malloc -I$(cfg_pistachio_dir)/include
+
+wedge_symbols := $(cfg_wedge_build_prefix)$(CONFIG_AFTERBURN_WEDGE_NAME)/afterburn-wedge.symbols
+LDFLAGS_MODULE += --just-symbols=$(wedge_symbols)
+
+EXTRA_CFLAGS += \
+  -I$(cfg_marzipan_dir) -I$(srctree)/afterburn/drivers -I$(dir $(obj)) \
+  -I$(cfg_pistachio_dir)/include $(shell $(IDL4_CONFIG) --cflags)
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/client.c linux-2.6.9/afterburn/drivers/net/client.c
--- linux-2.6.9.src/afterburn/drivers/net/client.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/client.c	2007-07-16 10:02:01.000000000 +0200
@@ -0,0 +1,1141 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/client.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+
+#include <asm/io.h>
+
+#include <glue/thread.h>
+#include <glue/bottomhalf.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+#include <glue/wedge.h>
+
+#define PREFIX "L4VMnet client: "
+
+#include "client.h"
+
+int L4VMnet_irq = 9;
+MODULE_PARM( L4VMnet_irq, "i" );
+
+static L4VMnet_client_adapter_t *L4VMnet_adapter_list = NULL;
+
+static irqreturn_t L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs );
+
+static int L4VMnet_irq_pending( void *data );
+static void L4VMnet_flush_tasklet_handler( unsigned long unused );
+
+DECLARE_TASKLET( L4VMnet_flush_tasklet, L4VMnet_flush_tasklet_handler, 0 );
+
+/*
+ * Initialization:
+ *   TODO: how do we determine thread ID of the server?
+ *   1.  Allocate an interrupt from Linux.
+ *   2.  Register interrupt with server (put this on the shared page).
+ *   3.  Allocate a descriptor ring for incoming packets.  The descriptor
+ *       ring holds both the empty+waiting buffers and the newly arrived
+ *       packets.  Visible only to the client.
+ *   4.  Allocate a descriptor ring for outgoing packets.  The server must
+ *       provide the memory region for the outgoing packet descriptors
+ *       (for trust reasons).  Thus the client allocates a virtual
+ *       memory region from Linux, and then receives a memory mapping
+ *       into that region from the server.
+ *   5.  Register some shared status words with the server.  One status
+ *       word will be writeable by the server, to indicate virtual
+ *       interrupt status.
+ *   6.  Tell the server the thread ID of the inbound packet thread.
+ *   7.  Ask the server for the feature list, such as hardware checksum,
+ *       and hardware fragment support.
+ *   8.  Generate a virtual LAN address.
+ *
+ * Sending packets:
+ *   1.  Physical address of outbound fragments are added to a descriptor ring.
+ *   2.  Send IPC to server, to inform that outbound packets are available.
+ *       TODO: determine mechanism for IPC mitigation (for example, we know
+ *       that a network driver will wake at some point to clean its old send
+ *       descriptors).
+ *   3.  When no more send descriptors, try to clean-out stale descriptors.
+ *
+ * Cleaning stale send descriptors:
+ *   1.  Listen for virtual interrupt from the server.
+ *
+ * Receiving packets:
+ *   1.  A device thread waits for inbound packet buffers to be allocated.
+ *       After the kernel driver thread allocates the packet buffers, and
+ *       adds them to a descriptor ring, the kernel thread sends an IPC to
+ *       the driver thread.
+ *   2.  The device thread waits in an infinite loop for IPC from server.
+ *       The IPC string-copies packets into receive buffers.  When low
+ *       on socket buffers, the device thread raises virtual interrupt,
+ *       and waits for an IPC from the kernel thread.
+ *   3.  Device thread adds new packets to a descriptor ring, and
+ *       raises a virtual interrupt.
+ *   4.  The interrupt handler fetches packets from the inbound descriptor
+ *       ring, and injects them into the kernel.  It allocates more buffers
+ *       as necessary, and adds them to a buffer descriptor ring.
+ */
+
+/****************************************************************************
+ *
+ * Receive thread functions.
+ *
+ ****************************************************************************/
+
+/*
+ *  1. Use a large descriptor ring.
+ *  2. Perform an IPC wait on a group of descriptors.
+ *  3. Move to the next group of descriptors for the next IPC.
+ *  4. If insufficient descriptors, then send an IPC to the device driver
+ *     and wait for replenishment of descriptors.
+ *  Note: when the device driver walks the ring of received packets, it must
+ *  be able to handle holes ... packets which were not delivered.  It should
+ *  just skip the holes and reuse the already allocated buffers.  We tell
+ *  the device driver to skip the packet by giving it a buffer length of zero.
+ */
+
+static void L4VMnet_rcv_thread_prepare(
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter,
+	L4_ThreadId_t *reply_tid,
+	L4_MsgTag_t *reply_tag )
+{
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    L4_Word16_t total, i;
+    L4_ThreadId_t from_tid;
+
+    while( 1 )
+    {
+	// Do we have enough buffers?
+	total = 0;
+	i = ring->start_free;
+	while( 1 )
+	{
+	    if( !group->rcv_shadow[i].skb ||
+		    !ring->desc_ring[i].status.X.server_owned )
+	    {
+		group->status |= L4VMNET_IRQ_RING_EMPTY;
+		break; // Insufficient buffers.
+	    }
+
+	    total++;
+	    if( total >= IVMnet_rcv_buffer_cnt )
+		break;  // We found enough buffers.
+
+	    i = (i + 1) % ring->cnt;
+	    if( i == ring->start_free )
+	    {
+		group->status |= L4VMNET_IRQ_RING_EMPTY;
+		break; // Wrapped around the ring.
+	    }
+	}
+
+	// Calculate whether to send an interrupt.
+	if( ((group->status & group->mask) != 0) && !adapter->irq_pending )
+	{
+	    adapter->irq_pending = TRUE;
+	    if( vcpu_in_dispatch_ipc() )
+		*reply_tid = L4VM_linux_main_thread( smp_processor_id() );
+	    else
+		*reply_tid = L4VM_linux_irq_thread( smp_processor_id() );
+	    reply_tag->raw = 0;
+	    reply_tag->X.label = L4_TAG_IRQ;
+	    reply_tag->X.u = 1;
+	    L4_LoadMR( 1, L4VMnet_irq );
+	}
+	else
+	    *reply_tid = L4_nilthread;
+
+	// Do the interrupt IPC.
+	if( group->status & L4VMNET_IRQ_RING_EMPTY )
+	{
+	    // We need a wait phase for a reply.
+	    L4_MsgTag_t tag;
+	    dprintk( 3, PREFIX "available buffers: %d, %d needed; device thread sleeping\n", total, IVMnet_rcv_buffer_cnt );
+	    L4_Set_MsgTag( *reply_tag );
+	    tag = L4_Ipc( *reply_tid, L4_anythread, 
+		    L4_Timeouts(L4_Never,L4_Never), &from_tid );
+	    if( L4_IpcFailed(tag) && ((L4_ErrorCode() & 1) == 0) )
+	    {
+		// Retry to the IRQ dispatcher.
+		L4_Set_MsgTag( *reply_tag );
+		*reply_tid = L4VM_linux_irq_thread( smp_processor_id() );
+		tag = L4_Ipc( *reply_tid, L4_anythread,
+			L4_Timeouts(L4_Never,L4_Never), &from_tid );
+	    }
+	    dprintk( 3, PREFIX "device thread wakeup.\n" );
+	}
+	else
+	{
+	    // TODO: study performance impact of a one-way IRQ IPC to notify
+	    // the client's IRQ thread of ready packets.  Currently we donate
+	    // the timeslice to the IRQ thread, which may be inappropriate.
+	    return;
+	}
+    }
+}
+
+static void L4VMnet_rcv_thread_wait(
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter,
+	L4_ThreadId_t reply_tid,
+	L4_MsgTag_t reply_tag )
+{
+    L4_Acceptor_t acceptor;
+    L4_StringItem_t string_item;
+    L4_MsgTag_t msg_tag;
+    L4_ThreadId_t server_tid;
+    L4_Word16_t i, pkt;
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    IVMnet_ring_descriptor_t *desc;
+
+    // Create and install the IPC mapping acceptor.
+    acceptor = L4_MapGrantItems( L4_Nilpage );	// No mappings permitted.
+    L4_Accept( acceptor );			// Install.
+    L4_Set_MsgTag( reply_tag );
+
+    // Install string item acceptors for each packet buffer.
+    dprintk( 4, PREFIX "start_free = %d\n", ring->start_free );
+    pkt = ring->start_free;
+    for( i = 0; i < IVMnet_rcv_buffer_cnt; i++ )
+    {
+	desc = &ring->desc_ring[ pkt ];
+	pkt = (pkt + 1) % ring->cnt;
+
+	ASSERT( desc->status.X.server_owned );
+
+	string_item = L4_StringItem( desc->buffer_len, (void *)desc->buffer );
+	if( i < (IVMnet_rcv_buffer_cnt-1) )
+	    string_item.X.C = 1; // More buffers comming.
+
+	// Write the message registers.
+	L4_LoadBRs( i*2 + 1, 2, string_item.raw );
+    }
+
+    // Wait for packets from a valid sender, and send an IRQ IPC if necessary.
+    do {
+	adapter->client_shared->receiver_tids[ group->group_no ] = L4_Myself();
+	msg_tag = L4_Ipc( reply_tid, L4_anythread, 
+		L4_Timeouts(L4_Never, L4_Never), &server_tid );
+	reply_tid = L4_nilthread;
+    } while( msg_tag.X.flags & 8 );
+
+    dprintk( 4, PREFIX "string copy received, string items: %lu\n",
+	    L4_TypedWords(msg_tag)/2 );
+
+    // Swallow the received packets.
+    for( i = L4_UntypedWords(msg_tag) + 1;
+	 (i+1) <= (L4_UntypedWords(msg_tag) + L4_TypedWords(msg_tag));
+	 i += 2 )
+    {
+	// Read the message registers.
+	L4_StoreMRs( i, 2, string_item.raw );
+	// Release ownership of the socket buffers.
+	// TODO: handle invalid IPC message format.
+	desc = &ring->desc_ring[ ring->start_free ];
+	ASSERT( L4_IsStringItem(&string_item) );
+	ASSERT( desc->status.X.server_owned );
+	desc->buffer_len = string_item.X.string_length;
+	dprintk( 4, PREFIX "string length: %u\n", desc->buffer_len );
+	desc->status.X.server_owned = 0;
+
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+	if( desc->buffer_len > 0 )
+	    group->status |= L4VMNET_IRQ_PKTS_AVAIL;
+    }
+}
+
+typedef struct
+{
+    L4VMnet_client_adapter_t *adapter;
+    L4VMnet_rcv_group_t *group;
+} L4VMnet_rcv_thread_params_t;
+
+static void
+L4VMnet_rcv_thread( void *arg )
+{
+    L4VMnet_rcv_thread_params_t *params = arg;
+    L4_MsgTag_t reply_tag;
+    L4_ThreadId_t reply_tid;
+
+    while( 1 )
+    {
+	L4VMnet_rcv_thread_prepare( params->group, params->adapter,
+		&reply_tid, &reply_tag );
+	L4VMnet_rcv_thread_wait( params->group, params->adapter,
+		reply_tid, reply_tag );
+    }
+}
+
+
+/****************************************************************************
+ *
+ * Server commands.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_server_stop( L4VMnet_client_adapter_t *adapter )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+    unsigned long irq_flags;
+
+    local_irq_save(irq_flags);
+    IVMnet_Control_stop( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+}
+
+static void
+L4VMnet_server_start( L4VMnet_client_adapter_t *adapter )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+    unsigned long irq_flags;
+
+    local_irq_save(irq_flags);
+    IVMnet_Control_start( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    // TODO: check for error
+    dprintk( 1, PREFIX "started the server connection.\n" );
+}
+
+/****************************************************************************
+ *
+ * Server connection functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_reset_ring( L4VMnet_desc_ring_t *ring,
+	L4VMnet_shadow_t *shadow )
+{
+    L4_Word16_t i;
+
+    for( i = 0; i < ring->cnt; i++ )
+    {
+	if( !ring->desc_ring[i].status.X.fragment && shadow[i].skb )
+	    dev_kfree_skb_any( shadow[i].skb );
+    }
+
+    memset( ring->desc_ring, 0, ring->cnt * sizeof(IVMnet_ring_descriptor_t));
+    memset( shadow, 0, ring->cnt * sizeof(L4VMnet_shadow_t) );
+}
+
+static int
+L4VMnet_open( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4_Fpage_t rcv_window;
+    idl4_fpage_t idl4_mapping, idl4_server_mapping;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    int err, line = __LINE__;
+    int i;
+    L4_Word_t log2size, size, size2, window_base;
+
+    if( adapter->connected == 1 )
+    {
+	L4VMnet_server_start( adapter );
+	netif_wake_queue( netdev );
+	return 0;
+    }
+
+    /* Try to obtain a LAN address if we still need one.
+     */
+    if( adapter->lanaddress_valid == FALSE )
+    {
+	err = L4VMnet_allocate_lan_address( netdev->dev_addr );
+	if( err )
+	{
+	    line = __LINE__;
+	    goto err_lan_address;
+	}
+	adapter->lanaddress_valid = TRUE;
+    }
+
+    // Locate the network server.
+    err = L4VM_server_locate( UUID_IVMnet, &adapter->server_tid );
+    if( err != 0 )
+    {
+	line = __LINE__;
+	goto err_locate_server;
+    }
+    dprintk( 2, PREFIX "server tid %lx\n", adapter->server_tid.raw );
+
+    /* Prepare the send ring.
+     */
+    adapter->send_ring.cnt = IVMnet_snd_descriptor_ring_cnt;
+    adapter->send_ring.start_free = 0;
+    adapter->send_ring.start_dirty = 0;
+    adapter->send_shadow = kmalloc( adapter->send_ring.cnt * sizeof(L4VMnet_shadow_t), GFP_KERNEL );
+    if( adapter->send_shadow == NULL )
+    {
+	err = -ENOMEM; line = __LINE__;
+	goto err_shadow_alloc;
+    }
+
+    /* Prepare the receive ring.
+     */
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].rcv_ring.cnt = IVMnet_rcv_buffer_cnt * 4;
+	adapter->rcv_group[i].rcv_ring.start_free = 0;
+	adapter->rcv_group[i].rcv_ring.start_dirty = 0;
+	adapter->rcv_group[i].rcv_ring.desc_ring = NULL;
+	adapter->rcv_group[i].rcv_shadow = NULL;
+    }
+
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].rcv_shadow = kmalloc( adapter->rcv_group[i].rcv_ring.cnt * sizeof(L4VMnet_shadow_t), GFP_KERNEL );
+	adapter->rcv_group[i].rcv_ring.desc_ring = kmalloc( adapter->rcv_group[i].rcv_ring.cnt * sizeof(IVMnet_ring_descriptor_t), GFP_KERNEL );
+	if( !adapter->rcv_group[i].rcv_shadow || !adapter->rcv_group[i].rcv_ring.desc_ring )
+	{
+	    err = -ENOMEM; line = __LINE__;
+	    goto err_rcv_alloc;
+	}
+    }
+
+    // Allocate an oversized shared window region, to adjust for alignment.
+    // We receive two fpages in the area.  We will align to the largest fpage.
+    size  = L4_Size( L4_Fpage(0,sizeof(IVMnet_client_shared_t)) );
+    size2 = L4_Size( L4_Fpage(0,sizeof(IVMnet_server_shared_t)) );
+    log2size = L4_SizeLog2( L4_Fpage(0, size+size2) );
+    err = L4VM_fpage_vmarea_get( log2size, &adapter->shared_window );
+    if( err < 0 )
+    {
+	line = __LINE__;
+	goto err_get_vm_area;
+    }
+    else
+	rcv_window = adapter->shared_window.fpage;
+    ASSERT( L4_Address(adapter->shared_window.fpage) >=
+	    (L4_Word_t)adapter->shared_window.ioremap_addr );
+    dprintk( 2, KERN_INFO PREFIX "receive window at %p, size %lu "
+	    "(padding starts at %p)\n",
+	    (void *)L4_Address(adapter->shared_window.fpage),
+	    L4_Size(adapter->shared_window.fpage),
+	    adapter->shared_window.ioremap_addr );
+
+    // Attach to the server, and request the shared mapping area.
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    idl4_set_rcv_window( &ipc_env, rcv_window );
+    IVMnet_Control_attach( adapter->server_tid, "eth1", netdev->dev_addr, &adapter->ivmnet_handle, &idl4_mapping, &idl4_server_mapping, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	err = -EIO; line = __LINE__;
+	goto err_server_register;
+    }
+
+    dprintk( 1, PREFIX "shared region at base 0x%x, offset 0x%lx, "
+	    "size %ld\n",
+	    idl4_fpage_get_base(idl4_mapping),
+	    L4_Address(idl4_fpage_get_page(idl4_mapping)),
+	    L4_Size(idl4_fpage_get_page(idl4_mapping)) );
+    dprintk( 1, PREFIX "server status region at base 0x%x, offset 0x%lx, "
+	    "size %ld\n",
+	    idl4_fpage_get_base(idl4_server_mapping),
+	    L4_Address(idl4_fpage_get_page(idl4_server_mapping)),
+	    L4_Size(idl4_fpage_get_page(idl4_server_mapping)) );
+
+    // Finish up initialization.
+    window_base = L4_Address( rcv_window );
+    adapter->client_shared = (IVMnet_client_shared_t *)
+	(window_base + idl4_fpage_get_base(idl4_mapping));
+    adapter->client_shared->client_irq_tid = L4_nilthread;
+    adapter->client_shared->client_irq_pending = 0;
+    adapter->send_ring.desc_ring = adapter->client_shared->snd_desc_ring;
+    adapter->client_shared->receiver_cnt = 0;
+
+    adapter->server_status = (IVMnet_server_shared_t *)
+	(window_base + idl4_fpage_get_base(idl4_server_mapping));
+
+    // Zero the send rings.
+    memset( adapter->send_ring.desc_ring, 0, adapter->send_ring.cnt * sizeof(IVMnet_ring_descriptor_t) );
+    memset( adapter->send_shadow, 0, adapter->send_ring.cnt * sizeof(L4VMnet_shadow_t) );
+
+    // Zero the receive rings.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	memset( adapter->rcv_group[i].rcv_ring.desc_ring, 0, adapter->rcv_group[i].rcv_ring.cnt * sizeof(IVMnet_ring_descriptor_t) );
+	memset( adapter->rcv_group[i].rcv_shadow, 0, adapter->rcv_group[i].rcv_ring.cnt * sizeof(L4VMnet_shadow_t) );
+    }
+
+    // More IRQ related initialization.
+    adapter->irq_pending = FALSE;
+
+    // Init the receiver threads.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].group_no = i;
+	adapter->rcv_group[i].dev_tid = L4_nilthread;
+	adapter->rcv_group[i].status = 0;
+	adapter->rcv_group[i].mask = 0;
+    }
+
+    // Allocate a virtual interrupt.
+    if( L4VMnet_irq > NR_IRQS )
+    {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	goto err_vmpic_reserve;
+    }
+    l4ka_wedge_add_virtual_irq( L4VMnet_irq );
+    err = request_irq( L4VMnet_irq, L4VMnet_irq_handler, 0, 
+	    netdev->name, netdev );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Tell the server our irq tid.
+    adapter->client_shared->client_irq_tid = get_vcpu()->irq_gtid;
+
+    // Start the receiver threads.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	// Prepare parameters for the new thread.
+	L4VMnet_rcv_thread_params_t params;
+	params.adapter = adapter;
+	params.group = &adapter->rcv_group[i];
+	// Start the thread.
+	adapter->rcv_group[i].dev_tid = L4VM_thread_create( GFP_KERNEL,
+		L4VMnet_rcv_thread, l4ka_wedge_get_irq_prio(), 
+		0, &params, sizeof(params) );
+	if( L4_IsNilThread(adapter->rcv_group[i].dev_tid) )
+	{
+	    printk( KERN_ERR PREFIX "failed to start a receiver thread\n" );
+	    continue;
+	}
+
+	// Tell the server about our receiver threads.
+	adapter->client_shared->receiver_tids[ adapter->client_shared->receiver_cnt ] = adapter->rcv_group[i].dev_tid;
+	adapter->client_shared->receiver_cnt++;
+    }
+
+    // Tell the server that we are ready to handle packets.
+    adapter->connected = 1;
+    L4VMnet_server_start( adapter );
+    netif_wake_queue( netdev );
+
+    // Enable the interrupts.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	adapter->rcv_group[i].mask = ~0;
+
+    return 0;
+
+err_request_irq:
+err_vmpic_reserve:
+err_server_register:
+    L4VM_fpage_vmarea_release( &adapter->shared_window );
+err_get_vm_area:
+    kfree( adapter->send_shadow );
+err_rcv_alloc:
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	if( adapter->rcv_group[i].rcv_shadow )
+	    kfree( adapter->rcv_group[i].rcv_shadow );
+	if( adapter->rcv_group[i].rcv_ring.desc_ring )
+	    kfree( adapter->rcv_group[i].rcv_ring.desc_ring );
+    }
+err_shadow_alloc:
+err_locate_server:
+err_lan_address:
+    printk( KERN_ERR PREFIX "configuration error (%s:%d)\n", __FILE__, line );
+    return err;
+}
+
+static int
+L4VMnet_close( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    int i;
+
+    // Stop the server, so that we can clean packets.
+    netif_stop_queue( netdev );
+    L4VMnet_server_stop( adapter );
+
+    // Preserve our server connection.
+    return 0;
+
+    // TODO: coordinate with the device threads!  They are waiting
+    // on these buffers!!
+
+    // Drop all packets in the send descriptor ring.
+    L4VMnet_reset_ring( &adapter->send_ring, adapter->send_shadow );
+
+    // Drop all packets in the receive descriptor rings.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	L4VMnet_reset_ring( &adapter->rcv_group[i].rcv_ring,
+		adapter->rcv_group[i].rcv_shadow );
+
+    // Disconnect from the server.
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMnet_Control_detach( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_ERR PREFIX "server doesn't acknowledge our connection\n" );
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_ERR PREFIX "unable to disconnect from server\n" );
+	return -EIO;
+    }
+
+    // Free receiver stuff.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	L4VM_thread_delete( adapter->rcv_group[i].dev_tid );
+	kfree( adapter->rcv_group[i].rcv_shadow );
+	kfree( adapter->rcv_group[i].rcv_ring.desc_ring );
+    }
+
+    // Unregister the IRQ handler.
+    // TODO: release the IRQ handler.
+
+    // Free resources.
+    L4VM_fpage_vmarea_release( &adapter->shared_window );
+    kfree( adapter->send_shadow );
+
+    adapter->connected = 0;
+    return 0;
+}
+
+static struct net_device_stats *
+L4VMnet_get_stats( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+
+    if( NULL != adapter && FALSE != adapter->connected ) {
+	CORBA_Environment ipc_env;
+	unsigned long irq_flags;
+
+	ipc_env = idl4_default_environment;
+	local_irq_save(irq_flags);
+	IVMnet_Control_update_stats( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+	local_irq_restore(irq_flags);
+
+	if( ipc_env._major == CORBA_NO_EXCEPTION ) {
+	    return (struct net_device_stats *)&(adapter->server_status->stats);
+	} else if( ipc_env._major == CORBA_USER_EXCEPTION ) {
+	    printk( KERN_ERR PREFIX "exception %d while requesting stats update from server\n",
+		    ipc_env._minor );
+	} else {
+	    printk( KERN_ERR PREFIX "unable to request stats update from server\n" );
+	}
+
+	L4_KDB_Enter("get_stats");
+	CORBA_exception_free(&ipc_env);
+    }
+
+    return NULL;
+}
+
+/****************************************************************************
+ *
+ * Packet send functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_clean_send_ring( L4VMnet_client_adapter_t *adapter )
+{
+    IVMnet_ring_descriptor_t *desc;
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    L4_Word16_t cleaned = 0;
+
+    while( 1 )
+    {
+	desc = &ring->desc_ring[ ring->start_dirty ];
+	if( desc->status.X.server_owned )
+	    break;
+
+	if( adapter->send_shadow[ ring->start_dirty ].skb == NULL )
+	{
+	    ASSERT( ring->start_dirty == ring->start_free );
+	    break;
+	}
+
+	if( !desc->status.X.fragment )
+	    dev_kfree_skb_any( adapter->send_shadow[ ring->start_dirty ].skb );
+	adapter->send_shadow[ ring->start_dirty ].skb = NULL;
+	desc->buffer = 0;
+	desc->buffer_len = 0;
+
+	ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	cleaned++;
+    }
+
+    // Wake the queue *after* we finish playing with the ring descriptors,
+    // so that we avoid concurrent access to this function!
+    if( cleaned && netif_queue_stopped(adapter->dev) )
+    {
+	netif_wake_queue( adapter->dev );
+	dprintk( 3, PREFIX "rewaking packet queue\n" );
+    }
+    else if( netif_queue_stopped(adapter->dev) )
+    {
+//	dprintk( 3, PREFIX "send packet queue jammed\n" );
+    }
+}
+
+static void
+L4VMnet_trigger_send( L4VMnet_client_adapter_t *adapter )
+{
+    L4_MsgTag_t msg_tag, result_tag;
+
+    adapter->server_status->irq_status |= 1;
+    if( adapter->server_status->irq_pending )
+	return;
+
+    adapter->server_status->irq_pending = TRUE;
+
+    {
+
+	unsigned long flags;
+	local_irq_save(flags);
+	{
+	    L4_ThreadId_t from_tid;
+	    msg_tag.raw = 0; msg_tag.X.label = L4_TAG_IRQ; msg_tag.X.u = 1;
+	    L4_Set_MsgTag( msg_tag );
+	    L4_LoadMR( 1, adapter->client_shared->server_irq );
+	    result_tag = L4_Reply( adapter->client_shared->server_main_tid );
+	    if( !L4_IpcSucceeded(result_tag) )
+	    {
+		L4_Set_MsgTag( msg_tag );
+		L4_Ipc( adapter->client_shared->server_irq_tid, L4_nilthread,
+	    		L4_Timeouts(L4_Never, L4_Never), &from_tid);
+	    }
+	}
+	local_irq_restore(flags);
+    }
+}
+
+static int
+L4VMnet_xmit_available(
+	L4VMnet_client_adapter_t *adapter,
+	L4_Word16_t fragments )
+{
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    IVMnet_ring_descriptor_t *desc;
+    int dirty = FALSE;
+    int frag, i;
+
+    ASSERT( fragments < ring->cnt );
+
+    if( L4VMnet_ring_available(ring) <= fragments )
+    {
+	i = ring->start_dirty;
+	for( frag = 0; frag < fragments; frag++ )
+	{
+	    desc = &ring->desc_ring[ i ];
+	    if( desc->status.X.server_owned )
+		return FALSE;
+	    dirty = TRUE;
+	    i = (i + 1) % ring->cnt;
+	}
+    }
+
+    if( dirty )
+    {
+	L4VMnet_clean_send_ring( adapter );
+	ASSERT( adapter->send_shadow[ ring->start_free ].skb == NULL );
+    }
+
+    return TRUE;
+}
+
+static int
+L4VMnet_xmit_frame( struct sk_buff *skb, struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    IVMnet_ring_descriptor_t *desc = &ring->desc_ring[ ring->start_free ];
+    IVMnet_ring_descriptor_t *frag_desc;
+    L4_Word16_t fragments, f;
+    unsigned long flags;
+
+    fragments = 1 + skb_shinfo(skb)->nr_frags;
+    if( unlikely(fragments >= ring->cnt) )
+    {
+	// Too many fragments in the packet, so drop the packet.
+	dev_kfree_skb_any( skb );
+	return 0;
+    }
+
+    local_irq_save(flags);
+    if( !L4VMnet_xmit_available(adapter, fragments) )
+    {
+	// We are still waiting for the server to empty the send queue.
+	netif_stop_queue( netdev );
+	local_irq_restore(flags);
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+	dprintk( 3, PREFIX "send queue full\n" );
+	return 1;
+    }
+    local_irq_restore(flags);
+
+    dprintk( 4, PREFIX "skb protocol %d, pkt_type %d\n",
+	    skb->protocol, skb->pkt_type );
+
+    ASSERT( 0 == desc->status.X.server_owned );
+    ASSERT( NULL == adapter->send_shadow[ ring->start_free ].skb );
+    desc->status.raw	= 0;
+    desc->buffer	= virt_to_phys( skb->data );
+    desc->buffer_len	= skb_headlen( skb );
+    desc->pkt_type	= skb->pkt_type;
+    desc->protocol	= skb->protocol;
+    adapter->send_shadow[ ring->start_free ].skb = skb;
+    // Where the device must start the checksum, relative to the buffer.
+    desc->csum.tx.start = skb->h.raw - skb->data;
+    // Where the device must store the checksum, relative to the buffer.
+    desc->csum.tx.offset = desc->csum.tx.start + skb->csum;
+    if( skb->ip_summed == CHECKSUM_HW )
+	desc->status.X.csum = 1;
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    dprintk( 4, PREFIX "send pkt @ virt %lx, bus %lx, size %d\n",
+	    skb->data, desc->buffer, desc->buffer_len );
+
+    // Queue packet fragments, for scatter/gather.
+    frag_desc = desc;
+    for( f = 0; f < skb_shinfo(skb)->nr_frags; f++ )
+    {
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
+
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %d --> %lx\n",
+		f,
+		(L4_Word_t)frag->page, (L4_Word_t)frag->page_offset,
+		frag->size, L4VMnet_fragment_buffer(frag));
+
+	// Mark the *prior* descriptor as a fragment.  The last descriptor
+	// in the sequence is not marked.
+	frag_desc->status.X.fragment = 1;
+
+	frag_desc = &ring->desc_ring[ ring->start_free ];
+	ASSERT( 0 == frag_desc->status.X.server_owned );
+	ASSERT( NULL == adapter->send_shadow[ ring->start_free ].skb );
+
+	frag_desc->status.raw	= 0;
+	frag_desc->buffer	= L4VMnet_fragment_buffer(frag);
+	frag_desc->buffer_len	= frag->size;
+	adapter->send_shadow[ ring->start_free ].skb = skb;
+	frag_desc->status.X.server_owned = 1;
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+    }
+
+    // Assign ownership of the packet sequence to the server.  Do this last!
+    desc->status.X.server_owned = 1;
+
+    // Deal with sending the packets.
+    netdev->trans_start = jiffies;
+    dprintk( 4, PREFIX "send %u fragments\n", fragments );
+
+    tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+
+    return 0;
+}
+
+static void
+L4VMnet_flush_tasklet_handler( unsigned long unused )
+{
+    L4VMnet_client_adapter_t *adapter = L4VMnet_adapter_list;
+
+    L4VMnet_trigger_send( adapter );
+}
+
+/****************************************************************************
+ *
+ * Packet receive functions.
+ *
+ ****************************************************************************/
+
+static void L4VMnet_wake_driver_thread(
+	L4VMnet_rcv_group_t *rcv_group,
+	L4VMnet_client_adapter_t *adapter )
+{
+    L4_MsgTag_t tag;
+    unsigned long flags;
+
+    local_irq_save(flags);
+    tag.raw = 0;
+    L4_Set_MsgTag( tag );
+    L4_Reply( rcv_group->dev_tid );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_walk_rcv_ring(
+	int wake,
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter )
+{
+    volatile IVMnet_ring_descriptor_t *desc;
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    struct sk_buff *skb;
+    L4_Word16_t pkt_len = IVMnet_rcv_buffer_size;
+    L4_Word16_t reserve_len = 2;
+    int total_transferred = 0;
+
+    // Descriptors can be in several states:
+    //  1. Zero buffer len: either the buffer doesn't exist, or it was a
+    //     dropped packet and the buffer exists.
+    //  2. Valid packet that should be handed off to Linux.
+    //  3. Packet still belongs to the driver thread.
+
+    dprintk( 4, PREFIX "start_dirty = %d\n", ring->start_dirty );
+    while( 1 )
+    {
+	desc = &ring->desc_ring[ ring->start_dirty ];
+	skb = group->rcv_shadow[ ring->start_dirty ].skb;
+
+	if( desc->status.X.server_owned )
+	    break;
+
+	if( likely((desc->buffer_len > 0) && skb) )
+	{
+	    // Give the skb to the Linux network stack.
+	    dprintk( 4, PREFIX "injesting packet\n" );
+	    skb_put( skb, desc->buffer_len );
+	    // Assume that the DD/OS performed the checksum.
+	    skb->ip_summed = CHECKSUM_UNNECESSARY;
+	    skb->protocol = eth_type_trans( skb, adapter->dev );
+	    netif_rx( skb );
+	    adapter->dev->last_rx = jiffies;
+	    skb = NULL;  // Release the skb.
+	}
+
+	if( skb == NULL )
+	{
+	    // Allocate a new skb.
+	    skb = dev_alloc_skb( pkt_len + reserve_len );
+	    group->rcv_shadow[ ring->start_dirty ].skb = skb;
+
+	    if( unlikely(skb == NULL) )
+	    {
+		desc->buffer = 0;
+		desc->buffer_len = 0;
+	    }
+	    else
+	    {
+		// Align the buffer 2 beyond a 16-byte boundary, to result
+		// in a 16-byte aligned IP header after the 14-byte MAC header
+		// is removed.
+		skb_reserve( skb, reserve_len );
+		skb->dev = adapter->dev;
+	    }
+	}
+
+	if( likely(skb != NULL) )
+	{
+	    // This could be a reused buffer, or a new buffer.
+	    desc->buffer = (L4_Word_t)skb->data;
+	    desc->buffer_len = pkt_len;
+	    // Transfer ownership to the device.
+	    desc->status.raw = 0;
+	    desc->status.X.server_owned = 1;
+	    total_transferred++;
+
+	    ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	}
+	else
+	    break; // Insufficient memory!
+    }
+
+    // TODO: deal with insufficient memory conditions.
+
+    dprintk( 3, PREFIX "allocated %d receive buffers.\n", total_transferred );
+    if( total_transferred && wake )
+	L4VMnet_wake_driver_thread( group, adapter );
+}
+
+static irqreturn_t
+L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    struct net_device *netdev = data;
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4VMnet_rcv_group_t *rcv_group;
+    int i, finished, wake, cnt = 0;
+
+    do
+    {
+	// No one needs to deliver IRQ messages.  They would be
+	// superfluous.
+	adapter->irq_pending = TRUE;
+	finished = TRUE;
+
+	// Process receive interrupts.
+	for( i = 0; i < NR_RCV_GROUP; i++ )
+	{
+	    unsigned events;
+	    rcv_group = &adapter->rcv_group[i];
+
+	    do {
+		events = rcv_group->status;
+	    } while( cmpxchg(&rcv_group->status, events, 0) != events );
+
+	    if( !events )
+		continue;
+
+	    finished = FALSE;
+
+	    wake = 0 != (events & L4VMNET_IRQ_RING_EMPTY);
+	    dprintk( 3, PREFIX "irq handler, group %d, wake %d: 0x%x\n",
+		    i, wake, events );
+	    L4VMnet_walk_rcv_ring( wake, rcv_group, adapter );
+	}
+
+	// Process transmit related problems.
+	if( netif_queue_stopped(adapter->dev) )
+	    L4VMnet_clean_send_ring( adapter );
+
+	// Enable interrupt message delivery.
+	adapter->irq_pending = FALSE;
+	cnt++;
+	if( cnt > 100 )
+	    dprintk( 1, PREFIX "too many interrupts.\n" );
+    } while( !finished );
+
+    return IRQ_HANDLED;
+}
+
+static int
+L4VMnet_irq_pending( void *data )
+{
+    L4VMnet_client_adapter_t *adapter = (L4VMnet_client_adapter_t *)data;
+    int i;
+
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	if( (adapter->rcv_group[i].status & adapter->rcv_group[i].mask) != 0 )
+	{
+	    dprintk( 4, PREFIX "irq pending.\n" );
+	    return TRUE;
+	}
+    return FALSE;
+}
+
+/****************************************************************************
+ *
+ * Module-level functions.
+ *
+ ****************************************************************************/
+
+static int __init
+L4VMnet_client_init_module( void )
+{
+    struct net_device *netdev;
+    L4VMnet_client_adapter_t *adapter;
+    int err;
+
+    netdev = alloc_etherdev( sizeof(L4VMnet_client_adapter_t) );
+    if( !netdev )
+	return -ENOMEM;
+
+    SET_MODULE_OWNER( netdev );
+
+    /* Adapter should already be zeroed. */
+    adapter = (L4VMnet_client_adapter_t *)netdev->priv;
+    adapter->dev = netdev;
+    adapter->connected = 0;
+    L4VMnet_adapter_list = adapter;
+
+    /* Initialize the netdev. */
+    netdev->open = L4VMnet_open;
+    netdev->stop = L4VMnet_close;
+    netdev->hard_start_xmit = L4VMnet_xmit_frame;
+    netdev->features = NETIF_F_SG | NETIF_F_HW_CSUM | NETIF_F_FRAGLIST;
+    netdev->get_stats = L4VMnet_get_stats;
+
+    /* Try to obtain a LAN address.  We can retry later if necessary. */
+    memset( netdev->dev_addr, 0, ETH_ALEN );
+    adapter->lanaddress_valid =
+	( 0 == L4VMnet_allocate_lan_address(netdev->dev_addr) );
+    printk( KERN_INFO PREFIX "LAN address: " );
+    L4VMnet_print_lan_address( netdev->dev_addr );
+    printk( "\n" );
+
+    /* Register the netdev. */
+    err = register_netdev( netdev );
+    if( err )
+	goto err_register_netdev;
+    netif_stop_queue( netdev );
+
+    printk( KERN_INFO PREFIX "L4VMnet client device %s\n", netdev->name );
+
+    return 0;
+
+err_register_netdev:
+    kfree( netdev );
+    return err;
+}
+
+static void __exit
+L4VMnet_client_exit_module( void )
+{
+    if( L4VMnet_adapter_list )
+    {
+	unregister_netdev( L4VMnet_adapter_list->dev );
+	kfree( L4VMnet_adapter_list->dev );
+	L4VMnet_adapter_list = NULL;
+    }
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4 network client driver" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM net" );
+MODULE_VERSION("slink");
+
+module_init( L4VMnet_client_init_module );
+module_exit( L4VMnet_client_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/client.c~ linux-2.6.9/afterburn/drivers/net/client.c~
--- linux-2.6.9.src/afterburn/drivers/net/client.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/client.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,1141 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/client.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+
+#include <asm/io.h>
+
+#include <glue/thread.h>
+#include <glue/bottomhalf.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+#include <glue/wedge.h>
+
+#define PREFIX "L4VMnet client: "
+
+#include "client.h"
+
+int L4VMnet_irq = 9;
+MODULE_PARM( L4VMnet_irq, "i" );
+
+static L4VMnet_client_adapter_t *L4VMnet_adapter_list = NULL;
+
+static irqreturn_t L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs );
+
+static int L4VMnet_irq_pending( void *data );
+static void L4VMnet_flush_tasklet_handler( unsigned long unused );
+
+DECLARE_TASKLET( L4VMnet_flush_tasklet, L4VMnet_flush_tasklet_handler, 0 );
+
+/*
+ * Initialization:
+ *   TODO: how do we determine thread ID of the server?
+ *   1.  Allocate an interrupt from Linux.
+ *   2.  Register interrupt with server (put this on the shared page).
+ *   3.  Allocate a descriptor ring for incoming packets.  The descriptor
+ *       ring holds both the empty+waiting buffers and the newly arrived
+ *       packets.  Visible only to the client.
+ *   4.  Allocate a descriptor ring for outgoing packets.  The server must
+ *       provide the memory region for the outgoing packet descriptors
+ *       (for trust reasons).  Thus the client allocates a virtual
+ *       memory region from Linux, and then receives a memory mapping
+ *       into that region from the server.
+ *   5.  Register some shared status words with the server.  One status
+ *       word will be writeable by the server, to indicate virtual
+ *       interrupt status.
+ *   6.  Tell the server the thread ID of the inbound packet thread.
+ *   7.  Ask the server for the feature list, such as hardware checksum,
+ *       and hardware fragment support.
+ *   8.  Generate a virtual LAN address.
+ *
+ * Sending packets:
+ *   1.  Physical address of outbound fragments are added to a descriptor ring.
+ *   2.  Send IPC to server, to inform that outbound packets are available.
+ *       TODO: determine mechanism for IPC mitigation (for example, we know
+ *       that a network driver will wake at some point to clean its old send
+ *       descriptors).
+ *   3.  When no more send descriptors, try to clean-out stale descriptors.
+ *
+ * Cleaning stale send descriptors:
+ *   1.  Listen for virtual interrupt from the server.
+ *
+ * Receiving packets:
+ *   1.  A device thread waits for inbound packet buffers to be allocated.
+ *       After the kernel driver thread allocates the packet buffers, and
+ *       adds them to a descriptor ring, the kernel thread sends an IPC to
+ *       the driver thread.
+ *   2.  The device thread waits in an infinite loop for IPC from server.
+ *       The IPC string-copies packets into receive buffers.  When low
+ *       on socket buffers, the device thread raises virtual interrupt,
+ *       and waits for an IPC from the kernel thread.
+ *   3.  Device thread adds new packets to a descriptor ring, and
+ *       raises a virtual interrupt.
+ *   4.  The interrupt handler fetches packets from the inbound descriptor
+ *       ring, and injects them into the kernel.  It allocates more buffers
+ *       as necessary, and adds them to a buffer descriptor ring.
+ */
+
+/****************************************************************************
+ *
+ * Receive thread functions.
+ *
+ ****************************************************************************/
+
+/*
+ *  1. Use a large descriptor ring.
+ *  2. Perform an IPC wait on a group of descriptors.
+ *  3. Move to the next group of descriptors for the next IPC.
+ *  4. If insufficient descriptors, then send an IPC to the device driver
+ *     and wait for replenishment of descriptors.
+ *  Note: when the device driver walks the ring of received packets, it must
+ *  be able to handle holes ... packets which were not delivered.  It should
+ *  just skip the holes and reuse the already allocated buffers.  We tell
+ *  the device driver to skip the packet by giving it a buffer length of zero.
+ */
+
+static void L4VMnet_rcv_thread_prepare(
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter,
+	L4_ThreadId_t *reply_tid,
+	L4_MsgTag_t *reply_tag )
+{
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    L4_Word16_t total, i;
+    L4_ThreadId_t from_tid;
+
+    while( 1 )
+    {
+	// Do we have enough buffers?
+	total = 0;
+	i = ring->start_free;
+	while( 1 )
+	{
+	    if( !group->rcv_shadow[i].skb ||
+		    !ring->desc_ring[i].status.X.server_owned )
+	    {
+		group->status |= L4VMNET_IRQ_RING_EMPTY;
+		break; // Insufficient buffers.
+	    }
+
+	    total++;
+	    if( total >= IVMnet_rcv_buffer_cnt )
+		break;  // We found enough buffers.
+
+	    i = (i + 1) % ring->cnt;
+	    if( i == ring->start_free )
+	    {
+		group->status |= L4VMNET_IRQ_RING_EMPTY;
+		break; // Wrapped around the ring.
+	    }
+	}
+
+	// Calculate whether to send an interrupt.
+	if( ((group->status & group->mask) != 0) && !adapter->irq_pending )
+	{
+	    adapter->irq_pending = TRUE;
+	    if( vcpu_in_dispatch_ipc() )
+		*reply_tid = L4VM_linux_main_thread( smp_processor_id() );
+	    else
+		*reply_tid = L4VM_linux_irq_thread( smp_processor_id() );
+	    reply_tag->raw = 0;
+	    reply_tag->X.label = L4_TAG_IRQ;
+	    reply_tag->X.u = 1;
+	    L4_LoadMR( 1, L4VMnet_irq );
+	}
+	else
+	    *reply_tid = L4_nilthread;
+
+	// Do the interrupt IPC.
+	if( group->status & L4VMNET_IRQ_RING_EMPTY )
+	{
+	    // We need a wait phase for a reply.
+	    L4_MsgTag_t tag;
+	    dprintk( 3, PREFIX "available buffers: %d, %d needed; device thread sleeping\n", total, IVMnet_rcv_buffer_cnt );
+	    L4_Set_MsgTag( *reply_tag );
+	    tag = L4_Ipc( *reply_tid, L4_anythread, 
+		    L4_Timeouts(L4_Never,L4_Never), &from_tid );
+	    if( L4_IpcFailed(tag) && ((L4_ErrorCode() & 1) == 0) )
+	    {
+		// Retry to the IRQ dispatcher.
+		L4_Set_MsgTag( *reply_tag );
+		*reply_tid = L4VM_linux_irq_thread( smp_processor_id() );
+		tag = L4_Ipc( *reply_tid, L4_anythread,
+			L4_Timeouts(L4_Never,L4_Never), &from_tid );
+	    }
+	    dprintk( 3, PREFIX "device thread wakeup.\n" );
+	}
+	else
+	{
+	    // TODO: study performance impact of a one-way IRQ IPC to notify
+	    // the client's IRQ thread of ready packets.  Currently we donate
+	    // the timeslice to the IRQ thread, which may be inappropriate.
+	    return;
+	}
+    }
+}
+
+static void L4VMnet_rcv_thread_wait(
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter,
+	L4_ThreadId_t reply_tid,
+	L4_MsgTag_t reply_tag )
+{
+    L4_Acceptor_t acceptor;
+    L4_StringItem_t string_item;
+    L4_MsgTag_t msg_tag;
+    L4_ThreadId_t server_tid;
+    L4_Word16_t i, pkt;
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    IVMnet_ring_descriptor_t *desc;
+
+    // Create and install the IPC mapping acceptor.
+    acceptor = L4_MapGrantItems( L4_Nilpage );	// No mappings permitted.
+    L4_Accept( acceptor );			// Install.
+    L4_Set_MsgTag( reply_tag );
+
+    // Install string item acceptors for each packet buffer.
+    dprintk( 4, PREFIX "start_free = %d\n", ring->start_free );
+    pkt = ring->start_free;
+    for( i = 0; i < IVMnet_rcv_buffer_cnt; i++ )
+    {
+	desc = &ring->desc_ring[ pkt ];
+	pkt = (pkt + 1) % ring->cnt;
+
+	ASSERT( desc->status.X.server_owned );
+
+	string_item = L4_StringItem( desc->buffer_len, (void *)desc->buffer );
+	if( i < (IVMnet_rcv_buffer_cnt-1) )
+	    string_item.X.C = 1; // More buffers comming.
+
+	// Write the message registers.
+	L4_LoadBRs( i*2 + 1, 2, string_item.raw );
+    }
+
+    // Wait for packets from a valid sender, and send an IRQ IPC if necessary.
+    do {
+	adapter->client_shared->receiver_tids[ group->group_no ] = L4_Myself();
+	msg_tag = L4_Ipc( reply_tid, L4_anythread, 
+		L4_Timeouts(L4_Never, L4_Never), &server_tid );
+	reply_tid = L4_nilthread;
+    } while( msg_tag.X.flags & 8 );
+
+    dprintk( 4, PREFIX "string copy received, string items: %lu\n",
+	    L4_TypedWords(msg_tag)/2 );
+
+    // Swallow the received packets.
+    for( i = L4_UntypedWords(msg_tag) + 1;
+	 (i+1) <= (L4_UntypedWords(msg_tag) + L4_TypedWords(msg_tag));
+	 i += 2 )
+    {
+	// Read the message registers.
+	L4_StoreMRs( i, 2, string_item.raw );
+	// Release ownership of the socket buffers.
+	// TODO: handle invalid IPC message format.
+	desc = &ring->desc_ring[ ring->start_free ];
+	ASSERT( L4_IsStringItem(&string_item) );
+	ASSERT( desc->status.X.server_owned );
+	desc->buffer_len = string_item.X.string_length;
+	dprintk( 4, PREFIX "string length: %u\n", desc->buffer_len );
+	desc->status.X.server_owned = 0;
+
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+	if( desc->buffer_len > 0 )
+	    group->status |= L4VMNET_IRQ_PKTS_AVAIL;
+    }
+}
+
+typedef struct
+{
+    L4VMnet_client_adapter_t *adapter;
+    L4VMnet_rcv_group_t *group;
+} L4VMnet_rcv_thread_params_t;
+
+static void
+L4VMnet_rcv_thread( void *arg )
+{
+    L4VMnet_rcv_thread_params_t *params = arg;
+    L4_MsgTag_t reply_tag;
+    L4_ThreadId_t reply_tid;
+
+    while( 1 )
+    {
+	L4VMnet_rcv_thread_prepare( params->group, params->adapter,
+		&reply_tid, &reply_tag );
+	L4VMnet_rcv_thread_wait( params->group, params->adapter,
+		reply_tid, reply_tag );
+    }
+}
+
+
+/****************************************************************************
+ *
+ * Server commands.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_server_stop( L4VMnet_client_adapter_t *adapter )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+    unsigned long irq_flags;
+
+    local_irq_save(irq_flags);
+    IVMnet_Control_stop( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+}
+
+static void
+L4VMnet_server_start( L4VMnet_client_adapter_t *adapter )
+{
+    CORBA_Environment ipc_env = idl4_default_environment;
+    unsigned long irq_flags;
+
+    local_irq_save(irq_flags);
+    IVMnet_Control_start( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+
+    // TODO: check for error
+    dprintk( 1, PREFIX "started the server connection.\n" );
+}
+
+/****************************************************************************
+ *
+ * Server connection functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_reset_ring( L4VMnet_desc_ring_t *ring,
+	L4VMnet_shadow_t *shadow )
+{
+    L4_Word16_t i;
+
+    for( i = 0; i < ring->cnt; i++ )
+    {
+	if( !ring->desc_ring[i].status.X.fragment && shadow[i].skb )
+	    dev_kfree_skb_any( shadow[i].skb );
+    }
+
+    memset( ring->desc_ring, 0, ring->cnt * sizeof(IVMnet_ring_descriptor_t));
+    memset( shadow, 0, ring->cnt * sizeof(L4VMnet_shadow_t) );
+}
+
+static int
+L4VMnet_open( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4_Fpage_t rcv_window;
+    idl4_fpage_t idl4_mapping, idl4_server_mapping;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    int err, line = __LINE__;
+    int i;
+    L4_Word_t log2size, size, size2, window_base;
+
+    if( adapter->connected == 1 )
+    {
+	L4VMnet_server_start( adapter );
+	netif_wake_queue( netdev );
+	return 0;
+    }
+
+    /* Try to obtain a LAN address if we still need one.
+     */
+    if( adapter->lanaddress_valid == FALSE )
+    {
+	err = L4VMnet_allocate_lan_address( netdev->dev_addr );
+	if( err )
+	{
+	    line = __LINE__;
+	    goto err_lan_address;
+	}
+	adapter->lanaddress_valid = TRUE;
+    }
+
+    // Locate the network server.
+    err = L4VM_server_locate( UUID_IVMnet, &adapter->server_tid );
+    if( err != 0 )
+    {
+	line = __LINE__;
+	goto err_locate_server;
+    }
+    dprintk( 2, PREFIX "server tid %lx\n", adapter->server_tid.raw );
+
+    /* Prepare the send ring.
+     */
+    adapter->send_ring.cnt = IVMnet_snd_descriptor_ring_cnt;
+    adapter->send_ring.start_free = 0;
+    adapter->send_ring.start_dirty = 0;
+    adapter->send_shadow = kmalloc( adapter->send_ring.cnt * sizeof(L4VMnet_shadow_t), GFP_KERNEL );
+    if( adapter->send_shadow == NULL )
+    {
+	err = -ENOMEM; line = __LINE__;
+	goto err_shadow_alloc;
+    }
+
+    /* Prepare the receive ring.
+     */
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].rcv_ring.cnt = IVMnet_rcv_buffer_cnt * 4;
+	adapter->rcv_group[i].rcv_ring.start_free = 0;
+	adapter->rcv_group[i].rcv_ring.start_dirty = 0;
+	adapter->rcv_group[i].rcv_ring.desc_ring = NULL;
+	adapter->rcv_group[i].rcv_shadow = NULL;
+    }
+
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].rcv_shadow = kmalloc( adapter->rcv_group[i].rcv_ring.cnt * sizeof(L4VMnet_shadow_t), GFP_KERNEL );
+	adapter->rcv_group[i].rcv_ring.desc_ring = kmalloc( adapter->rcv_group[i].rcv_ring.cnt * sizeof(IVMnet_ring_descriptor_t), GFP_KERNEL );
+	if( !adapter->rcv_group[i].rcv_shadow || !adapter->rcv_group[i].rcv_ring.desc_ring )
+	{
+	    err = -ENOMEM; line = __LINE__;
+	    goto err_rcv_alloc;
+	}
+    }
+
+    // Allocate an oversized shared window region, to adjust for alignment.
+    // We receive two fpages in the area.  We will align to the largest fpage.
+    size  = L4_Size( L4_Fpage(0,sizeof(IVMnet_client_shared_t)) );
+    size2 = L4_Size( L4_Fpage(0,sizeof(IVMnet_server_shared_t)) );
+    log2size = L4_SizeLog2( L4_Fpage(0, size+size2) );
+    err = L4VM_fpage_vmarea_get( log2size, &adapter->shared_window );
+    if( err < 0 )
+    {
+	line = __LINE__;
+	goto err_get_vm_area;
+    }
+    else
+	rcv_window = adapter->shared_window.fpage;
+    ASSERT( L4_Address(adapter->shared_window.fpage) >=
+	    (L4_Word_t)adapter->shared_window.ioremap_addr );
+    dprintk( 2, KERN_INFO PREFIX "receive window at %p, size %lu "
+	    "(padding starts at %p)\n",
+	    (void *)L4_Address(adapter->shared_window.fpage),
+	    L4_Size(adapter->shared_window.fpage),
+	    adapter->shared_window.ioremap_addr );
+
+    // Attach to the server, and request the shared mapping area.
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    idl4_set_rcv_window( &ipc_env, rcv_window );
+    IVMnet_Control_attach( adapter->server_tid, "eth1", netdev->dev_addr, &adapter->ivmnet_handle, &idl4_mapping, &idl4_server_mapping, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	err = -EIO; line = __LINE__;
+	goto err_server_register;
+    }
+
+    dprintk( 1, PREFIX "shared region at base 0x%x, offset 0x%lx, "
+	    "size %ld\n",
+	    idl4_fpage_get_base(idl4_mapping),
+	    L4_Address(idl4_fpage_get_page(idl4_mapping)),
+	    L4_Size(idl4_fpage_get_page(idl4_mapping)) );
+    dprintk( 1, PREFIX "server status region at base 0x%x, offset 0x%lx, "
+	    "size %ld\n",
+	    idl4_fpage_get_base(idl4_server_mapping),
+	    L4_Address(idl4_fpage_get_page(idl4_server_mapping)),
+	    L4_Size(idl4_fpage_get_page(idl4_server_mapping)) );
+
+    // Finish up initialization.
+    window_base = L4_Address( rcv_window );
+    adapter->client_shared = (IVMnet_client_shared_t *)
+	(window_base + idl4_fpage_get_base(idl4_mapping));
+    adapter->client_shared->client_irq_tid = L4_nilthread;
+    adapter->client_shared->client_irq_pending = 0;
+    adapter->send_ring.desc_ring = adapter->client_shared->snd_desc_ring;
+    adapter->client_shared->receiver_cnt = 0;
+
+    adapter->server_status = (IVMnet_server_shared_t *)
+	(window_base + idl4_fpage_get_base(idl4_server_mapping));
+
+    // Zero the send rings.
+    memset( adapter->send_ring.desc_ring, 0, adapter->send_ring.cnt * sizeof(IVMnet_ring_descriptor_t) );
+    memset( adapter->send_shadow, 0, adapter->send_ring.cnt * sizeof(L4VMnet_shadow_t) );
+
+    // Zero the receive rings.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	memset( adapter->rcv_group[i].rcv_ring.desc_ring, 0, adapter->rcv_group[i].rcv_ring.cnt * sizeof(IVMnet_ring_descriptor_t) );
+	memset( adapter->rcv_group[i].rcv_shadow, 0, adapter->rcv_group[i].rcv_ring.cnt * sizeof(L4VMnet_shadow_t) );
+    }
+
+    // More IRQ related initialization.
+    adapter->irq_pending = FALSE;
+
+    // Init the receiver threads.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	adapter->rcv_group[i].group_no = i;
+	adapter->rcv_group[i].dev_tid = L4_nilthread;
+	adapter->rcv_group[i].status = 0;
+	adapter->rcv_group[i].mask = 0;
+    }
+
+    // Allocate a virtual interrupt.
+    if( L4VMnet_irq > NR_IRQS )
+    {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	goto err_vmpic_reserve;
+    }
+    l4ka_wedge_add_virtual_irq( L4VMnet_irq );
+    err = request_irq( L4VMnet_irq, L4VMnet_irq_handler, 0, 
+	    netdev->name, netdev );
+    if( err < 0 )
+    {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Tell the server our irq tid.
+    adapter->client_shared->client_irq_tid = get_vcpu()->irq_gtid;
+
+    // Start the receiver threads.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	// Prepare parameters for the new thread.
+	L4VMnet_rcv_thread_params_t params;
+	params.adapter = adapter;
+	params.group = &adapter->rcv_group[i];
+	// Start the thread.
+	adapter->rcv_group[i].dev_tid = L4VM_thread_create( GFP_KERNEL,
+		L4VMnet_rcv_thread, l4ka_wedge_get_irq_prio(), 
+		0, &params, sizeof(params) );
+	if( L4_IsNilThread(adapter->rcv_group[i].dev_tid) )
+	{
+	    printk( KERN_ERR PREFIX "failed to start a receiver thread\n" );
+	    continue;
+	}
+
+	// Tell the server about our receiver threads.
+	adapter->client_shared->receiver_tids[ adapter->client_shared->receiver_cnt ] = adapter->rcv_group[i].dev_tid;
+	adapter->client_shared->receiver_cnt++;
+    }
+
+    // Tell the server that we are ready to handle packets.
+    adapter->connected = 1;
+    L4VMnet_server_start( adapter );
+    netif_wake_queue( netdev );
+
+    // Enable the interrupts.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	adapter->rcv_group[i].mask = ~0;
+
+    return 0;
+
+err_request_irq:
+err_vmpic_reserve:
+err_server_register:
+    L4VM_fpage_vmarea_release( &adapter->shared_window );
+err_get_vm_area:
+    kfree( adapter->send_shadow );
+err_rcv_alloc:
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	if( adapter->rcv_group[i].rcv_shadow )
+	    kfree( adapter->rcv_group[i].rcv_shadow );
+	if( adapter->rcv_group[i].rcv_ring.desc_ring )
+	    kfree( adapter->rcv_group[i].rcv_ring.desc_ring );
+    }
+err_shadow_alloc:
+err_locate_server:
+err_lan_address:
+    printk( KERN_ERR PREFIX "configuration error (%s:%d)\n", __FILE__, line );
+    return err;
+}
+
+static int
+L4VMnet_close( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    CORBA_Environment ipc_env;
+    unsigned long irq_flags;
+    int i;
+
+    // Stop the server, so that we can clean packets.
+    netif_stop_queue( netdev );
+    L4VMnet_server_stop( adapter );
+
+    // Preserve our server connection.
+    return 0;
+
+    // TODO: coordinate with the device threads!  They are waiting
+    // on these buffers!!
+
+    // Drop all packets in the send descriptor ring.
+    L4VMnet_reset_ring( &adapter->send_ring, adapter->send_shadow );
+
+    // Drop all packets in the receive descriptor rings.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	L4VMnet_reset_ring( &adapter->rcv_group[i].rcv_ring,
+		adapter->rcv_group[i].rcv_shadow );
+
+    // Disconnect from the server.
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    IVMnet_Control_detach( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_ERR PREFIX "server doesn't acknowledge our connection\n" );
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	printk( KERN_ERR PREFIX "unable to disconnect from server\n" );
+	return -EIO;
+    }
+
+    // Free receiver stuff.
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+    {
+	L4VM_thread_delete( adapter->rcv_group[i].dev_tid );
+	kfree( adapter->rcv_group[i].rcv_shadow );
+	kfree( adapter->rcv_group[i].rcv_ring.desc_ring );
+    }
+
+    // Unregister the IRQ handler.
+    // TODO: release the IRQ handler.
+
+    // Free resources.
+    L4VM_fpage_vmarea_release( &adapter->shared_window );
+    kfree( adapter->send_shadow );
+
+    adapter->connected = 0;
+    return 0;
+}
+
+static struct net_device_stats *
+L4VMnet_get_stats( struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+
+    if( NULL != adapter && FALSE != adapter->connected ) {
+	CORBA_Environment ipc_env;
+	unsigned long irq_flags;
+
+	ipc_env = idl4_default_environment;
+	local_irq_save(irq_flags);
+	IVMnet_Control_update_stats( adapter->server_tid, adapter->ivmnet_handle, &ipc_env );
+	local_irq_restore(irq_flags);
+
+	if( ipc_env._major == CORBA_NO_EXCEPTION ) {
+	    return (struct net_device_stats *)&(adapter->server_status->stats);
+	} else if( ipc_env._major == CORBA_USER_EXCEPTION ) {
+	    printk( KERN_ERR PREFIX "exception %d while requesting stats update from server\n",
+		    ipc_env._minor );
+	} else {
+	    printk( KERN_ERR PREFIX "unable to request stats update from server\n" );
+	}
+
+	L4_KDB_Enter();
+	CORBA_exception_free(&ipc_env);
+    }
+
+    return NULL;
+}
+
+/****************************************************************************
+ *
+ * Packet send functions.
+ *
+ ****************************************************************************/
+
+static void
+L4VMnet_clean_send_ring( L4VMnet_client_adapter_t *adapter )
+{
+    IVMnet_ring_descriptor_t *desc;
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    L4_Word16_t cleaned = 0;
+
+    while( 1 )
+    {
+	desc = &ring->desc_ring[ ring->start_dirty ];
+	if( desc->status.X.server_owned )
+	    break;
+
+	if( adapter->send_shadow[ ring->start_dirty ].skb == NULL )
+	{
+	    ASSERT( ring->start_dirty == ring->start_free );
+	    break;
+	}
+
+	if( !desc->status.X.fragment )
+	    dev_kfree_skb_any( adapter->send_shadow[ ring->start_dirty ].skb );
+	adapter->send_shadow[ ring->start_dirty ].skb = NULL;
+	desc->buffer = 0;
+	desc->buffer_len = 0;
+
+	ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	cleaned++;
+    }
+
+    // Wake the queue *after* we finish playing with the ring descriptors,
+    // so that we avoid concurrent access to this function!
+    if( cleaned && netif_queue_stopped(adapter->dev) )
+    {
+	netif_wake_queue( adapter->dev );
+	dprintk( 3, PREFIX "rewaking packet queue\n" );
+    }
+    else if( netif_queue_stopped(adapter->dev) )
+    {
+//	dprintk( 3, PREFIX "send packet queue jammed\n" );
+    }
+}
+
+static void
+L4VMnet_trigger_send( L4VMnet_client_adapter_t *adapter )
+{
+    L4_MsgTag_t msg_tag, result_tag;
+
+    adapter->server_status->irq_status |= 1;
+    if( adapter->server_status->irq_pending )
+	return;
+
+    adapter->server_status->irq_pending = TRUE;
+
+    {
+
+	unsigned long flags;
+	local_irq_save(flags);
+	{
+	    L4_ThreadId_t from_tid;
+	    msg_tag.raw = 0; msg_tag.X.label = L4_TAG_IRQ; msg_tag.X.u = 1;
+	    L4_Set_MsgTag( msg_tag );
+	    L4_LoadMR( 1, adapter->client_shared->server_irq );
+	    result_tag = L4_Reply( adapter->client_shared->server_main_tid );
+	    if( !L4_IpcSucceeded(result_tag) )
+	    {
+		L4_Set_MsgTag( msg_tag );
+		L4_Ipc( adapter->client_shared->server_irq_tid, L4_nilthread,
+	    		L4_Timeouts(L4_Never, L4_Never), &from_tid);
+	    }
+	}
+	local_irq_restore(flags);
+    }
+}
+
+static int
+L4VMnet_xmit_available(
+	L4VMnet_client_adapter_t *adapter,
+	L4_Word16_t fragments )
+{
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    IVMnet_ring_descriptor_t *desc;
+    int dirty = FALSE;
+    int frag, i;
+
+    ASSERT( fragments < ring->cnt );
+
+    if( L4VMnet_ring_available(ring) <= fragments )
+    {
+	i = ring->start_dirty;
+	for( frag = 0; frag < fragments; frag++ )
+	{
+	    desc = &ring->desc_ring[ i ];
+	    if( desc->status.X.server_owned )
+		return FALSE;
+	    dirty = TRUE;
+	    i = (i + 1) % ring->cnt;
+	}
+    }
+
+    if( dirty )
+    {
+	L4VMnet_clean_send_ring( adapter );
+	ASSERT( adapter->send_shadow[ ring->start_free ].skb == NULL );
+    }
+
+    return TRUE;
+}
+
+static int
+L4VMnet_xmit_frame( struct sk_buff *skb, struct net_device *netdev )
+{
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4VMnet_desc_ring_t *ring = &adapter->send_ring;
+    IVMnet_ring_descriptor_t *desc = &ring->desc_ring[ ring->start_free ];
+    IVMnet_ring_descriptor_t *frag_desc;
+    L4_Word16_t fragments, f;
+    unsigned long flags;
+
+    fragments = 1 + skb_shinfo(skb)->nr_frags;
+    if( unlikely(fragments >= ring->cnt) )
+    {
+	// Too many fragments in the packet, so drop the packet.
+	dev_kfree_skb_any( skb );
+	return 0;
+    }
+
+    local_irq_save(flags);
+    if( !L4VMnet_xmit_available(adapter, fragments) )
+    {
+	// We are still waiting for the server to empty the send queue.
+	netif_stop_queue( netdev );
+	local_irq_restore(flags);
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+	dprintk( 3, PREFIX "send queue full\n" );
+	return 1;
+    }
+    local_irq_restore(flags);
+
+    dprintk( 4, PREFIX "skb protocol %d, pkt_type %d\n",
+	    skb->protocol, skb->pkt_type );
+
+    ASSERT( 0 == desc->status.X.server_owned );
+    ASSERT( NULL == adapter->send_shadow[ ring->start_free ].skb );
+    desc->status.raw	= 0;
+    desc->buffer	= virt_to_phys( skb->data );
+    desc->buffer_len	= skb_headlen( skb );
+    desc->pkt_type	= skb->pkt_type;
+    desc->protocol	= skb->protocol;
+    adapter->send_shadow[ ring->start_free ].skb = skb;
+    // Where the device must start the checksum, relative to the buffer.
+    desc->csum.tx.start = skb->h.raw - skb->data;
+    // Where the device must store the checksum, relative to the buffer.
+    desc->csum.tx.offset = desc->csum.tx.start + skb->csum;
+    if( skb->ip_summed == CHECKSUM_HW )
+	desc->status.X.csum = 1;
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    dprintk( 4, PREFIX "send pkt @ virt %lx, bus %lx, size %d\n",
+	    skb->data, desc->buffer, desc->buffer_len );
+
+    // Queue packet fragments, for scatter/gather.
+    frag_desc = desc;
+    for( f = 0; f < skb_shinfo(skb)->nr_frags; f++ )
+    {
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
+
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %d --> %lx\n",
+		f,
+		(L4_Word_t)frag->page, (L4_Word_t)frag->page_offset,
+		frag->size, L4VMnet_fragment_buffer(frag));
+
+	// Mark the *prior* descriptor as a fragment.  The last descriptor
+	// in the sequence is not marked.
+	frag_desc->status.X.fragment = 1;
+
+	frag_desc = &ring->desc_ring[ ring->start_free ];
+	ASSERT( 0 == frag_desc->status.X.server_owned );
+	ASSERT( NULL == adapter->send_shadow[ ring->start_free ].skb );
+
+	frag_desc->status.raw	= 0;
+	frag_desc->buffer	= L4VMnet_fragment_buffer(frag);
+	frag_desc->buffer_len	= frag->size;
+	adapter->send_shadow[ ring->start_free ].skb = skb;
+	frag_desc->status.X.server_owned = 1;
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+    }
+
+    // Assign ownership of the packet sequence to the server.  Do this last!
+    desc->status.X.server_owned = 1;
+
+    // Deal with sending the packets.
+    netdev->trans_start = jiffies;
+    dprintk( 4, PREFIX "send %u fragments\n", fragments );
+
+    tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+
+    return 0;
+}
+
+static void
+L4VMnet_flush_tasklet_handler( unsigned long unused )
+{
+    L4VMnet_client_adapter_t *adapter = L4VMnet_adapter_list;
+
+    L4VMnet_trigger_send( adapter );
+}
+
+/****************************************************************************
+ *
+ * Packet receive functions.
+ *
+ ****************************************************************************/
+
+static void L4VMnet_wake_driver_thread(
+	L4VMnet_rcv_group_t *rcv_group,
+	L4VMnet_client_adapter_t *adapter )
+{
+    L4_MsgTag_t tag;
+    unsigned long flags;
+
+    local_irq_save(flags);
+    tag.raw = 0;
+    L4_Set_MsgTag( tag );
+    L4_Reply( rcv_group->dev_tid );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_walk_rcv_ring(
+	int wake,
+	L4VMnet_rcv_group_t *group,
+	L4VMnet_client_adapter_t *adapter )
+{
+    volatile IVMnet_ring_descriptor_t *desc;
+    L4VMnet_desc_ring_t *ring = &group->rcv_ring;
+    struct sk_buff *skb;
+    L4_Word16_t pkt_len = IVMnet_rcv_buffer_size;
+    L4_Word16_t reserve_len = 2;
+    int total_transferred = 0;
+
+    // Descriptors can be in several states:
+    //  1. Zero buffer len: either the buffer doesn't exist, or it was a
+    //     dropped packet and the buffer exists.
+    //  2. Valid packet that should be handed off to Linux.
+    //  3. Packet still belongs to the driver thread.
+
+    dprintk( 4, PREFIX "start_dirty = %d\n", ring->start_dirty );
+    while( 1 )
+    {
+	desc = &ring->desc_ring[ ring->start_dirty ];
+	skb = group->rcv_shadow[ ring->start_dirty ].skb;
+
+	if( desc->status.X.server_owned )
+	    break;
+
+	if( likely((desc->buffer_len > 0) && skb) )
+	{
+	    // Give the skb to the Linux network stack.
+	    dprintk( 4, PREFIX "injesting packet\n" );
+	    skb_put( skb, desc->buffer_len );
+	    // Assume that the DD/OS performed the checksum.
+	    skb->ip_summed = CHECKSUM_UNNECESSARY;
+	    skb->protocol = eth_type_trans( skb, adapter->dev );
+	    netif_rx( skb );
+	    adapter->dev->last_rx = jiffies;
+	    skb = NULL;  // Release the skb.
+	}
+
+	if( skb == NULL )
+	{
+	    // Allocate a new skb.
+	    skb = dev_alloc_skb( pkt_len + reserve_len );
+	    group->rcv_shadow[ ring->start_dirty ].skb = skb;
+
+	    if( unlikely(skb == NULL) )
+	    {
+		desc->buffer = 0;
+		desc->buffer_len = 0;
+	    }
+	    else
+	    {
+		// Align the buffer 2 beyond a 16-byte boundary, to result
+		// in a 16-byte aligned IP header after the 14-byte MAC header
+		// is removed.
+		skb_reserve( skb, reserve_len );
+		skb->dev = adapter->dev;
+	    }
+	}
+
+	if( likely(skb != NULL) )
+	{
+	    // This could be a reused buffer, or a new buffer.
+	    desc->buffer = (L4_Word_t)skb->data;
+	    desc->buffer_len = pkt_len;
+	    // Transfer ownership to the device.
+	    desc->status.raw = 0;
+	    desc->status.X.server_owned = 1;
+	    total_transferred++;
+
+	    ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	}
+	else
+	    break; // Insufficient memory!
+    }
+
+    // TODO: deal with insufficient memory conditions.
+
+    dprintk( 3, PREFIX "allocated %d receive buffers.\n", total_transferred );
+    if( total_transferred && wake )
+	L4VMnet_wake_driver_thread( group, adapter );
+}
+
+static irqreturn_t
+L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    struct net_device *netdev = data;
+    L4VMnet_client_adapter_t *adapter = netdev->priv;
+    L4VMnet_rcv_group_t *rcv_group;
+    int i, finished, wake, cnt = 0;
+
+    do
+    {
+	// No one needs to deliver IRQ messages.  They would be
+	// superfluous.
+	adapter->irq_pending = TRUE;
+	finished = TRUE;
+
+	// Process receive interrupts.
+	for( i = 0; i < NR_RCV_GROUP; i++ )
+	{
+	    unsigned events;
+	    rcv_group = &adapter->rcv_group[i];
+
+	    do {
+		events = rcv_group->status;
+	    } while( cmpxchg(&rcv_group->status, events, 0) != events );
+
+	    if( !events )
+		continue;
+
+	    finished = FALSE;
+
+	    wake = 0 != (events & L4VMNET_IRQ_RING_EMPTY);
+	    dprintk( 3, PREFIX "irq handler, group %d, wake %d: 0x%x\n",
+		    i, wake, events );
+	    L4VMnet_walk_rcv_ring( wake, rcv_group, adapter );
+	}
+
+	// Process transmit related problems.
+	if( netif_queue_stopped(adapter->dev) )
+	    L4VMnet_clean_send_ring( adapter );
+
+	// Enable interrupt message delivery.
+	adapter->irq_pending = FALSE;
+	cnt++;
+	if( cnt > 100 )
+	    dprintk( 1, PREFIX "too many interrupts.\n" );
+    } while( !finished );
+
+    return IRQ_HANDLED;
+}
+
+static int
+L4VMnet_irq_pending( void *data )
+{
+    L4VMnet_client_adapter_t *adapter = (L4VMnet_client_adapter_t *)data;
+    int i;
+
+    for( i = 0; i < NR_RCV_GROUP; i++ )
+	if( (adapter->rcv_group[i].status & adapter->rcv_group[i].mask) != 0 )
+	{
+	    dprintk( 4, PREFIX "irq pending.\n" );
+	    return TRUE;
+	}
+    return FALSE;
+}
+
+/****************************************************************************
+ *
+ * Module-level functions.
+ *
+ ****************************************************************************/
+
+static int __init
+L4VMnet_client_init_module( void )
+{
+    struct net_device *netdev;
+    L4VMnet_client_adapter_t *adapter;
+    int err;
+
+    netdev = alloc_etherdev( sizeof(L4VMnet_client_adapter_t) );
+    if( !netdev )
+	return -ENOMEM;
+
+    SET_MODULE_OWNER( netdev );
+
+    /* Adapter should already be zeroed. */
+    adapter = (L4VMnet_client_adapter_t *)netdev->priv;
+    adapter->dev = netdev;
+    adapter->connected = 0;
+    L4VMnet_adapter_list = adapter;
+
+    /* Initialize the netdev. */
+    netdev->open = L4VMnet_open;
+    netdev->stop = L4VMnet_close;
+    netdev->hard_start_xmit = L4VMnet_xmit_frame;
+    netdev->features = NETIF_F_SG | NETIF_F_HW_CSUM | NETIF_F_FRAGLIST;
+    netdev->get_stats = L4VMnet_get_stats;
+
+    /* Try to obtain a LAN address.  We can retry later if necessary. */
+    memset( netdev->dev_addr, 0, ETH_ALEN );
+    adapter->lanaddress_valid =
+	( 0 == L4VMnet_allocate_lan_address(netdev->dev_addr) );
+    printk( KERN_INFO PREFIX "LAN address: " );
+    L4VMnet_print_lan_address( netdev->dev_addr );
+    printk( "\n" );
+
+    /* Register the netdev. */
+    err = register_netdev( netdev );
+    if( err )
+	goto err_register_netdev;
+    netif_stop_queue( netdev );
+
+    printk( KERN_INFO PREFIX "L4VMnet client device %s\n", netdev->name );
+
+    return 0;
+
+err_register_netdev:
+    kfree( netdev );
+    return err;
+}
+
+static void __exit
+L4VMnet_client_exit_module( void )
+{
+    if( L4VMnet_adapter_list )
+    {
+	unregister_netdev( L4VMnet_adapter_list->dev );
+	kfree( L4VMnet_adapter_list->dev );
+	L4VMnet_adapter_list = NULL;
+    }
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4 network client driver" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM net" );
+MODULE_VERSION("slink");
+
+module_init( L4VMnet_client_init_module );
+module_exit( L4VMnet_client_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/client.h linux-2.6.9/afterburn/drivers/net/client.h
--- linux-2.6.9.src/afterburn/drivers/net/client.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/client.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,75 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/client.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__NET__CLIENT_H__
+#define __L4KA_DRIVERS__NET__CLIENT_H__
+
+#include "L4VMnet_idl_client.h"
+#include "net.h"
+
+#define NR_RCV_GROUP	1
+
+#define L4VMNET_IRQ_PKTS_AVAIL	1
+#define L4VMNET_IRQ_RING_EMPTY	2
+
+typedef struct
+{
+    int group_no;
+    L4_ThreadId_t dev_tid;
+    volatile unsigned status;
+    volatile unsigned mask;
+    L4VMnet_desc_ring_t rcv_ring;
+    L4VMnet_shadow_t *rcv_shadow;
+} L4VMnet_rcv_group_t;
+
+typedef struct
+{
+    /* Data valid for an unopened device.
+     */
+    struct net_device *dev;
+    int connected;
+    int lanaddress_valid;
+
+    /* Data valid after a connection is successfully opened to the server.
+     */
+    L4_ThreadId_t server_tid;	/* Server thread ID. */
+    IVMnet_handle_t ivmnet_handle; /* Handle to the server. */
+    int irq_pending;
+
+    L4VM_alligned_vmarea_t shared_window;
+    IVMnet_client_shared_t *client_shared;
+    IVMnet_server_shared_t *server_status;
+
+    L4VMnet_desc_ring_t send_ring;
+    L4VMnet_shadow_t *send_shadow;
+
+    L4VMnet_rcv_group_t rcv_group[NR_RCV_GROUP];
+} L4VMnet_client_adapter_t;
+
+#endif	/* __LINUXNET__L4VMNET_CLIENT_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/net/L4VMnet_idl_reply.h linux-2.6.9/afterburn/drivers/net/L4VMnet_idl_reply.h
--- linux-2.6.9.src/afterburn/drivers/net/L4VMnet_idl_reply.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/L4VMnet_idl_reply.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,254 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/linux/net/L4VMnet_idl_reply.h
+ * Description:	Hand-written deceiving IPC stubs; to be replaced with IDL4
+ * 		automation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__L4VMNET_IDL_REPLY_H__
+#define __L4KA_DRIVERS__L4VMNET_IDL_REPLY_H__
+
+static inline void IVMnet_Control_attach_propagate_reply(CORBA_Object _client, IVMnet_handle_t *handle, idl4_fpage_t *shared_window, idl4_fpage_t *server_status, idl4_server_environment *_env)
+
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      IVMnet_handle_t handle;
+      idl4_mapitem shared_window;
+      idl4_mapitem server_status;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      _par->_out.handle = *handle;
+      _par->_out.shared_window = *shared_window;
+      _par->_out.server_status = *server_status;
+      msgtag = (1 << 12)+(1+(4<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMnet_Control_reattach_propagate_reply(CORBA_Object _client, idl4_fpage_t *shared_window, idl4_fpage_t *server_status, idl4_server_environment *_env)
+
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+      idl4_mapitem shared_window;
+      idl4_mapitem server_status;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      _par->_out.shared_window = *shared_window;
+      _par->_out.server_status = *server_status;
+      msgtag = (1 << 12)+(0+(4<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+
+static inline void IVMnet_Control_start_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0+(0<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMnet_Control_stop_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0+(0<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMnet_Control_detach_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0+(0<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMnet_Control_update_stats_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0+(0<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+static inline void IVMnet_Control_register_dp83820_tx_ring_propagate_reply(CORBA_Object _client, idl4_server_environment *_env)
+{
+  long dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long _msgtag;
+    } _out;
+  } *_par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+  {
+      msgtag = (1 << 12)+(0+(0<<6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+#endif	/* __L4KA_DRIVERS__L4VMNET_IDL_REPLY_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/net/Makefile linux-2.6.9/afterburn/drivers/net/Makefile
--- linux-2.6.9.src/afterburn/drivers/net/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/Makefile	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,9 @@
+
+include $(srctree)/afterburn/drivers/Makesupport
+
+obj-$(CONFIG_AFTERBURN_DRIVERS_NET_SERVER) += l4ka_net_server.o
+obj-$(CONFIG_AFTERBURN_DRIVERS_NET_CLIENT) += l4ka_net_client.o
+l4ka_net_server-y := net.o server.o
+l4ka_net_client-y := net.o client.o
+
+$(addprefix $(obj)/,$(obj-m)): $(wedge_symbols)
diff -Naur linux-2.6.9.src/afterburn/drivers/net/net.c linux-2.6.9/afterburn/drivers/net/net.c
--- linux-2.6.9.src/afterburn/drivers/net/net.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/net.c	2007-07-03 16:22:40.000000000 +0200
@@ -0,0 +1,89 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/net.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/version.h>
+
+#include "L4VMnet_idl_client.h"
+#include "net.h"
+#include "lanaddress_idl_client.h"
+
+#if !defined(CONFIG_DRIVERS_NET_OPTIMIZE)
+int L4VMnet_debug_level = 2;
+MODULE_PARM( L4VMnet_debug_level, "i" );
+#endif
+
+int L4VMnet_allocate_lan_address( char lanaddress[ETH_ALEN] )
+{
+    L4_ThreadId_t lanaddress_tid;
+    CORBA_Environment ipc_env;
+    lanaddress_t reply_addr;
+    unsigned long irq_flags;
+    int err;
+
+    err = L4VM_server_locate( UUID_ILanAddress, &lanaddress_tid );
+    if( err != 0 )
+	return err;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    ILanAddress_generate_lan_address( lanaddress_tid, &reply_addr, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -EIO; 
+    }
+
+    memcpy( lanaddress, reply_addr.raw, ETH_ALEN );
+    return 0;
+}
+
+void L4VMnet_print_lan_address( unsigned char *lanaddr )
+{
+    int i;
+    for( i = 0; i < ETH_ALEN; i++ )
+    {
+	if(i) printk(":");
+	printk( "%02x", lanaddr[i] );
+    }
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/net.c~ linux-2.6.9/afterburn/drivers/net/net.c~
--- linux-2.6.9.src/afterburn/drivers/net/net.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/net.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,89 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/net.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/version.h>
+
+#include "L4VMnet_idl_client.h"
+#include "net.h"
+#include "lanaddress_idl_client.h"
+
+#if !defined(CONFIG_DRIVERS_NET_OPTIMIZE)
+int L4VMnet_debug_level = 4;
+MODULE_PARM( L4VMnet_debug_level, "i" );
+#endif
+
+int L4VMnet_allocate_lan_address( char lanaddress[ETH_ALEN] )
+{
+    L4_ThreadId_t lanaddress_tid;
+    CORBA_Environment ipc_env;
+    lanaddress_t reply_addr;
+    unsigned long irq_flags;
+    int err;
+
+    err = L4VM_server_locate( UUID_ILanAddress, &lanaddress_tid );
+    if( err != 0 )
+	return err;
+
+    ipc_env = idl4_default_environment;
+    local_irq_save(irq_flags);
+    ILanAddress_generate_lan_address( lanaddress_tid, &reply_addr, &ipc_env );
+    local_irq_restore(irq_flags);
+    if( ipc_env._major == CORBA_USER_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -ENODEV;
+    }
+    else if( ipc_env._major != CORBA_NO_EXCEPTION )
+    {
+	CORBA_exception_free(&ipc_env);
+	return -EIO; 
+    }
+
+    memcpy( lanaddress, reply_addr.raw, ETH_ALEN );
+    return 0;
+}
+
+void L4VMnet_print_lan_address( unsigned char *lanaddr )
+{
+    int i;
+    for( i = 0; i < ETH_ALEN; i++ )
+    {
+	if(i) printk(":");
+	printk( "%02x", lanaddr[i] );
+    }
+}
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/net.h linux-2.6.9/afterburn/drivers/net/net.h
--- linux-2.6.9.src/afterburn/drivers/net/net.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/net.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,128 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/net.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__NET__NET_H__
+#define __L4KA_DRIVERS__NET__NET_H__
+
+#include <l4/types.h>
+#include <l4/kdebug.h>
+
+#include <linux/config.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <net/sock.h>
+#include <net/pkt_sched.h>
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+
+#include <glue/vmserver.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+
+#if IDL4_HEADER_REVISION < 20030403
+# error "Your version of IDL4 is too old.  Please upgrade to the latest."
+#endif
+
+#if defined(TRUE)
+#undef TRUE
+#endif
+#define TRUE	(1 == 1)
+
+#if defined(FALSE)
+#undef FALSE
+#endif
+
+#define FALSE	(1 == 0)
+
+#define RAW(a)	((void *)((a).raw))
+
+#if defined(AFTERBURN_DRIVERS_NET_OPTIMIZE)
+#define L4VMnet_debug_level	2
+
+#define PARANOID(a)		// Don't execute paranoid stuff.
+#define ASSERT(a)		// Don't execute asserts.
+
+#else
+
+extern int L4VMnet_debug_level;
+
+#define PARANOID(a)		a
+#define ASSERT(a)		do { if(!(a)) { printk( KERN_CRIT PREFIX "assert failure %s:%s:%d\n", __FILE__, __FUNCTION__, __LINE__); L4_KDB_Enter("assert"); } } while(0)
+
+#endif	/* AFTERBURN_DRIVERS_NET_OPTIMIZE */
+
+#define dprintk(n,a...) do { if(L4VMnet_debug_level >= (n)) printk(a); } while(0)
+
+
+#define L4VMNET_SKB_RING_LEN	256
+
+#define L4_TAG_IRQ	0x100
+
+typedef struct L4VMnet_skb_ring
+{
+    struct sk_buff *skb_ring[L4VMNET_SKB_RING_LEN];
+    L4_Word16_t cnt;
+    L4_Word16_t start_free;
+    L4_Word16_t start_dirty;
+} L4VMnet_skb_ring_t;
+
+extern inline L4_Word16_t L4VMnet_skb_ring_pending( L4VMnet_skb_ring_t *ring )
+{
+    return (ring->start_free + ring->cnt - ring->start_dirty) % ring->cnt;
+}
+
+typedef struct
+{
+    struct sk_buff *skb;
+} L4VMnet_shadow_t;
+
+typedef struct
+{
+    IVMnet_ring_descriptor_t *desc_ring;
+    L4_Word16_t cnt;
+    L4_Word16_t start_free;
+    L4_Word16_t start_dirty;
+} L4VMnet_desc_ring_t;
+
+extern inline L4_Word16_t
+L4VMnet_ring_available( L4VMnet_desc_ring_t *ring )
+{
+    return (ring->start_dirty + ring->cnt - (ring->start_free + 1)) % ring->cnt;
+}
+
+extern inline L4_Word_t
+L4VMnet_fragment_buffer( struct skb_frag_struct *frag )
+{
+    return (L4_Word_t)(frag->page - mem_map) * PAGE_SIZE + frag->page_offset;
+}
+
+extern int L4VMnet_allocate_lan_address( char lanaddress[ETH_ALEN] );
+extern void L4VMnet_print_lan_address( unsigned char *lanaddr );
+
+#endif	/* __L4KA_DRIVERS__NET__NET_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/net/server.c linux-2.6.9/afterburn/drivers/net/server.c
--- linux-2.6.9.src/afterburn/drivers/net/server.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/server.c	2007-07-03 16:22:40.000000000 +0200
@@ -0,0 +1,1922 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/server.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/kip.h>
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/spinlock.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/interrupt.h>
+
+#include <linux/if_bridge.h>
+#if defined(DO_LINUX_BRIDGE)
+#include <br_private.h>
+#endif
+
+#include <asm/io.h>
+
+#include <glue/thread.h>
+#include <glue/bottomhalf.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+#include <glue/wedge.h>
+
+#if defined(CONFIG_X86_IO_APIC)
+#include <acpi/acpi.h>
+#include <linux/acpi.h>
+#endif
+
+#if defined(PREFIX)
+#undef PREFIX
+#endif
+#define PREFIX "L4VMnet server: "
+
+
+#include "server.h"
+#include "lanaddress.h"
+
+#define TXDP (0x20/4)
+#define ISR  (0x10/4)
+#define IMR  (0x14/4)
+#define IER  (0x18/4)
+
+int L4VMnet_server_irq = 0;
+MODULE_PARM( L4VMnet_server_irq, "i" );
+
+L4VMnet_server_t L4VMnet_server = { 
+    L4_nilthread, L4_nilthread, L4_nilthread
+};
+
+static int L4VMnet_absorb_frame( struct sk_buff *skb );
+static void L4VMnet_flush_tasklet_handler( unsigned long unused );
+
+/*
+ * Our L4 clients hand us unreadable packets.  The packets are not mapped
+ * into the address space.  So no routing decisions by Linux are possible.
+ * This driver must perform the routing
+ * decisions.  It shouldn't be a problem in our experimentation.
+ *
+ * The driver must watch for notifier events: NETDEV_UP and
+ * NETDEV_DOWN.  Thus we can detect if our target device goes down.  We
+ * mustn't send a packet once a device goes down, or we'd give the target
+ * device driver unexpected behavior from Linux.  Likewise, if the target
+ * device driver is paused, we mustn't try to deliver packets.
+ */
+
+DECLARE_TASKLET( L4VMnet_flush_tasklet, L4VMnet_flush_tasklet_handler, 0 );
+
+/***************************************************************************
+ *
+ * Interface handle mapper.
+ *
+ ***************************************************************************/
+
+#define L4VMNET_MAX_CLIENTS	16
+
+static L4VMnet_client_info_t *L4VMnet_client_list[ L4VMNET_MAX_CLIENTS ];
+
+static atomic_t dirty_tx_pkt_cnt = ATOMIC_INIT(0);
+
+extern void
+L4VMnet_client_list_init( void )
+{
+    int i;
+
+    for( i = 0; i < L4VMNET_MAX_CLIENTS; i++ )
+	L4VMnet_client_list[i] = NULL;
+}
+
+#if 0
+static void
+L4VMnet_client_handle_free( IVMnet_handle_t handle )
+{
+    L4VMnet_client_list[ handle ] = NULL;
+}
+#endif
+
+extern int L4VMnet_client_handle_set(
+	IVMnet_handle_t handle,
+	L4VMnet_client_info_t *client )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    if( handle >= L4VMNET_MAX_CLIENTS )
+	return FALSE;
+
+    spin_lock( &lock );
+    {
+	if( L4VMnet_client_list[handle] != NULL )
+	{
+	    spin_unlock( &lock );
+	    return FALSE;
+	}
+	L4VMnet_client_list[ handle ] = client;
+    }
+    spin_unlock( &lock );
+
+    return TRUE;
+}
+
+extern inline L4VMnet_client_info_t *
+L4VMnet_client_handle_lookup( IVMnet_handle_t handle )
+{
+    if( handle < L4VMNET_MAX_CLIENTS )
+	return L4VMnet_client_list[ handle ];
+    return NULL;
+}
+
+/***************************************************************************
+ *
+ * Receiving packets.
+ *
+ ***************************************************************************/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,9)
+static int L4VMnet_bridge_handler( struct net_bridge_port *p, struct sk_buff **pskb )
+{
+    struct sk_buff *skb = *pskb;
+
+    dprintk( 4, PREFIX "snarfed packet\n" );
+
+    skb_push( skb, ETH_HLEN ); // Uncover the LAN address.
+    if( L4VMnet_absorb_frame(skb) )
+	return 1;
+
+    skb_pull( skb, ETH_HLEN ); // Cover the LAN address back up.
+    return 0;	// We didn't handle the packet.
+}
+#else
+static int L4VMnet_bridge_handler( struct sk_buff *skb )
+{
+    dprintk( 4, PREFIX "snarfed packet\n" );
+
+    skb_push( skb, ETH_HLEN ); // Uncover the LAN address.
+    if( L4VMnet_absorb_frame(skb) )
+	return 0;
+
+    skb_pull( skb, ETH_HLEN ); // Cover the LAN address back up.
+    return 1;	// We didn't handle the packet.
+}
+#endif
+/***************************************************************************
+ *
+ * Sending packets outbound from the client.
+ *
+ ***************************************************************************/
+
+/*
+ * When deallocating memory, Linux uses skb->head.
+ * When accessing the shared skb info (skb_shinfo), Linux uses skb->end.
+ * When transmitting data, Linux uses skb->data.
+ *
+ * skb->head points to the beginning of allocated space.
+ * skb->data points to the beginning of valid data.
+ * skb->tail points to the end of valid data.
+ * skb->end points to the maximum valid address for tail.
+ */
+
+typedef struct L4VMnet_skb_shadow
+{
+    struct sk_buff *skb;
+    L4VMnet_desc_ring_t *ring;
+    L4_Word16_t start_frag;
+    L4_Word16_t frag_count;
+} L4VMnet_skb_shadow_t;
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+
+static void
+L4VMnet_skb_destructor( struct sk_buff *skb )
+{
+    L4VMnet_skb_shadow_t *shadow = (L4VMnet_skb_shadow_t *)skb->head;
+    IVMnet_ring_descriptor_t *desc, *frag_desc;
+
+    ASSERT(shadow);
+    ASSERT(shadow->skb == skb);
+    ASSERT(shadow->start_frag < shadow->ring->cnt);
+
+    desc = &shadow->ring->desc_ring[ shadow->start_frag ];
+    ASSERT( desc->status.X.server_owned && desc->status.X.dropped );
+
+    // Handle the fragments first.  The last fragment in the list has
+    // its fragment bit cleared.
+    if( desc->status.X.fragment )
+    {
+	L4_Word16_t idx = (shadow->start_frag + 1) % shadow->ring->cnt;
+	while( shadow->frag_count )
+	{
+	    // Release the fragment descriptor back to the client.
+	    frag_desc = &shadow->ring->desc_ring[ idx ];
+	    ASSERT( frag_desc->status.X.server_owned );
+	    frag_desc->status.X.server_owned = 0;
+	    idx = (idx + 1) % shadow->ring->cnt;
+	    shadow->frag_count--;
+	}
+    }
+
+    // Handle the head fragment last.
+    desc->status.X.server_owned = 0;
+
+    // Clean-up the skb.
+    ASSERT( !skb_shinfo(skb)->frag_list );
+    skb_shinfo(skb)->nr_frags = 0;
+    skb->data = skb->head;
+    skb->tail = skb->data;
+    skb->h.raw = skb->data;
+    skb->len = 0;
+    skb->data_len = 0;
+
+    dprintk( 4, PREFIX "skb destructor\n" );
+}
+
+static struct sk_buff *
+L4VMnet_skb_wrap_packet( L4VMnet_client_info_t *client )
+{
+    // TODO: validate that buffers + fragments together create valid
+    // skb buffs.  Lots of overlapping state must be validated.
+    L4VMnet_desc_ring_t *ring;
+    IVMnet_ring_descriptor_t *desc, *frag_desc;
+    struct sk_buff *skb;
+    L4VMnet_skb_shadow_t *skb_shadow;
+    int fragment;
+
+    ring = &client->send_ring;
+
+    // Grab the first packet, and move to the next.
+    desc = &ring->desc_ring[ ring->start_free ];
+    if( !desc->status.X.server_owned )
+	return NULL;
+    if( unlikely(desc->status.X.dropped) )
+    {
+	dprintk( 2, KERN_INFO PREFIX "send wrap-around.\n" );
+	return NULL;
+    }
+
+    skb = alloc_skb( sizeof(L4VMnet_skb_shadow_t), GFP_ATOMIC );
+    if( unlikely(skb == NULL) )
+	return NULL;
+
+    skb_shadow = (L4VMnet_skb_shadow_t *)skb->head;
+    skb_shadow->ring = ring;
+    skb_shadow->start_frag = ring->start_free;
+    skb_shadow->frag_count = 0;
+    skb_shadow->skb = skb;
+
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    // TODO: validate that the packet lives in the client's address space.
+    ASSERT( desc->buffer && desc->buffer_len );
+    ASSERT( desc->buffer + client->client_space->bus_start < num_physpages * PAGE_SIZE );
+    skb->data = bus_to_virt(desc->buffer + client->client_space->bus_start);
+    skb->len  = desc->buffer_len;
+    ASSERT( skb->len && skb->data );
+    skb->tail = skb->data + skb->len;
+
+    skb->destructor = L4VMnet_skb_destructor;
+    skb->ip_summed  = CHECKSUM_NONE;
+    skb->protocol   = desc->protocol;
+    skb->pkt_type   = desc->pkt_type;
+
+    // Where the device must start the checksum.
+    skb->h.raw = desc->csum.tx.start + skb->data;
+    // Where the device must store the checksum.
+    skb->csum = desc->csum.tx.offset + skb->data - skb->h.raw;
+
+    if( likely(desc->status.X.csum) )
+    {
+	if( (skb->h.raw >= skb->data) && (skb->h.raw <= skb->tail) &&
+		((skb->csum + skb->h.raw) <= skb->tail) )
+	    skb->ip_summed = CHECKSUM_HW;
+	else
+	    dprintk( 2, PREFIX "bad hardware checksum request.\n" );
+    }
+
+    dprintk( 4, PREFIX "skb protocol %x, pkt_type %x, "
+	    "data %p (bus %p/%p)\n",
+	    skb->protocol, skb->pkt_type, skb->data, (void *)desc->buffer,
+	    (void *)virt_to_bus(skb->data) );
+
+    fragment = 0;
+    frag_desc = desc;
+    while( frag_desc->status.X.fragment )
+    {
+	L4_Word_t paddr;
+
+	if( unlikely(fragment >= MAX_SKB_FRAGS) )
+	    break;
+	frag_desc = &ring->desc_ring[ ring->start_free ];
+	if( unlikely(!frag_desc->status.X.server_owned) )
+	    break;
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+	skb_shinfo(skb)->frags[fragment].size = frag_desc->buffer_len;
+	skb_shinfo(skb)->frags[fragment].page_offset =
+	    (unsigned long)frag_desc->buffer & (PAGE_SIZE-1);
+	ASSERT(frag_desc->buffer);
+	ASSERT( frag_desc->buffer + client->client_space->bus_start < num_physpages * PAGE_SIZE );
+	paddr = virt_to_phys(bus_to_virt(frag_desc->buffer + client->client_space->bus_start));
+	skb_shinfo(skb)->frags[fragment].page = mem_map + (paddr >> PAGE_SHIFT);
+	ASSERT( paddr && frag_desc->buffer_len );
+	skb->len += frag_desc->buffer_len;
+	skb->data_len += frag_desc->buffer_len;
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %u\n",
+		fragment,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page_offset,
+		skb_shinfo(skb)->frags[fragment].size );
+	ASSERT( page_to_bus(skb_shinfo(skb)->frags[fragment].page) == (frag_desc->buffer & PAGE_MASK) );
+
+	fragment++;
+    }
+    skb_shinfo(skb)->nr_frags = fragment;
+    skb_shadow->frag_count = fragment;
+
+    ASSERT( skb_headlen(skb) == desc->buffer_len );
+    if( fragment )
+	dprintk( 4, PREFIX "fragments %d, tot len %d, fragment len %d\n",
+		fragment, skb->len, skb->data_len );
+
+    desc->status.X.dropped = 1; // Remember that we have submitted this packet.
+
+    return skb;
+}
+
+static int L4VMnet_dev_queue_client_pkt( L4VMnet_client_info_t *client )
+{
+    struct sk_buff *skb;
+
+    skb = L4VMnet_skb_wrap_packet( client );
+    if( skb == NULL )
+	return FALSE;
+
+    // TODO: if the skb exceeds the device's queue length, it will call
+    // kfree_skb(skb) at interrupt time, which isn't compatible with
+    // desctructors.  And it will drop our packets.
+    ASSERT( client->real_dev );
+    skb->dev = client->real_dev;
+    dev_queue_xmit( skb );	// Can be called from an interrupt.
+
+    return TRUE;
+}
+
+static void
+L4VMnet_transmit_client_pkts( void )
+{
+    L4VMnet_client_info_t *client;
+    L4_Word_t pkts_sent, total_pkts;
+
+    total_pkts = 0;
+    do {
+	pkts_sent = 0;
+	for( client = L4VMnet_server.client_list; client; client = client->next)
+	{
+	    if( !client->operating )
+		continue;
+
+	    if( L4VMnet_dev_queue_client_pkt(client) )
+		pkts_sent++;
+	}
+
+	total_pkts += pkts_sent;
+    } while( pkts_sent );
+
+    dprintk( 4, PREFIX "%lu packets transmitted\n", total_pkts );
+}
+#endif
+
+
+/***************************************************************************
+ *
+ * Sending packets outbound from the client via the dp83820 interface.
+ *
+ ***************************************************************************/
+
+#if 1
+static void notify_dp83820_client( L4VMnet_client_info_t *client )
+{
+    IVMnet_client_shared_t *shared = client->shared_data;
+
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+    if( !shared->client_irq_pending 
+	    && (shared->dp83820_regs[ISR] & shared->dp83820_regs[IMR])
+	    && shared->dp83820_regs[IER] )
+    {
+	unsigned long irq_flags;
+	L4_MsgTag_t tag, result_tag;
+	tag.raw = 0;
+	tag.X.u = 1;
+	tag.X.label = 0x100;
+
+	shared->client_irq_pending = 1;
+
+	local_irq_save(irq_flags); ASSERT( !vcpu_interrupts_enabled() );
+#if 0
+	L4_Set_MsgTag( tag );
+	L4_LoadMR( 1, shared->client_irq );
+	result_tag = L4_Reply( shared->client_main_tid );
+	if( L4_IpcFailed(result_tag) && ((L4_ErrorCode()&1) == 0) ) 
+#endif
+	{
+	    L4_Set_MsgTag( tag );
+	    L4_LoadMR( 1, shared->client_irq );
+	    result_tag = L4_Send( shared->client_irq_tid );
+	}
+	local_irq_restore(irq_flags);
+
+	if( L4_IpcFailed(result_tag) ) {
+	    dprintk( 2, PREFIX "skb destructor notify failed\n" );
+	    shared->client_irq_pending = 0;
+	}
+    }
+}
+#endif
+typedef struct L4VMnet_skb_dp83820_shadow
+{
+    struct sk_buff *skb;
+    L4VMnet_client_info_t *client;
+    L4_Word_t link;
+} L4VMnet_skb_dp83820_shadow_t;
+
+
+extern inline IVMnet_dp83820_descriptor_t *
+get_dp83820_desc( L4VMnet_client_info_t *client, L4_Word_t link )
+{
+    return (IVMnet_dp83820_descriptor_t *)
+	(link - client->dp83820_tx.first + client->dp83820_tx.ring_start);
+}
+
+extern inline IVMnet_dp83820_descriptor_t *
+get_dp83820_current_desc( L4VMnet_client_info_t *client )
+{
+    // TODO: make SMP safe
+    return get_dp83820_desc( client, client->shared_data->dp83820_regs[TXDP] );
+}
+
+static int reignite_transmit( L4VMnet_client_info_t *client )
+{
+    IVMnet_dp83820_descriptor_t *desc;
+
+    desc = get_dp83820_current_desc( client );
+    if( !desc->cmd_status.tx.own )
+	return 0;
+
+    L4VMnet_server.server_status->irq_status |= 1;
+    L4VMnet_server.server_status->irq_pending = 1;
+
+    l4ka_wedge_raise_irq( client->shared_data->server_irq );
+    return 1;
+}
+
+
+static void
+L4VMnet_skb_dp83820_destructor( struct sk_buff *skb )
+{
+    L4VMnet_skb_dp83820_shadow_t *shadow = 
+	(L4VMnet_skb_dp83820_shadow_t *)skb->head;
+    IVMnet_dp83820_descriptor_t *desc, *frag_desc;
+    int do_irq = 0, delay;
+    L4_Word_t link;
+
+    ASSERT(shadow);
+    ASSERT(shadow->skb == skb);
+    ASSERT(shadow->client);
+    ASSERT(shadow->link);
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+
+    desc = get_dp83820_desc( shadow->client, shadow->link );
+    ASSERT( desc->cmd_status.tx.own && desc->cmd_status.tx.ok );
+    if( desc->cmd_status.tx.intr )
+	do_irq = 1;
+    link = desc->cmd_status.tx.more ? desc->link : 0;
+    desc->cmd_status.tx.own = 0;
+    atomic_dec( &dirty_tx_pkt_cnt );
+
+    while( link )
+    {
+	// Release the fragment descriptor back to the client.
+	ASSERT(link);
+	frag_desc = get_dp83820_desc( shadow->client, link );
+    	ASSERT(frag_desc->cmd_status.tx.own && frag_desc->cmd_status.tx.ok);
+	if( frag_desc->cmd_status.tx.intr )
+	    do_irq = 1;
+	link = frag_desc->cmd_status.tx.more ? frag_desc->link : 0;
+	frag_desc->cmd_status.tx.own = 0;
+    }
+
+    delay = reignite_transmit( shadow->client );
+    set_bit( 6, (volatile unsigned long *)&shadow->client->shared_data->dp83820_regs[ISR] );
+    if( do_irq )
+	set_bit( 7, (volatile unsigned long *)&shadow->client->shared_data->dp83820_regs[ISR] );
+    if( !delay )
+	notify_dp83820_client( shadow->client );
+
+    // Clean-up the skb.
+    ASSERT( !skb_shinfo(skb)->frag_list );
+    skb_shinfo(skb)->nr_frags = 0;
+    skb->data = skb->head;
+    skb->tail = skb->data;
+    skb->h.raw = skb->data;
+    skb->len = 0;
+    skb->data_len = 0;
+
+    dprintk( 4, PREFIX "skb destructor\n" );
+}
+
+static struct sk_buff *
+L4VMnet_skb_wrap_dp83820_packet( L4VMnet_client_info_t *client )
+{
+    // TODO: validate that buffers + fragments together create valid
+    // skb buffs.  Lots of overlapping state must be validated.
+    IVMnet_dp83820_descriptor_t *desc, *frag_desc;
+    struct sk_buff *skb;
+    L4VMnet_skb_dp83820_shadow_t *skb_shadow;
+    int fragment;
+
+    // Grab the first packet.
+    ASSERT( client->shared_data->dp83820_regs[TXDP] );
+    desc = get_dp83820_current_desc( client );
+    if( !desc->cmd_status.tx.own )
+	return NULL;
+    if( unlikely(desc->cmd_status.tx.ok) )
+    {
+	dprintk( 2, KERN_INFO PREFIX "send wrap-around.\n" );
+	return NULL;
+    }
+
+    skb = alloc_skb( sizeof(L4VMnet_skb_dp83820_shadow_t), GFP_ATOMIC );
+    if( unlikely(skb == NULL) )
+	return NULL;
+
+    skb_shadow = (L4VMnet_skb_dp83820_shadow_t *)skb->head;
+    skb_shadow->client = client;
+    skb_shadow->link = client->shared_data->dp83820_regs[TXDP];
+    skb_shadow->skb = skb;
+
+    // Move to the next packet.
+    client->shared_data->dp83820_regs[TXDP]= desc->link;
+
+    // TODO: validate that the packet lives in the client's address space.
+    ASSERT( desc->buffer && desc->buffer_len );
+    skb->data = bus_to_virt(desc->buffer + client->client_space->bus_start);
+    skb->len  = desc->buffer_len;
+    skb->tail = skb->data + skb->len;
+
+    skb->destructor = L4VMnet_skb_dp83820_destructor;
+    skb->ip_summed  = CHECKSUM_NONE;
+    skb->pkt_type   = PACKET_OUTGOING;
+
+    // Where the device must start the checksum.
+    skb->h.raw = 34 + skb->data;
+    // Where the device must store the checksum.
+    if( desc->extended_status.udp_pkt ) {
+	skb->csum = 40 + skb->data - skb->h.raw;
+	skb->ip_summed = CHECKSUM_HW;
+    }
+    else if( desc->extended_status.tcp_pkt ) {
+	skb->csum = 50 + skb->data - skb->h.raw;
+	skb->ip_summed = CHECKSUM_HW;
+    }
+    else if( desc->extended_status.ip_pkt )
+	L4_KDB_Enter("oops, ip csum not configured");
+
+    dprintk( 4, PREFIX "skb protocol %x, pkt_type %x, "
+	    "data %p (bus %p/%p)\n",
+	    skb->protocol, skb->pkt_type, skb->data, (void *)desc->buffer,
+	    (void *)virt_to_bus(skb->data) );
+
+    desc->cmd_status.tx.ok = 1; // Remember that we have submitted this packet.
+
+    fragment = 0;
+    frag_desc = desc;
+    while( frag_desc->cmd_status.tx.more )
+    {
+	L4_Word_t paddr;
+	IVMnet_dp83820_descriptor_t *last = frag_desc;
+
+	if( unlikely(fragment >= MAX_SKB_FRAGS) ) {
+	    last->cmd_status.tx.more = 0;
+	    break;
+	}
+	ASSERT( client->shared_data->dp83820_regs[TXDP] );
+	frag_desc = get_dp83820_current_desc( client );
+	if( unlikely(!frag_desc->cmd_status.tx.own) ) {
+	    last->cmd_status.tx.more = 0;
+	    break;
+	}
+	frag_desc->cmd_status.tx.ok = 1;
+	client->shared_data->dp83820_regs[TXDP] = frag_desc->link;
+
+	ASSERT( frag_desc->buffer && frag_desc->buffer_len );
+	skb_shinfo(skb)->frags[fragment].size = frag_desc->buffer_len;
+	skb_shinfo(skb)->frags[fragment].page_offset =
+	    (unsigned long)frag_desc->buffer & (PAGE_SIZE-1);
+	paddr = l4ka_wedge_bus_to_phys(frag_desc->buffer + client->client_space->bus_start);
+	skb_shinfo(skb)->frags[fragment].page = mem_map + (paddr >> PAGE_SHIFT);
+	skb->len += frag_desc->buffer_len;
+	skb->data_len += frag_desc->buffer_len;
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %u\n",
+		fragment,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page_offset,
+		skb_shinfo(skb)->frags[fragment].size );
+	ASSERT( page_to_bus(skb_shinfo(skb)->frags[fragment].page) == ((frag_desc->buffer + client->client_space->bus_start) & PAGE_MASK) );
+
+	fragment++;
+    }
+    skb_shinfo(skb)->nr_frags = fragment;
+
+    ASSERT( skb_headlen(skb) == desc->buffer_len );
+    if( fragment )
+	dprintk( 4, PREFIX "fragments %d, tot len %d, fragment len %d\n",
+		fragment, skb->len, skb->data_len );
+
+    return skb;
+}
+
+
+static int L4VMnet_dev_queue_dp83820_pkt( L4VMnet_client_info_t *client )
+{
+    struct sk_buff *skb;
+
+    skb = L4VMnet_skb_wrap_dp83820_packet( client );
+    if( skb == NULL ) {
+	set_bit( 9 /* TXIDLE */, (volatile unsigned long *)&client->shared_data->dp83820_regs[ISR] );
+//	notify_dp83820_client( client );
+	return FALSE;
+    }
+
+    // TODO: if the skb exceeds the device's queue length, it will call
+    // kfree_skb(skb) at interrupt time, which isn't compatible with
+    // desctructors.  And it will drop our packets.
+    ASSERT( client->real_dev );
+    skb->dev = client->real_dev;
+    dev_queue_xmit( skb );	// Can be called from an interrupt.
+    atomic_inc( &dirty_tx_pkt_cnt );
+
+    return TRUE;
+}
+
+static void
+L4VMnet_dp83820_tx_pkts( void )
+{
+    L4VMnet_client_info_t *client;
+    L4_Word_t pkts_sent, total_pkts;
+
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+    total_pkts = 0;
+    do {
+	pkts_sent = 0;
+	for( client = L4VMnet_server.client_list; client; client = client->next)
+	{
+	    if( !client->operating )
+		continue;
+
+	    if( L4VMnet_dev_queue_dp83820_pkt(client) )
+		pkts_sent++;
+	}
+
+	total_pkts += pkts_sent;
+    } while( pkts_sent );
+
+    dprintk( 4, PREFIX "%lu packets transmitted\n", total_pkts );
+
+}
+
+/***************************************************************************
+ *
+ * Server handlers for executing in top halves and bottom halves.
+ *
+ ***************************************************************************/
+
+static void L4VMnet_attach_handler( L4VMnet_server_cmd_t *params )
+{
+    IVMnet_handle_t handle;
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    L4VM_client_space_info_t *client_space;
+    idl4_fpage_t idl4_fp, idl4_fp2;
+    L4VM_alligned_alloc_t alloc_info;
+    int log2size;
+    unsigned long flags;
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    // Prepare the reply.
+    ipc_env._action = 0;
+
+    // Derive the client's handle from its LAN address.
+    handle = lanaddress_get_handle(
+	    (lanaddress_t *)params->params.attach.lan_address );
+
+    printk( KERN_INFO PREFIX "attach request from handle %u, lan address ",
+	    handle);
+    L4VMnet_print_lan_address( params->params.attach.lan_address );
+    printk( ", for device %s\n", params->params.attach.dev_name );
+
+    // Get access to the client's pages.  This is a HACK.
+    client_space = L4VM_get_client_space_info( params->reply_to_tid );
+    if( client_space == NULL )
+	goto err_client_space;
+
+    // Allocate memory for the datastructures to be mapped into the client.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMnet_client_shared_t)) );
+    if( L4VM_fpage_alloc(log2size, &alloc_info) < 0 )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	goto err_shared_region_alloc;
+    }
+    dprintk( 4, KERN_INFO "client shared region at 0x%p, size %lu, "
+	    "requested size %d\n", (void *)L4_Address(alloc_info.fpage),
+	    L4_Size(alloc_info.fpage), 1 << log2size );
+
+    // Allocate a client structure.
+    client = kmalloc( sizeof(L4VMnet_client_info_t), GFP_KERNEL );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	goto err_client_info_alloc;
+    }
+
+    // Initialize the client structure.
+    client->ivmnet_handle = handle;
+    memcpy( client->lan_address, params->params.attach.lan_address, ETH_ALEN );
+    client->shared_alloc_info = alloc_info;
+    client->client_space = client_space;
+    memset( &client->rcv_ring, 0, sizeof(L4VMnet_skb_ring_t) );
+    client->rcv_ring.cnt = L4VMNET_SKB_RING_LEN;
+
+    // Initialize the pointers to the shared region.
+    client->shared_data = (IVMnet_client_shared_t *)
+	L4_Address(client->shared_alloc_info.fpage);
+    // Page-in the client shared region (zero all the pages)
+    memset( client->shared_data, 0, L4_Size(client->shared_alloc_info.fpage) );
+    client->send_ring.desc_ring = client->shared_data->snd_desc_ring;
+    client->send_ring.start_free = 0;
+    client->send_ring.start_dirty = 0;
+    client->send_ring.cnt = IVMnet_snd_descriptor_ring_cnt;
+
+    client->shared_data->server_irq = L4VMnet_server_irq;
+    client->shared_data->server_irq_tid = L4VMnet_server.my_irq_tid;
+    client->shared_data->server_main_tid = L4VMnet_server.my_main_tid;
+
+    client->operating = FALSE;
+
+    strcpy( client->real_dev_name, params->params.attach.dev_name );
+    client->real_dev = dev_get_by_name( client->real_dev_name );
+    if( client->real_dev ) {
+	// Enable bridging on the device.  This is never disabled,
+	// because we don't maintain reference counts to the real devices.
+	client->real_dev->br_port = (struct net_bridge_port *)&L4VMnet_server;
+    }
+
+    // Enable the client.
+    if( !L4VMnet_client_handle_set(handle, client) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_client, NULL );
+	goto err_client_handle;
+    }
+
+    client->next = L4VMnet_server.client_list;
+    L4VMnet_server.client_list = client;
+
+    // Reply to the client.
+    idl4_fpage_set_base( &idl4_fp, 0 );
+    idl4_fpage_set_mode( &idl4_fp, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp, client->shared_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp, IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    idl4_fpage_set_base( &idl4_fp2, L4_Size(client->shared_alloc_info.fpage) );
+    idl4_fpage_set_mode( &idl4_fp2, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp2, L4VMnet_server.status_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp2, IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_attach_propagate_reply( params->reply_to_tid,
+	    &handle, &idl4_fp, &idl4_fp2, &ipc_env );
+    local_irq_restore(flags);
+    // TODO: check for IPC error, and free resources if necessary.
+    return;
+
+err_client_handle:
+    kfree( client );
+err_client_info_alloc:
+    L4VM_fpage_dealloc( &alloc_info );
+err_shared_region_alloc:
+err_client_space:
+    printk( KERN_ERR PREFIX "error attaching client.\n" );
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    IVMnet_Control_attach_propagate_reply( params->reply_to_tid,
+	    NULL, NULL, NULL, &ipc_env );
+    local_irq_restore(flags);
+    return;
+}
+
+static void L4VMnet_reattach_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+    L4VMnet_client_info_t *client;
+
+    dprintk( 2, PREFIX "reattach request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.reattach.handle );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_VirtualSender( L4VMnet_server.server_tid );
+	IVMnet_Control_reattach_propagate_reply( params->reply_to_tid,
+		NULL, NULL, &ipc_env );
+	local_irq_restore(flags);
+    }
+    else
+    {
+	idl4_fpage_t idl4_fp, idl4_fp2;
+
+	ipc_env._action = 0;
+
+	L4VM_page_in( client->shared_alloc_info.fpage, PAGE_SIZE );
+	L4VM_page_in( L4VMnet_server.status_alloc_info.fpage, PAGE_SIZE );
+
+	idl4_fpage_set_base( &idl4_fp, 0 );
+	idl4_fpage_set_mode( &idl4_fp, IDL4_MODE_MAP );
+	idl4_fpage_set_page( &idl4_fp, client->shared_alloc_info.fpage );
+	idl4_fpage_set_permissions( &idl4_fp, 
+		IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+	idl4_fpage_set_base( &idl4_fp2, 
+		L4_Size(client->shared_alloc_info.fpage) );
+	idl4_fpage_set_mode( &idl4_fp2, IDL4_MODE_MAP );
+	idl4_fpage_set_page( &idl4_fp2, 
+		L4VMnet_server.status_alloc_info.fpage );
+	idl4_fpage_set_permissions( &idl4_fp2, 
+		IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_VirtualSender( L4VMnet_server.server_tid );
+	IVMnet_Control_reattach_propagate_reply( params->reply_to_tid,
+		&idl4_fp, &idl4_fp2, &ipc_env );
+	local_irq_restore(flags);
+    }
+}
+
+static void L4VMnet_detach_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+    L4VMnet_client_info_t *client;
+    ipc_env._action = 0;
+
+    dprintk( 2, PREFIX "detach request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	client->operating = FALSE;
+	if( client->real_dev ) {
+	    dev_put( client->real_dev );
+	    client->real_dev = NULL;
+	}
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_detach_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_start_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 2, PREFIX "start request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	if( client->real_dev ) {
+	    client->operating = TRUE;
+
+	    rtnl_lock();
+	    dev_change_flags( client->real_dev, 
+		    IFF_UP | IFF_BROADCAST | IFF_MULTICAST | IFF_PROMISC );
+	    rtnl_unlock();
+	}
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_start_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_stop_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 2, PREFIX "stop request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	client->operating = FALSE;
+	// TODO: wait for client operations to quiesce.
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_stop_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_update_stats_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 3, PREFIX "update stats request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+    ASSERT( sizeof( IVMnet_stats_t ) == sizeof( struct net_device_stats ) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else {
+	struct net_device *netdev;
+
+#warning stg: reporting physical instead of virtual device stats
+	ipc_env._action = 0;
+	netdev = dev_get_by_name( "eth0" );
+	memcpy( &L4VMnet_server.server_status->stats, netdev->get_stats( netdev ), sizeof( struct net_device_stats ) );
+	dev_put( netdev );
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_update_stats_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_register_dp83820_tx_ring_handler(
+    L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 3, PREFIX "register dp83820 tx ring from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.register_dp83820_tx_ring.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else {
+	// Extract the client parameters.
+	L4_Word_t client_paddr = 
+	    params->params.register_dp83820_tx_ring.client_paddr;
+	L4_Word_t client_size = 
+	    params->params.register_dp83820_tx_ring.ring_size_bytes;
+	L4_Word_t has_extended_status = 
+	    params->params.register_dp83820_tx_ring.has_extended_status;
+	L4_Fpage_t fp_req = L4_Fpage( client_paddr, client_size );
+	CORBA_Environment req_env = idl4_default_environment;
+	idl4_fpage_t fp;
+	int err;
+
+	ipc_env._action = 0;
+
+	if( !has_extended_status ) {
+	    printk( KERN_ERR PREFIX "The client must use the dp83820 extended status.  Fatal error.\n" );
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Request a virtual address region from Linux.
+	err = L4VM_fpage_vmarea_get( L4_SizeLog2(fp_req), 
+		&client->dp83820_tx.vmarea );
+	if( err < 0 ) {
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Request the client pages from the resourcemon.
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	idl4_set_rcv_window( &req_env, client->dp83820_tx.vmarea.fpage );
+	IResourcemon_request_client_pages( 
+		resourcemon_shared.cpu[L4_ProcessorNo()].resourcemon_tid,
+		&params->reply_to_tid, fp_req.raw, &fp, &req_env );
+	local_irq_restore(flags);
+
+	if( req_env._major != CORBA_NO_EXCEPTION ) {
+	    CORBA_exception_free( &req_env );
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Point at our local address for the descriptor ring.
+	client->dp83820_tx.first = client_paddr;
+	client->dp83820_tx.ring_start =
+	    (L4_Address(client->dp83820_tx.vmarea.fpage) +
+	     ((L4_Size(client->dp83820_tx.vmarea.fpage)-1) & client_paddr) );
+	dprintk( 2, PREFIX "dp83820 tx ring, client %08lx, DD/OS %08lx\n",
+		client_paddr, client->dp83820_tx.ring_start );
+    }
+
+out:
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_register_dp83820_tx_ring_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+
+static void L4VMnet_cmd_dispatcher( L4VMnet_server_cmd_ring_t *cmd_ring )
+{
+    while( 1 )
+    {
+	L4VMnet_server_cmd_t *cmd = &cmd_ring->cmds[ cmd_ring->next_cmd ];
+	if( !cmd->handler )
+	    break;
+
+	// Dispatch the command entry.
+	cmd->handler( cmd );
+
+	// Release the command entry and get the next command.
+	cmd->handler = NULL;
+	cmd_ring->next_cmd = (cmd_ring->next_cmd + 1) % L4VMNET_SERVER_CMD_RING_LEN;
+    }
+
+    if( cmd_ring->wake_server )
+    {
+	unsigned long flags;
+
+	cmd_ring->wake_server = FALSE;
+
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_MsgTag( (L4_MsgTag_t){ raw:0 } );
+	L4_Reply( L4VMnet_server.server_tid );
+	local_irq_restore(flags);
+    }
+}
+
+/***************************************************************************
+ *
+ * Server thread.
+ *
+ ***************************************************************************/
+
+static L4VMnet_server_cmd_t * L4VMnet_allocate_cmd(
+	L4VMnet_server_cmd_ring_t *ring,
+	L4_Word_t irq_flag )
+{
+    L4VMnet_server_cmd_t *cmd = &ring->cmds[ ring->next_free ];
+
+    // Wait until the command is available for use.  We wait for an
+    // IPC from *any* thread, so confirm that we actually have
+    // a free cmd slot when we wake.
+    while( cmd->handler )
+    {
+	L4_ThreadId_t to_tid, from_tid;
+
+	dprintk( 1, PREFIX "command queue full, sending IRQ to Linux\n" );
+
+	// Tell the handler that it needs to wake server thread.
+	ring->wake_server = TRUE;
+
+	// 1. Raise IRQ flag.
+	L4VMnet_server.irq_status |= irq_flag;
+
+	// 2. Deliver IRQ.
+	if( ((L4VMnet_server.irq_status & L4VMnet_server.irq_mask) != 0) &&
+		!L4VMnet_server.irq_pending )
+	{
+	    L4_MsgTag_t msgtag = (L4_MsgTag_t){raw:0};
+	    msgtag.X.label = L4_TAG_IRQ; msgtag.X.u = 1;
+	    L4_LoadMR( 1, L4VMnet_server_irq );
+	    L4_Set_MsgTag(msgtag);
+
+	    to_tid = L4VMnet_server.my_irq_tid;
+	    L4VMnet_server.irq_pending = TRUE;
+	}
+	else
+	    to_tid = L4_nilthread;
+
+	// 3. Wait for reactivation.
+	L4_Ipc( to_tid, L4_anythread, 
+		L4_Timeouts(L4_Never, L4_Never), &from_tid);
+	// Ignore IPC errors and IPC from unknown clients.
+	// We restart the loop, which will correct for errors.
+    }
+
+    // Move to the next command.
+    ring->next_free = (ring->next_free + 1) % L4VMNET_SERVER_CMD_RING_LEN;
+
+    return cmd;
+}
+
+static void
+L4VMnet_xdeliver_irq( unsigned event )
+    // Delivers an IRQ request to the IRQ thread, from any L4 thread
+    // running in the kernel.  Thus this function doesn't necessary
+    // execute in the main VM thread.  This function isn't performance
+    // oriented.
+{
+    L4VMnet_server.irq_status |= event;
+
+    if( ((L4VMnet_server.irq_status & L4VMnet_server.irq_mask) != 0) &&
+	    !L4VMnet_server.irq_pending )
+    {
+	L4_MsgTag_t msgtag;
+
+	dprintk( 3, PREFIX "delivering irq to Linux tid %lx\n",
+		L4VMnet_server.my_irq_tid.raw );
+
+	L4VMnet_server.irq_pending = TRUE;
+
+	msgtag.raw = 0;
+	msgtag.X.u = 1;
+	msgtag.X.label = 0x100;
+	L4_Set_MsgTag( msgtag );
+	L4_LoadMR( 1, L4VMnet_server_irq );
+
+	L4_Send( L4VMnet_server.my_irq_tid );
+    }
+}
+
+IDL4_INLINE void IVMnet_Control_attach_implementation(
+	CORBA_Object _caller,
+	const char *dev_name,
+	const char *lan_address,
+	IVMnet_handle_t *handle,
+	idl4_fpage_t *shared_window,
+	idl4_fpage_t *server_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    memcpy( cmd->params.attach.dev_name, dev_name, IFNAMSIZ );
+    cmd->params.attach.dev_name[IFNAMSIZ-1] = '\0';
+    memcpy( cmd->params.attach.lan_address, lan_address, ETH_ALEN );
+    cmd->handler = L4VMnet_attach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_ATTACH(IVMnet_Control_attach_implementation);
+
+IDL4_INLINE void IVMnet_Control_reattach_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_fpage_t *shared_window,
+	idl4_fpage_t *server_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.reattach.handle = handle;
+    cmd->handler = L4VMnet_reattach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_REATTACH(IVMnet_Control_reattach_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_detach_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.detach.handle = handle;
+    cmd->handler = L4VMnet_detach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_DETACH(IVMnet_Control_detach_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_start_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.start.handle = handle;
+    cmd->handler = L4VMnet_start_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_START(IVMnet_Control_start_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_stop_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.stop.handle = handle;
+    cmd->handler = L4VMnet_stop_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_STOP(IVMnet_Control_stop_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_update_stats_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.update_stats.handle = handle;
+    cmd->handler = L4VMnet_update_stats_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_UPDATE_STATS(IVMnet_Control_update_stats_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_run_dispatcher_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_DISPATCH );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_RUN_DISPATCHER(IVMnet_Control_run_dispatcher_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_dp83820_tx_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_DP83820_TX );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_DP83820_TX(IVMnet_Control_dp83820_tx_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_register_dp83820_tx_ring_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	const L4_Word_t client_paddr,
+	const L4_Word_t ring_size_bytes,
+	const L4_Word_t has_extended_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.register_dp83820_tx_ring.handle = handle;
+    cmd->params.register_dp83820_tx_ring.client_paddr = client_paddr;
+    cmd->params.register_dp83820_tx_ring.ring_size_bytes = ring_size_bytes;
+    cmd->params.register_dp83820_tx_ring.has_extended_status = has_extended_status;
+    cmd->handler = L4VMnet_register_dp83820_tx_ring_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_REGISTER_DP83820_TX_RING(IVMnet_Control_register_dp83820_tx_ring_implementation);
+
+
+void L4VMnet_server_thread( void *param )
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    char dev_name[IFNAMSIZ];
+    char addr_buf[ETH_ALEN];
+    static void *IVMnet_Control_vtable[IVMNET_CONTROL_DEFAULT_VTABLE_SIZE] = IVMNET_CONTROL_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init(&msgbuf);
+    idl4_msgbuf_add_buffer( &msgbuf, dev_name, sizeof(dev_name) );
+    idl4_msgbuf_add_buffer( &msgbuf, addr_buf, sizeof(addr_buf) );
+
+    while( 1 )
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while( 1 )
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    dprintk( 1, PREFIX "server thread request from %lx\n", partner.raw);
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt,
+		    IVMnet_Control_vtable[
+		      idl4_get_function_id(&msgtag) & IVMNET_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMnet_Control_discard(void)
+{
+    // Invoked in response to invalid IPC messages.
+}
+
+/***************************************************************************
+ *
+ * IRQ and bottom half handling.
+ *
+ ***************************************************************************/
+
+static void
+L4VMnet_bottom_half_handler( void * param )
+{
+    dprintk( 2, PREFIX "bottom half handler, in interrupt: %lu\n",
+	    in_interrupt() );
+
+    L4VMnet_cmd_dispatcher( &L4VMnet_server.bottom_half_cmds );
+}
+
+static irqreturn_t
+L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    while( 1 )
+    {
+	// Outstanding events?  Read them and reset without losing events.
+	unsigned events, client_events;
+
+	do {
+	    events = L4VMnet_server.irq_status;
+	} while( cmpxchg(&L4VMnet_server.irq_status, events, 0) != events );
+	do {
+	    client_events = L4VMnet_server.server_status->irq_status;
+	} while( cmpxchg(&L4VMnet_server.server_status->irq_status, client_events, 0) != client_events );
+
+	if( !events && !client_events )
+	    return IRQ_HANDLED;
+	dprintk( 3, PREFIX "irq handler: 0x%x, 0x%x\n", events, client_events );
+
+	// No one needs to deliver IRQ messages.  They would be superfluous.
+	L4VMnet_server.irq_pending = TRUE;
+	L4VMnet_server.server_status->irq_pending = TRUE;
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+	if( (events & L4VMNET_IRQ_DISPATCH) || client_events )
+	{
+	    // TODO: can we do this at IRQ time?
+	    L4VMnet_transmit_client_pkts();
+	}
+#else
+	if( (events & L4VMNET_IRQ_DP83820_TX) || client_events )
+	{
+	    L4VMnet_dp83820_tx_pkts();
+	}
+#endif
+
+	if( (events & L4VMNET_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VMnet_cmd_dispatcher( &L4VMnet_server.top_half_cmds );
+
+	if( (events & L4VMNET_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &L4VMnet_server.bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	L4VMnet_server.irq_pending = FALSE;
+	L4VMnet_server.server_status->irq_pending = FALSE;
+    }
+}
+
+#if 0
+
+static int
+L4VMnet_irq_pending( void *data )
+{
+    return (L4VMnet_server.irq_status & L4VMnet_server.irq_mask) ||
+	   L4VMnet_server.server_status->irq_status;
+}
+
+#endif
+/***************************************************************************
+ *
+ * Packet send functions.
+ *
+ ***************************************************************************/
+
+static int L4VMnet_xmit_packets_to_client_thread(
+	L4VMnet_client_info_t *client,
+	int receiver )
+{
+    L4VMnet_skb_ring_t *ring = &client->rcv_ring;
+    L4_Word_t idx;
+    L4_Word_t string_items = 0;
+    L4_MsgTag_t tag;
+    L4_StringItem_t str_item;
+    L4_ThreadId_t receiver_tid;
+    unsigned long flags;
+    L4_Word_t len, transferred_bytes = 0;
+    L4_Word_t cnt=0;
+
+    dprintk( 4, PREFIX "xmit_packets_to_client %p\n", client);
+    
+    if( receiver >= client->shared_data->receiver_cnt )
+    {
+	dprintk( 4, PREFIX "inbound packets for unavailable client\n" );
+	return FALSE;
+    }
+    receiver_tid = client->shared_data->receiver_tids[ receiver ];
+
+    local_irq_save(flags); // Protect our message registers.
+    ASSERT( !vcpu_interrupts_enabled() );
+
+    // Install string items for the message.
+    idx = ring->start_dirty;
+    while( string_items < IVMnet_rcv_buffer_cnt )
+    {
+	struct sk_buff *skb = ring->skb_ring[ idx ];
+	if( skb == NULL )
+	    break;	// No more packets.
+
+	if( string_items > 0 )
+	{
+	    // Store the string item from the last iteration.  We delay
+	    // by one iteration to update the C bit.
+	    str_item.X.C = 1;
+	    L4_LoadMR( 2*string_items-1, str_item.raw[0] );
+	    L4_LoadMR( 2*string_items,   str_item.raw[1] );
+	}
+
+	len = skb_headlen(skb);
+	str_item = L4_StringItem( len, skb->data );
+	transferred_bytes += len;
+
+	string_items++;
+	idx = (idx + 1) % ring->cnt;
+	if( idx == ring->start_dirty )
+	    break;	// Wrap-around.
+    }
+    dprintk( 4, PREFIX "%lu inbound packets, %lu bytes to transfer.\n", 
+	    string_items, transferred_bytes );
+
+    if( string_items == 0 )
+    {
+	local_irq_restore(flags);
+	return FALSE;
+    }
+
+    // Write the last string item.
+    L4_LoadMR( 2*string_items-1, str_item.raw[0] );
+    L4_LoadMR( 2*string_items,   str_item.raw[1] );
+
+    // Initialize the IPC message tag.
+    tag.raw = 0;
+    tag.X.t = 2*string_items;
+    L4_Set_MsgTag( tag );
+
+    client->shared_data->receiver_tids[receiver] = L4_nilthread;
+
+    // Deliver the IPC.
+    L4_Set_XferTimeouts( L4_Timeouts(L4_Never, L4_Never) );
+#warning "Move the outer loop here, and retry the various client threads!"
+    tag = L4_Reply( receiver_tid );
+    if( unlikely(L4_IpcFailed(tag)) )
+    {
+	dprintk( 4, PREFIX "message overflow %x.\n", receiver_tid.raw );
+	L4_Word_t err = L4_ErrorCode();
+	if( ((err >> 1) & 7) <= 3 ) {
+	    // Send-phase error.
+	    client->shared_data->receiver_tids[receiver] = receiver_tid;
+	    transferred_bytes = 0;
+	}
+	else {
+
+	    // Message overflow.
+	    transferred_bytes = err >> 4;
+	}
+    }
+
+    dprintk( 4, PREFIX "%lu bytes transferred.\n", transferred_bytes );
+
+    // Commit the mappings.  We may have had partial transfer.  We resend
+    // any skbuffs that had partial transfer.
+    for( idx = 0; (idx < string_items) && transferred_bytes; idx++ )
+    {
+	struct sk_buff *skb = ring->skb_ring[ ring->start_dirty ];
+	len = skb_headlen(skb);
+	if( transferred_bytes >= len ) {
+	    cnt++;
+	    transferred_bytes -= len;
+	    dev_kfree_skb_any( ring->skb_ring[ ring->start_dirty ] );
+	    ring->skb_ring[ ring->start_dirty ] = NULL;
+	    ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	}
+	else 
+	    transferred_bytes = 0;
+    }
+
+    local_irq_restore(flags);
+
+    return TRUE;
+}
+
+static void L4VMnet_xmit_packets_to_client( L4VMnet_client_info_t *client )
+{
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+    int receiver = 0;
+
+    while( (receiver < client->shared_data->receiver_cnt) &&
+	    (receiver < IVMnet_max_receiver_cnt) )
+    {
+	if( L4_IsNilThread( client->shared_data->receiver_tids[receiver] ) )
+	{
+	    receiver++;
+	    continue;
+	}
+	if( !L4VMnet_xmit_packets_to_client_thread(client, receiver) )
+	    return;
+	receiver++;
+    }
+    dprintk( 3, PREFIX "wow, not enough client receiver threads!\n" );
+#else
+    if( !L4_IsNilThread( client->shared_data->receiver_tids[0] ) )
+	L4VMnet_xmit_packets_to_client_thread( client, 0 );
+    else 
+    {
+	if (client)
+	{
+	    dprintk( 4, PREFIX "client receiver thread is nilthread %p\n", 
+		    client->shared_data->receiver_tids );
+	}
+	
+    }
+#endif
+}
+
+static void L4VMnet_queue_pkt_to_client(
+	L4VMnet_client_info_t *client,
+	struct sk_buff *skb )
+{
+    L4VMnet_skb_ring_t *ring;
+
+    if( !client->operating )
+    {
+	dev_kfree_skb_any( skb );
+	return;
+    }
+
+    ring = &client->rcv_ring;
+    dprintk( 4, PREFIX "queue packet %p to client %p ring %p\n", skb, client, ring);
+
+    // Make room on the queue.
+    if( ring->skb_ring[ring->start_free] != NULL )
+    {
+	// Ring is full.  So drop the packet.
+	dev_kfree_skb_any( skb );
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+	dprintk( 2, PREFIX "wow, too many packets for client!\n" );
+	return;
+    }
+
+    ring->skb_ring[ ring->start_free ] = skb;
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    if( L4VMnet_skb_ring_pending(&client->rcv_ring) > 2 || 
+	    (atomic_read(&dirty_tx_pkt_cnt) == 0) )
+    {
+	dprintk( 4, PREFIX "schedule flush tasklet\n");
+
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+    }
+}
+
+
+static int
+L4VMnet_absorb_frame( struct sk_buff *skb )
+    // Returns TRUE if the packet has been handled.
+    // Returns FALSE if the packet is supposed to be handled by Linux.
+{
+    L4VMnet_client_info_t *client;
+    IVMnet_handle_t handle;
+    lanaddress_t *lanaddr = (lanaddress_t *)skb->mac.raw;
+    lanaddress_t *linux_lanaddr;
+
+    // Is the packet destined for the local Linux?
+    linux_lanaddr = (lanaddress_t *)skb->dev->dev_addr;
+    if( (linux_lanaddr->align4.lsb == lanaddr->align4.lsb) &&
+	    (linux_lanaddr->align4.msb == lanaddr->align4.msb) )
+    {
+	return FALSE;
+    }
+
+    if( unlikely(skb_shinfo(skb)->nr_frags > 0) )
+    {
+	dprintk( 4, PREFIX "fragmented packet, linearize\n");
+	skb_linearize(skb, GFP_ATOMIC);
+	// We don't yet support fragmentation, so drop the packet.
+	//dev_kfree_skb_any( skb );
+	//return TRUE;
+    }
+    
+    // Fast map the destination LAN address into a handle, and then lookup
+    // the client structure.
+    handle = lanaddress_get_handle( lanaddr );
+    client = L4VMnet_client_handle_lookup( handle );
+    if( client ) {
+	L4VMnet_queue_pkt_to_client( client, skb );
+	return TRUE;
+    }
+
+
+    for( client = L4VMnet_server.client_list; client; client = client->next )
+    {
+	struct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);
+	if( clone )
+	    L4VMnet_queue_pkt_to_client( client, clone );
+    }
+
+    return FALSE;	// Return broadcasts to Linux too.
+}
+
+static void
+L4VMnet_flush_tasklet_handler( unsigned long unused )
+{
+    dprintk( 4, PREFIX "flush tasklet handler client list %p (server %p)\n",
+	    L4VMnet_server.client_list, &L4VMnet_server);
+
+    L4VMnet_client_info_t *client;
+
+    for( client = L4VMnet_server.client_list; client; client = client->next )
+	if( client->operating )
+	    L4VMnet_xmit_packets_to_client( client );
+	else
+	    dprintk( 4, PREFIX "flush tasklet handler client %p not operating\n", client);
+
+}
+
+
+/***************************************************************************
+ *
+ * Linux Module functions.
+ *
+ ***************************************************************************/
+
+static int __init
+L4VMnet_server_alloc( void )
+{
+    int err, log2size;
+
+    // Initialize the server.
+    L4VMnet_server.server_tid = L4_nilthread;
+
+    SET_MODULE_OWNER( L4VMnet_server );
+
+    INIT_TQUEUE( &L4VMnet_server.bottom_half_task,
+	    L4VMnet_bottom_half_handler, NULL );
+
+    // Skip server cmd ring init ... already zeroed.
+
+    // Allocate a status page, shared by all clients.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMnet_server_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &L4VMnet_server.status_alloc_info );
+    if( err < 0 )
+	goto err_server_status;
+    L4VMnet_server.server_status = (IVMnet_server_shared_t *)
+	L4_Address( L4VMnet_server.status_alloc_info.fpage );
+    L4VMnet_server.server_status->irq_status = 0;
+    L4VMnet_server.server_status->irq_pending = 0;
+
+    // Some IRQ init.
+    L4VMnet_server.irq_status = 0;
+    L4VMnet_server.irq_mask = ~0;
+    L4VMnet_server.irq_pending = FALSE;
+    L4VMnet_server.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    L4VMnet_server.my_main_tid = L4VM_linux_main_thread( smp_processor_id() );
+
+    if (L4VMnet_server_irq == 0)
+    {
+#if defined(CONFIG_X86_IO_APIC)
+        L4_KernelInterfacePage_t *kip = (L4_KernelInterfacePage_t *) L4_GetKernelInterface();
+        L4VMnet_server_irq = L4_ThreadIdSystemBase(kip) + 3;	
+	acpi_register_gsi(L4VMnet_server_irq, ACPI_LEVEL_SENSITIVE, ACPI_ACTIVE_LOW);
+
+#else
+	L4VMnet_server_irq = 9;
+#endif
+    }
+    printk( KERN_INFO PREFIX "L4VMnet server irq %d\n", L4VMnet_server_irq );
+
+    // Allocate a virtual interrupt.
+    L4VMnet_server.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    // TODO: in case of error, we don't want the irq to remain virtual.
+    l4ka_wedge_add_virtual_irq( L4VMnet_server_irq );
+    
+
+
+
+    err = request_irq( L4VMnet_server_irq, L4VMnet_irq_handler, 0,
+	    "l4ka_net_server", &L4VMnet_server );
+    if( err < 0 )
+    {
+	    printk( KERN_ERR PREFIX "unable to allocate virtual interrupt %d, err: %d\n", 
+		    L4VMnet_server_irq, err);
+	goto err_request_irq;
+    }
+
+    // Start the server loop.
+    L4VMnet_server.server_tid = L4VM_thread_create( GFP_KERNEL, 
+	    L4VMnet_server_thread, l4ka_wedge_get_irq_prio(), 
+	    smp_processor_id(), NULL, 0 );
+    if( L4_IsNilThread(L4VMnet_server.server_tid) )
+    {
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	err = -ENOMEM;
+	goto err_thread_start;
+    }
+
+    return 0;
+
+err_thread_start:
+    free_irq( L4VMnet_server_irq, &L4VMnet_server );
+err_request_irq:
+    L4VM_fpage_dealloc( &L4VMnet_server.status_alloc_info );
+err_server_status:
+    return -ENODEV;
+}
+
+static int __init
+L4VMnet_server_init_module( void )
+{
+    int result;
+
+    if( br_handle_frame_hook ) {
+	printk( KERN_ERR "Error: The bridging module is already loaded!  This\n"
+		"module is incompatible with the bridging module.\n" );
+	return -ENODEV;
+    }
+
+    br_handle_frame_hook = L4VMnet_bridge_handler;
+    L4VMnet_client_list_init();
+
+    // TODO: register device notifiers!!
+
+    result = L4VMnet_server_alloc();
+    if( result )
+	return result;
+
+    // Register with the locator.
+    L4VM_server_register_location( UUID_IVMnet, L4VMnet_server.server_tid );
+
+    printk( KERN_INFO PREFIX "L4VMnet server initialized.\n" );
+
+    return 0;
+}
+
+static void __exit
+L4VMnet_server_exit_module( void )
+{
+    if( !L4_IsNilThread(L4VMnet_server.server_tid) )
+	L4VM_thread_delete( L4VMnet_server.server_tid );
+    free_irq( L4VMnet_server_irq, &L4VMnet_server );
+    L4VM_fpage_dealloc( &L4VMnet_server.status_alloc_info );
+    br_handle_frame_hook = NULL;
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4 network device driver server stub" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM net" );
+MODULE_VERSION( "yoda" );
+
+module_init( L4VMnet_server_init_module );
+module_exit( L4VMnet_server_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/server.c~ linux-2.6.9/afterburn/drivers/net/server.c~
--- linux-2.6.9.src/afterburn/drivers/net/server.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/server.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,1919 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/server.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+
+#include <l4/kip.h>
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/spinlock.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/interrupt.h>
+
+#include <linux/if_bridge.h>
+#if defined(DO_LINUX_BRIDGE)
+#include <br_private.h>
+#endif
+
+#include <asm/io.h>
+
+#include <glue/thread.h>
+#include <glue/bottomhalf.h>
+#include <glue/vmirq.h>
+#include <glue/vmmemory.h>
+#include <glue/wedge.h>
+
+#if defined(CONFIG_X86_IO_APIC)
+#include <acpi/acpi.h>
+#include <linux/acpi.h>
+#endif
+
+#if defined(PREFIX)
+#undef PREFIX
+#endif
+#define PREFIX "L4VMnet server: "
+
+
+#include "server.h"
+#include "lanaddress.h"
+
+#define TXDP (0x20/4)
+#define ISR  (0x10/4)
+#define IMR  (0x14/4)
+#define IER  (0x18/4)
+
+int L4VMnet_server_irq = 0;
+MODULE_PARM( L4VMnet_server_irq, "i" );
+
+L4VMnet_server_t L4VMnet_server = { 
+    L4_nilthread, L4_nilthread, L4_nilthread
+};
+
+static int L4VMnet_absorb_frame( struct sk_buff *skb );
+static void L4VMnet_flush_tasklet_handler( unsigned long unused );
+
+/*
+ * Our L4 clients hand us unreadable packets.  The packets are not mapped
+ * into the address space.  So no routing decisions by Linux are possible.
+ * This driver must perform the routing
+ * decisions.  It shouldn't be a problem in our experimentation.
+ *
+ * The driver must watch for notifier events: NETDEV_UP and
+ * NETDEV_DOWN.  Thus we can detect if our target device goes down.  We
+ * mustn't send a packet once a device goes down, or we'd give the target
+ * device driver unexpected behavior from Linux.  Likewise, if the target
+ * device driver is paused, we mustn't try to deliver packets.
+ */
+
+DECLARE_TASKLET( L4VMnet_flush_tasklet, L4VMnet_flush_tasklet_handler, 0 );
+
+/***************************************************************************
+ *
+ * Interface handle mapper.
+ *
+ ***************************************************************************/
+
+#define L4VMNET_MAX_CLIENTS	16
+
+static L4VMnet_client_info_t *L4VMnet_client_list[ L4VMNET_MAX_CLIENTS ];
+
+static atomic_t dirty_tx_pkt_cnt = ATOMIC_INIT(0);
+
+extern void
+L4VMnet_client_list_init( void )
+{
+    int i;
+
+    for( i = 0; i < L4VMNET_MAX_CLIENTS; i++ )
+	L4VMnet_client_list[i] = NULL;
+}
+
+#if 0
+static void
+L4VMnet_client_handle_free( IVMnet_handle_t handle )
+{
+    L4VMnet_client_list[ handle ] = NULL;
+}
+#endif
+
+extern int L4VMnet_client_handle_set(
+	IVMnet_handle_t handle,
+	L4VMnet_client_info_t *client )
+{
+    static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+
+    if( handle >= L4VMNET_MAX_CLIENTS )
+	return FALSE;
+
+    spin_lock( &lock );
+    {
+	if( L4VMnet_client_list[handle] != NULL )
+	{
+	    spin_unlock( &lock );
+	    return FALSE;
+	}
+	L4VMnet_client_list[ handle ] = client;
+    }
+    spin_unlock( &lock );
+
+    return TRUE;
+}
+
+extern inline L4VMnet_client_info_t *
+L4VMnet_client_handle_lookup( IVMnet_handle_t handle )
+{
+    if( handle < L4VMNET_MAX_CLIENTS )
+	return L4VMnet_client_list[ handle ];
+    return NULL;
+}
+
+/***************************************************************************
+ *
+ * Receiving packets.
+ *
+ ***************************************************************************/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,9)
+static int L4VMnet_bridge_handler( struct net_bridge_port *p, struct sk_buff **pskb )
+{
+    struct sk_buff *skb = *pskb;
+
+    dprintk( 4, PREFIX "snarfed packet\n" );
+
+    skb_push( skb, ETH_HLEN ); // Uncover the LAN address.
+    if( L4VMnet_absorb_frame(skb) )
+	return 1;
+
+    skb_pull( skb, ETH_HLEN ); // Cover the LAN address back up.
+    return 0;	// We didn't handle the packet.
+}
+#else
+static int L4VMnet_bridge_handler( struct sk_buff *skb )
+{
+    dprintk( 4, PREFIX "snarfed packet\n" );
+
+    skb_push( skb, ETH_HLEN ); // Uncover the LAN address.
+    if( L4VMnet_absorb_frame(skb) )
+	return 0;
+
+    skb_pull( skb, ETH_HLEN ); // Cover the LAN address back up.
+    return 1;	// We didn't handle the packet.
+}
+#endif
+/***************************************************************************
+ *
+ * Sending packets outbound from the client.
+ *
+ ***************************************************************************/
+
+/*
+ * When deallocating memory, Linux uses skb->head.
+ * When accessing the shared skb info (skb_shinfo), Linux uses skb->end.
+ * When transmitting data, Linux uses skb->data.
+ *
+ * skb->head points to the beginning of allocated space.
+ * skb->data points to the beginning of valid data.
+ * skb->tail points to the end of valid data.
+ * skb->end points to the maximum valid address for tail.
+ */
+
+typedef struct L4VMnet_skb_shadow
+{
+    struct sk_buff *skb;
+    L4VMnet_desc_ring_t *ring;
+    L4_Word16_t start_frag;
+    L4_Word16_t frag_count;
+} L4VMnet_skb_shadow_t;
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+
+static void
+L4VMnet_skb_destructor( struct sk_buff *skb )
+{
+    L4VMnet_skb_shadow_t *shadow = (L4VMnet_skb_shadow_t *)skb->head;
+    IVMnet_ring_descriptor_t *desc, *frag_desc;
+
+    ASSERT(shadow);
+    ASSERT(shadow->skb == skb);
+    ASSERT(shadow->start_frag < shadow->ring->cnt);
+
+    desc = &shadow->ring->desc_ring[ shadow->start_frag ];
+    ASSERT( desc->status.X.server_owned && desc->status.X.dropped );
+
+    // Handle the fragments first.  The last fragment in the list has
+    // its fragment bit cleared.
+    if( desc->status.X.fragment )
+    {
+	L4_Word16_t idx = (shadow->start_frag + 1) % shadow->ring->cnt;
+	while( shadow->frag_count )
+	{
+	    // Release the fragment descriptor back to the client.
+	    frag_desc = &shadow->ring->desc_ring[ idx ];
+	    ASSERT( frag_desc->status.X.server_owned );
+	    frag_desc->status.X.server_owned = 0;
+	    idx = (idx + 1) % shadow->ring->cnt;
+	    shadow->frag_count--;
+	}
+    }
+
+    // Handle the head fragment last.
+    desc->status.X.server_owned = 0;
+
+    // Clean-up the skb.
+    ASSERT( !skb_shinfo(skb)->frag_list );
+    skb_shinfo(skb)->nr_frags = 0;
+    skb->data = skb->head;
+    skb->tail = skb->data;
+    skb->h.raw = skb->data;
+    skb->len = 0;
+    skb->data_len = 0;
+
+    dprintk( 4, PREFIX "skb destructor\n" );
+}
+
+static struct sk_buff *
+L4VMnet_skb_wrap_packet( L4VMnet_client_info_t *client )
+{
+    // TODO: validate that buffers + fragments together create valid
+    // skb buffs.  Lots of overlapping state must be validated.
+    L4VMnet_desc_ring_t *ring;
+    IVMnet_ring_descriptor_t *desc, *frag_desc;
+    struct sk_buff *skb;
+    L4VMnet_skb_shadow_t *skb_shadow;
+    int fragment;
+
+    ring = &client->send_ring;
+
+    // Grab the first packet, and move to the next.
+    desc = &ring->desc_ring[ ring->start_free ];
+    if( !desc->status.X.server_owned )
+	return NULL;
+    if( unlikely(desc->status.X.dropped) )
+    {
+	dprintk( 2, KERN_INFO PREFIX "send wrap-around.\n" );
+	return NULL;
+    }
+
+    skb = alloc_skb( sizeof(L4VMnet_skb_shadow_t), GFP_ATOMIC );
+    if( unlikely(skb == NULL) )
+	return NULL;
+
+    skb_shadow = (L4VMnet_skb_shadow_t *)skb->head;
+    skb_shadow->ring = ring;
+    skb_shadow->start_frag = ring->start_free;
+    skb_shadow->frag_count = 0;
+    skb_shadow->skb = skb;
+
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    // TODO: validate that the packet lives in the client's address space.
+    ASSERT( desc->buffer && desc->buffer_len );
+    ASSERT( desc->buffer + client->client_space->bus_start < num_physpages * PAGE_SIZE );
+    skb->data = bus_to_virt(desc->buffer + client->client_space->bus_start);
+    skb->len  = desc->buffer_len;
+    ASSERT( skb->len && skb->data );
+    skb->tail = skb->data + skb->len;
+
+    skb->destructor = L4VMnet_skb_destructor;
+    skb->ip_summed  = CHECKSUM_NONE;
+    skb->protocol   = desc->protocol;
+    skb->pkt_type   = desc->pkt_type;
+
+    // Where the device must start the checksum.
+    skb->h.raw = desc->csum.tx.start + skb->data;
+    // Where the device must store the checksum.
+    skb->csum = desc->csum.tx.offset + skb->data - skb->h.raw;
+
+    if( likely(desc->status.X.csum) )
+    {
+	if( (skb->h.raw >= skb->data) && (skb->h.raw <= skb->tail) &&
+		((skb->csum + skb->h.raw) <= skb->tail) )
+	    skb->ip_summed = CHECKSUM_HW;
+	else
+	    dprintk( 2, PREFIX "bad hardware checksum request.\n" );
+    }
+
+    dprintk( 4, PREFIX "skb protocol %x, pkt_type %x, "
+	    "data %p (bus %p/%p)\n",
+	    skb->protocol, skb->pkt_type, skb->data, (void *)desc->buffer,
+	    (void *)virt_to_bus(skb->data) );
+
+    fragment = 0;
+    frag_desc = desc;
+    while( frag_desc->status.X.fragment )
+    {
+	L4_Word_t paddr;
+
+	if( unlikely(fragment >= MAX_SKB_FRAGS) )
+	    break;
+	frag_desc = &ring->desc_ring[ ring->start_free ];
+	if( unlikely(!frag_desc->status.X.server_owned) )
+	    break;
+	ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+	skb_shinfo(skb)->frags[fragment].size = frag_desc->buffer_len;
+	skb_shinfo(skb)->frags[fragment].page_offset =
+	    (unsigned long)frag_desc->buffer & (PAGE_SIZE-1);
+	ASSERT(frag_desc->buffer);
+	ASSERT( frag_desc->buffer + client->client_space->bus_start < num_physpages * PAGE_SIZE );
+	paddr = virt_to_phys(bus_to_virt(frag_desc->buffer + client->client_space->bus_start));
+	skb_shinfo(skb)->frags[fragment].page = mem_map + (paddr >> PAGE_SHIFT);
+	ASSERT( paddr && frag_desc->buffer_len );
+	skb->len += frag_desc->buffer_len;
+	skb->data_len += frag_desc->buffer_len;
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %u\n",
+		fragment,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page_offset,
+		skb_shinfo(skb)->frags[fragment].size );
+	ASSERT( page_to_bus(skb_shinfo(skb)->frags[fragment].page) == (frag_desc->buffer & PAGE_MASK) );
+
+	fragment++;
+    }
+    skb_shinfo(skb)->nr_frags = fragment;
+    skb_shadow->frag_count = fragment;
+
+    ASSERT( skb_headlen(skb) == desc->buffer_len );
+    if( fragment )
+	dprintk( 4, PREFIX "fragments %d, tot len %d, fragment len %d\n",
+		fragment, skb->len, skb->data_len );
+
+    desc->status.X.dropped = 1; // Remember that we have submitted this packet.
+
+    return skb;
+}
+
+static int L4VMnet_dev_queue_client_pkt( L4VMnet_client_info_t *client )
+{
+    struct sk_buff *skb;
+
+    skb = L4VMnet_skb_wrap_packet( client );
+    if( skb == NULL )
+	return FALSE;
+
+    // TODO: if the skb exceeds the device's queue length, it will call
+    // kfree_skb(skb) at interrupt time, which isn't compatible with
+    // desctructors.  And it will drop our packets.
+    ASSERT( client->real_dev );
+    skb->dev = client->real_dev;
+    dev_queue_xmit( skb );	// Can be called from an interrupt.
+
+    return TRUE;
+}
+
+static void
+L4VMnet_transmit_client_pkts( void )
+{
+    L4VMnet_client_info_t *client;
+    L4_Word_t pkts_sent, total_pkts;
+
+    total_pkts = 0;
+    do {
+	pkts_sent = 0;
+	for( client = L4VMnet_server.client_list; client; client = client->next)
+	{
+	    if( !client->operating )
+		continue;
+
+	    if( L4VMnet_dev_queue_client_pkt(client) )
+		pkts_sent++;
+	}
+
+	total_pkts += pkts_sent;
+    } while( pkts_sent );
+
+    dprintk( 4, PREFIX "%lu packets transmitted\n", total_pkts );
+}
+#endif
+
+
+/***************************************************************************
+ *
+ * Sending packets outbound from the client via the dp83820 interface.
+ *
+ ***************************************************************************/
+
+#if 0
+static void notify_dp83820_client( L4VMnet_client_info_t *client )
+{
+    IVMnet_client_shared_t *shared = client->shared_data;
+
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+    if( !shared->client_irq_pending 
+	    && (shared->dp83820_regs[ISR] & shared->dp83820_regs[IMR])
+	    && shared->dp83820_regs[IER] )
+    {
+	unsigned long irq_flags;
+	L4_MsgTag_t tag, result_tag;
+	tag.raw = 0;
+	tag.X.u = 1;
+	tag.X.label = 0x100;
+
+	shared->client_irq_pending = 1;
+
+	local_irq_save(irq_flags); ASSERT( !vcpu_interrupts_enabled() );
+#if 0
+	L4_Set_MsgTag( tag );
+	L4_LoadMR( 1, shared->client_irq );
+	result_tag = L4_Reply( shared->client_main_tid );
+	if( L4_IpcFailed(result_tag) && ((L4_ErrorCode()&1) == 0) ) 
+#endif
+	{
+	    L4_Set_MsgTag( tag );
+	    L4_LoadMR( 1, shared->client_irq );
+	    result_tag = L4_Send( shared->client_irq_tid );
+	}
+	local_irq_restore(irq_flags);
+
+	if( L4_IpcFailed(result_tag) ) {
+	    dprintk( 2, PREFIX "skb destructor notify failed\n" );
+	    shared->client_irq_pending = 0;
+	}
+    }
+}
+#endif
+typedef struct L4VMnet_skb_dp83820_shadow
+{
+    struct sk_buff *skb;
+    L4VMnet_client_info_t *client;
+    L4_Word_t link;
+} L4VMnet_skb_dp83820_shadow_t;
+
+
+extern inline IVMnet_dp83820_descriptor_t *
+get_dp83820_desc( L4VMnet_client_info_t *client, L4_Word_t link )
+{
+    return (IVMnet_dp83820_descriptor_t *)
+	(link - client->dp83820_tx.first + client->dp83820_tx.ring_start);
+}
+
+extern inline IVMnet_dp83820_descriptor_t *
+get_dp83820_current_desc( L4VMnet_client_info_t *client )
+{
+    // TODO: make SMP safe
+    return get_dp83820_desc( client, client->shared_data->dp83820_regs[TXDP] );
+}
+
+static int reignite_transmit( L4VMnet_client_info_t *client )
+{
+    IVMnet_dp83820_descriptor_t *desc;
+
+    desc = get_dp83820_current_desc( client );
+    if( !desc->cmd_status.tx.own )
+	return 0;
+
+    L4VMnet_server.server_status->irq_status |= 1;
+    L4VMnet_server.server_status->irq_pending = 1;
+
+    l4ka_wedge_raise_irq( client->shared_data->server_irq );
+    return 1;
+}
+
+
+static void
+L4VMnet_skb_dp83820_destructor( struct sk_buff *skb )
+{
+    L4VMnet_skb_dp83820_shadow_t *shadow = 
+	(L4VMnet_skb_dp83820_shadow_t *)skb->head;
+    IVMnet_dp83820_descriptor_t *desc, *frag_desc;
+    int do_irq = 0, delay;
+    L4_Word_t link;
+
+    ASSERT(shadow);
+    ASSERT(shadow->skb == skb);
+    ASSERT(shadow->client);
+    ASSERT(shadow->link);
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+
+    desc = get_dp83820_desc( shadow->client, shadow->link );
+    ASSERT( desc->cmd_status.tx.own && desc->cmd_status.tx.ok );
+    if( desc->cmd_status.tx.intr )
+	do_irq = 1;
+    link = desc->cmd_status.tx.more ? desc->link : 0;
+    desc->cmd_status.tx.own = 0;
+    atomic_dec( &dirty_tx_pkt_cnt );
+
+    while( link )
+    {
+	// Release the fragment descriptor back to the client.
+	ASSERT(link);
+	frag_desc = get_dp83820_desc( shadow->client, link );
+    	ASSERT(frag_desc->cmd_status.tx.own && frag_desc->cmd_status.tx.ok);
+	if( frag_desc->cmd_status.tx.intr )
+	    do_irq = 1;
+	link = frag_desc->cmd_status.tx.more ? frag_desc->link : 0;
+	frag_desc->cmd_status.tx.own = 0;
+    }
+
+    delay = reignite_transmit( shadow->client );
+    set_bit( 6, (volatile unsigned long *)&shadow->client->shared_data->dp83820_regs[ISR] );
+    if( do_irq )
+	set_bit( 7, (volatile unsigned long *)&shadow->client->shared_data->dp83820_regs[ISR] );
+//    if( !delay )
+//	notify_dp83820_client( shadow->client );
+
+    // Clean-up the skb.
+    ASSERT( !skb_shinfo(skb)->frag_list );
+    skb_shinfo(skb)->nr_frags = 0;
+    skb->data = skb->head;
+    skb->tail = skb->data;
+    skb->h.raw = skb->data;
+    skb->len = 0;
+    skb->data_len = 0;
+
+    dprintk( 4, PREFIX "skb destructor\n" );
+}
+
+static struct sk_buff *
+L4VMnet_skb_wrap_dp83820_packet( L4VMnet_client_info_t *client )
+{
+    // TODO: validate that buffers + fragments together create valid
+    // skb buffs.  Lots of overlapping state must be validated.
+    IVMnet_dp83820_descriptor_t *desc, *frag_desc;
+    struct sk_buff *skb;
+    L4VMnet_skb_dp83820_shadow_t *skb_shadow;
+    int fragment;
+
+    // Grab the first packet.
+    ASSERT( client->shared_data->dp83820_regs[TXDP] );
+    desc = get_dp83820_current_desc( client );
+    if( !desc->cmd_status.tx.own )
+	return NULL;
+    if( unlikely(desc->cmd_status.tx.ok) )
+    {
+	dprintk( 2, KERN_INFO PREFIX "send wrap-around.\n" );
+	return NULL;
+    }
+
+    skb = alloc_skb( sizeof(L4VMnet_skb_dp83820_shadow_t), GFP_ATOMIC );
+    if( unlikely(skb == NULL) )
+	return NULL;
+
+    skb_shadow = (L4VMnet_skb_dp83820_shadow_t *)skb->head;
+    skb_shadow->client = client;
+    skb_shadow->link = client->shared_data->dp83820_regs[TXDP];
+    skb_shadow->skb = skb;
+
+    // Move to the next packet.
+    client->shared_data->dp83820_regs[TXDP]= desc->link;
+
+    // TODO: validate that the packet lives in the client's address space.
+    ASSERT( desc->buffer && desc->buffer_len );
+    skb->data = bus_to_virt(desc->buffer + client->client_space->bus_start);
+    skb->len  = desc->buffer_len;
+    skb->tail = skb->data + skb->len;
+
+    skb->destructor = L4VMnet_skb_dp83820_destructor;
+    skb->ip_summed  = CHECKSUM_NONE;
+    skb->pkt_type   = PACKET_OUTGOING;
+
+    // Where the device must start the checksum.
+    skb->h.raw = 34 + skb->data;
+    // Where the device must store the checksum.
+    if( desc->extended_status.udp_pkt ) {
+	skb->csum = 40 + skb->data - skb->h.raw;
+	skb->ip_summed = CHECKSUM_HW;
+    }
+    else if( desc->extended_status.tcp_pkt ) {
+	skb->csum = 50 + skb->data - skb->h.raw;
+	skb->ip_summed = CHECKSUM_HW;
+    }
+    else if( desc->extended_status.ip_pkt )
+	L4_KDB_Enter("oops, ip csum not configured");
+
+    dprintk( 4, PREFIX "skb protocol %x, pkt_type %x, "
+	    "data %p (bus %p/%p)\n",
+	    skb->protocol, skb->pkt_type, skb->data, (void *)desc->buffer,
+	    (void *)virt_to_bus(skb->data) );
+
+    desc->cmd_status.tx.ok = 1; // Remember that we have submitted this packet.
+
+    fragment = 0;
+    frag_desc = desc;
+    while( frag_desc->cmd_status.tx.more )
+    {
+	L4_Word_t paddr;
+	IVMnet_dp83820_descriptor_t *last = frag_desc;
+
+	if( unlikely(fragment >= MAX_SKB_FRAGS) ) {
+	    last->cmd_status.tx.more = 0;
+	    break;
+	}
+	ASSERT( client->shared_data->dp83820_regs[TXDP] );
+	frag_desc = get_dp83820_current_desc( client );
+	if( unlikely(!frag_desc->cmd_status.tx.own) ) {
+	    last->cmd_status.tx.more = 0;
+	    break;
+	}
+	frag_desc->cmd_status.tx.ok = 1;
+	client->shared_data->dp83820_regs[TXDP] = frag_desc->link;
+
+	ASSERT( frag_desc->buffer && frag_desc->buffer_len );
+	skb_shinfo(skb)->frags[fragment].size = frag_desc->buffer_len;
+	skb_shinfo(skb)->frags[fragment].page_offset =
+	    (unsigned long)frag_desc->buffer & (PAGE_SIZE-1);
+	paddr = l4ka_wedge_bus_to_phys(frag_desc->buffer + client->client_space->bus_start);
+	skb_shinfo(skb)->frags[fragment].page = mem_map + (paddr >> PAGE_SHIFT);
+	skb->len += frag_desc->buffer_len;
+	skb->data_len += frag_desc->buffer_len;
+	dprintk( 4, PREFIX "frag %d @ %lx + %lx, %u\n",
+		fragment,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page,
+		(L4_Word_t)skb_shinfo(skb)->frags[fragment].page_offset,
+		skb_shinfo(skb)->frags[fragment].size );
+	ASSERT( page_to_bus(skb_shinfo(skb)->frags[fragment].page) == ((frag_desc->buffer + client->client_space->bus_start) & PAGE_MASK) );
+
+	fragment++;
+    }
+    skb_shinfo(skb)->nr_frags = fragment;
+
+    ASSERT( skb_headlen(skb) == desc->buffer_len );
+    if( fragment )
+	dprintk( 4, PREFIX "fragments %d, tot len %d, fragment len %d\n",
+		fragment, skb->len, skb->data_len );
+
+    return skb;
+}
+
+
+static int L4VMnet_dev_queue_dp83820_pkt( L4VMnet_client_info_t *client )
+{
+    struct sk_buff *skb;
+
+    skb = L4VMnet_skb_wrap_dp83820_packet( client );
+    if( skb == NULL ) {
+	set_bit( 9 /* TXIDLE */, (volatile unsigned long *)&client->shared_data->dp83820_regs[ISR] );
+//	notify_dp83820_client( client );
+	return FALSE;
+    }
+
+    // TODO: if the skb exceeds the device's queue length, it will call
+    // kfree_skb(skb) at interrupt time, which isn't compatible with
+    // desctructors.  And it will drop our packets.
+    ASSERT( client->real_dev );
+    skb->dev = client->real_dev;
+    dev_queue_xmit( skb );	// Can be called from an interrupt.
+    atomic_inc( &dirty_tx_pkt_cnt );
+
+    return TRUE;
+}
+
+static void
+L4VMnet_dp83820_tx_pkts( void )
+{
+    L4VMnet_client_info_t *client;
+    L4_Word_t pkts_sent, total_pkts;
+
+    ASSERT( L4_MyLocalId().raw == get_vcpu()->main_ltid.raw );
+    total_pkts = 0;
+    do {
+	pkts_sent = 0;
+	for( client = L4VMnet_server.client_list; client; client = client->next)
+	{
+	    if( !client->operating )
+		continue;
+
+	    if( L4VMnet_dev_queue_dp83820_pkt(client) )
+		pkts_sent++;
+	}
+
+	total_pkts += pkts_sent;
+    } while( pkts_sent );
+
+    dprintk( 4, PREFIX "%lu packets transmitted\n", total_pkts );
+
+}
+
+/***************************************************************************
+ *
+ * Server handlers for executing in top halves and bottom halves.
+ *
+ ***************************************************************************/
+
+static void L4VMnet_attach_handler( L4VMnet_server_cmd_t *params )
+{
+    IVMnet_handle_t handle;
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    L4VM_client_space_info_t *client_space;
+    idl4_fpage_t idl4_fp, idl4_fp2;
+    L4VM_alligned_alloc_t alloc_info;
+    int log2size;
+    unsigned long flags;
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    // Prepare the reply.
+    ipc_env._action = 0;
+
+    // Derive the client's handle from its LAN address.
+    handle = lanaddress_get_handle(
+	    (lanaddress_t *)params->params.attach.lan_address );
+
+    printk( KERN_INFO PREFIX "attach request from handle %u, lan address ",
+	    handle);
+    L4VMnet_print_lan_address( params->params.attach.lan_address );
+    printk( ", for device %s\n", params->params.attach.dev_name );
+
+    // Get access to the client's pages.  This is a HACK.
+    client_space = L4VM_get_client_space_info( params->reply_to_tid );
+    if( client_space == NULL )
+	goto err_client_space;
+
+    // Allocate memory for the datastructures to be mapped into the client.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMnet_client_shared_t)) );
+    if( L4VM_fpage_alloc(log2size, &alloc_info) < 0 )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	goto err_shared_region_alloc;
+    }
+    dprintk( 4, KERN_INFO "client shared region at 0x%p, size %lu, "
+	    "requested size %d\n", (void *)L4_Address(alloc_info.fpage),
+	    L4_Size(alloc_info.fpage), 1 << log2size );
+
+    // Allocate a client structure.
+    client = kmalloc( sizeof(L4VMnet_client_info_t), GFP_KERNEL );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	goto err_client_info_alloc;
+    }
+
+    // Initialize the client structure.
+    client->ivmnet_handle = handle;
+    memcpy( client->lan_address, params->params.attach.lan_address, ETH_ALEN );
+    client->shared_alloc_info = alloc_info;
+    client->client_space = client_space;
+    memset( &client->rcv_ring, 0, sizeof(L4VMnet_skb_ring_t) );
+    client->rcv_ring.cnt = L4VMNET_SKB_RING_LEN;
+
+    // Initialize the pointers to the shared region.
+    client->shared_data = (IVMnet_client_shared_t *)
+	L4_Address(client->shared_alloc_info.fpage);
+    // Page-in the client shared region (zero all the pages)
+    memset( client->shared_data, 0, L4_Size(client->shared_alloc_info.fpage) );
+    client->send_ring.desc_ring = client->shared_data->snd_desc_ring;
+    client->send_ring.start_free = 0;
+    client->send_ring.start_dirty = 0;
+    client->send_ring.cnt = IVMnet_snd_descriptor_ring_cnt;
+
+    client->shared_data->server_irq = L4VMnet_server_irq;
+    client->shared_data->server_irq_tid = L4VMnet_server.my_irq_tid;
+    client->shared_data->server_main_tid = L4VMnet_server.my_main_tid;
+
+    client->operating = FALSE;
+
+    strcpy( client->real_dev_name, params->params.attach.dev_name );
+    client->real_dev = dev_get_by_name( client->real_dev_name );
+    if( client->real_dev ) {
+	// Enable bridging on the device.  This is never disabled,
+	// because we don't maintain reference counts to the real devices.
+	client->real_dev->br_port = (struct net_bridge_port *)&L4VMnet_server;
+    }
+
+    // Enable the client.
+    if( !L4VMnet_client_handle_set(handle, client) )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_client, NULL );
+	goto err_client_handle;
+    }
+
+    client->next = L4VMnet_server.client_list;
+    L4VMnet_server.client_list = client;
+
+    // Reply to the client.
+    idl4_fpage_set_base( &idl4_fp, 0 );
+    idl4_fpage_set_mode( &idl4_fp, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp, client->shared_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp, IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+    idl4_fpage_set_base( &idl4_fp2, L4_Size(client->shared_alloc_info.fpage) );
+    idl4_fpage_set_mode( &idl4_fp2, IDL4_MODE_MAP );
+    idl4_fpage_set_page( &idl4_fp2, L4VMnet_server.status_alloc_info.fpage );
+    idl4_fpage_set_permissions( &idl4_fp2, IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_attach_propagate_reply( params->reply_to_tid,
+	    &handle, &idl4_fp, &idl4_fp2, &ipc_env );
+    local_irq_restore(flags);
+    // TODO: check for IPC error, and free resources if necessary.
+    return;
+
+err_client_handle:
+    kfree( client );
+err_client_info_alloc:
+    L4VM_fpage_dealloc( &alloc_info );
+err_shared_region_alloc:
+err_client_space:
+    printk( KERN_ERR PREFIX "error attaching client.\n" );
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    IVMnet_Control_attach_propagate_reply( params->reply_to_tid,
+	    NULL, NULL, NULL, &ipc_env );
+    local_irq_restore(flags);
+    return;
+}
+
+static void L4VMnet_reattach_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+    L4VMnet_client_info_t *client;
+
+    dprintk( 2, PREFIX "reattach request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.reattach.handle );
+    if( client == NULL )
+    {
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_VirtualSender( L4VMnet_server.server_tid );
+	IVMnet_Control_reattach_propagate_reply( params->reply_to_tid,
+		NULL, NULL, &ipc_env );
+	local_irq_restore(flags);
+    }
+    else
+    {
+	idl4_fpage_t idl4_fp, idl4_fp2;
+
+	ipc_env._action = 0;
+
+	L4VM_page_in( client->shared_alloc_info.fpage, PAGE_SIZE );
+	L4VM_page_in( L4VMnet_server.status_alloc_info.fpage, PAGE_SIZE );
+
+	idl4_fpage_set_base( &idl4_fp, 0 );
+	idl4_fpage_set_mode( &idl4_fp, IDL4_MODE_MAP );
+	idl4_fpage_set_page( &idl4_fp, client->shared_alloc_info.fpage );
+	idl4_fpage_set_permissions( &idl4_fp, 
+		IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+	idl4_fpage_set_base( &idl4_fp2, 
+		L4_Size(client->shared_alloc_info.fpage) );
+	idl4_fpage_set_mode( &idl4_fp2, IDL4_MODE_MAP );
+	idl4_fpage_set_page( &idl4_fp2, 
+		L4VMnet_server.status_alloc_info.fpage );
+	idl4_fpage_set_permissions( &idl4_fp2, 
+		IDL4_PERM_WRITE | IDL4_PERM_READ );
+
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_VirtualSender( L4VMnet_server.server_tid );
+	IVMnet_Control_reattach_propagate_reply( params->reply_to_tid,
+		&idl4_fp, &idl4_fp2, &ipc_env );
+	local_irq_restore(flags);
+    }
+}
+
+static void L4VMnet_detach_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    unsigned long flags;
+    L4VMnet_client_info_t *client;
+    ipc_env._action = 0;
+
+    dprintk( 2, PREFIX "detach request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	client->operating = FALSE;
+	if( client->real_dev ) {
+	    dev_put( client->real_dev );
+	    client->real_dev = NULL;
+	}
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_detach_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_start_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 2, PREFIX "start request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	if( client->real_dev ) {
+	    client->operating = TRUE;
+
+	    rtnl_lock();
+	    dev_change_flags( client->real_dev, 
+		    IFF_UP | IFF_BROADCAST | IFF_MULTICAST | IFF_PROMISC );
+	    rtnl_unlock();
+	}
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_start_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_stop_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 2, PREFIX "stop request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else
+    {
+	ipc_env._action = 0;
+	client->operating = FALSE;
+	// TODO: wait for client operations to quiesce.
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_stop_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_update_stats_handler( L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 3, PREFIX "update stats request from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+    ASSERT( sizeof( IVMnet_stats_t ) == sizeof( struct net_device_stats ) );
+
+    client = L4VMnet_client_handle_lookup( params->params.start.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else {
+	struct net_device *netdev;
+
+#warning stg: reporting physical instead of virtual device stats
+	ipc_env._action = 0;
+	netdev = dev_get_by_name( "eth0" );
+	memcpy( &L4VMnet_server.server_status->stats, netdev->get_stats( netdev ), sizeof( struct net_device_stats ) );
+	dev_put( netdev );
+    }
+
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_update_stats_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+static void L4VMnet_register_dp83820_tx_ring_handler(
+    L4VMnet_server_cmd_t *params )
+{
+    idl4_server_environment ipc_env;
+    L4VMnet_client_info_t *client;
+    unsigned long flags;
+
+    dprintk( 3, PREFIX "register dp83820 tx ring from 0x%lx\n",
+	    params->reply_to_tid.raw );
+
+    ASSERT( params->reply_to_tid.raw );
+    ASSERT( !L4_IsNilThread(L4VMnet_server.server_tid) );
+
+    client = L4VMnet_client_handle_lookup( params->params.register_dp83820_tx_ring.handle );
+    if( client == NULL )
+	CORBA_exception_set( &ipc_env, ex_IVMnet_invalid_handle, NULL );
+    else {
+	// Extract the client parameters.
+	L4_Word_t client_paddr = 
+	    params->params.register_dp83820_tx_ring.client_paddr;
+	L4_Word_t client_size = 
+	    params->params.register_dp83820_tx_ring.ring_size_bytes;
+	L4_Word_t has_extended_status = 
+	    params->params.register_dp83820_tx_ring.has_extended_status;
+	L4_Fpage_t fp_req = L4_Fpage( client_paddr, client_size );
+	CORBA_Environment req_env = idl4_default_environment;
+	idl4_fpage_t fp;
+	int err;
+
+	ipc_env._action = 0;
+
+	if( !has_extended_status ) {
+	    printk( KERN_ERR PREFIX "The client must use the dp83820 extended status.  Fatal error.\n" );
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Request a virtual address region from Linux.
+	err = L4VM_fpage_vmarea_get( L4_SizeLog2(fp_req), 
+		&client->dp83820_tx.vmarea );
+	if( err < 0 ) {
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Request the client pages from the resourcemon.
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	idl4_set_rcv_window( &req_env, client->dp83820_tx.vmarea.fpage );
+	IResourcemon_request_client_pages( 
+		resourcemon_shared.cpu[L4_ProcessorNo()].resourcemon_tid,
+		&params->reply_to_tid, fp_req.raw, &fp, &req_env );
+	local_irq_restore(flags);
+
+	if( req_env._major != CORBA_NO_EXCEPTION ) {
+	    CORBA_exception_free( &req_env );
+	    CORBA_exception_set( &ipc_env, ex_IVMnet_no_memory, NULL );
+	    goto out;
+	}
+
+	// Point at our local address for the descriptor ring.
+	client->dp83820_tx.first = client_paddr;
+	client->dp83820_tx.ring_start =
+	    (L4_Address(client->dp83820_tx.vmarea.fpage) +
+	     ((L4_Size(client->dp83820_tx.vmarea.fpage)-1) & client_paddr) );
+	dprintk( 2, PREFIX "dp83820 tx ring, client %08lx, DD/OS %08lx\n",
+		client_paddr, client->dp83820_tx.ring_start );
+    }
+
+out:
+    local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+    L4_Set_VirtualSender( L4VMnet_server.server_tid );
+    IVMnet_Control_register_dp83820_tx_ring_propagate_reply( params->reply_to_tid, &ipc_env );
+    local_irq_restore(flags);
+}
+
+
+static void L4VMnet_cmd_dispatcher( L4VMnet_server_cmd_ring_t *cmd_ring )
+{
+    while( 1 )
+    {
+	L4VMnet_server_cmd_t *cmd = &cmd_ring->cmds[ cmd_ring->next_cmd ];
+	if( !cmd->handler )
+	    break;
+
+	// Dispatch the command entry.
+	cmd->handler( cmd );
+
+	// Release the command entry and get the next command.
+	cmd->handler = NULL;
+	cmd_ring->next_cmd = (cmd_ring->next_cmd + 1) % L4VMNET_SERVER_CMD_RING_LEN;
+    }
+
+    if( cmd_ring->wake_server )
+    {
+	unsigned long flags;
+
+	cmd_ring->wake_server = FALSE;
+
+	local_irq_save(flags); ASSERT( !vcpu_interrupts_enabled() );
+	L4_Set_MsgTag( (L4_MsgTag_t){ raw:0 } );
+	L4_Reply( L4VMnet_server.server_tid );
+	local_irq_restore(flags);
+    }
+}
+
+/***************************************************************************
+ *
+ * Server thread.
+ *
+ ***************************************************************************/
+
+static L4VMnet_server_cmd_t * L4VMnet_allocate_cmd(
+	L4VMnet_server_cmd_ring_t *ring,
+	L4_Word_t irq_flag )
+{
+    L4VMnet_server_cmd_t *cmd = &ring->cmds[ ring->next_free ];
+
+    // Wait until the command is available for use.  We wait for an
+    // IPC from *any* thread, so confirm that we actually have
+    // a free cmd slot when we wake.
+    while( cmd->handler )
+    {
+	L4_ThreadId_t to_tid, from_tid;
+
+	dprintk( 1, PREFIX "command queue full, sending IRQ to Linux\n" );
+
+	// Tell the handler that it needs to wake server thread.
+	ring->wake_server = TRUE;
+
+	// 1. Raise IRQ flag.
+	L4VMnet_server.irq_status |= irq_flag;
+
+	// 2. Deliver IRQ.
+	if( ((L4VMnet_server.irq_status & L4VMnet_server.irq_mask) != 0) &&
+		!L4VMnet_server.irq_pending )
+	{
+	    L4_MsgTag_t msgtag = (L4_MsgTag_t){raw:0};
+	    msgtag.X.label = L4_TAG_IRQ; msgtag.X.u = 1;
+	    L4_LoadMR( 1, L4VMnet_server_irq );
+	    L4_Set_MsgTag(msgtag);
+
+	    to_tid = L4VMnet_server.my_irq_tid;
+	    L4VMnet_server.irq_pending = TRUE;
+	}
+	else
+	    to_tid = L4_nilthread;
+
+	// 3. Wait for reactivation.
+	L4_Ipc( to_tid, L4_anythread, 
+		L4_Timeouts(L4_Never, L4_Never), &from_tid);
+	// Ignore IPC errors and IPC from unknown clients.
+	// We restart the loop, which will correct for errors.
+    }
+
+    // Move to the next command.
+    ring->next_free = (ring->next_free + 1) % L4VMNET_SERVER_CMD_RING_LEN;
+
+    return cmd;
+}
+
+static void
+L4VMnet_xdeliver_irq( unsigned event )
+    // Delivers an IRQ request to the IRQ thread, from any L4 thread
+    // running in the kernel.  Thus this function doesn't necessary
+    // execute in the main VM thread.  This function isn't performance
+    // oriented.
+{
+    L4VMnet_server.irq_status |= event;
+
+    if( ((L4VMnet_server.irq_status & L4VMnet_server.irq_mask) != 0) &&
+	    !L4VMnet_server.irq_pending )
+    {
+	L4_MsgTag_t msgtag;
+
+	dprintk( 3, PREFIX "delivering irq to Linux tid %lx\n",
+		L4VMnet_server.my_irq_tid.raw );
+
+	L4VMnet_server.irq_pending = TRUE;
+
+	msgtag.raw = 0;
+	msgtag.X.u = 1;
+	msgtag.X.label = 0x100;
+	L4_Set_MsgTag( msgtag );
+	L4_LoadMR( 1, L4VMnet_server_irq );
+
+	L4_Send( L4VMnet_server.my_irq_tid );
+    }
+}
+
+IDL4_INLINE void IVMnet_Control_attach_implementation(
+	CORBA_Object _caller,
+	const char *dev_name,
+	const char *lan_address,
+	IVMnet_handle_t *handle,
+	idl4_fpage_t *shared_window,
+	idl4_fpage_t *server_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    memcpy( cmd->params.attach.dev_name, dev_name, IFNAMSIZ );
+    cmd->params.attach.dev_name[IFNAMSIZ-1] = '\0';
+    memcpy( cmd->params.attach.lan_address, lan_address, ETH_ALEN );
+    cmd->handler = L4VMnet_attach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_ATTACH(IVMnet_Control_attach_implementation);
+
+IDL4_INLINE void IVMnet_Control_reattach_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_fpage_t *shared_window,
+	idl4_fpage_t *server_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.reattach.handle = handle;
+    cmd->handler = L4VMnet_reattach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_REATTACH(IVMnet_Control_reattach_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_detach_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.detach.handle = handle;
+    cmd->handler = L4VMnet_detach_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_DETACH(IVMnet_Control_detach_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_start_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.start.handle = handle;
+    cmd->handler = L4VMnet_start_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_START(IVMnet_Control_start_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_stop_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.stop.handle = handle;
+    cmd->handler = L4VMnet_stop_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_STOP(IVMnet_Control_stop_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_update_stats_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.top_half_cmds,
+	    L4VMNET_IRQ_TOP_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.update_stats.handle = handle;
+    cmd->handler = L4VMnet_update_stats_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_TOP_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_UPDATE_STATS(IVMnet_Control_update_stats_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_run_dispatcher_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_DISPATCH );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_RUN_DISPATCHER(IVMnet_Control_run_dispatcher_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_dp83820_tx_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	idl4_server_environment *_env)
+{
+
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_DP83820_TX );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_DP83820_TX(IVMnet_Control_dp83820_tx_implementation);
+
+
+IDL4_INLINE void IVMnet_Control_register_dp83820_tx_ring_implementation(
+	CORBA_Object _caller,
+	const IVMnet_handle_t handle,
+	const L4_Word_t client_paddr,
+	const L4_Word_t ring_size_bytes,
+	const L4_Word_t has_extended_status,
+	idl4_server_environment *_env)
+{
+    L4VMnet_server_cmd_t *cmd;
+
+    cmd = L4VMnet_allocate_cmd( &L4VMnet_server.bottom_half_cmds,
+	    L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    cmd->reply_to_tid = _caller;
+    cmd->params.register_dp83820_tx_ring.handle = handle;
+    cmd->params.register_dp83820_tx_ring.client_paddr = client_paddr;
+    cmd->params.register_dp83820_tx_ring.ring_size_bytes = ring_size_bytes;
+    cmd->params.register_dp83820_tx_ring.has_extended_status = has_extended_status;
+    cmd->handler = L4VMnet_register_dp83820_tx_ring_handler;
+    L4VMnet_xdeliver_irq( L4VMNET_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMNET_CONTROL_REGISTER_DP83820_TX_RING(IVMnet_Control_register_dp83820_tx_ring_implementation);
+
+
+void L4VMnet_server_thread( void *param )
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    char dev_name[IFNAMSIZ];
+    char addr_buf[ETH_ALEN];
+    static void *IVMnet_Control_vtable[IVMNET_CONTROL_DEFAULT_VTABLE_SIZE] = IVMNET_CONTROL_DEFAULT_VTABLE;
+
+    idl4_msgbuf_init(&msgbuf);
+    idl4_msgbuf_add_buffer( &msgbuf, dev_name, sizeof(dev_name) );
+    idl4_msgbuf_add_buffer( &msgbuf, addr_buf, sizeof(addr_buf) );
+
+    while( 1 )
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while( 1 )
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    dprintk( 1, PREFIX "server thread request from %lx\n", partner.raw);
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt,
+		    IVMnet_Control_vtable[
+		      idl4_get_function_id(&msgtag) & IVMNET_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMnet_Control_discard(void)
+{
+    // Invoked in response to invalid IPC messages.
+}
+
+/***************************************************************************
+ *
+ * IRQ and bottom half handling.
+ *
+ ***************************************************************************/
+
+static void
+L4VMnet_bottom_half_handler( void * param )
+{
+    dprintk( 2, PREFIX "bottom half handler, in interrupt: %lu\n",
+	    in_interrupt() );
+
+    L4VMnet_cmd_dispatcher( &L4VMnet_server.bottom_half_cmds );
+}
+
+static irqreturn_t
+L4VMnet_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    while( 1 )
+    {
+	// Outstanding events?  Read them and reset without losing events.
+	unsigned events, client_events;
+
+	do {
+	    events = L4VMnet_server.irq_status;
+	} while( cmpxchg(&L4VMnet_server.irq_status, events, 0) != events );
+	do {
+	    client_events = L4VMnet_server.server_status->irq_status;
+	} while( cmpxchg(&L4VMnet_server.server_status->irq_status, client_events, 0) != client_events );
+
+	if( !events && !client_events )
+	    return IRQ_HANDLED;
+	dprintk( 3, PREFIX "irq handler: 0x%x, 0x%x\n", events, client_events );
+
+	// No one needs to deliver IRQ messages.  They would be superfluous.
+	L4VMnet_server.irq_pending = TRUE;
+	L4VMnet_server.server_status->irq_pending = TRUE;
+
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+	if( (events & L4VMNET_IRQ_DISPATCH) || client_events )
+	{
+	    // TODO: can we do this at IRQ time?
+	    L4VMnet_transmit_client_pkts();
+	}
+#else
+	if( (events & L4VMNET_IRQ_DP83820_TX) || client_events )
+	{
+	    L4VMnet_dp83820_tx_pkts();
+	}
+#endif
+
+	if( (events & L4VMNET_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VMnet_cmd_dispatcher( &L4VMnet_server.top_half_cmds );
+
+	if( (events & L4VMNET_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &L4VMnet_server.bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	L4VMnet_server.irq_pending = FALSE;
+	L4VMnet_server.server_status->irq_pending = FALSE;
+    }
+}
+
+#if 0
+
+static int
+L4VMnet_irq_pending( void *data )
+{
+    return (L4VMnet_server.irq_status & L4VMnet_server.irq_mask) ||
+	   L4VMnet_server.server_status->irq_status;
+}
+
+#endif
+/***************************************************************************
+ *
+ * Packet send functions.
+ *
+ ***************************************************************************/
+
+static int L4VMnet_xmit_packets_to_client_thread(
+	L4VMnet_client_info_t *client,
+	int receiver )
+{
+    L4VMnet_skb_ring_t *ring = &client->rcv_ring;
+    L4_Word_t idx;
+    L4_Word_t string_items = 0;
+    L4_MsgTag_t tag;
+    L4_StringItem_t str_item;
+    L4_ThreadId_t receiver_tid;
+    unsigned long flags;
+    L4_Word_t len, transferred_bytes = 0;
+    L4_Word_t cnt=0;
+
+    dprintk( 4, PREFIX "xmit_packets_to_client %p\n", client);
+    
+    if( receiver >= client->shared_data->receiver_cnt )
+    {
+	dprintk( 4, PREFIX "inbound packets for unavailable client\n" );
+	return FALSE;
+    }
+    receiver_tid = client->shared_data->receiver_tids[ receiver ];
+
+    local_irq_save(flags); // Protect our message registers.
+    ASSERT( !vcpu_interrupts_enabled() );
+
+    // Install string items for the message.
+    idx = ring->start_dirty;
+    while( string_items < IVMnet_rcv_buffer_cnt )
+    {
+	struct sk_buff *skb = ring->skb_ring[ idx ];
+	if( skb == NULL )
+	    break;	// No more packets.
+
+	if( string_items > 0 )
+	{
+	    // Store the string item from the last iteration.  We delay
+	    // by one iteration to update the C bit.
+	    str_item.X.C = 1;
+	    L4_LoadMR( 2*string_items-1, str_item.raw[0] );
+	    L4_LoadMR( 2*string_items,   str_item.raw[1] );
+	}
+
+	len = skb_headlen(skb);
+	str_item = L4_StringItem( len, skb->data );
+	transferred_bytes += len;
+
+	string_items++;
+	idx = (idx + 1) % ring->cnt;
+	if( idx == ring->start_dirty )
+	    break;	// Wrap-around.
+    }
+    dprintk( 4, PREFIX "%lu inbound packets, %lu bytes to transfer.\n", 
+	    string_items, transferred_bytes );
+
+    if( string_items == 0 )
+    {
+	local_irq_restore(flags);
+	return FALSE;
+    }
+
+    // Write the last string item.
+    L4_LoadMR( 2*string_items-1, str_item.raw[0] );
+    L4_LoadMR( 2*string_items,   str_item.raw[1] );
+
+    // Initialize the IPC message tag.
+    tag.raw = 0;
+    tag.X.t = 2*string_items;
+    L4_Set_MsgTag( tag );
+
+    client->shared_data->receiver_tids[receiver] = L4_nilthread;
+
+    // Deliver the IPC.
+    L4_Set_XferTimeouts( L4_Timeouts(L4_Never, L4_Never) );
+#warning "Move the outer loop here, and retry the various client threads!"
+    tag = L4_Reply( receiver_tid );
+    if( unlikely(L4_IpcFailed(tag)) )
+    {
+	L4_KDB_Enter("Message overflow");
+	dprintk( 4, PREFIX "message overflow %x.\n", receiver_tid.raw );
+	L4_Word_t err = L4_ErrorCode();
+	if( ((err >> 1) & 7) <= 3 ) {
+	    // Send-phase error.
+	    client->shared_data->receiver_tids[receiver] = receiver_tid;
+	    transferred_bytes = 0;
+	}
+	else {
+
+	    // Message overflow.
+	    transferred_bytes = err >> 4;
+	}
+    }
+
+    dprintk( 4, PREFIX "%lu bytes transferred.\n", transferred_bytes );
+
+    // Commit the mappings.  We may have had partial transfer.  We resend
+    // any skbuffs that had partial transfer.
+    for( idx = 0; (idx < string_items) && transferred_bytes; idx++ )
+    {
+	struct sk_buff *skb = ring->skb_ring[ ring->start_dirty ];
+	len = skb_headlen(skb);
+	if( transferred_bytes >= len ) {
+	    cnt++;
+	    transferred_bytes -= len;
+	    dev_kfree_skb_any( ring->skb_ring[ ring->start_dirty ] );
+	    ring->skb_ring[ ring->start_dirty ] = NULL;
+	    ring->start_dirty = (ring->start_dirty + 1) % ring->cnt;
+	}
+	else 
+	    transferred_bytes = 0;
+    }
+
+    local_irq_restore(flags);
+
+    return TRUE;
+}
+
+static void L4VMnet_xmit_packets_to_client( L4VMnet_client_info_t *client )
+{
+#if !defined(CONFIG_AFTERBURN_DRIVERS_NET_DP83820)
+    int receiver = 0;
+
+    while( (receiver < client->shared_data->receiver_cnt) &&
+	    (receiver < IVMnet_max_receiver_cnt) )
+    {
+	if( L4_IsNilThread( client->shared_data->receiver_tids[receiver] ) )
+	{
+	    receiver++;
+	    continue;
+	}
+	if( !L4VMnet_xmit_packets_to_client_thread(client, receiver) )
+	    return;
+	receiver++;
+    }
+    dprintk( 3, PREFIX "wow, not enough client receiver threads!\n" );
+#else
+    if( !L4_IsNilThread( client->shared_data->receiver_tids[0] ) )
+	L4VMnet_xmit_packets_to_client_thread( client, 0 );
+    else 
+    {
+	if (client)
+	{
+	    dprintk( 4, PREFIX "client receiver thread is nilthread %p\n", 
+		    client->shared_data->receiver_tids );
+	}
+	
+    }
+#endif
+}
+
+static void L4VMnet_queue_pkt_to_client(
+	L4VMnet_client_info_t *client,
+	struct sk_buff *skb )
+{
+    L4VMnet_skb_ring_t *ring;
+
+    if( !client->operating )
+    {
+	dev_kfree_skb_any( skb );
+	return;
+    }
+
+    ring = &client->rcv_ring;
+    dprintk( 4, PREFIX "queue packet %p to client %p ring %p\n", skb, client, ring);
+
+    // Make room on the queue.
+    if( ring->skb_ring[ring->start_free] != NULL )
+    {
+	// Ring is full.  So drop the packet.
+	dev_kfree_skb_any( skb );
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+	dprintk( 2, PREFIX "wow, too many packets for client!\n" );
+	return;
+    }
+
+    ring->skb_ring[ ring->start_free ] = skb;
+    ring->start_free = (ring->start_free + 1) % ring->cnt;
+
+    if( L4VMnet_skb_ring_pending(&client->rcv_ring) > 2 || 
+	    (atomic_read(&dirty_tx_pkt_cnt) == 0) )
+    {
+	dprintk( 4, PREFIX "schedule flush tasklet\n");
+
+	tasklet_hi_schedule( &L4VMnet_flush_tasklet );
+    }
+}
+
+
+static int
+L4VMnet_absorb_frame( struct sk_buff *skb )
+    // Returns TRUE if the packet has been handled.
+    // Returns FALSE if the packet is supposed to be handled by Linux.
+{
+    L4VMnet_client_info_t *client;
+    IVMnet_handle_t handle;
+    lanaddress_t *lanaddr = (lanaddress_t *)skb->mac.raw;
+    lanaddress_t *linux_lanaddr;
+
+    if( unlikely(skb_shinfo(skb)->nr_frags > 0) )
+    {
+	// We don't yet support fragmentation, so drop the packet.
+	dev_kfree_skb_any( skb );
+	return TRUE;
+    }
+    // Fast map the destination LAN address into a handle, and then lookup
+    // the client structure.
+    handle = lanaddress_get_handle( lanaddr );
+    client = L4VMnet_client_handle_lookup( handle );
+    if( client ) {
+	L4VMnet_queue_pkt_to_client( client, skb );
+	return TRUE;
+    }
+
+    // Is the packet destined for the local Linux?
+    linux_lanaddr = (lanaddress_t *)skb->dev->dev_addr;
+    if( (linux_lanaddr->align4.lsb == lanaddr->align4.lsb) &&
+	    (linux_lanaddr->align4.msb == lanaddr->align4.msb) )
+    {
+	return FALSE;
+    }
+    
+    for( client = L4VMnet_server.client_list; client; client = client->next )
+    {
+	struct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);
+	if( clone )
+	    L4VMnet_queue_pkt_to_client( client, clone );
+    }
+
+    return FALSE;	// Return broadcasts to Linux too.
+}
+
+static void
+L4VMnet_flush_tasklet_handler( unsigned long unused )
+{
+    dprintk( 4, PREFIX "flush tasklet handler client list %p (server %p)\n",
+	    L4VMnet_server.client_list, &L4VMnet_server);
+
+    L4VMnet_client_info_t *client;
+
+    for( client = L4VMnet_server.client_list; client; client = client->next )
+	if( client->operating )
+	    L4VMnet_xmit_packets_to_client( client );
+	else
+	    dprintk( 4, PREFIX "flush tasklet handler client %p not operating\n", client);
+
+}
+
+
+/***************************************************************************
+ *
+ * Linux Module functions.
+ *
+ ***************************************************************************/
+
+static int __init
+L4VMnet_server_alloc( void )
+{
+    int err, log2size;
+
+    // Initialize the server.
+    L4VMnet_server.server_tid = L4_nilthread;
+
+    SET_MODULE_OWNER( L4VMnet_server );
+
+    INIT_TQUEUE( &L4VMnet_server.bottom_half_task,
+	    L4VMnet_bottom_half_handler, NULL );
+
+    // Skip server cmd ring init ... already zeroed.
+
+    // Allocate a status page, shared by all clients.
+    log2size = L4_SizeLog2( L4_Fpage(0,sizeof(IVMnet_server_shared_t)) );
+    err = L4VM_fpage_alloc( log2size, &L4VMnet_server.status_alloc_info );
+    if( err < 0 )
+	goto err_server_status;
+    L4VMnet_server.server_status = (IVMnet_server_shared_t *)
+	L4_Address( L4VMnet_server.status_alloc_info.fpage );
+    L4VMnet_server.server_status->irq_status = 0;
+    L4VMnet_server.server_status->irq_pending = 0;
+
+    // Some IRQ init.
+    L4VMnet_server.irq_status = 0;
+    L4VMnet_server.irq_mask = ~0;
+    L4VMnet_server.irq_pending = FALSE;
+    L4VMnet_server.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    L4VMnet_server.my_main_tid = L4VM_linux_main_thread( smp_processor_id() );
+
+    if (L4VMnet_server_irq == 0)
+    {
+#if defined(CONFIG_X86_IO_APIC)
+        L4_KernelInterfacePage_t *kip = (L4_KernelInterfacePage_t *) L4_GetKernelInterface();
+        L4VMnet_server_irq = L4_ThreadIdSystemBase(kip) + 3;	
+	acpi_register_gsi(L4VMnet_server_irq, ACPI_LEVEL_SENSITIVE, ACPI_ACTIVE_LOW);
+
+#else
+	L4VMnet_server_irq = 9;
+#endif
+    }
+    printk( KERN_INFO PREFIX "L4VMnet server irq %d\n", L4VMnet_server_irq );
+
+    // Allocate a virtual interrupt.
+    L4VMnet_server.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    // TODO: in case of error, we don't want the irq to remain virtual.
+    l4ka_wedge_add_virtual_irq( L4VMnet_server_irq );
+    
+
+
+
+    err = request_irq( L4VMnet_server_irq, L4VMnet_irq_handler, 0,
+	    "l4ka_net_server", &L4VMnet_server );
+    if( err < 0 )
+    {
+	    printk( KERN_ERR PREFIX "unable to allocate virtual interrupt %d, err: %d\n", 
+		    L4VMnet_server_irq, err);
+	goto err_request_irq;
+    }
+
+    // Start the server loop.
+    L4VMnet_server.server_tid = L4VM_thread_create( GFP_KERNEL, 
+	    L4VMnet_server_thread, l4ka_wedge_get_irq_prio(), 
+	    smp_processor_id(), NULL, 0 );
+    if( L4_IsNilThread(L4VMnet_server.server_tid) )
+    {
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	err = -ENOMEM;
+	goto err_thread_start;
+    }
+
+    return 0;
+
+err_thread_start:
+    free_irq( L4VMnet_server_irq, &L4VMnet_server );
+err_request_irq:
+    L4VM_fpage_dealloc( &L4VMnet_server.status_alloc_info );
+err_server_status:
+    return -ENODEV;
+}
+
+static int __init
+L4VMnet_server_init_module( void )
+{
+    int result;
+
+    if( br_handle_frame_hook ) {
+	printk( KERN_ERR "Error: The bridging module is already loaded!  This\n"
+		"module is incompatible with the bridging module.\n" );
+	return -ENODEV;
+    }
+
+    br_handle_frame_hook = L4VMnet_bridge_handler;
+    L4VMnet_client_list_init();
+
+    // TODO: register device notifiers!!
+
+    result = L4VMnet_server_alloc();
+    if( result )
+	return result;
+
+    // Register with the locator.
+    L4VM_server_register_location( UUID_IVMnet, L4VMnet_server.server_tid );
+
+    printk( KERN_INFO PREFIX "L4VMnet server initialized.\n" );
+
+    return 0;
+}
+
+static void __exit
+L4VMnet_server_exit_module( void )
+{
+    if( !L4_IsNilThread(L4VMnet_server.server_tid) )
+	L4VM_thread_delete( L4VMnet_server.server_tid );
+    free_irq( L4VMnet_server_irq, &L4VMnet_server );
+    L4VM_fpage_dealloc( &L4VMnet_server.status_alloc_info );
+    br_handle_frame_hook = NULL;
+}
+
+MODULE_AUTHOR( "Joshua LeVasseur <jtl@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4 network device driver server stub" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM net" );
+MODULE_VERSION( "yoda" );
+
+module_init( L4VMnet_server_init_module );
+module_exit( L4VMnet_server_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
diff -Naur linux-2.6.9.src/afterburn/drivers/net/server.h linux-2.6.9/afterburn/drivers/net/server.h
--- linux-2.6.9.src/afterburn/drivers/net/server.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/net/server.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,146 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2005,  University of Karlsruhe
+ *
+ * File path:	l4ka-drivers/net/server.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVERS__NET__SERVER_H__
+#define __L4KA_DRIVERS__NET__SERVER_H__
+
+#include "L4VMnet_idl_server.h"
+#include "L4VMnet_idl_reply.h"
+#include "net.h"
+
+#define L4VMNET_IRQ_BOTTOM_HALF_CMD	1
+#define L4VMNET_IRQ_TOP_HALF_CMD	2
+#define L4VMNET_IRQ_DISPATCH		4
+#define L4VMNET_IRQ_DP83820_TX		8
+
+// Forward declarations.
+struct L4VMnet_server_cmd;
+typedef struct L4VMnet_server_cmd L4VMnet_server_cmd_t;
+
+typedef void (*L4VMnet_server_cmd_handler_t)( L4VMnet_server_cmd_t * );
+
+struct L4VMnet_server_cmd
+{
+    volatile L4VMnet_server_cmd_handler_t handler;
+    CORBA_Object reply_to_tid;
+
+    union
+    {
+	struct {
+	    char dev_name[IFNAMSIZ];
+	    char lan_address[ETH_ALEN];
+	} attach;
+
+	struct {
+	    IVMnet_handle_t handle;
+	} reattach;
+
+	struct {
+	    IVMnet_handle_t handle;
+	} detach;
+
+	struct {
+	    IVMnet_handle_t handle;
+	} start;
+
+	struct {
+	    IVMnet_handle_t handle;
+	} stop;
+
+	struct {
+	    IVMnet_handle_t handle;
+	} update_stats;
+
+	struct {
+	    IVMnet_handle_t handle;
+	    L4_Word_t client_paddr;
+	    L4_Word_t ring_size_bytes;
+	    L4_Word_t has_extended_status;
+	} register_dp83820_tx_ring;
+
+    } params;
+};
+
+#define L4VMNET_SERVER_CMD_RING_LEN	8
+
+typedef struct L4VMnet_server_cmd_ring
+{
+    L4_Word16_t next_free;
+    L4_Word16_t next_cmd;
+    volatile int wake_server;
+    L4VMnet_server_cmd_t cmds[L4VMNET_SERVER_CMD_RING_LEN];
+} L4VMnet_server_cmd_ring_t;
+
+typedef struct L4VMnet_client_info
+{
+    struct L4VMnet_client_info *next;
+    L4VM_client_space_info_t *client_space;
+    struct net_device *real_dev; /* Our partner Linux network device. */
+    int operating;
+
+    IVMnet_client_shared_t *shared_data;
+    L4VMnet_desc_ring_t send_ring;
+    L4VMnet_skb_ring_t rcv_ring; /* Packets to send to client. */
+
+    struct {
+	L4_Word_t first;
+	L4_Word_t ring_start;
+	L4VM_alligned_vmarea_t vmarea;
+    } dp83820_tx;
+
+    L4VM_alligned_alloc_t shared_alloc_info;
+    IVMnet_handle_t ivmnet_handle;	/* Client's handle. */
+    char lan_address[ETH_ALEN];
+    char real_dev_name[IFNAMSIZ];
+} L4VMnet_client_info_t;
+
+typedef struct
+{
+    L4_ThreadId_t server_tid; /* Our server thread ID. */
+    L4_ThreadId_t my_irq_tid; /* The L4Linux IRQ thread. */
+    L4_ThreadId_t my_main_tid; /* The L4Linux main thread. */
+
+    int irq_pending;
+    volatile unsigned irq_status;
+    volatile unsigned irq_mask;
+
+    L4VMnet_server_cmd_ring_t top_half_cmds;
+    L4VMnet_server_cmd_ring_t bottom_half_cmds;
+
+    struct tq_struct bottom_half_task;
+
+    IVMnet_server_shared_t *server_status;
+    L4VM_alligned_alloc_t status_alloc_info;
+
+    L4VMnet_client_info_t *client_list;
+} L4VMnet_server_t;
+
+extern L4VMnet_server_t L4VMnet_server;
+
+#endif	/* __L4KA_DRIVERS__NET__SERVER_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/common.h linux-2.6.9/afterburn/drivers/pci/common.h
--- linux-2.6.9.src/afterburn/drivers/pci/common.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/common.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,53 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-driver-reuse/kernel/pci/common.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVER_REUSE__KERNEL__PCI__COMMON_H__
+#define __L4KA_DRIVER_REUSE__KERNEL__PCI__COMMON_H__
+
+#include <l4/types.h>
+
+#ifndef TRUE
+# define TRUE	1
+#endif
+#ifndef FALSE
+# define FALSE	0
+#endif
+
+#define RAW(a)	((void *)((a).raw))
+
+#define PARANOID(a)		a
+#define ASSERT(a)		do { if(!(a)) { printk( PREFIX "assert failure %s:%s:%d\n", __FILE__, __FUNCTION__, __LINE__); L4_KDB_Enter("assert"); } } while(0)
+
+#define dprintk(n,a...) do { if(L4VMpci_debug_level >= (n)) printk(a); } while(0)
+
+
+extern int L4VMpci_debug_level;
+
+
+#endif	/* __L4KA_DRIVER_REUSE__KERNEL__PCI__COMMON_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/L4VMpci_idl_reply.h linux-2.6.9/afterburn/drivers/pci/L4VMpci_idl_reply.h
--- linux-2.6.9.src/afterburn/drivers/pci/L4VMpci_idl_reply.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/L4VMpci_idl_reply.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,150 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-driver-reuse/kernel/pci/L4VMpci_idl_reply.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVER_REUSE__KERNEL__PCI__L4VMPCI_IDL_REPLY_H__
+#define __L4KA_DRIVER_REUSE__KERNEL__PCI__L4VMPCI_IDL_REPLY_H__
+
+static inline void  IVMpci_Control_register_propagate_reply(CORBA_Object  _client, idl4_server_environment *_env)
+{
+  long  dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long  _msgtag;
+    }  _out;
+  } * _par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+      msgtag = (1 << 12)+(0 + (0 << 6));
+ 
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+static inline void  IVMpci_Control_deregister_propagate_reply(CORBA_Object  _client, idl4_server_environment *_env)
+{
+  long  dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long  _msgtag;
+    }  _out;
+  } * _par = (struct _reply_buffer *)MyUTCB();
+
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else
+      msgtag = (1 << 12)+(0 + (0 << 6));
+ 
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+static inline void  IVMpci_Control_read_propagate_reply(CORBA_Object  _client, L4_Word32_t * val, int  __retval, idl4_server_environment *_env)
+{
+  long  dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long  _msgtag;
+      L4_Word32_t  val;
+      int  __retval;
+    }  _out;
+  } * _par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else {
+      _par->_out.val = *val;
+      _par->_out.__retval = __retval;
+      msgtag = (1 << 12)+(2 + (0 << 6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+static inline void  IVMpci_Control_write_propagate_reply(CORBA_Object  _client, int  __retval, idl4_server_environment *_env)
+{
+  long  dummy, msgtag;
+  struct _reply_buffer {
+    struct {
+      long  _msgtag;
+      int  __retval;
+    }  _out;
+  } * _par = (struct _reply_buffer *)MyUTCB();
+
+  /* marshal reply */
+  
+  if( IDL4_EXPECT_FALSE(_env && _env->_action) )
+      msgtag = (1 << 12)+(_env->_action << 16)+(0 << 6)+0;
+  else {
+      _par->_out.__retval = __retval;
+      msgtag = (1 << 12)+(1 + (0 << 6));
+  }
+
+  /* send message */
+  
+  __asm__ __volatile__(
+    "push %%ebp                         \n\t"
+    "call __L4_Ipc                      \n\t"
+    "pop %%ebp                          \n\t"
+    : "=a" (dummy), "=c" (dummy), "=d" (dummy), "=S" (dummy), "=D" (dummy)
+    : "a" (_client), "c" (0x4000000), "d" (0), "S" (msgtag), "D" (_par)
+    : "ebx", "memory", "cc"
+  );
+}
+
+
+#endif	/* __L4KA_DRIVER_REUSE__KERNEL__PCI__L4VMPCI_IDL_REPLY_H__ */
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/Makefile linux-2.6.9/afterburn/drivers/pci/Makefile
--- linux-2.6.9.src/afterburn/drivers/pci/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/Makefile	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,7 @@
+
+include $(srctree)/afterburn/drivers/Makesupport
+
+obj-$(CONFIG_AFTERBURN_DRIVERS_PCI_SERVER) += l4ka_pci_server.o
+l4ka_pci_server-y := server.o
+
+$(addprefix $(obj)/,$(obj-m)): $(wedge_symbols)
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/server.c linux-2.6.9/afterburn/drivers/pci/server.c
--- linux-2.6.9.src/afterburn/drivers/pci/server.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/server.c	2007-07-03 16:22:40.000000000 +0200
@@ -0,0 +1,738 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-driver-reuse/kernel/pci/server.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#include <linux/version.h>
+#include <linux/list.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+
+#include <l4/types.h>
+#include <l4/space.h>
+#include <l4/kdebug.h>
+
+#include <glue/wedge.h>
+#include <glue/vmserver.h>
+#include <glue/thread.h>
+#include <glue/vmirq.h>
+
+#include "common.h"
+#include "L4VMpci_idl_server.h"
+#include "L4VMpci_idl_reply.h"
+
+
+#if defined(CONFIG_X86_IO_APIC)
+#include <acpi/acpi.h>
+#include <linux/acpi.h>
+#endif
+
+#if defined(PREFIX)
+#undef PREFIX
+#endif
+#include "server.h"
+
+
+#define L4VMPCI_DEV_INIT(name) { LIST_HEAD_INIT(name->devs), NULL, 0UL }
+
+int L4VMpci_debug_level = 3;
+
+int L4VMpci_server_irq = 0;
+MODULE_PARM( L4VMpci_server_irq, "i" );
+
+/**
+ * Single instance of server context.
+ */
+static L4VMpci_server_t L4VMpci_server = {
+    server_tid: L4_nilthread,
+    clients: LIST_HEAD_INIT(L4VMpci_server.clients)
+};
+
+
+/*********************************************************************
+ *
+ * client management
+ *
+ ********************************************************************/
+
+static L4VMpci_client_t *
+L4VMpci_server_create_client( L4VMpci_server_t *server, L4_ThreadId_t tid )
+{
+    L4VMpci_client_t *client;
+
+    client = kmalloc( sizeof( L4VMpci_client_t ), GFP_KERNEL );
+    if( NULL != client ) {
+	client->l_server.next = &client->l_server;
+	client->l_server.prev = &client->l_server;
+	client->tid = tid;
+	client->devs.next = &client->devs;
+	client->devs.prev = &client->devs;
+	list_add( &client->l_server, &server->clients );
+    } else {
+	printk( KERN_ERR PREFIX "unable to allocate client object, out of mem!\n" );
+    }
+
+    return client;
+}
+
+static void
+L4VMpci_server_deregister_client( L4VMpci_server_t *server, L4VMpci_client_t *client )
+{
+    struct list_head *elem = NULL;
+
+    list_for_each( elem, &client->devs ) {
+	kfree( list_entry( elem, L4VMpci_dev_t, devs ) );
+    }
+    kfree( client );
+}
+
+static int
+L4VMpci_client_add_device( L4VMpci_client_t *client, unsigned idx, struct pci_dev *dev, int writable )
+{
+    L4VMpci_dev_t *l4dev;
+
+    l4dev = kmalloc( sizeof( L4VMpci_dev_t ), GFP_KERNEL );
+    if( NULL != l4dev )
+    {
+	l4dev->idx = idx;
+	l4dev->devs.next = &l4dev->devs;
+	l4dev->devs.prev = &l4dev->devs;
+	l4dev->dev = dev;
+	l4dev->writable = writable;
+	printk(PREFIX "adding device %02x.%02x\n", dev->bus->number, dev->devfn);
+	list_add( &l4dev->devs, &client->devs );
+	return 0;
+    }
+
+    return -ENOMEM;
+}
+
+static L4VMpci_client_t *
+L4VMpci_server_lookup_client( L4VMpci_server_t *server, L4_ThreadId_t tid )
+{
+    struct list_head *elem;
+    L4VMpci_client_t *client;
+
+    list_for_each( elem, &server->clients ) {
+	client = list_entry( elem, L4VMpci_client_t, l_server );
+	if( L4_IsThreadEqual( client->tid, tid ) )
+	    return client;
+    }
+
+    return NULL;
+}
+
+static L4VMpci_dev_t *
+L4VMpci_server_lookup_device( L4VMpci_client_t *client, unsigned idx )
+{
+    struct list_head *elem = NULL;
+
+    list_for_each( elem, &client->devs ) {
+	L4VMpci_dev_t *dev = list_entry( elem, L4VMpci_dev_t, devs );
+	if( dev->idx == idx )
+	    return dev;
+    }
+
+    return NULL;
+}
+
+
+/*********************************************************************
+ *
+ * request handling
+ *
+ ********************************************************************/
+
+#define idl4_default_server_environment {0,0}
+
+static void
+L4VMpci_server_register_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    int dev_num, idx;
+    IVMpci_dev_id_t *devs;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    int err = -ENODEV;
+    struct pci_dev *dev;
+    unsigned long flags;
+
+    dev_num = params->reg.dev_num;
+    devs = params->reg.devs;
+
+    dprintk( 3, KERN_INFO PREFIX "reserving devices ");
+    for( idx = 0; idx < dev_num; idx++ ) {
+	dprintk( 3, "[%hx:%hx] ", devs[idx].vendor_id, devs[idx].device_id );
+    }
+    dprintk( 3, "\n");
+
+    // scan all devices Linux found on the real PCI bus
+    dev = NULL;
+#if 0
+    for( ; ; )
+    {
+	int add_dev = 0, writable = 1;
+	dev = pci_find_device( PCI_ANY_ID, PCI_ANY_ID, dev );
+	if( !dev )
+	    break;
+
+	for ( idx = 0; idx < dev_num; idx++ ) {
+	    // check whether this is one of the requested devices
+	    if( devs[idx].vendor_id == dev->vendor && devs[idx].device_id == dev->device ) {
+		add_dev = 1;
+		goto found;
+	    }
+	} 
+	// always add bridges, but read-only
+	if( dev->hdr_type == PCI_HEADER_TYPE_BRIDGE ) {
+	    writable = 0;
+	    add_dev = 1;
+	}
+	
+found:
+#endif
+
+    for( idx = 0; idx < dev_num; idx++ )
+    {
+	dev = pci_find_device( devs[idx].vendor_id, devs[idx].device_id, NULL );
+
+    	// create client object
+       	if( NULL == client)
+    	    client = L4VMpci_server_create_client( &L4VMpci_server, cmd->reply_to_tid );
+       	if( NULL == client ) {
+    	    printk( KERN_ERR PREFIX "failed to create client descriptor\n" );
+    	    CORBA_exception_set( &_env, ex_IVMpci_unknown, NULL );
+    	    goto exit;
+       	}
+
+	// add linux device to client object
+	err = L4VMpci_client_add_device( client, idx, dev, TRUE );
+	if( 0 != err ) {
+	    printk( KERN_ERR PREFIX "failed to associate VM client with device\n" );
+	    L4_KDB_Enter("ugh");
+	}
+    }
+
+exit:
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_register_propagate_reply( cmd->reply_to_tid, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_deregister_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    L4_ThreadId_t tid;
+    unsigned long flags;
+
+    client = L4VMpci_server_lookup_client( server, params->dereg.tid );
+    if( NULL != client ) {
+	L4VMpci_server_deregister_client( server, client );
+    } else {
+	dprintk( 1, KERN_INFO PREFIX "unable to find specified client %p\n",
+		 RAW(tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_deregister_propagate_reply( cmd->reply_to_tid, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_read_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    unsigned long flags;
+
+    u32 val = 0;
+    int ret_val = 0;
+    u32 reg = params->r_w.reg*4 + params->r_w.offset;
+
+    client = L4VMpci_server_lookup_client( server, cmd->reply_to_tid );
+    if( NULL != client ) {
+	L4VMpci_dev_t *dev;
+
+	dev = L4VMpci_server_lookup_device( client, params->r_w.idx );
+	if( NULL != dev ) {
+	    struct pci_dev *pci_dev = dev->dev;
+
+	    switch( params->r_w.len ) {
+		case 1:
+		    ret_val = pci_read_config_byte( pci_dev, reg, (u8*)&val );
+		    break;
+		case 2:
+		    ret_val = pci_read_config_word( pci_dev, reg, (u16*)&val );
+		    break;
+		case 4:
+		    ret_val = pci_read_config_dword( pci_dev, reg, &val );
+		    break;
+
+	    }
+	    dprintk( 3, KERN_INFO PREFIX "client tried to read from device %02x  (%x.%x) %d/%d/%d -> %x\n",
+	    	     params->r_w.idx, pci_dev->vendor, pci_dev->device, params->r_w.len, params->r_w.reg, params->r_w.offset, val);
+	} else {
+	    dprintk( 5, KERN_INFO PREFIX "client tried to read from invalid device\n" );
+	    ret_val = 0;
+	    val = 0xFFFFFFFF;
+	}
+    } else {
+	dprintk( 3, KERN_INFO PREFIX "unable to find client %p\n",
+		 RAW(cmd->reply_to_tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_read_propagate_reply( cmd->reply_to_tid, 
+	    &val, ret_val, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_write_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    int ret_val = 0;
+    unsigned long flags;
+    u32 reg = params->r_w.reg*4 + params->r_w.offset;
+
+    client = L4VMpci_server_lookup_client( server, cmd->reply_to_tid );
+    if( NULL != client ) {
+	L4VMpci_dev_t *dev;
+
+	dev = L4VMpci_server_lookup_device( client, params->r_w.idx );
+	if( NULL != dev ) {
+	    if( 1 == dev->writable ) {
+		struct pci_dev *pci_dev = dev->dev;
+		switch( params->r_w.len ) {
+		    case 1:
+			ret_val = pci_write_config_byte( pci_dev, reg, (u8)(params->r_w.val & 0xFF) );
+			break;
+		    case 2:
+			ret_val = pci_write_config_word( pci_dev, reg, (u16)(params->r_w.val & 0xFFFF) );
+			break;
+		    case 4:
+			ret_val = pci_write_config_dword( pci_dev, reg, params->r_w.val );
+			break;
+		}
+		dprintk( 1, KERN_INFO PREFIX "client tried to write to device %02x %d/%d/%d -> %x\n",
+			 params->r_w.idx, params->r_w.len, params->r_w.reg, params->r_w.offset, params->r_w.val);
+			    
+	    } else {
+		dprintk( 4, KERN_INFO PREFIX "client tried to write to read-only device\n" );
+		ret_val = 0;
+	    }
+
+	} else {
+	    dprintk( 5, KERN_INFO PREFIX "client tried to write to invalid device\n" );
+	    ret_val = 0;
+	}
+    } else {
+	dprintk( 1, KERN_INFO PREFIX "unable to find specified client %p\n",
+		 RAW(cmd->reply_to_tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_write_propagate_reply( cmd->reply_to_tid, ret_val, &_env );
+    local_irq_restore(flags);
+}
+
+
+/***************************************************************************
+ *
+ * Linux interrupt functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMpci_bottom_half_handler( void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    dprintk( 5, KERN_INFO PREFIX "bottom half handler\n" );
+
+//    ASSERT( !in_interrupt() );
+
+    L4VM_server_cmd_dispatcher( &server->bottom_half_cmds, server,
+	    server->server_tid );
+}
+
+static irqreturn_t
+L4VMpci_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    while( 1 )
+    {
+	// Outstanding events? Read them and reset without losing events.
+	L4_Word_t events = L4VM_irq_status_reset( &server->irq.status );
+	if( !events )
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+	    return;
+#else
+	    return IRQ_HANDLED;
+#endif
+
+	dprintk( 5, KERN_INFO PREFIX "irq handler: 0x%lx\n",
+		events );
+
+	server->irq.pending = TRUE;
+//	server->server_info->irq_pending = TRUE;
+
+	if( (events & L4VMPCI_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VM_server_cmd_dispatcher( &server->top_half_cmds, server,
+		    server->server_tid );
+
+	if( (events & L4VMPCI_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &server->bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	server->irq.pending = FALSE;
+//	server->server_info->irq_pending = FALSE;
+    }
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+static int
+L4VMpci_irq_pending( void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    return (server->irq.status & server->irq.mask);// ||
+//	server->server_info->irq_status;
+}
+#endif
+
+/*********************************************************************
+ *
+ * module level functions
+ *
+ ********************************************************************/
+
+static void 
+L4VMpci_server_thread( void *data )
+{
+    IVMpci_Control_server();
+}
+
+static int __init
+L4VMpci_server_init_module( void )
+{
+    int err;
+    L4VMpci_server_t *server = &L4VMpci_server;
+
+    L4VM_irq_init( &server->irq );
+    L4VM_server_cmd_init( &server->top_half_cmds );
+    L4VM_server_cmd_init( &server->bottom_half_cmds );
+
+    INIT_TQUEUE( &server->bottom_half_task,
+		 L4VMpci_bottom_half_handler, server );
+
+    printk( KERN_INFO PREFIX "initializing...\n" );
+
+    // Allocate a virtual interrupt.
+    if (L4VMpci_server_irq == 0)
+    {
+#if defined(CONFIG_X86_IO_APIC)
+        L4_KernelInterfacePage_t *kip = (L4_KernelInterfacePage_t *) L4_GetKernelInterface();
+        L4VMpci_server_irq = L4_ThreadIdSystemBase(kip) + 2;	
+	acpi_register_gsi(L4VMpci_server_irq, ACPI_LEVEL_SENSITIVE, ACPI_ACTIVE_LOW);
+
+#else
+	L4VMpci_server_irq = 3;
+#endif
+    }
+    
+    printk( KERN_INFO PREFIX "L4VMpci server irq %d\n", L4VMpci_server_irq );
+    
+    server->irq.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    server->irq_no = L4VMpci_server_irq;
+    
+    if( server->irq_no >= NR_IRQS ) {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	err = -ENOMEM;
+	goto err_vmpic_reserve;
+    }
+    // TODO: in case of error, we don't want the irq to remain virtual.
+    l4ka_wedge_add_virtual_irq( server->irq_no );
+    err = request_irq( server->irq_no, L4VMpci_irq_handler, 0,
+	    "L4VMpci", server );
+    if( err < 0 ) {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Start the server thread.
+    server->server_tid = L4VM_thread_create( GFP_KERNEL,
+	    L4VMpci_server_thread, l4ka_wedge_get_irq_prio(), 
+	    smp_processor_id(), NULL, 0 );
+    if( L4_IsNilThread(server->server_tid) ) {
+	err = -ENOMEM;
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	goto err_thread_create;
+    }
+
+    // Register with the locator.
+    err = L4VM_server_register_location( UUID_IVMpci, server->server_tid );
+    if( err == -ENODEV ) {
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+	goto err_register;
+    }
+    else if( err ) {
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+	goto err_register;
+    }
+
+    return 0;
+
+err_register:
+err_thread_create:
+    free_irq( server->irq_no, server );
+err_request_irq:
+err_vmpic_reserve:
+    return err;
+}
+
+static void __exit
+L4VMpci_server_exit_module( void )
+{
+    L4_KDB_Enter("pci server exit");
+}
+
+MODULE_AUTHOR( "Stefan Goetz <sgoetz@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4Linux virtual PCI config space server" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM PCI" );
+MODULE_VERSION( "monkey" );
+
+module_init( L4VMpci_server_init_module );
+module_exit( L4VMpci_server_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
+/*********************************************************************
+ *
+ * IDL server
+ *
+ ********************************************************************/
+
+IDL4_INLINE void IVMpci_Control_register_implementation(CORBA_Object _caller, const int dev_num, const IVMpci_dev_id_t *devs, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client register\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, 
+					&server->irq, server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->reg.dev_num = dev_num;
+    params->reg.devs = (IVMpci_dev_id_t *)devs;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_register_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR 
+IDL4_PUBLISH_IVMPCI_CONTROL_REGISTER(IVMpci_Control_register_implementation);
+
+IDL4_INLINE void IVMpci_Control_deregister_implementation(CORBA_Object _caller, const L4_ThreadId_t *tid, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: deregister request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->dereg.tid = *tid;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_deregister_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_DEREGISTER(IVMpci_Control_deregister_implementation);
+
+IDL4_INLINE int IVMpci_Control_read_implementation(CORBA_Object _caller, const int idx, const int reg, const int len, const int offset, unsigned int *val, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 5, KERN_INFO PREFIX "server thread: read request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->r_w.idx = idx;
+    params->r_w.reg = reg;
+    params->r_w.len = len;
+    params->r_w.offset = offset;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_read_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+
+    return 0;
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_READ(IVMpci_Control_read_implementation);
+
+IDL4_INLINE int IVMpci_Control_write_implementation(CORBA_Object _caller, const int idx, const int reg, const int len, const int offset, const unsigned int val, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 5, KERN_INFO PREFIX "server thread: write request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->r_w.idx = idx;
+    params->r_w.reg = reg;
+    params->r_w.len = len;
+    params->r_w.val = val;
+    params->r_w.offset = offset;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_write_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+
+    return 0;
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_WRITE(IVMpci_Control_write_implementation);
+
+static void *IVMpci_Control_vtable[IVMPCI_CONTROL_DEFAULT_VTABLE_SIZE] = IVMPCI_CONTROL_DEFAULT_VTABLE;
+
+void IVMpci_Control_server(void)
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    IVMpci_dev_id_t dev_ids[20];
+
+    idl4_msgbuf_init(&msgbuf);
+    idl4_msgbuf_add_buffer( &msgbuf, dev_ids, sizeof(dev_ids) );
+
+    while (1)
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while (1)
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt, IVMpci_Control_vtable[idl4_get_function_id(&msgtag) & IVMPCI_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMpci_Control_discard(void)
+{
+    L4_KDB_Enter( "received IPC with invalid format" );
+}
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/server.c~ linux-2.6.9/afterburn/drivers/pci/server.c~
--- linux-2.6.9.src/afterburn/drivers/pci/server.c~	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/server.c~	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,738 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe
+ *
+ * File path:	l4ka-driver-reuse/kernel/pci/server.c
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#include <linux/version.h>
+#include <linux/list.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+
+#include <l4/types.h>
+#include <l4/space.h>
+#include <l4/kdebug.h>
+
+#include <glue/wedge.h>
+#include <glue/vmserver.h>
+#include <glue/thread.h>
+#include <glue/vmirq.h>
+
+#include "common.h"
+#include "L4VMpci_idl_server.h"
+#include "L4VMpci_idl_reply.h"
+
+
+#if defined(CONFIG_X86_IO_APIC)
+#include <acpi/acpi.h>
+#include <linux/acpi.h>
+#endif
+
+#if defined(PREFIX)
+#undef PREFIX
+#endif
+#include "server.h"
+
+
+#define L4VMPCI_DEV_INIT(name) { LIST_HEAD_INIT(name->devs), NULL, 0UL }
+
+int L4VMpci_debug_level = 3;
+
+int L4VMpci_server_irq = 0;
+MODULE_PARM( L4VMpci_server_irq, "i" );
+
+/**
+ * Single instance of server context.
+ */
+static L4VMpci_server_t L4VMpci_server = {
+    server_tid: L4_nilthread,
+    clients: LIST_HEAD_INIT(L4VMpci_server.clients)
+};
+
+
+/*********************************************************************
+ *
+ * client management
+ *
+ ********************************************************************/
+
+static L4VMpci_client_t *
+L4VMpci_server_create_client( L4VMpci_server_t *server, L4_ThreadId_t tid )
+{
+    L4VMpci_client_t *client;
+
+    client = kmalloc( sizeof( L4VMpci_client_t ), GFP_KERNEL );
+    if( NULL != client ) {
+	client->l_server.next = &client->l_server;
+	client->l_server.prev = &client->l_server;
+	client->tid = tid;
+	client->devs.next = &client->devs;
+	client->devs.prev = &client->devs;
+	list_add( &client->l_server, &server->clients );
+    } else {
+	printk( KERN_ERR PREFIX "unable to allocate client object, out of mem!\n" );
+    }
+
+    return client;
+}
+
+static void
+L4VMpci_server_deregister_client( L4VMpci_server_t *server, L4VMpci_client_t *client )
+{
+    struct list_head *elem = NULL;
+
+    list_for_each( elem, &client->devs ) {
+	kfree( list_entry( elem, L4VMpci_dev_t, devs ) );
+    }
+    kfree( client );
+}
+
+static int
+L4VMpci_client_add_device( L4VMpci_client_t *client, unsigned idx, struct pci_dev *dev, int writable )
+{
+    L4VMpci_dev_t *l4dev;
+
+    l4dev = kmalloc( sizeof( L4VMpci_dev_t ), GFP_KERNEL );
+    if( NULL != l4dev )
+    {
+	l4dev->idx = idx;
+	l4dev->devs.next = &l4dev->devs;
+	l4dev->devs.prev = &l4dev->devs;
+	l4dev->dev = dev;
+	l4dev->writable = writable;
+	printk(PREFIX "adding device %02x.%02x\n", dev->bus->number, dev->devfn);
+	list_add( &l4dev->devs, &client->devs );
+	return 0;
+    }
+
+    return -ENOMEM;
+}
+
+static L4VMpci_client_t *
+L4VMpci_server_lookup_client( L4VMpci_server_t *server, L4_ThreadId_t tid )
+{
+    struct list_head *elem;
+    L4VMpci_client_t *client;
+
+    list_for_each( elem, &server->clients ) {
+	client = list_entry( elem, L4VMpci_client_t, l_server );
+	if( L4_IsThreadEqual( client->tid, tid ) )
+	    return client;
+    }
+
+    return NULL;
+}
+
+static L4VMpci_dev_t *
+L4VMpci_server_lookup_device( L4VMpci_client_t *client, unsigned idx )
+{
+    struct list_head *elem = NULL;
+
+    list_for_each( elem, &client->devs ) {
+	L4VMpci_dev_t *dev = list_entry( elem, L4VMpci_dev_t, devs );
+	if( dev->idx == idx )
+	    return dev;
+    }
+
+    return NULL;
+}
+
+
+/*********************************************************************
+ *
+ * request handling
+ *
+ ********************************************************************/
+
+#define idl4_default_server_environment {0,0}
+
+static void
+L4VMpci_server_register_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    int dev_num, idx;
+    IVMpci_dev_id_t *devs;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    int err = -ENODEV;
+    struct pci_dev *dev;
+    unsigned long flags;
+
+    dev_num = params->reg.dev_num;
+    devs = params->reg.devs;
+
+    dprintk( 3, KERN_INFO PREFIX "reserving devices ");
+    for( idx = 0; idx < dev_num; idx++ ) {
+	dprintk( 3, "[%hx:%hx] ", devs[idx].vendor_id, devs[idx].device_id );
+    }
+    dprintk( 3, "\n");
+
+    // scan all devices Linux found on the real PCI bus
+    dev = NULL;
+#if 0
+    for( ; ; )
+    {
+	int add_dev = 0, writable = 1;
+	dev = pci_find_device( PCI_ANY_ID, PCI_ANY_ID, dev );
+	if( !dev )
+	    break;
+
+	for ( idx = 0; idx < dev_num; idx++ ) {
+	    // check whether this is one of the requested devices
+	    if( devs[idx].vendor_id == dev->vendor && devs[idx].device_id == dev->device ) {
+		add_dev = 1;
+		goto found;
+	    }
+	} 
+	// always add bridges, but read-only
+	if( dev->hdr_type == PCI_HEADER_TYPE_BRIDGE ) {
+	    writable = 0;
+	    add_dev = 1;
+	}
+	
+found:
+#endif
+
+    for( idx = 0; idx < dev_num; idx++ )
+    {
+	dev = pci_find_device( devs[idx].vendor_id, devs[idx].device_id, NULL );
+
+    	// create client object
+       	if( NULL == client)
+    	    client = L4VMpci_server_create_client( &L4VMpci_server, cmd->reply_to_tid );
+       	if( NULL == client ) {
+    	    printk( KERN_ERR PREFIX "failed to create client descriptor\n" );
+    	    CORBA_exception_set( &_env, ex_IVMpci_unknown, NULL );
+    	    goto exit;
+       	}
+
+	// add linux device to client object
+	err = L4VMpci_client_add_device( client, idx, dev, TRUE );
+	if( 0 != err ) {
+	    printk( KERN_ERR PREFIX "failed to associate VM client with device\n" );
+	    L4_KDB_Enter("ugh");
+	}
+    }
+
+exit:
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_register_propagate_reply( cmd->reply_to_tid, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_deregister_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    L4_ThreadId_t tid;
+    unsigned long flags;
+
+    client = L4VMpci_server_lookup_client( server, params->dereg.tid );
+    if( NULL != client ) {
+	L4VMpci_server_deregister_client( server, client );
+    } else {
+	dprintk( 1, KERN_INFO PREFIX "unable to find specified client %p\n",
+		 RAW(tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_deregister_propagate_reply( cmd->reply_to_tid, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_read_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    unsigned long flags;
+
+    u32 val = 0;
+    int ret_val = 0;
+    u32 reg = params->r_w.reg*4 + params->r_w.offset;
+
+    client = L4VMpci_server_lookup_client( server, cmd->reply_to_tid );
+    if( NULL != client ) {
+	L4VMpci_dev_t *dev;
+
+	dev = L4VMpci_server_lookup_device( client, params->r_w.idx );
+	if( NULL != dev ) {
+	    struct pci_dev *pci_dev = dev->dev;
+
+	    switch( params->r_w.len ) {
+		case 1:
+		    ret_val = pci_read_config_byte( pci_dev, reg, (u8*)&val );
+		    break;
+		case 2:
+		    ret_val = pci_read_config_word( pci_dev, reg, (u16*)&val );
+		    break;
+		case 4:
+		    ret_val = pci_read_config_dword( pci_dev, reg, &val );
+		    break;
+
+	    }
+	    dprintk( 3, KERN_INFO PREFIX "client tried to read from device %02x  (%x.%x) %d/%d/%d -> %x\n",
+	    	     params->r_w.idx, pci_dev->vendor, pci_dev->device, params->r_w.len, params->r_w.reg, params->r_w.offset, val);
+	} else {
+	    dprintk( 5, KERN_INFO PREFIX "client tried to read from invalid device\n" );
+	    ret_val = 0;
+	    val = 0xFFFFFFFF;
+	}
+    } else {
+	dprintk( 3, KERN_INFO PREFIX "unable to find client %p\n",
+		 RAW(cmd->reply_to_tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_read_propagate_reply( cmd->reply_to_tid, 
+	    &val, ret_val, &_env );
+    local_irq_restore(flags);
+}
+
+static void
+L4VMpci_server_write_handler( L4VM_server_cmd_t *cmd, void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+    L4VM_server_cmd_params_t *params = (L4VM_server_cmd_params_t *)cmd->data;
+    idl4_server_environment _env = idl4_default_server_environment;
+    L4VMpci_client_t *client = NULL;
+    int ret_val = 0;
+    unsigned long flags;
+    u32 reg = params->r_w.reg*4 + params->r_w.offset;
+
+    client = L4VMpci_server_lookup_client( server, cmd->reply_to_tid );
+    if( NULL != client ) {
+	L4VMpci_dev_t *dev;
+
+	dev = L4VMpci_server_lookup_device( client, params->r_w.idx );
+	if( NULL != dev ) {
+	    if( 1 == dev->writable ) {
+		struct pci_dev *pci_dev = dev->dev;
+		switch( params->r_w.len ) {
+		    case 1:
+			ret_val = pci_write_config_byte( pci_dev, reg, (u8)(params->r_w.val & 0xFF) );
+			break;
+		    case 2:
+			ret_val = pci_write_config_word( pci_dev, reg, (u16)(params->r_w.val & 0xFFFF) );
+			break;
+		    case 4:
+			ret_val = pci_write_config_dword( pci_dev, reg, params->r_w.val );
+			break;
+		}
+		dprintk( 1, KERN_INFO PREFIX "client tried to write to device %02x %d/%d/%d -> %x\n",
+			 params->r_w.idx, params->r_w.len, params->r_w.reg, params->r_w.offset, params->r_w.val);
+			    
+	    } else {
+		dprintk( 4, KERN_INFO PREFIX "client tried to write to read-only device\n" );
+		ret_val = 0;
+	    }
+
+	} else {
+	    dprintk( 5, KERN_INFO PREFIX "client tried to write to invalid device\n" );
+	    ret_val = 0;
+	}
+    } else {
+	dprintk( 1, KERN_INFO PREFIX "unable to find specified client %p\n",
+		 RAW(cmd->reply_to_tid) );
+	CORBA_exception_set( &_env, ex_IVMpci_invalid_client, NULL );
+    }
+
+    local_irq_save(flags);
+    L4_Set_VirtualSender( server->server_tid );
+    IVMpci_Control_write_propagate_reply( cmd->reply_to_tid, ret_val, &_env );
+    local_irq_restore(flags);
+}
+
+
+/***************************************************************************
+ *
+ * Linux interrupt functions.
+ *
+ ***************************************************************************/
+
+static void
+L4VMpci_bottom_half_handler( void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    dprintk( 5, KERN_INFO PREFIX "bottom half handler\n" );
+
+//    ASSERT( !in_interrupt() );
+
+    L4VM_server_cmd_dispatcher( &server->bottom_half_cmds, server,
+	    server->server_tid );
+}
+
+static irqreturn_t
+L4VMpci_irq_handler( int irq, void *data, struct pt_regs *regs )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    while( 1 )
+    {
+	// Outstanding events? Read them and reset without losing events.
+	L4_Word_t events = L4VM_irq_status_reset( &server->irq.status );
+	if( !events )
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+	    return;
+#else
+	    return IRQ_HANDLED;
+#endif
+
+	dprintk( 5, KERN_INFO PREFIX "irq handler: 0x%lx\n",
+		events );
+
+	server->irq.pending = TRUE;
+//	server->server_info->irq_pending = TRUE;
+
+	if( (events & L4VMPCI_IRQ_TOP_HALF_CMD) != 0 )
+	    L4VM_server_cmd_dispatcher( &server->top_half_cmds, server,
+		    server->server_tid );
+
+	if( (events & L4VMPCI_IRQ_BOTTOM_HALF_CMD) != 0 )
+	{
+	    // Make sure that we tackle bottom halves outside the
+	    // interrupt context!!
+	    schedule_task( &server->bottom_half_task );
+	}
+
+	// Enable interrupt message delivery.
+	server->irq.pending = FALSE;
+//	server->server_info->irq_pending = FALSE;
+    }
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+static int
+L4VMpci_irq_pending( void *data )
+{
+    L4VMpci_server_t *server = (L4VMpci_server_t *)data;
+
+    return (server->irq.status & server->irq.mask);// ||
+//	server->server_info->irq_status;
+}
+#endif
+
+/*********************************************************************
+ *
+ * module level functions
+ *
+ ********************************************************************/
+
+static void 
+L4VMpci_server_thread( void *data )
+{
+    IVMpci_Control_server();
+}
+
+static int __init
+L4VMpci_server_init_module( void )
+{
+    int err;
+    L4VMpci_server_t *server = &L4VMpci_server;
+
+    L4VM_irq_init( &server->irq );
+    L4VM_server_cmd_init( &server->top_half_cmds );
+    L4VM_server_cmd_init( &server->bottom_half_cmds );
+
+    INIT_TQUEUE( &server->bottom_half_task,
+		 L4VMpci_bottom_half_handler, server );
+
+    printk( KERN_INFO PREFIX "initializing...\n" );
+
+    // Allocate a virtual interrupt.
+    if (L4VMpci_server_irq == 0)
+    {
+#if defined(CONFIG_X86_IO_APIC)
+        L4_KernelInterfacePage_t *kip = (L4_KernelInterfacePage_t *) L4_GetKernelInterface();
+        L4VMpci_server_irq = L4_ThreadIdSystemBase(kip) + 2;	
+	acpi_register_gsi(L4VMpci_server_irq, ACPI_LEVEL_SENSITIVE, ACPI_ACTIVE_LOW);
+
+#else
+	L4VMpci_server_irq = 9;
+#endif
+    }
+    
+    printk( KERN_INFO PREFIX "L4VMpci server irq %d\n", L4VMpci_server_irq );
+    
+    server->irq.my_irq_tid = L4VM_linux_irq_thread( smp_processor_id() );
+    server->irq_no = L4VMpci_server_irq;
+    
+    if( server->irq_no >= NR_IRQS ) {
+	printk( KERN_ERR PREFIX "unable to reserve a virtual interrupt.\n" );
+	err = -ENOMEM;
+	goto err_vmpic_reserve;
+    }
+    // TODO: in case of error, we don't want the irq to remain virtual.
+    l4ka_wedge_add_virtual_irq( server->irq_no );
+    err = request_irq( server->irq_no, L4VMpci_irq_handler, 0,
+	    "L4VMpci", server );
+    if( err < 0 ) {
+	printk( KERN_ERR PREFIX "unable to allocate an interrupt.\n" );
+	goto err_request_irq;
+    }
+
+    // Start the server thread.
+    server->server_tid = L4VM_thread_create( GFP_KERNEL,
+	    L4VMpci_server_thread, l4ka_wedge_get_irq_prio(), 
+	    smp_processor_id(), NULL, 0 );
+    if( L4_IsNilThread(server->server_tid) ) {
+	err = -ENOMEM;
+	printk( KERN_ERR PREFIX "failed to start the server thread.\n" );
+	goto err_thread_create;
+    }
+
+    // Register with the locator.
+    err = L4VM_server_register_location( UUID_IVMpci, server->server_tid );
+    if( err == -ENODEV ) {
+	printk( KERN_ERR PREFIX "failed to register with the locator.\n" );
+	goto err_register;
+    }
+    else if( err ) {
+	printk( KERN_ERR PREFIX "failed to contact the locator service.\n" );
+	goto err_register;
+    }
+
+    return 0;
+
+err_register:
+err_thread_create:
+    free_irq( server->irq_no, server );
+err_request_irq:
+err_vmpic_reserve:
+    return err;
+}
+
+static void __exit
+L4VMpci_server_exit_module( void )
+{
+    L4_KDB_Enter("pci server exit");
+}
+
+MODULE_AUTHOR( "Stefan Goetz <sgoetz@ira.uka.de>" );
+MODULE_DESCRIPTION( "L4Linux virtual PCI config space server" );
+MODULE_LICENSE( "Dual BSD/GPL" );
+MODULE_SUPPORTED_DEVICE( "L4 VM PCI" );
+MODULE_VERSION( "monkey" );
+
+module_init( L4VMpci_server_init_module );
+module_exit( L4VMpci_server_exit_module );
+
+#if defined(MODULE)
+// Define a .afterburn_wedge_module section, to activate symbol resolution
+// for this module against the wedge's symbols.
+__attribute__ ((section(".afterburn_wedge_module")))
+burn_wedge_header_t burn_wedge_header = {
+    BURN_WEDGE_MODULE_INTERFACE_VERSION
+};
+#endif
+
+/*********************************************************************
+ *
+ * IDL server
+ *
+ ********************************************************************/
+
+IDL4_INLINE void IVMpci_Control_register_implementation(CORBA_Object _caller, const int dev_num, const IVMpci_dev_id_t *devs, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: client register\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, 
+					&server->irq, server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->reg.dev_num = dev_num;
+    params->reg.devs = (IVMpci_dev_id_t *)devs;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_register_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR 
+IDL4_PUBLISH_IVMPCI_CONTROL_REGISTER(IVMpci_Control_register_implementation);
+
+IDL4_INLINE void IVMpci_Control_deregister_implementation(CORBA_Object _caller, const L4_ThreadId_t *tid, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 3, KERN_INFO PREFIX "server thread: deregister request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->dereg.tid = *tid;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_deregister_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_DEREGISTER(IVMpci_Control_deregister_implementation);
+
+IDL4_INLINE int IVMpci_Control_read_implementation(CORBA_Object _caller, const int idx, const int reg, const int len, const int offset, unsigned int *val, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 5, KERN_INFO PREFIX "server thread: read request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->r_w.idx = idx;
+    params->r_w.reg = reg;
+    params->r_w.len = len;
+    params->r_w.offset = offset;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_read_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+
+    return 0;
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_READ(IVMpci_Control_read_implementation);
+
+IDL4_INLINE int IVMpci_Control_write_implementation(CORBA_Object _caller, const int idx, const int reg, const int len, const int offset, const unsigned int val, idl4_server_environment *_env)
+
+{
+    L4VMpci_server_t *server = &L4VMpci_server;
+    L4_Word16_t cmd_idx;
+    L4VM_server_cmd_t *cmd;
+    L4VM_server_cmd_params_t *params;
+
+    dprintk( 5, KERN_INFO PREFIX "server thread: write request\n" );
+
+    cmd_idx = L4VM_server_cmd_allocate( &server->bottom_half_cmds,
+					L4VMPCI_IRQ_BOTTOM_HALF_CMD, &server->irq,
+					server->irq_no );
+    cmd = &server->bottom_half_cmds.cmds[ cmd_idx ];
+    params = &server->bottom_half_params[ cmd_idx ];
+    params->r_w.idx = idx;
+    params->r_w.reg = reg;
+    params->r_w.len = len;
+    params->r_w.val = val;
+    params->r_w.offset = offset;
+
+    cmd->reply_to_tid = _caller;
+    cmd->data = params;
+    cmd->handler = L4VMpci_server_write_handler;
+
+    L4VM_irq_deliver( &server->irq, server->irq_no,
+	    L4VMPCI_IRQ_BOTTOM_HALF_CMD );
+
+    idl4_set_no_response( _env );  // Handle the request in a safe context.
+
+    return 0;
+}
+IDL4_PUBLISH_ATTR
+IDL4_PUBLISH_IVMPCI_CONTROL_WRITE(IVMpci_Control_write_implementation);
+
+static void *IVMpci_Control_vtable[IVMPCI_CONTROL_DEFAULT_VTABLE_SIZE] = IVMPCI_CONTROL_DEFAULT_VTABLE;
+
+void IVMpci_Control_server(void)
+{
+    L4_ThreadId_t partner;
+    L4_MsgTag_t msgtag;
+    idl4_msgbuf_t msgbuf;
+    long cnt;
+    IVMpci_dev_id_t dev_ids[20];
+
+    idl4_msgbuf_init(&msgbuf);
+    idl4_msgbuf_add_buffer( &msgbuf, dev_ids, sizeof(dev_ids) );
+
+    while (1)
+    {
+	partner = L4_nilthread;
+	msgtag.raw = 0;
+	cnt = 0;
+
+	while (1)
+	{
+	    idl4_msgbuf_sync(&msgbuf);
+
+	    idl4_reply_and_wait(&partner, &msgtag, &msgbuf, &cnt);
+
+	    if (idl4_is_error(&msgtag))
+		break;
+
+	    idl4_process_request(&partner, &msgtag, &msgbuf, &cnt, IVMpci_Control_vtable[idl4_get_function_id(&msgtag) & IVMPCI_CONTROL_FID_MASK]);
+	}
+    }
+}
+
+void IVMpci_Control_discard(void)
+{
+    L4_KDB_Enter( "received IPC with invalid format" );
+}
diff -Naur linux-2.6.9.src/afterburn/drivers/pci/server.h linux-2.6.9/afterburn/drivers/pci/server.h
--- linux-2.6.9.src/afterburn/drivers/pci/server.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/drivers/pci/server.h	2007-06-06 14:59:14.000000000 +0200
@@ -0,0 +1,116 @@
+/*********************************************************************
+ *
+ * Copyright (C) 2004-2006,  University of Karlsruhe,  
+ *                           University of New South Wales, Sydney
+ *
+ * File path:	l4ka-driver-reuse/kernel/pci/server.h
+ * Description:	
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ ********************************************************************/
+#ifndef __L4KA_DRIVER_REUSE__KERNEL__PCI__SERVER_H__
+#define __L4KA_DRIVER_REUSE__KERNEL__PCI__SERVER_H__
+
+#include <linux/list.h>
+#include <linux/pci.h>
+
+#include <l4/types.h>
+#include <l4/kip.h>
+
+#include <glue/vmserver.h>
+#include <glue/bottomhalf.h>
+
+#define PREFIX "L4VMpci server: "
+
+#define L4VMPCI_IRQ_BOTTOM_HALF_CMD	(1)
+#define L4VMPCI_IRQ_TOP_HALF_CMD	(2)
+
+
+/**
+ * Describes a PCI device managed by a client VM.
+ */
+typedef struct {
+    /** node in the list of client-specific devices */
+    struct list_head devs;
+    /** the PCI device */
+    struct pci_dev *dev;
+    /** indicates whether a client may write to this device or not */
+    int writable;
+    /** the client's ID for this device */
+    unsigned idx;
+} L4VMpci_dev_t;
+
+/**
+ * Describes context associated with a client VM managing a specific
+ * set of devices.
+ */
+typedef struct {
+    /** node in the list of clients in a server context */
+    struct list_head l_server;
+    /** ID of the thread in the client VM */
+    L4_ThreadId_t tid;
+    /** The devices managed by this client VM */
+    struct list_head devs;
+} L4VMpci_client_t;
+
+/**
+ * Describes the parameters of a queued server command.
+ */
+typedef union
+{
+    struct {
+	int dev_num;
+	IVMpci_dev_id_t *devs;
+    } reg;
+    struct {
+	L4_ThreadId_t tid;
+    } dereg;
+    struct {
+	unsigned idx;
+	unsigned reg;
+	int len;
+	int offset;
+	unsigned int val;
+    } r_w;
+} L4VM_server_cmd_params_t;
+
+/**
+ * Describes context of a PCI config space server.
+ */
+typedef struct {
+    L4_ThreadId_t server_tid;
+
+    L4VM_irq_t irq;
+    int irq_no;
+
+    struct list_head clients;
+
+    struct tq_struct bottom_half_task;
+    L4VM_server_cmd_ring_t top_half_cmds;
+    L4VM_server_cmd_params_t top_half_params[L4VM_SERVER_CMD_RING_LEN];
+    L4VM_server_cmd_ring_t bottom_half_cmds;
+    L4VM_server_cmd_params_t bottom_half_params[L4VM_SERVER_CMD_RING_LEN];
+} L4VMpci_server_t;
+
+
+#endif	/* __L4KA_DRIVER_REUSE__KERNEL__PCI__SERVER_H__ */
diff -Naur linux-2.6.9.src/afterburn/Kconfig linux-2.6.9/afterburn/Kconfig
--- linux-2.6.9.src/afterburn/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/Kconfig	2007-07-13 11:11:50.000000000 +0200
@@ -0,0 +1,114 @@
+
+menu "Afterburn"
+
+config AFTERBURN
+	bool "Afterburn pre-virtualization"
+	help
+	  Support for running Linux as a virtual machine in a hypervisor
+	  environment.
+
+#config AFTERBURN_STATIC
+#	depends on AFTERBURN
+#	bool "Staticly link against wedge"
+#	default y
+#	help
+#          Instruction afterburning is normally staticaly linked at build time
+#          and you limit yourself to the particular wedge. Use this option to instead
+#          generate dynamic patchup symbols instead.
+
+config AFTERBURN_L4KA
+	depends on AFTERBURN
+	bool "Support the L4Ka::Pistachio environment?"
+	default y
+	select AFTERBURN_RELINK
+	select AFTERBURN_HOOK_UACCESS
+	select AFTERBURN_ANNOTATIONS
+	select AFTERBURN_THREAD_HOOKS
+	select AFTERBURN_MODULE_HOOKS
+
+config AFTERBURN_XEN
+	depends on AFTERBURN
+	bool "Support the Xen environment?"
+	default y
+	select AFTERBURN_RELINK
+	select AFTERBURN_ANNOTATIONS
+	select AFTERBURN_XEN_HOOKS
+	select AFTERBURN_MODULE_HOOKS
+
+config AFTERBURN_DEVICE_PASSTHRU
+	depends on AFTERBURN
+	bool "Permit pass through device access?"
+	default y
+	select AFTERBURN_HOOK_DMA
+
+config AFTERBURN_KERNEL_PROFILING
+	depends on AFTERBURN
+	bool "Profile the kernel? (Requires frame pointers)"
+	select DEBUG_KERNEL
+	select FRAME_POINTER
+
+menu "Advanced"
+depends on AFTERBURN
+
+config AFTERBURN_RELINK
+	depends on AFTERBURN
+	bool "Relink the Linux kernel at a new address?"
+	default y
+
+config AFTERBURN_LINK_BASE
+	depends on AFTERBURN && AFTERBURN_RELINK
+	hex "The kernel link address"
+	default "0x80000000"
+	help
+	  This address is the first byte location of where the kernel's
+	  available virtual address space starts.
+
+config AFTERBURN_VADDR_END
+	depends on AFTERBURN && AFTERBURN_RELINK
+	hex "The end of the kernel's virtual address space"
+	default "0xbc000000"
+	help
+	  This address is the first byte location of where *unavailable*
+	  memory starts.  It is thus the end of the kernel's available
+	  virtual address space.  If you use a dynamic wedge, consider
+	  leaving enough room for the wedge to link itself at the
+	  end of the kernel's available virtual address space.
+
+config AFTERBURN_USER_VADDR_END
+	depends on AFTERBURN && AFTERBURN_RELINK
+	hex "The end of a user application's virtual address space"
+	default "0x80000000"
+	help
+	  This address marks the end of the user address space managed
+	  by Linux.  All addresses starting here and beyond are managed 
+	  by the hypervisor.
+
+config AFTERBURN_HOOK_UACCESS
+	depends on AFTERBURN
+	bool "Permit the virtual machine to hook user data access?"
+
+config AFTERBURN_HOOK_DMA
+	depends on AFTERBURN
+	bool "Permit the virtual machine to hook DMA translation?"
+
+config AFTERBURN_XEN_HOOKS
+	depends on AFTERBURN
+	bool "Extra afterburn hooks required for Xen"
+
+config AFTERBURN_ANNOTATIONS
+	depends on AFTERBURN
+	bool "Generate Afterburn annotations and NOPs for optimizations"
+
+config AFTERBURN_THREAD_HOOKS
+	depends on AFTERBURN
+	bool "Permit the virtual machine to hook high-level thread operations?"
+
+config AFTERBURN_MODULE_HOOKS
+	depends on AFTERBURN
+	bool "Support dynamic kernel module loading?"
+	default y
+
+endmenu
+endmenu
+
+source "afterburn/drivers/Kconfig"
diff -Naur linux-2.6.9.src/afterburn/Makefile linux-2.6.9/afterburn/Makefile
--- linux-2.6.9.src/afterburn/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/afterburn/Makefile	2007-07-13 11:11:50.000000000 +0200
@@ -0,0 +1,2 @@
+obj-$(CONFIG_AFTERBURN) += afterburn.o
+obj-$(CONFIG_AFTERBURN_DRIVERS) += drivers/
diff -Naur linux-2.6.9.src/arch/i386/boot/compressed/#head.S# linux-2.6.9/arch/i386/boot/compressed/#head.S#
--- linux-2.6.9.src/arch/i386/boot/compressed/#head.S#	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/arch/i386/boot/compressed/#head.S#	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,128 @@
+/*
+ *  linux/boot/head.S
+ *
+ *  Copyright (C) 1991, 1992, 1993  Linus Torvalds
+ */
+
+/*
+ *  head.S contains the 32-bit startup code.
+ *
+ * NOTE!!! Startup happens at absolute address 0x00001000, which is also where
+ * the page directory will exist. The startup code will be overwritten by
+ * the page directory. [According to comments etc elsewhere on a compressed
+ * kernel it will end up at 0x1000 + 1Mb I hope so as I assume this. - AC]
+ *
+ * Page 0 is deliberately kept safe, since System Management Mode code in 
+ * laptops may need to access the BIOS data stored there.  This is also
+ * useful for future device drivers that either access the BIOS via VM86 
+ * mode.
+ */
+
+/*
+ * High loaded stuff by Hans Lermen & Werner Almesberger, Feb. 1996
+ */
+.text
+
+#include <linux/linkage.h>
+#include <asm/segment.h>
+
+	.globl startup_32
+	
+startup_32:
+	cld
+	cli
+	movl $(__BOOT_DS),%eax
+	movl %eax,%ds
+	movl %eax,%es
+	movl %eax,%fs
+	movl %eax,%gs
+
+	lss stack_start,%esp
+	xorl %eax,%eax
+1:	incl %eax		# check that A20 really IS enabled
+	movl %eax,0x000000	# loop forever if it isn't
+	cmpl %eax,0x100000
+	je 1b
+
+/*
+ * Initialize eflags.  Some BIOS's leave bits like NT set.  This would
+ * confuse the debugger if this code is traced.
+ * XXX - best to initialize before switching to protected mode.
+ */
+	pushl $0
+	popfl
+/*
+ * Clear BSS
+ */
+	xorl %eax,%eax
+	movl $_edata,%edi
+	movl $_end,%ecx
+	subl %edi,%ecx
+	cld
+	rep
+	stosb
+/*
+ * Do the decompression, and jump to the new kernel..
+ */
+	subl $16,%esp	# place for structure on the stack
+	movl %esp,%eax
+	pushl %esi	# real mode pointer as second arg
+	pushl %eax	# address of structure as first arg
+	call decompress_kernel
+	orl  %eax,%eax 
+	jnz  3f
+	popl %esi	# discard address
+	popl %esi	# real mode pointer
+	xorl %ebx,%ebx
+	ljmp $(__BOOT_CS), $0x100000
+
+/*
+ * We come here, if we were loaded high.
+ * We need to move the move-in-place routine down to 0x1000
+ * and then start it with the buffer addresses in registers,
+ * which we got from the stack.
+ */
+3:
+	movl $move_routine_start,%esi
+	movl $0x1000,%edi
+	movl $move_routine_end,%ecx
+	subl %esi,%ecx
+	addl $3,%ecx
+	shrl $2,%ecx
+	cld
+	rep
+	movsl
+
+	popl %esi	# discard the address
+	popl %ebx	# real mode pointer
+	popl %esi	# low_buffer_start
+	popl %ecx	# lcount
+	popl %edx	# high_buffer_start
+	popl %eax	# hcount
+	movl $0x100000,%edi
+	cli		# make sure we don't get interrupted
+	ljmp $(__BOOT_CS), $0x1000 # and jump to the move routine
+
+/*
+ * Routine (template) for moving the decompressed kernel in place,
+ * if we were high loaded. This _must_ PIC-code !
+ */
+move_routine_start:
+	movl %ecx,%ebp
+	shrl $2,%ecx
+	rep
+	movsl
+	movl %ebp,%ecx
+	andl $3,%ecx
+	rep
+	movsb
+	movl %edx,%esi
+	movl %eax,%ecx	# NOTE: rep movsb won't move if %ecx == 0
+	addl $3,%ecx
+	shrl $2,%ecx
+	rep
+	movsl
+	movl %ebx,%esi	# Restore setup pointer
+	xorl %ebx,%ebx
+	ljmp $(__BOOT_CS), $0x100000
+move_routine_end:
diff -Naur linux-2.6.9.src/arch/i386/Kconfig linux-2.6.9/arch/i386/Kconfig
--- linux-2.6.9.src/arch/i386/Kconfig	2004-10-18 23:53:22.000000000 +0200
+++ linux-2.6.9/arch/i386/Kconfig	2007-07-03 11:07:40.000000000 +0200
@@ -870,6 +870,7 @@
 
 endmenu
 
+source "afterburn/Kconfig"
 
 menu "Power management options (ACPI, APM)"
 	depends on !X86_VOYAGER
diff -Naur linux-2.6.9.src/arch/i386/kernel/entry.S linux-2.6.9/arch/i386/kernel/entry.S
--- linux-2.6.9.src/arch/i386/kernel/entry.S	2004-10-18 23:53:44.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/entry.S	2007-07-03 11:07:40.000000000 +0200
@@ -119,7 +119,7 @@
 	.align 4;	\
 	.long 1b,3b;	\
 	.long 2b,4b;	\
-.previous
+.previous;
 
 
 #define RESTORE_ALL	\
@@ -137,7 +137,7 @@
 .section __ex_table,"a";\
 	.align 4;	\
 	.long 1b,2b;	\
-.previous
+.previous;
 
 
 
diff -Naur linux-2.6.9.src/arch/i386/kernel/io_apic.c linux-2.6.9/arch/i386/kernel/io_apic.c
--- linux-2.6.9.src/arch/i386/kernel/io_apic.c	2004-10-18 23:53:46.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/io_apic.c	2007-07-03 11:07:40.000000000 +0200
@@ -246,7 +246,8 @@
 # include <linux/kernel_stat.h>	/* kstat */
 # include <linux/slab.h>		/* kmalloc() */
 # include <linux/timer.h>	/* time_after() */
- 
+
+#define CONFIG_BALANCED_IRQ_DEBUG
 # ifdef CONFIG_BALANCED_IRQ_DEBUG
 #  define TDprintk(x...) do { printk("<%ld:%s:%d>: ", jiffies, __FILE__, __LINE__); printk(x); } while (0)
 #  define Dprintk(x...) do { TDprintk(x); } while (0)
@@ -1758,7 +1759,11 @@
 
 	local_irq_enable();
 	/* Let ten ticks pass... */
+#if defined(CONFIG_AFTERBURN)
+	mdelay((50 * 1000) / HZ);
+#else
 	mdelay((10 * 1000) / HZ);
+#endif	
 
 	/*
 	 * Expect a few ticks at least, to be sure some possible
diff -Naur linux-2.6.9.src/arch/i386/kernel/pci-dma.c linux-2.6.9/arch/i386/kernel/pci-dma.c
--- linux-2.6.9.src/arch/i386/kernel/pci-dma.c	2004-10-18 23:53:43.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/pci-dma.c	2007-07-03 11:07:40.000000000 +0200
@@ -21,6 +21,10 @@
 	unsigned long	*bitmap;
 };
 
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+extern unsigned long (*afterburn_phys_to_dma_hook)(unsigned long phys, unsigned long size);
+#endif
+
 void *dma_alloc_coherent(struct device *dev, size_t size,
 			   dma_addr_t *dma_handle, int gfp)
 {
@@ -50,6 +54,11 @@
 
 	if (ret != NULL) {
 		memset(ret, 0, size);
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+		if (afterburn_phys_to_dma_hook)
+			*dma_handle = afterburn_phys_to_dma_hook(virt_to_phys(ret), size);
+		else
+#endif
 		*dma_handle = virt_to_phys(ret);
 	}
 	return ret;
diff -Naur linux-2.6.9.src/arch/i386/kernel/process.c linux-2.6.9/arch/i386/kernel/process.c
--- linux-2.6.9.src/arch/i386/kernel/process.c	2004-10-18 23:53:05.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/process.c	2007-07-03 11:07:40.000000000 +0200
@@ -574,8 +574,8 @@
 	 * Save away %fs and %gs. No need to save %es and %ds, as
 	 * those are always kernel segments while inside the kernel.
 	 */
-	asm volatile("movl %%fs,%0":"=m" (*(int *)&prev->fs));
-	asm volatile("movl %%gs,%0":"=m" (*(int *)&prev->gs));
+	asm volatile("mov %%fs,%0":"=m" (prev->fs));
+	asm volatile("mov %%gs,%0":"=m" (prev->gs));
 
 	/*
 	 * Restore %fs and %gs if needed.
diff -Naur linux-2.6.9.src/arch/i386/kernel/process.c.orig linux-2.6.9/arch/i386/kernel/process.c.orig
--- linux-2.6.9.src/arch/i386/kernel/process.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/arch/i386/kernel/process.c.orig	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,806 @@
+/*
+ *  linux/arch/i386/kernel/process.c
+ *
+ *  Copyright (C) 1995  Linus Torvalds
+ *
+ *  Pentium III FXSR, SSE support
+ *	Gareth Hughes <gareth@valinux.com>, May 2000
+ */
+
+/*
+ * This file handles the architecture-dependent parts of process handling..
+ */
+
+#include <stdarg.h>
+
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/elfcore.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/stddef.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/user.h>
+#include <linux/a.out.h>
+#include <linux/interrupt.h>
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/delay.h>
+#include <linux/reboot.h>
+#include <linux/init.h>
+#include <linux/mc146818rtc.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/ptrace.h>
+
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/ldt.h>
+#include <asm/processor.h>
+#include <asm/i387.h>
+#include <asm/irq.h>
+#include <asm/desc.h>
+#ifdef CONFIG_MATH_EMULATION
+#include <asm/math_emu.h>
+#endif
+
+#include <linux/irq.h>
+#include <linux/err.h>
+
+asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
+
+int hlt_counter;
+
+/*
+ * Return saved PC of a blocked thread.
+ */
+unsigned long thread_saved_pc(struct task_struct *tsk)
+{
+	return ((unsigned long *)tsk->thread.esp)[3];
+}
+
+/*
+ * Powermanagement idle function, if any..
+ */
+void (*pm_idle)(void);
+
+void disable_hlt(void)
+{
+	hlt_counter++;
+}
+
+EXPORT_SYMBOL(disable_hlt);
+
+void enable_hlt(void)
+{
+	hlt_counter--;
+}
+
+EXPORT_SYMBOL(enable_hlt);
+
+/*
+ * We use this if we don't have any better
+ * idle routine..
+ */
+void default_idle(void)
+{
+	if (!hlt_counter && current_cpu_data.hlt_works_ok) {
+		local_irq_disable();
+		if (!need_resched())
+			safe_halt();
+		else
+			local_irq_enable();
+	}
+}
+
+/*
+ * On SMP it's slightly faster (but much more power-consuming!)
+ * to poll the ->work.need_resched flag instead of waiting for the
+ * cross-CPU IPI to arrive. Use this option with caution.
+ */
+static void poll_idle (void)
+{
+	int oldval;
+
+	local_irq_enable();
+
+	/*
+	 * Deal with another CPU just having chosen a thread to
+	 * run here:
+	 */
+	oldval = test_and_clear_thread_flag(TIF_NEED_RESCHED);
+
+	if (!oldval) {
+		set_thread_flag(TIF_POLLING_NRFLAG);
+		asm volatile(
+			"2:"
+			"testl %0, %1;"
+			"rep; nop;"
+			"je 2b;"
+			: : "i"(_TIF_NEED_RESCHED), "m" (current_thread_info()->flags));
+
+		clear_thread_flag(TIF_POLLING_NRFLAG);
+	} else {
+		set_need_resched();
+	}
+}
+
+/*
+ * The idle thread. There's no useful work to be
+ * done, so just try to conserve power and have a
+ * low exit latency (ie sit in a loop waiting for
+ * somebody to say that they'd like to reschedule)
+ */
+void cpu_idle (void)
+{
+	/* endless idle loop with no priority at all */
+	while (1) {
+		while (!need_resched()) {
+			void (*idle)(void);
+			/*
+			 * Mark this as an RCU critical section so that
+			 * synchronize_kernel() in the unload path waits
+			 * for our completion.
+			 */
+			rcu_read_lock();
+			idle = pm_idle;
+
+			if (!idle)
+				idle = default_idle;
+
+			irq_stat[smp_processor_id()].idle_timestamp = jiffies;
+			idle();
+			rcu_read_unlock();
+		}
+		schedule();
+	}
+}
+
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ */
+static void mwait_idle(void)
+{
+	local_irq_enable();
+
+	if (!need_resched()) {
+		set_thread_flag(TIF_POLLING_NRFLAG);
+		do {
+			__monitor((void *)&current_thread_info()->flags, 0, 0);
+			if (need_resched())
+				break;
+			__mwait(0, 0);
+		} while (!need_resched());
+		clear_thread_flag(TIF_POLLING_NRFLAG);
+	}
+}
+
+void __init select_idle_routine(const struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+		printk("monitor/mwait feature present.\n");
+		/*
+		 * Skip, if setup has overridden idle.
+		 * One CPU supports mwait => All CPUs supports mwait
+		 */
+		if (!pm_idle) {
+			printk("using mwait in idle threads.\n");
+			pm_idle = mwait_idle;
+		}
+	}
+}
+
+static int __init idle_setup (char *str)
+{
+	if (!strncmp(str, "poll", 4)) {
+		printk("using polling idle threads.\n");
+		pm_idle = poll_idle;
+#ifdef CONFIG_X86_SMP
+		if (smp_num_siblings > 1)
+			printk("WARNING: polling idle and HT enabled, performance may degrade.\n");
+#endif
+	} else if (!strncmp(str, "halt", 4)) {
+		printk("using halt in idle threads.\n");
+		pm_idle = default_idle;
+	}
+
+	return 1;
+}
+
+__setup("idle=", idle_setup);
+
+void show_regs(struct pt_regs * regs)
+{
+	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L;
+
+	printk("\n");
+	printk("Pid: %d, comm: %20s\n", current->pid, current->comm);
+	printk("EIP: %04x:[<%08lx>] CPU: %d\n",0xffff & regs->xcs,regs->eip, smp_processor_id());
+	print_symbol("EIP is at %s\n", regs->eip);
+
+	if (regs->xcs & 3)
+		printk(" ESP: %04x:%08lx",0xffff & regs->xss,regs->esp);
+	printk(" EFLAGS: %08lx    %s  (%s)\n",regs->eflags, print_tainted(),UTS_RELEASE);
+	printk("EAX: %08lx EBX: %08lx ECX: %08lx EDX: %08lx\n",
+		regs->eax,regs->ebx,regs->ecx,regs->edx);
+	printk("ESI: %08lx EDI: %08lx EBP: %08lx",
+		regs->esi, regs->edi, regs->ebp);
+	printk(" DS: %04x ES: %04x\n",
+		0xffff & regs->xds,0xffff & regs->xes);
+
+	__asm__("movl %%cr0, %0": "=r" (cr0));
+	__asm__("movl %%cr2, %0": "=r" (cr2));
+	__asm__("movl %%cr3, %0": "=r" (cr3));
+	/* This could fault if %cr4 does not exist */
+	__asm__("1: movl %%cr4, %0		\n"
+		"2:				\n"
+		".section __ex_table,\"a\"	\n"
+		".long 1b,2b			\n"
+		".previous			\n"
+		: "=r" (cr4): "0" (0));
+	printk("CR0: %08lx CR2: %08lx CR3: %08lx CR4: %08lx\n", cr0, cr2, cr3, cr4);
+	show_trace(NULL, &regs->esp);
+}
+
+/*
+ * This gets run with %ebx containing the
+ * function to call, and %edx containing
+ * the "args".
+ */
+extern void kernel_thread_helper(void);
+__asm__(".section .text\n"
+	".align 4\n"
+	"kernel_thread_helper:\n\t"
+	"movl %edx,%eax\n\t"
+	"pushl %edx\n\t"
+	"call *%ebx\n\t"
+	"pushl %eax\n\t"
+	"call do_exit\n"
+	".previous");
+
+/*
+ * Create a kernel thread
+ */
+int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
+{
+	struct pt_regs regs;
+
+	memset(&regs, 0, sizeof(regs));
+
+	regs.ebx = (unsigned long) fn;
+	regs.edx = (unsigned long) arg;
+
+	regs.xds = __USER_DS;
+	regs.xes = __USER_DS;
+	regs.orig_eax = -1;
+	regs.eip = (unsigned long) kernel_thread_helper;
+	regs.xcs = __KERNEL_CS;
+	regs.eflags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;
+
+	/* Ok, create the new process.. */
+	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+}
+
+/*
+ * Free current thread data structures etc..
+ */
+void exit_thread(void)
+{
+	struct task_struct *tsk = current;
+	struct thread_struct *t = &tsk->thread;
+
+	/* The process may have allocated an io port bitmap... nuke it. */
+	if (unlikely(NULL != t->io_bitmap_ptr)) {
+		int cpu = get_cpu();
+		struct tss_struct *tss = &per_cpu(init_tss, cpu);
+
+		kfree(t->io_bitmap_ptr);
+		t->io_bitmap_ptr = NULL;
+		/*
+		 * Careful, clear this in the TSS too:
+		 */
+		memset(tss->io_bitmap, 0xff, tss->io_bitmap_max);
+		t->io_bitmap_max = 0;
+		tss->io_bitmap_owner = NULL;
+		tss->io_bitmap_max = 0;
+		tss->io_bitmap_base = INVALID_IO_BITMAP_OFFSET;
+		put_cpu();
+	}
+}
+
+void flush_thread(void)
+{
+	struct task_struct *tsk = current;
+
+	memset(tsk->thread.debugreg, 0, sizeof(unsigned long)*8);
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
+	/*
+	 * Forget coprocessor state..
+	 */
+	clear_fpu(tsk);
+	tsk->used_math = 0;
+}
+
+void release_thread(struct task_struct *dead_task)
+{
+	if (dead_task->mm) {
+		// temporary debugging check
+		if (dead_task->mm->context.size) {
+			printk("WARNING: dead process %8s still has LDT? <%p/%d>\n",
+					dead_task->comm,
+					dead_task->mm->context.ldt,
+					dead_task->mm->context.size);
+			BUG();
+		}
+	}
+
+	release_x86_irqs(dead_task);
+}
+
+/*
+ * This gets called before we allocate a new thread and copy
+ * the current task into it.
+ */
+void prepare_to_copy(struct task_struct *tsk)
+{
+	unlazy_fpu(tsk);
+}
+
+int copy_thread(int nr, unsigned long clone_flags, unsigned long esp,
+	unsigned long unused,
+	struct task_struct * p, struct pt_regs * regs)
+{
+	struct pt_regs * childregs;
+	struct task_struct *tsk;
+	int err;
+
+	childregs = ((struct pt_regs *) (THREAD_SIZE + (unsigned long) p->thread_info)) - 1;
+	*childregs = *regs;
+	childregs->eax = 0;
+	childregs->esp = esp;
+	p->set_child_tid = p->clear_child_tid = NULL;
+
+	p->thread.esp = (unsigned long) childregs;
+	p->thread.esp0 = (unsigned long) (childregs+1);
+
+	p->thread.eip = (unsigned long) ret_from_fork;
+
+	savesegment(fs,p->thread.fs);
+	savesegment(gs,p->thread.gs);
+
+	tsk = current;
+	if (unlikely(NULL != tsk->thread.io_bitmap_ptr)) {
+		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
+		if (!p->thread.io_bitmap_ptr) {
+			p->thread.io_bitmap_max = 0;
+			return -ENOMEM;
+		}
+		memcpy(p->thread.io_bitmap_ptr, tsk->thread.io_bitmap_ptr,
+			IO_BITMAP_BYTES);
+	}
+
+	/*
+	 * Set a new TLS for the child thread?
+	 */
+	if (clone_flags & CLONE_SETTLS) {
+		struct desc_struct *desc;
+		struct user_desc info;
+		int idx;
+
+		err = -EFAULT;
+		if (copy_from_user(&info, (void __user *)childregs->esi, sizeof(info)))
+			goto out;
+		err = -EINVAL;
+		if (LDT_empty(&info))
+			goto out;
+
+		idx = info.entry_number;
+		if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+			goto out;
+
+		desc = p->thread.tls_array + idx - GDT_ENTRY_TLS_MIN;
+		desc->a = LDT_entry_a(&info);
+		desc->b = LDT_entry_b(&info);
+	}
+
+	err = 0;
+ out:
+	if (err && p->thread.io_bitmap_ptr) {
+		kfree(p->thread.io_bitmap_ptr);
+		p->thread.io_bitmap_max = 0;
+	}
+	return err;
+}
+
+/*
+ * fill in the user structure for a core dump..
+ */
+void dump_thread(struct pt_regs * regs, struct user * dump)
+{
+	int i;
+
+/* changed the size calculations - should hopefully work better. lbt */
+	dump->magic = CMAGIC;
+	dump->start_code = 0;
+	dump->start_stack = regs->esp & ~(PAGE_SIZE - 1);
+	dump->u_tsize = ((unsigned long) current->mm->end_code) >> PAGE_SHIFT;
+	dump->u_dsize = ((unsigned long) (current->mm->brk + (PAGE_SIZE-1))) >> PAGE_SHIFT;
+	dump->u_dsize -= dump->u_tsize;
+	dump->u_ssize = 0;
+	for (i = 0; i < 8; i++)
+		dump->u_debugreg[i] = current->thread.debugreg[i];  
+
+	if (dump->start_stack < TASK_SIZE)
+		dump->u_ssize = ((unsigned long) (TASK_SIZE - dump->start_stack)) >> PAGE_SHIFT;
+
+	dump->regs.ebx = regs->ebx;
+	dump->regs.ecx = regs->ecx;
+	dump->regs.edx = regs->edx;
+	dump->regs.esi = regs->esi;
+	dump->regs.edi = regs->edi;
+	dump->regs.ebp = regs->ebp;
+	dump->regs.eax = regs->eax;
+	dump->regs.ds = regs->xds;
+	dump->regs.es = regs->xes;
+	savesegment(fs,dump->regs.fs);
+	savesegment(gs,dump->regs.gs);
+	dump->regs.orig_eax = regs->orig_eax;
+	dump->regs.eip = regs->eip;
+	dump->regs.cs = regs->xcs;
+	dump->regs.eflags = regs->eflags;
+	dump->regs.esp = regs->esp;
+	dump->regs.ss = regs->xss;
+
+	dump->u_fpvalid = dump_fpu (regs, &dump->i387);
+}
+
+/* 
+ * Capture the user space registers if the task is not running (in user space)
+ */
+int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
+{
+	struct pt_regs ptregs;
+	
+	ptregs = *(struct pt_regs *)
+		((unsigned long)tsk->thread_info+THREAD_SIZE - sizeof(ptregs));
+	ptregs.xcs &= 0xffff;
+	ptregs.xds &= 0xffff;
+	ptregs.xes &= 0xffff;
+	ptregs.xss &= 0xffff;
+
+	elf_core_copy_regs(regs, &ptregs);
+
+	return 1;
+}
+
+static inline void
+handle_io_bitmap(struct thread_struct *next, struct tss_struct *tss)
+{
+	if (!next->io_bitmap_ptr) {
+		/*
+		 * Disable the bitmap via an invalid offset. We still cache
+		 * the previous bitmap owner and the IO bitmap contents:
+		 */
+		tss->io_bitmap_base = INVALID_IO_BITMAP_OFFSET;
+		return;
+	}
+	if (likely(next == tss->io_bitmap_owner)) {
+		/*
+		 * Previous owner of the bitmap (hence the bitmap content)
+		 * matches the next task, we dont have to do anything but
+		 * to set a valid offset in the TSS:
+		 */
+		tss->io_bitmap_base = IO_BITMAP_OFFSET;
+		return;
+	}
+	/*
+	 * Lazy TSS's I/O bitmap copy. We set an invalid offset here
+	 * and we let the task to get a GPF in case an I/O instruction
+	 * is performed.  The handler of the GPF will verify that the
+	 * faulting task has a valid I/O bitmap and, it true, does the
+	 * real copy and restart the instruction.  This will save us
+	 * redundant copies when the currently switched task does not
+	 * perform any I/O during its timeslice.
+	 */
+	tss->io_bitmap_base = INVALID_IO_BITMAP_OFFSET_LAZY;
+}
+/*
+ * This special macro can be used to load a debugging register
+ */
+#define loaddebug(thread,register) \
+		__asm__("movl %0,%%db" #register  \
+			: /* no output */ \
+			:"r" (thread->debugreg[register]))
+
+/*
+ *	switch_to(x,yn) should switch tasks from x to y.
+ *
+ * We fsave/fwait so that an exception goes off at the right time
+ * (as a call from the fsave or fwait in effect) rather than to
+ * the wrong process. Lazy FP saving no longer makes any sense
+ * with modern CPU's, and this simplifies a lot of things (SMP
+ * and UP become the same).
+ *
+ * NOTE! We used to use the x86 hardware context switching. The
+ * reason for not using it any more becomes apparent when you
+ * try to recover gracefully from saved state that is no longer
+ * valid (stale segment register values in particular). With the
+ * hardware task-switch, there is no way to fix up bad state in
+ * a reasonable manner.
+ *
+ * The fact that Intel documents the hardware task-switching to
+ * be slow is a fairly red herring - this code is not noticeably
+ * faster. However, there _is_ some room for improvement here,
+ * so the performance issues may eventually be a valid point.
+ * More important, however, is the fact that this allows us much
+ * more flexibility.
+ *
+ * The return value (in %eax) will be the "prev" task after
+ * the task-switch, and shows up in ret_from_fork in entry.S,
+ * for example.
+ */
+struct task_struct fastcall * __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	struct thread_struct *prev = &prev_p->thread,
+				 *next = &next_p->thread;
+	int cpu = smp_processor_id();
+	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+
+	/* never put a printk in __switch_to... printk() calls wake_up*() indirectly */
+
+	__unlazy_fpu(prev_p);
+
+	/*
+	 * Reload esp0, LDT and the page table pointer:
+	 */
+	load_esp0(tss, next);
+
+	/*
+	 * Load the per-thread Thread-Local Storage descriptor.
+	 */
+	load_TLS(next, cpu);
+
+	/*
+	 * Save away %fs and %gs. No need to save %es and %ds, as
+	 * those are always kernel segments while inside the kernel.
+	 */
+	asm volatile("movl %%fs,%0":"=m" (*(int *)&prev->fs));
+	asm volatile("movl %%gs,%0":"=m" (*(int *)&prev->gs));
+
+	/*
+	 * Restore %fs and %gs if needed.
+	 */
+	if (unlikely(prev->fs | prev->gs | next->fs | next->gs)) {
+		loadsegment(fs, next->fs);
+		loadsegment(gs, next->gs);
+	}
+
+	/*
+	 * Now maybe reload the debug registers
+	 */
+	if (unlikely(next->debugreg[7])) {
+		loaddebug(next, 0);
+		loaddebug(next, 1);
+		loaddebug(next, 2);
+		loaddebug(next, 3);
+		/* no 4 and 5 */
+		loaddebug(next, 6);
+		loaddebug(next, 7);
+	}
+
+	if (unlikely(prev->io_bitmap_ptr || next->io_bitmap_ptr))
+		handle_io_bitmap(next, tss);
+
+	return prev_p;
+}
+
+asmlinkage int sys_fork(struct pt_regs regs)
+{
+	return do_fork(SIGCHLD, regs.esp, &regs, 0, NULL, NULL);
+}
+
+asmlinkage int sys_clone(struct pt_regs regs)
+{
+	unsigned long clone_flags;
+	unsigned long newsp;
+	int __user *parent_tidptr, *child_tidptr;
+
+	clone_flags = regs.ebx;
+	newsp = regs.ecx;
+	parent_tidptr = (int __user *)regs.edx;
+	child_tidptr = (int __user *)regs.edi;
+	if (!newsp)
+		newsp = regs.esp;
+	return do_fork(clone_flags, newsp, &regs, 0, parent_tidptr, child_tidptr);
+}
+
+/*
+ * This is trivial, and on the face of it looks like it
+ * could equally well be done in user mode.
+ *
+ * Not so, for quite unobvious reasons - register pressure.
+ * In user mode vfork() cannot have a stack frame, and if
+ * done by calling the "clone()" system call directly, you
+ * do not have enough call-clobbered registers to hold all
+ * the information you need.
+ */
+asmlinkage int sys_vfork(struct pt_regs regs)
+{
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.esp, &regs, 0, NULL, NULL);
+}
+
+/*
+ * sys_execve() executes a new program.
+ */
+asmlinkage int sys_execve(struct pt_regs regs)
+{
+	int error;
+	char * filename;
+
+	filename = getname((char __user *) regs.ebx);
+	error = PTR_ERR(filename);
+	if (IS_ERR(filename))
+		goto out;
+	error = do_execve(filename,
+			(char __user * __user *) regs.ecx,
+			(char __user * __user *) regs.edx,
+			&regs);
+	if (error == 0) {
+		current->ptrace &= ~PT_DTRACE;
+		/* Make sure we don't return using sysenter.. */
+		set_thread_flag(TIF_IRET);
+	}
+	putname(filename);
+out:
+	return error;
+}
+
+#define top_esp                (THREAD_SIZE - sizeof(unsigned long))
+#define top_ebp                (THREAD_SIZE - 2*sizeof(unsigned long))
+
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long ebp, esp, eip;
+	unsigned long stack_page;
+	int count = 0;
+	if (!p || p == current || p->state == TASK_RUNNING)
+		return 0;
+	stack_page = (unsigned long)p->thread_info;
+	esp = p->thread.esp;
+	if (!stack_page || esp < stack_page || esp > top_esp+stack_page)
+		return 0;
+	/* include/asm-i386/system.h:switch_to() pushes ebp last. */
+	ebp = *(unsigned long *) esp;
+	do {
+		if (ebp < stack_page || ebp > top_ebp+stack_page)
+			return 0;
+		eip = *(unsigned long *) (ebp+4);
+		if (!in_sched_functions(eip))
+			return eip;
+		ebp = *(unsigned long *) ebp;
+	} while (count++ < 16);
+	return 0;
+}
+
+/*
+ * sys_alloc_thread_area: get a yet unused TLS descriptor index.
+ */
+static int get_free_idx(void)
+{
+	struct thread_struct *t = &current->thread;
+	int idx;
+
+	for (idx = 0; idx < GDT_ENTRY_TLS_ENTRIES; idx++)
+		if (desc_empty(t->tls_array + idx))
+			return idx + GDT_ENTRY_TLS_MIN;
+	return -ESRCH;
+}
+
+/*
+ * Set a given TLS descriptor:
+ */
+asmlinkage int sys_set_thread_area(struct user_desc __user *u_info)
+{
+	struct thread_struct *t = &current->thread;
+	struct user_desc info;
+	struct desc_struct *desc;
+	int cpu, idx;
+
+	if (copy_from_user(&info, u_info, sizeof(info)))
+		return -EFAULT;
+	idx = info.entry_number;
+
+	/*
+	 * index -1 means the kernel should try to find and
+	 * allocate an empty descriptor:
+	 */
+	if (idx == -1) {
+		idx = get_free_idx();
+		if (idx < 0)
+			return idx;
+		if (put_user(idx, &u_info->entry_number))
+			return -EFAULT;
+	}
+
+	if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+		return -EINVAL;
+
+	desc = t->tls_array + idx - GDT_ENTRY_TLS_MIN;
+
+	/*
+	 * We must not get preempted while modifying the TLS.
+	 */
+	cpu = get_cpu();
+
+	if (LDT_empty(&info)) {
+		desc->a = 0;
+		desc->b = 0;
+	} else {
+		desc->a = LDT_entry_a(&info);
+		desc->b = LDT_entry_b(&info);
+	}
+	load_TLS(t, cpu);
+
+	put_cpu();
+
+	return 0;
+}
+
+/*
+ * Get the current Thread-Local Storage area:
+ */
+
+#define GET_BASE(desc) ( \
+	(((desc)->a >> 16) & 0x0000ffff) | \
+	(((desc)->b << 16) & 0x00ff0000) | \
+	( (desc)->b        & 0xff000000)   )
+
+#define GET_LIMIT(desc) ( \
+	((desc)->a & 0x0ffff) | \
+	 ((desc)->b & 0xf0000) )
+	
+#define GET_32BIT(desc)		(((desc)->b >> 22) & 1)
+#define GET_CONTENTS(desc)	(((desc)->b >> 10) & 3)
+#define GET_WRITABLE(desc)	(((desc)->b >>  9) & 1)
+#define GET_LIMIT_PAGES(desc)	(((desc)->b >> 23) & 1)
+#define GET_PRESENT(desc)	(((desc)->b >> 15) & 1)
+#define GET_USEABLE(desc)	(((desc)->b >> 20) & 1)
+
+asmlinkage int sys_get_thread_area(struct user_desc __user *u_info)
+{
+	struct user_desc info;
+	struct desc_struct *desc;
+	int idx;
+
+	if (get_user(idx, &u_info->entry_number))
+		return -EFAULT;
+	if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+		return -EINVAL;
+
+	desc = current->thread.tls_array + idx - GDT_ENTRY_TLS_MIN;
+
+	info.entry_number = idx;
+	info.base_addr = GET_BASE(desc);
+	info.limit = GET_LIMIT(desc);
+	info.seg_32bit = GET_32BIT(desc);
+	info.contents = GET_CONTENTS(desc);
+	info.read_exec_only = !GET_WRITABLE(desc);
+	info.limit_in_pages = GET_LIMIT_PAGES(desc);
+	info.seg_not_present = !GET_PRESENT(desc);
+	info.useable = GET_USEABLE(desc);
+
+	if (copy_to_user(u_info, &info, sizeof(info)))
+		return -EFAULT;
+	return 0;
+}
+
diff -Naur linux-2.6.9.src/arch/i386/kernel/signal.c linux-2.6.9/arch/i386/kernel/signal.c
--- linux-2.6.9.src/arch/i386/kernel/signal.c	2004-10-18 23:54:07.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/signal.c	2007-07-03 11:07:40.000000000 +0200
@@ -576,6 +576,14 @@
 	int signr;
 	struct k_sigaction ka;
 
+#if defined(CONFIG_AFTERBURN_THREAD_HOOKS)
+	{
+	    extern int (*afterburn_signal_hook)(void *handle);
+	    if( afterburn_signal_hook &&
+		    afterburn_signal_hook(current->afterburn_handle) )
+		goto no_signal;
+	}
+#endif
 	/*
 	 * We want the common case to go fast, which
 	 * is why we may in certain cases get here from
diff -Naur linux-2.6.9.src/arch/i386/kernel/vm86.c linux-2.6.9/arch/i386/kernel/vm86.c
--- linux-2.6.9.src/arch/i386/kernel/vm86.c	2004-10-18 23:53:13.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/vm86.c	2007-07-03 11:07:40.000000000 +0200
@@ -300,8 +300,8 @@
  */
 	info->regs32->eax = 0;
 	tsk->thread.saved_esp0 = tsk->thread.esp0;
-	asm volatile("movl %%fs,%0":"=m" (tsk->thread.saved_fs));
-	asm volatile("movl %%gs,%0":"=m" (tsk->thread.saved_gs));
+	asm volatile("mov %%fs,%0":"=m" (tsk->thread.saved_fs));
+	asm volatile("mov %%gs,%0":"=m" (tsk->thread.saved_gs));
 
 	tss = &per_cpu(init_tss, get_cpu());
 	tsk->thread.esp0 = (unsigned long) &info->VM86_TSS_ESP0;
diff -Naur linux-2.6.9.src/arch/i386/kernel/vm86.c.orig linux-2.6.9/arch/i386/kernel/vm86.c.orig
--- linux-2.6.9.src/arch/i386/kernel/vm86.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/arch/i386/kernel/vm86.c.orig	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,803 @@
+/*
+ *  linux/kernel/vm86.c
+ *
+ *  Copyright (C) 1994  Linus Torvalds
+ *
+ *  29 dec 2001 - Fixed oopses caused by unchecked access to the vm86
+ *                stack - Manfred Spraul <manfreds@colorfullife.com>
+ *
+ *  22 mar 2002 - Manfred detected the stackfaults, but didn't handle
+ *                them correctly. Now the emulation will be in a
+ *                consistent state after stackfaults - Kasper Dupont
+ *                <kasperd@daimi.au.dk>
+ *
+ *  22 mar 2002 - Added missing clear_IF in set_vflags_* Kasper Dupont
+ *                <kasperd@daimi.au.dk>
+ *
+ *  ?? ??? 2002 - Fixed premature returns from handle_vm86_fault
+ *                caused by Kasper Dupont's changes - Stas Sergeev
+ *
+ *   4 apr 2002 - Fixed CHECK_IF_IN_TRAP broken by Stas' changes.
+ *                Kasper Dupont <kasperd@daimi.au.dk>
+ *
+ *   9 apr 2002 - Changed syntax of macros in handle_vm86_fault.
+ *                Kasper Dupont <kasperd@daimi.au.dk>
+ *
+ *   9 apr 2002 - Changed stack access macros to jump to a label
+ *                instead of returning to userspace. This simplifies
+ *                do_int, and is needed by handle_vm6_fault. Kasper
+ *                Dupont <kasperd@daimi.au.dk>
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/signal.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/highmem.h>
+#include <linux/ptrace.h>
+
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/tlbflush.h>
+#include <asm/irq.h>
+
+/*
+ * Known problems:
+ *
+ * Interrupt handling is not guaranteed:
+ * - a real x86 will disable all interrupts for one instruction
+ *   after a "mov ss,xx" to make stack handling atomic even without
+ *   the 'lss' instruction. We can't guarantee this in v86 mode,
+ *   as the next instruction might result in a page fault or similar.
+ * - a real x86 will have interrupts disabled for one instruction
+ *   past the 'sti' that enables them. We don't bother with all the
+ *   details yet.
+ *
+ * Let's hope these problems do not actually matter for anything.
+ */
+
+
+#define KVM86	((struct kernel_vm86_struct *)regs)
+#define VMPI 	KVM86->vm86plus
+
+
+/*
+ * 8- and 16-bit register defines..
+ */
+#define AL(regs)	(((unsigned char *)&((regs)->eax))[0])
+#define AH(regs)	(((unsigned char *)&((regs)->eax))[1])
+#define IP(regs)	(*(unsigned short *)&((regs)->eip))
+#define SP(regs)	(*(unsigned short *)&((regs)->esp))
+
+/*
+ * virtual flags (16 and 32-bit versions)
+ */
+#define VFLAGS	(*(unsigned short *)&(current->thread.v86flags))
+#define VEFLAGS	(current->thread.v86flags)
+
+#define set_flags(X,new,mask) \
+((X) = ((X) & ~(mask)) | ((new) & (mask)))
+
+#define SAFE_MASK	(0xDD5)
+#define RETURN_MASK	(0xDFF)
+
+#define VM86_REGS_PART2 orig_eax
+#define VM86_REGS_SIZE1 \
+        ( (unsigned)( & (((struct kernel_vm86_regs *)0)->VM86_REGS_PART2) ) )
+#define VM86_REGS_SIZE2 (sizeof(struct kernel_vm86_regs) - VM86_REGS_SIZE1)
+
+struct pt_regs * FASTCALL(save_v86_state(struct kernel_vm86_regs * regs));
+struct pt_regs * fastcall save_v86_state(struct kernel_vm86_regs * regs)
+{
+	struct tss_struct *tss;
+	struct pt_regs *ret;
+	unsigned long tmp;
+
+	/*
+	 * This gets called from entry.S with interrupts disabled, but
+	 * from process context. Enable interrupts here, before trying
+	 * to access user space.
+	 */
+	local_irq_enable();
+
+	if (!current->thread.vm86_info) {
+		printk("no vm86_info: BAD\n");
+		do_exit(SIGSEGV);
+	}
+	set_flags(regs->eflags, VEFLAGS, VIF_MASK | current->thread.v86mask);
+	tmp = copy_to_user(&current->thread.vm86_info->regs,regs, VM86_REGS_SIZE1);
+	tmp += copy_to_user(&current->thread.vm86_info->regs.VM86_REGS_PART2,
+		&regs->VM86_REGS_PART2, VM86_REGS_SIZE2);
+	tmp += put_user(current->thread.screen_bitmap,&current->thread.vm86_info->screen_bitmap);
+	if (tmp) {
+		printk("vm86: could not access userspace vm86_info\n");
+		do_exit(SIGSEGV);
+	}
+
+	tss = &per_cpu(init_tss, get_cpu());
+	current->thread.esp0 = current->thread.saved_esp0;
+	current->thread.sysenter_cs = __KERNEL_CS;
+	load_esp0(tss, &current->thread);
+	current->thread.saved_esp0 = 0;
+	put_cpu();
+
+	loadsegment(fs, current->thread.saved_fs);
+	loadsegment(gs, current->thread.saved_gs);
+	ret = KVM86->regs32;
+	return ret;
+}
+
+static void mark_screen_rdonly(struct task_struct * tsk)
+{
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *pte, *mapped;
+	int i;
+
+	preempt_disable();
+	spin_lock(&tsk->mm->page_table_lock);
+	pgd = pgd_offset(tsk->mm, 0xA0000);
+	if (pgd_none(*pgd))
+		goto out;
+	if (pgd_bad(*pgd)) {
+		pgd_ERROR(*pgd);
+		pgd_clear(pgd);
+		goto out;
+	}
+	pmd = pmd_offset(pgd, 0xA0000);
+	if (pmd_none(*pmd))
+		goto out;
+	if (pmd_bad(*pmd)) {
+		pmd_ERROR(*pmd);
+		pmd_clear(pmd);
+		goto out;
+	}
+	pte = mapped = pte_offset_map(pmd, 0xA0000);
+	for (i = 0; i < 32; i++) {
+		if (pte_present(*pte))
+			set_pte(pte, pte_wrprotect(*pte));
+		pte++;
+	}
+	pte_unmap(mapped);
+out:
+	spin_unlock(&tsk->mm->page_table_lock);
+	preempt_enable();
+	flush_tlb();
+}
+
+
+
+static int do_vm86_irq_handling(int subfunction, int irqnumber);
+static void do_sys_vm86(struct kernel_vm86_struct *info, struct task_struct *tsk);
+
+asmlinkage int sys_vm86old(struct pt_regs regs)
+{
+	struct vm86_struct __user *v86 = (struct vm86_struct __user *)regs.ebx;
+	struct kernel_vm86_struct info; /* declare this _on top_,
+					 * this avoids wasting of stack space.
+					 * This remains on the stack until we
+					 * return to 32 bit user space.
+					 */
+	struct task_struct *tsk;
+	int tmp, ret = -EPERM;
+
+	tsk = current;
+	if (tsk->thread.saved_esp0)
+		goto out;
+	tmp  = copy_from_user(&info, v86, VM86_REGS_SIZE1);
+	tmp += copy_from_user(&info.regs.VM86_REGS_PART2, &v86->regs.VM86_REGS_PART2,
+		(long)&info.vm86plus - (long)&info.regs.VM86_REGS_PART2);
+	ret = -EFAULT;
+	if (tmp)
+		goto out;
+	memset(&info.vm86plus, 0, (int)&info.regs32 - (int)&info.vm86plus);
+	info.regs32 = &regs;
+	tsk->thread.vm86_info = v86;
+	do_sys_vm86(&info, tsk);
+	ret = 0;	/* we never return here */
+out:
+	return ret;
+}
+
+
+asmlinkage int sys_vm86(struct pt_regs regs)
+{
+	struct kernel_vm86_struct info; /* declare this _on top_,
+					 * this avoids wasting of stack space.
+					 * This remains on the stack until we
+					 * return to 32 bit user space.
+					 */
+	struct task_struct *tsk;
+	int tmp, ret;
+	struct vm86plus_struct __user *v86;
+
+	tsk = current;
+	switch (regs.ebx) {
+		case VM86_REQUEST_IRQ:
+		case VM86_FREE_IRQ:
+		case VM86_GET_IRQ_BITS:
+		case VM86_GET_AND_RESET_IRQ:
+			ret = do_vm86_irq_handling(regs.ebx, (int)regs.ecx);
+			goto out;
+		case VM86_PLUS_INSTALL_CHECK:
+			/* NOTE: on old vm86 stuff this will return the error
+			   from verify_area(), because the subfunction is
+			   interpreted as (invalid) address to vm86_struct.
+			   So the installation check works.
+			 */
+			ret = 0;
+			goto out;
+	}
+
+	/* we come here only for functions VM86_ENTER, VM86_ENTER_NO_BYPASS */
+	ret = -EPERM;
+	if (tsk->thread.saved_esp0)
+		goto out;
+	v86 = (struct vm86plus_struct __user *)regs.ecx;
+	tmp  = copy_from_user(&info, v86, VM86_REGS_SIZE1);
+	tmp += copy_from_user(&info.regs.VM86_REGS_PART2, &v86->regs.VM86_REGS_PART2,
+		(long)&info.regs32 - (long)&info.regs.VM86_REGS_PART2);
+	ret = -EFAULT;
+	if (tmp)
+		goto out;
+	info.regs32 = &regs;
+	info.vm86plus.is_vm86pus = 1;
+	tsk->thread.vm86_info = (struct vm86_struct __user *)v86;
+	do_sys_vm86(&info, tsk);
+	ret = 0;	/* we never return here */
+out:
+	return ret;
+}
+
+
+static void do_sys_vm86(struct kernel_vm86_struct *info, struct task_struct *tsk)
+{
+	struct tss_struct *tss;
+/*
+ * make sure the vm86() system call doesn't try to do anything silly
+ */
+	info->regs.__null_ds = 0;
+	info->regs.__null_es = 0;
+
+/* we are clearing fs,gs later just before "jmp resume_userspace",
+ * because starting with Linux 2.1.x they aren't no longer saved/restored
+ */
+
+/*
+ * The eflags register is also special: we cannot trust that the user
+ * has set it up safely, so this makes sure interrupt etc flags are
+ * inherited from protected mode.
+ */
+ 	VEFLAGS = info->regs.eflags;
+	info->regs.eflags &= SAFE_MASK;
+	info->regs.eflags |= info->regs32->eflags & ~SAFE_MASK;
+	info->regs.eflags |= VM_MASK;
+
+	switch (info->cpu_type) {
+		case CPU_286:
+			tsk->thread.v86mask = 0;
+			break;
+		case CPU_386:
+			tsk->thread.v86mask = NT_MASK | IOPL_MASK;
+			break;
+		case CPU_486:
+			tsk->thread.v86mask = AC_MASK | NT_MASK | IOPL_MASK;
+			break;
+		default:
+			tsk->thread.v86mask = ID_MASK | AC_MASK | NT_MASK | IOPL_MASK;
+			break;
+	}
+
+/*
+ * Save old state, set default return value (%eax) to 0
+ */
+	info->regs32->eax = 0;
+	tsk->thread.saved_esp0 = tsk->thread.esp0;
+	asm volatile("movl %%fs,%0":"=m" (tsk->thread.saved_fs));
+	asm volatile("movl %%gs,%0":"=m" (tsk->thread.saved_gs));
+
+	tss = &per_cpu(init_tss, get_cpu());
+	tsk->thread.esp0 = (unsigned long) &info->VM86_TSS_ESP0;
+	if (cpu_has_sep)
+		tsk->thread.sysenter_cs = 0;
+	load_esp0(tss, &tsk->thread);
+	put_cpu();
+
+	tsk->thread.screen_bitmap = info->screen_bitmap;
+	if (info->flags & VM86_SCREEN_BITMAP)
+		mark_screen_rdonly(tsk);
+	__asm__ __volatile__(
+		"xorl %%eax,%%eax; movl %%eax,%%fs; movl %%eax,%%gs\n\t"
+		"movl %0,%%esp\n\t"
+		"movl %1,%%ebp\n\t"
+		"jmp resume_userspace"
+		: /* no outputs */
+		:"r" (&info->regs), "r" (tsk->thread_info) : "ax");
+	/* we never return here */
+}
+
+static inline void return_to_32bit(struct kernel_vm86_regs * regs16, int retval)
+{
+	struct pt_regs * regs32;
+
+	regs32 = save_v86_state(regs16);
+	regs32->eax = retval;
+	__asm__ __volatile__("movl %0,%%esp\n\t"
+		"movl %1,%%ebp\n\t"
+		"jmp resume_userspace"
+		: : "r" (regs32), "r" (current_thread_info()));
+}
+
+static inline void set_IF(struct kernel_vm86_regs * regs)
+{
+	VEFLAGS |= VIF_MASK;
+	if (VEFLAGS & VIP_MASK)
+		return_to_32bit(regs, VM86_STI);
+}
+
+static inline void clear_IF(struct kernel_vm86_regs * regs)
+{
+	VEFLAGS &= ~VIF_MASK;
+}
+
+static inline void clear_TF(struct kernel_vm86_regs * regs)
+{
+	regs->eflags &= ~TF_MASK;
+}
+
+static inline void clear_AC(struct kernel_vm86_regs * regs)
+{
+	regs->eflags &= ~AC_MASK;
+}
+
+/* It is correct to call set_IF(regs) from the set_vflags_*
+ * functions. However someone forgot to call clear_IF(regs)
+ * in the opposite case.
+ * After the command sequence CLI PUSHF STI POPF you should
+ * end up with interrups disabled, but you ended up with
+ * interrupts enabled.
+ *  ( I was testing my own changes, but the only bug I
+ *    could find was in a function I had not changed. )
+ * [KD]
+ */
+
+static inline void set_vflags_long(unsigned long eflags, struct kernel_vm86_regs * regs)
+{
+	set_flags(VEFLAGS, eflags, current->thread.v86mask);
+	set_flags(regs->eflags, eflags, SAFE_MASK);
+	if (eflags & IF_MASK)
+		set_IF(regs);
+	else
+		clear_IF(regs);
+}
+
+static inline void set_vflags_short(unsigned short flags, struct kernel_vm86_regs * regs)
+{
+	set_flags(VFLAGS, flags, current->thread.v86mask);
+	set_flags(regs->eflags, flags, SAFE_MASK);
+	if (flags & IF_MASK)
+		set_IF(regs);
+	else
+		clear_IF(regs);
+}
+
+static inline unsigned long get_vflags(struct kernel_vm86_regs * regs)
+{
+	unsigned long flags = regs->eflags & RETURN_MASK;
+
+	if (VEFLAGS & VIF_MASK)
+		flags |= IF_MASK;
+	flags |= IOPL_MASK;
+	return flags | (VEFLAGS & current->thread.v86mask);
+}
+
+static inline int is_revectored(int nr, struct revectored_struct * bitmap)
+{
+	__asm__ __volatile__("btl %2,%1\n\tsbbl %0,%0"
+		:"=r" (nr)
+		:"m" (*bitmap),"r" (nr));
+	return nr;
+}
+
+#define val_byte(val, n) (((__u8 *)&val)[n])
+
+#define pushb(base, ptr, val, err_label) \
+	do { \
+		__u8 __val = val; \
+		ptr--; \
+		if (put_user(__val, base + ptr) < 0) \
+			goto err_label; \
+	} while(0)
+
+#define pushw(base, ptr, val, err_label) \
+	do { \
+		__u16 __val = val; \
+		ptr--; \
+		if (put_user(val_byte(__val, 1), base + ptr) < 0) \
+			goto err_label; \
+		ptr--; \
+		if (put_user(val_byte(__val, 0), base + ptr) < 0) \
+			goto err_label; \
+	} while(0)
+
+#define pushl(base, ptr, val, err_label) \
+	do { \
+		__u32 __val = val; \
+		ptr--; \
+		if (put_user(val_byte(__val, 3), base + ptr) < 0) \
+			goto err_label; \
+		ptr--; \
+		if (put_user(val_byte(__val, 2), base + ptr) < 0) \
+			goto err_label; \
+		ptr--; \
+		if (put_user(val_byte(__val, 1), base + ptr) < 0) \
+			goto err_label; \
+		ptr--; \
+		if (put_user(val_byte(__val, 0), base + ptr) < 0) \
+			goto err_label; \
+	} while(0)
+
+#define popb(base, ptr, err_label) \
+	({ \
+		__u8 __res; \
+		if (get_user(__res, base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		__res; \
+	})
+
+#define popw(base, ptr, err_label) \
+	({ \
+		__u16 __res; \
+		if (get_user(val_byte(__res, 0), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		if (get_user(val_byte(__res, 1), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		__res; \
+	})
+
+#define popl(base, ptr, err_label) \
+	({ \
+		__u32 __res; \
+		if (get_user(val_byte(__res, 0), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		if (get_user(val_byte(__res, 1), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		if (get_user(val_byte(__res, 2), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		if (get_user(val_byte(__res, 3), base + ptr) < 0) \
+			goto err_label; \
+		ptr++; \
+		__res; \
+	})
+
+/* There are so many possible reasons for this function to return
+ * VM86_INTx, so adding another doesn't bother me. We can expect
+ * userspace programs to be able to handle it. (Getting a problem
+ * in userspace is always better than an Oops anyway.) [KD]
+ */
+static void do_int(struct kernel_vm86_regs *regs, int i,
+    unsigned char __user * ssp, unsigned short sp)
+{
+	unsigned long __user *intr_ptr;
+	unsigned long segoffs;
+
+	if (regs->cs == BIOSSEG)
+		goto cannot_handle;
+	if (is_revectored(i, &KVM86->int_revectored))
+		goto cannot_handle;
+	if (i==0x21 && is_revectored(AH(regs),&KVM86->int21_revectored))
+		goto cannot_handle;
+	intr_ptr = (unsigned long __user *) (i << 2);
+	if (get_user(segoffs, intr_ptr))
+		goto cannot_handle;
+	if ((segoffs >> 16) == BIOSSEG)
+		goto cannot_handle;
+	pushw(ssp, sp, get_vflags(regs), cannot_handle);
+	pushw(ssp, sp, regs->cs, cannot_handle);
+	pushw(ssp, sp, IP(regs), cannot_handle);
+	regs->cs = segoffs >> 16;
+	SP(regs) -= 6;
+	IP(regs) = segoffs & 0xffff;
+	clear_TF(regs);
+	clear_IF(regs);
+	clear_AC(regs);
+	return;
+
+cannot_handle:
+	return_to_32bit(regs, VM86_INTx + (i << 8));
+}
+
+int handle_vm86_trap(struct kernel_vm86_regs * regs, long error_code, int trapno)
+{
+	if (VMPI.is_vm86pus) {
+		if ( (trapno==3) || (trapno==1) )
+			return_to_32bit(regs, VM86_TRAP + (trapno << 8));
+		do_int(regs, trapno, (unsigned char __user *) (regs->ss << 4), SP(regs));
+		return 0;
+	}
+	if (trapno !=1)
+		return 1; /* we let this handle by the calling routine */
+	if (current->ptrace & PT_PTRACED) {
+		unsigned long flags;
+		spin_lock_irqsave(&current->sighand->siglock, flags);
+		sigdelset(&current->blocked, SIGTRAP);
+		recalc_sigpending();
+		spin_unlock_irqrestore(&current->sighand->siglock, flags);
+	}
+	send_sig(SIGTRAP, current, 1);
+	current->thread.trap_no = trapno;
+	current->thread.error_code = error_code;
+	return 0;
+}
+
+void handle_vm86_fault(struct kernel_vm86_regs * regs, long error_code)
+{
+	unsigned char opcode;
+	unsigned char __user *csp;
+	unsigned char __user *ssp;
+	unsigned short ip, sp;
+	int data32, pref_done;
+
+#define CHECK_IF_IN_TRAP \
+	if (VMPI.vm86dbg_active && VMPI.vm86dbg_TFpendig) \
+		newflags |= TF_MASK
+#define VM86_FAULT_RETURN do { \
+	if (VMPI.force_return_for_pic  && (VEFLAGS & (IF_MASK | VIF_MASK))) \
+		return_to_32bit(regs, VM86_PICRETURN); \
+	return; } while (0)
+
+	csp = (unsigned char __user *) (regs->cs << 4);
+	ssp = (unsigned char __user *) (regs->ss << 4);
+	sp = SP(regs);
+	ip = IP(regs);
+
+	data32 = 0;
+	pref_done = 0;
+	do {
+		switch (opcode = popb(csp, ip, simulate_sigsegv)) {
+			case 0x66:      /* 32-bit data */     data32=1; break;
+			case 0x67:      /* 32-bit address */  break;
+			case 0x2e:      /* CS */              break;
+			case 0x3e:      /* DS */              break;
+			case 0x26:      /* ES */              break;
+			case 0x36:      /* SS */              break;
+			case 0x65:      /* GS */              break;
+			case 0x64:      /* FS */              break;
+			case 0xf2:      /* repnz */       break;
+			case 0xf3:      /* rep */             break;
+			default: pref_done = 1;
+		}
+	} while (!pref_done);
+
+	switch (opcode) {
+
+	/* pushf */
+	case 0x9c:
+		if (data32) {
+			pushl(ssp, sp, get_vflags(regs), simulate_sigsegv);
+			SP(regs) -= 4;
+		} else {
+			pushw(ssp, sp, get_vflags(regs), simulate_sigsegv);
+			SP(regs) -= 2;
+		}
+		IP(regs) = ip;
+		VM86_FAULT_RETURN;
+
+	/* popf */
+	case 0x9d:
+		{
+		unsigned long newflags;
+		if (data32) {
+			newflags=popl(ssp, sp, simulate_sigsegv);
+			SP(regs) += 4;
+		} else {
+			newflags = popw(ssp, sp, simulate_sigsegv);
+			SP(regs) += 2;
+		}
+		IP(regs) = ip;
+		CHECK_IF_IN_TRAP;
+		if (data32) {
+			set_vflags_long(newflags, regs);
+		} else {
+			set_vflags_short(newflags, regs);
+		}
+		VM86_FAULT_RETURN;
+		}
+
+	/* int xx */
+	case 0xcd: {
+		int intno=popb(csp, ip, simulate_sigsegv);
+		IP(regs) = ip;
+		if (VMPI.vm86dbg_active) {
+			if ( (1 << (intno &7)) & VMPI.vm86dbg_intxxtab[intno >> 3] )
+				return_to_32bit(regs, VM86_INTx + (intno << 8));
+		}
+		do_int(regs, intno, ssp, sp);
+		return;
+	}
+
+	/* iret */
+	case 0xcf:
+		{
+		unsigned long newip;
+		unsigned long newcs;
+		unsigned long newflags;
+		if (data32) {
+			newip=popl(ssp, sp, simulate_sigsegv);
+			newcs=popl(ssp, sp, simulate_sigsegv);
+			newflags=popl(ssp, sp, simulate_sigsegv);
+			SP(regs) += 12;
+		} else {
+			newip = popw(ssp, sp, simulate_sigsegv);
+			newcs = popw(ssp, sp, simulate_sigsegv);
+			newflags = popw(ssp, sp, simulate_sigsegv);
+			SP(regs) += 6;
+		}
+		IP(regs) = newip;
+		regs->cs = newcs;
+		CHECK_IF_IN_TRAP;
+		if (data32) {
+			set_vflags_long(newflags, regs);
+		} else {
+			set_vflags_short(newflags, regs);
+		}
+		VM86_FAULT_RETURN;
+		}
+
+	/* cli */
+	case 0xfa:
+		IP(regs) = ip;
+		clear_IF(regs);
+		VM86_FAULT_RETURN;
+
+	/* sti */
+	/*
+	 * Damn. This is incorrect: the 'sti' instruction should actually
+	 * enable interrupts after the /next/ instruction. Not good.
+	 *
+	 * Probably needs some horsing around with the TF flag. Aiee..
+	 */
+	case 0xfb:
+		IP(regs) = ip;
+		set_IF(regs);
+		VM86_FAULT_RETURN;
+
+	default:
+		return_to_32bit(regs, VM86_UNKNOWN);
+	}
+
+	return;
+
+simulate_sigsegv:
+	/* FIXME: After a long discussion with Stas we finally
+	 *        agreed, that this is wrong. Here we should
+	 *        really send a SIGSEGV to the user program.
+	 *        But how do we create the correct context? We
+	 *        are inside a general protection fault handler
+	 *        and has just returned from a page fault handler.
+	 *        The correct context for the signal handler
+	 *        should be a mixture of the two, but how do we
+	 *        get the information? [KD]
+	 */
+	return_to_32bit(regs, VM86_UNKNOWN);
+}
+
+/* ---------------- vm86 special IRQ passing stuff ----------------- */
+
+#define VM86_IRQNAME		"vm86irq"
+
+static struct vm86_irqs {
+	struct task_struct *tsk;
+	int sig;
+} vm86_irqs[16];
+
+static spinlock_t irqbits_lock = SPIN_LOCK_UNLOCKED;
+static int irqbits;
+
+#define ALLOWED_SIGS ( 1 /* 0 = don't send a signal */ \
+	| (1 << SIGUSR1) | (1 << SIGUSR2) | (1 << SIGIO)  | (1 << SIGURG) \
+	| (1 << SIGUNUSED) )
+	
+static irqreturn_t irq_handler(int intno, void *dev_id, struct pt_regs * regs)
+{
+	int irq_bit;
+	unsigned long flags;
+
+	spin_lock_irqsave(&irqbits_lock, flags);	
+	irq_bit = 1 << intno;
+	if ((irqbits & irq_bit) || ! vm86_irqs[intno].tsk)
+		goto out;
+	irqbits |= irq_bit;
+	if (vm86_irqs[intno].sig)
+		send_sig(vm86_irqs[intno].sig, vm86_irqs[intno].tsk, 1);
+	/* else user will poll for IRQs */
+out:
+	spin_unlock_irqrestore(&irqbits_lock, flags);	
+	return IRQ_NONE;
+}
+
+static inline void free_vm86_irq(int irqnumber)
+{
+	unsigned long flags;
+
+	free_irq(irqnumber, NULL);
+	vm86_irqs[irqnumber].tsk = NULL;
+
+	spin_lock_irqsave(&irqbits_lock, flags);	
+	irqbits &= ~(1 << irqnumber);
+	spin_unlock_irqrestore(&irqbits_lock, flags);	
+}
+
+void release_x86_irqs(struct task_struct *task)
+{
+	int i;
+	for (i = FIRST_VM86_IRQ ; i <= LAST_VM86_IRQ; i++)
+	    if (vm86_irqs[i].tsk == task)
+		free_vm86_irq(i);
+}
+
+static inline int get_and_reset_irq(int irqnumber)
+{
+	int bit;
+	unsigned long flags;
+	
+	if (invalid_vm86_irq(irqnumber)) return 0;
+	if (vm86_irqs[irqnumber].tsk != current) return 0;
+	spin_lock_irqsave(&irqbits_lock, flags);	
+	bit = irqbits & (1 << irqnumber);
+	irqbits &= ~bit;
+	spin_unlock_irqrestore(&irqbits_lock, flags);	
+	if (!bit)
+		return 0;
+	enable_irq(irqnumber);
+	return 1;
+}
+
+
+static int do_vm86_irq_handling(int subfunction, int irqnumber)
+{
+	int ret;
+	switch (subfunction) {
+		case VM86_GET_AND_RESET_IRQ: {
+			return get_and_reset_irq(irqnumber);
+		}
+		case VM86_GET_IRQ_BITS: {
+			return irqbits;
+		}
+		case VM86_REQUEST_IRQ: {
+			int sig = irqnumber >> 8;
+			int irq = irqnumber & 255;
+			if (!capable(CAP_SYS_ADMIN)) return -EPERM;
+			if (!((1 << sig) & ALLOWED_SIGS)) return -EPERM;
+			if (invalid_vm86_irq(irq)) return -EPERM;
+			if (vm86_irqs[irq].tsk) return -EPERM;
+			ret = request_irq(irq, &irq_handler, 0, VM86_IRQNAME, NULL);
+			if (ret) return ret;
+			vm86_irqs[irq].sig = sig;
+			vm86_irqs[irq].tsk = current;
+			return irq;
+		}
+		case  VM86_FREE_IRQ: {
+			if (invalid_vm86_irq(irqnumber)) return -EPERM;
+			if (!vm86_irqs[irqnumber].tsk) return 0;
+			if (vm86_irqs[irqnumber].tsk != current) return -EPERM;
+			free_vm86_irq(irqnumber);
+			return 0;
+		}
+	}
+	return -EINVAL;
+}
+
diff -Naur linux-2.6.9.src/arch/i386/kernel/vmlinux.lds.S linux-2.6.9/arch/i386/kernel/vmlinux.lds.S
--- linux-2.6.9.src/arch/i386/kernel/vmlinux.lds.S	2004-10-18 23:53:43.000000000 +0200
+++ linux-2.6.9/arch/i386/kernel/vmlinux.lds.S	2007-07-03 11:07:40.000000000 +0200
@@ -38,6 +38,10 @@
 	CONSTRUCTORS
 	}
 
+  __burn_prof_counters_start = .;
+  .burn_prof_counters : { *(.burn_prof_counters) }
+  __burn_prof_counters_end = .;
+
   . = ALIGN(4096);
   __nosave_begin = .;
   .data_nosave : { *(.data.nosave) }
diff -Naur linux-2.6.9.src/arch/i386/lib/usercopy.c linux-2.6.9/arch/i386/lib/usercopy.c
--- linux-2.6.9.src/arch/i386/lib/usercopy.c	2004-10-18 23:53:06.000000000 +0200
+++ linux-2.6.9/arch/i386/lib/usercopy.c	2007-07-03 11:07:40.000000000 +0200
@@ -13,6 +13,12 @@
 #include <asm/uaccess.h>
 #include <asm/mmx.h>
 
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+extern unsigned long (*afterburn_clear_user_hook)(void *to, unsigned long n);
+extern unsigned long (*afterburn_strnlen_user_hook)(const char *s, unsigned long n);
+extern unsigned long (*afterburn_strncpy_from_user_hook)(char *dst, const char *src, unsigned long n, unsigned long *success);
+#endif
+
 static inline int __movsl_is_ok(unsigned long a1, unsigned long a2, unsigned long n)
 {
 #ifdef CONFIG_X86_INTEL_USERCOPY
@@ -81,6 +87,15 @@
 __strncpy_from_user(char *dst, const char __user *src, long count)
 {
 	long res;
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_strncpy_from_user_hook ) {
+		unsigned long success;
+		res = afterburn_strncpy_from_user_hook( dst, src, count, &success );
+		if( !success )
+			return -EFAULT;
+		return res;
+	}
+#endif
 	__do_strncpy_from_user(dst, src, count, res);
 	return res;
 }
@@ -107,8 +122,18 @@
 strncpy_from_user(char *dst, const char __user *src, long count)
 {
 	long res = -EFAULT;
-	if (access_ok(VERIFY_READ, src, 1))
+	if (access_ok(VERIFY_READ, src, 1)) {
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+		if( afterburn_strncpy_from_user_hook ) {
+			unsigned long success;
+			res = afterburn_strncpy_from_user_hook( dst, src, count, &success );
+			if( !success )
+				return -EFAULT;
+			return res;
+		}
+#endif
 		__do_strncpy_from_user(dst, src, count, res);
+	}
 	return res;
 }
 
@@ -152,6 +177,10 @@
 unsigned long
 clear_user(void __user *to, unsigned long n)
 {
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_clear_user_hook )
+		return n - afterburn_clear_user_hook( to, n );
+#endif
 	might_sleep();
 	if (access_ok(VERIFY_WRITE, to, n))
 		__do_clear_user(to, n);
@@ -172,6 +201,10 @@
 unsigned long
 __clear_user(void __user *to, unsigned long n)
 {
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_clear_user_hook )
+		return n - afterburn_clear_user_hook( to, n );
+#endif
 	__do_clear_user(to, n);
 	return n;
 }
@@ -194,6 +227,11 @@
 
 	might_sleep();
 
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_strnlen_user_hook )
+		return afterburn_strnlen_user_hook( s, n );
+#endif
+
 	__asm__ __volatile__(
 		"	testl %0, %0\n"
 		"	jz 3f\n"
@@ -514,6 +552,11 @@
 
 unsigned long __copy_to_user_ll(void __user *to, const void *from, unsigned long n)
 {
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_copy_to_user_hook )
+		return n - afterburn_copy_to_user_hook( to, from, n );
+#endif
+
 #ifndef CONFIG_X86_WP_WORKS_OK
 	if (unlikely(boot_cpu_data.wp_works_ok == 0) &&
 			((unsigned long )to) < TASK_SIZE) {
@@ -573,6 +616,10 @@
 unsigned long
 __copy_from_user_ll(void *to, const void __user *from, unsigned long n)
 {
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+	if( afterburn_copy_from_user_hook )
+		return n - afterburn_copy_from_user_hook( to, from, n );
+#endif
 	if (movsl_is_ok(to, from, n))
 		__copy_user_zeroing(to, from, n);
 	else
diff -Naur linux-2.6.9.src/arch/i386/mm/init.c linux-2.6.9/arch/i386/mm/init.c
--- linux-2.6.9.src/arch/i386/mm/init.c	2004-10-18 23:54:54.000000000 +0200
+++ linux-2.6.9/arch/i386/mm/init.c	2007-07-03 11:07:40.000000000 +0200
@@ -333,7 +333,7 @@
 	 * created - mappings will be set by set_fixmap():
 	 */
 	vaddr = __fix_to_virt(__end_of_fixed_addresses - 1) & PMD_MASK;
-	page_table_range_init(vaddr, 0, pgd_base);
+	page_table_range_init(vaddr, vaddr + PMD_SIZE, pgd_base);
 
 	permanent_kmaps_init(pgd_base);
 
@@ -345,7 +345,7 @@
 	 * All user-space mappings are explicitly cleared after
 	 * SMP startup.
 	 */
-	pgd_base[0] = pgd_base[USER_PTRS_PER_PGD];
+	pgd_base[0] = pgd_base[USER_PGD_PTRS];
 #endif
 }
 
@@ -379,7 +379,7 @@
 	 * Note that "pgd_clear()" doesn't do it for
 	 * us, because pgd_clear() is a no-op on i386.
 	 */
-	for (i = 0; i < USER_PTRS_PER_PGD; i++)
+	for (i = 0; i < USER_PGD_PTRS; i++)
 #ifdef CONFIG_X86_PAE
 		set_pgd(swapper_pg_dir+i, __pgd(1 + __pa(empty_zero_page)));
 #else
diff -Naur linux-2.6.9.src/arch/x86_64/kernel/process.c linux-2.6.9/arch/x86_64/kernel/process.c
--- linux-2.6.9.src/arch/x86_64/kernel/process.c	2004-10-18 23:54:31.000000000 +0200
+++ linux-2.6.9/arch/x86_64/kernel/process.c	2007-07-03 11:07:40.000000000 +0200
@@ -370,10 +370,10 @@
 	p->thread.fs = me->thread.fs;
 	p->thread.gs = me->thread.gs;
 
-	asm("movl %%gs,%0" : "=m" (p->thread.gsindex));
-	asm("movl %%fs,%0" : "=m" (p->thread.fsindex));
-	asm("movl %%es,%0" : "=m" (p->thread.es));
-	asm("movl %%ds,%0" : "=m" (p->thread.ds));
+	asm("mov %%gs,%0" : "=m" (p->thread.gsindex));
+	asm("mov %%fs,%0" : "=m" (p->thread.fsindex));
+	asm("mov %%es,%0" : "=m" (p->thread.es));
+	asm("mov %%ds,%0" : "=m" (p->thread.ds));
 
 	if (unlikely(me->thread.io_bitmap_ptr != NULL)) { 
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
@@ -436,11 +436,11 @@
 	 * Switch DS and ES.
 	 * This won't pick up thread selector changes, but I guess that is ok.
 	 */
-	asm volatile("movl %%es,%0" : "=m" (prev->es)); 
+	asm volatile("mov %%es,%0" : "=m" (prev->es)); 
 	if (unlikely(next->es | prev->es))
 		loadsegment(es, next->es); 
 	
-	asm volatile ("movl %%ds,%0" : "=m" (prev->ds)); 
+	asm volatile ("mov %%ds,%0" : "=m" (prev->ds)); 
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
@@ -451,7 +451,7 @@
 	 */
 	{ 
 		unsigned fsindex;
-		asm volatile("movl %%fs,%0" : "=g" (fsindex)); 
+		asm volatile("movl %%fs,%0" : "=r" (fsindex)); 
 		/* segment register != 0 always requires a reload. 
 		   also reload when it has changed. 
 		   when prev process used 64bit base always reload
@@ -472,7 +472,7 @@
 	}
 	{ 
 		unsigned gsindex;
-		asm volatile("movl %%gs,%0" : "=g" (gsindex)); 
+		asm volatile("movl %%gs,%0" : "=r" (gsindex)); 
 		if (unlikely(gsindex | next->gsindex | prev->gs)) {
 			load_gs_index(next->gsindex);
 			if (gsindex)
diff -Naur linux-2.6.9.src/arch/x86_64/kernel/process.c.orig linux-2.6.9/arch/x86_64/kernel/process.c.orig
--- linux-2.6.9.src/arch/x86_64/kernel/process.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/arch/x86_64/kernel/process.c.orig	2007-07-03 11:07:40.000000000 +0200
@@ -0,0 +1,722 @@
+/*
+ *  linux/arch/x86-64/kernel/process.c
+ *
+ *  Copyright (C) 1995  Linus Torvalds
+ *
+ *  Pentium III FXSR, SSE support
+ *	Gareth Hughes <gareth@valinux.com>, May 2000
+ * 
+ *  X86-64 port
+ *	Andi Kleen.
+ * 
+ *  $Id: process.c,v 1.38 2002/01/15 10:08:03 ak Exp $
+ */
+
+/*
+ * This file handles the architecture-dependent parts of process handling..
+ */
+
+#include <stdarg.h>
+
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/elfcore.h>
+#include <linux/smp.h>
+#include <linux/slab.h>
+#include <linux/user.h>
+#include <linux/module.h>
+#include <linux/a.out.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/ptrace.h>
+#include <linux/version.h>
+
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/processor.h>
+#include <asm/i387.h>
+#include <asm/mmu_context.h>
+#include <asm/pda.h>
+#include <asm/prctl.h>
+#include <asm/kdebug.h>
+#include <asm/desc.h>
+#include <asm/proto.h>
+#include <asm/ia32.h>
+
+asmlinkage extern void ret_from_fork(void);
+
+unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
+
+atomic_t hlt_counter = ATOMIC_INIT(0);
+
+/*
+ * Powermanagement idle function, if any..
+ */
+void (*pm_idle)(void);
+
+void disable_hlt(void)
+{
+	atomic_inc(&hlt_counter);
+}
+
+EXPORT_SYMBOL(disable_hlt);
+
+void enable_hlt(void)
+{
+	atomic_dec(&hlt_counter);
+}
+
+EXPORT_SYMBOL(enable_hlt);
+
+/*
+ * We use this if we don't have any better
+ * idle routine..
+ */
+void default_idle(void)
+{
+	if (!atomic_read(&hlt_counter)) {
+		local_irq_disable();
+		if (!need_resched())
+			safe_halt();
+		else
+			local_irq_enable();
+	}
+}
+
+/*
+ * On SMP it's slightly faster (but much more power-consuming!)
+ * to poll the ->need_resched flag instead of waiting for the
+ * cross-CPU IPI to arrive. Use this option with caution.
+ */
+static void poll_idle (void)
+{
+	int oldval;
+
+	local_irq_enable();
+
+	/*
+	 * Deal with another CPU just having chosen a thread to
+	 * run here:
+	 */
+	oldval = test_and_clear_thread_flag(TIF_NEED_RESCHED);
+
+	if (!oldval) {
+		set_thread_flag(TIF_POLLING_NRFLAG); 
+		asm volatile(
+			"2:"
+			"testl %0,%1;"
+			"rep; nop;"
+			"je 2b;"
+			: :
+			"i" (_TIF_NEED_RESCHED), 
+			"m" (current_thread_info()->flags));
+	} else {
+		set_need_resched();
+	}
+}
+
+/*
+ * The idle thread. There's no useful work to be
+ * done, so just try to conserve power and have a
+ * low exit latency (ie sit in a loop waiting for
+ * somebody to say that they'd like to reschedule)
+ */
+void cpu_idle (void)
+{
+	/* endless idle loop with no priority at all */
+	while (1) {
+		while (!need_resched()) {
+			void (*idle)(void);
+			/*
+			 * Mark this as an RCU critical section so that
+			 * synchronize_kernel() in the unload path waits
+			 * for our completion.
+			 */
+			rcu_read_lock();
+			idle = pm_idle;
+			if (!idle)
+				idle = default_idle;
+			idle();
+			rcu_read_unlock();
+		}
+		schedule();
+	}
+}
+
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ */
+static void mwait_idle(void)
+{
+	local_irq_enable();
+
+	if (!need_resched()) {
+		set_thread_flag(TIF_POLLING_NRFLAG);
+		do {
+			__monitor((void *)&current_thread_info()->flags, 0, 0);
+			if (need_resched())
+				break;
+			__mwait(0, 0);
+		} while (!need_resched());
+		clear_thread_flag(TIF_POLLING_NRFLAG);
+	}
+}
+
+void __init select_idle_routine(const struct cpuinfo_x86 *c)
+{
+	static int printed;
+	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+		/*
+		 * Skip, if setup has overridden idle.
+		 * One CPU supports mwait => All CPUs supports mwait
+		 */
+		if (!pm_idle) {
+			if (!printed) {
+				printk("using mwait in idle threads.\n");
+				printed = 1;
+			}
+			pm_idle = mwait_idle;
+		}
+	}
+}
+
+static int __init idle_setup (char *str)
+{
+	if (!strncmp(str, "poll", 4)) {
+		printk("using polling idle threads.\n");
+		pm_idle = poll_idle;
+	}
+
+	return 1;
+}
+
+__setup("idle=", idle_setup);
+
+/* Prints also some state that isn't saved in the pt_regs */ 
+void __show_regs(struct pt_regs * regs)
+{
+	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
+	unsigned int fsindex,gsindex;
+	unsigned int ds,cs,es; 
+
+	printk("\n");
+	print_modules();
+	printk("Pid: %d, comm: %.20s %s %s\n", 
+	       current->pid, current->comm, print_tainted(), UTS_RELEASE);
+	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->rip);
+	printk_address(regs->rip); 
+	printk("\nRSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->rsp, regs->eflags);
+	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
+	       regs->rax, regs->rbx, regs->rcx);
+	printk("RDX: %016lx RSI: %016lx RDI: %016lx\n",
+	       regs->rdx, regs->rsi, regs->rdi); 
+	printk("RBP: %016lx R08: %016lx R09: %016lx\n",
+	       regs->rbp, regs->r8, regs->r9); 
+	printk("R10: %016lx R11: %016lx R12: %016lx\n",
+	       regs->r10, regs->r11, regs->r12); 
+	printk("R13: %016lx R14: %016lx R15: %016lx\n",
+	       regs->r13, regs->r14, regs->r15); 
+
+	asm("movl %%ds,%0" : "=r" (ds)); 
+	asm("movl %%cs,%0" : "=r" (cs)); 
+	asm("movl %%es,%0" : "=r" (es)); 
+	asm("movl %%fs,%0" : "=r" (fsindex));
+	asm("movl %%gs,%0" : "=r" (gsindex));
+
+	rdmsrl(MSR_FS_BASE, fs);
+	rdmsrl(MSR_GS_BASE, gs); 
+	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs); 
+
+	asm("movq %%cr0, %0": "=r" (cr0));
+	asm("movq %%cr2, %0": "=r" (cr2));
+	asm("movq %%cr3, %0": "=r" (cr3));
+	asm("movq %%cr4, %0": "=r" (cr4));
+
+	printk("FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n", 
+	       fs,fsindex,gs,gsindex,shadowgs); 
+	printk("CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds, es, cr0); 
+	printk("CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3, cr4);
+}
+
+void show_regs(struct pt_regs *regs)
+{
+	__show_regs(regs);
+	show_trace(&regs->rsp);
+}
+
+/*
+ * Free current thread data structures etc..
+ */
+void exit_thread(void)
+{
+	struct task_struct *me = current;
+	struct thread_struct *t = &me->thread;
+	if (me->thread.io_bitmap_ptr) { 
+		struct tss_struct *tss = &per_cpu(init_tss, get_cpu());
+
+		kfree(t->io_bitmap_ptr);
+		t->io_bitmap_ptr = NULL;
+		/*
+		 * Careful, clear this in the TSS too:
+		 */
+		memset(tss->io_bitmap, 0xff, t->io_bitmap_max);
+		t->io_bitmap_max = 0;
+		put_cpu();
+	}
+}
+
+void flush_thread(void)
+{
+	struct task_struct *tsk = current;
+	struct thread_info *t = current_thread_info();
+
+	if (t->flags & _TIF_ABI_PENDING)
+		t->flags ^= (_TIF_ABI_PENDING | _TIF_IA32);
+
+	tsk->thread.debugreg0 = 0;
+	tsk->thread.debugreg1 = 0;
+	tsk->thread.debugreg2 = 0;
+	tsk->thread.debugreg3 = 0;
+	tsk->thread.debugreg6 = 0;
+	tsk->thread.debugreg7 = 0;
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
+	/*
+	 * Forget coprocessor state..
+	 */
+	clear_fpu(tsk);
+	tsk->used_math = 0;
+}
+
+void release_thread(struct task_struct *dead_task)
+{
+	if (dead_task->mm) {
+		if (dead_task->mm->context.size) {
+			printk("WARNING: dead process %8s still has LDT? <%p/%d>\n",
+					dead_task->comm,
+					dead_task->mm->context.ldt,
+					dead_task->mm->context.size);
+			BUG();
+		}
+	}
+}
+
+static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
+{
+	struct user_desc ud = { 
+		.base_addr = addr,
+		.limit = 0xfffff,
+		.seg_32bit = 1,
+		.limit_in_pages = 1,
+		.useable = 1,
+	};
+	struct n_desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	desc->a = LDT_entry_a(&ud); 
+	desc->b = LDT_entry_b(&ud); 
+}
+
+static inline u32 read_32bit_tls(struct task_struct *t, int tls)
+{
+	struct desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	return desc->base0 | 
+		(((u32)desc->base1) << 16) | 
+		(((u32)desc->base2) << 24);
+}
+
+/*
+ * This gets called before we allocate a new thread and copy
+ * the current task into it.
+ */
+void prepare_to_copy(struct task_struct *tsk)
+{
+	unlazy_fpu(tsk);
+}
+
+int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp, 
+		unsigned long unused,
+	struct task_struct * p, struct pt_regs * regs)
+{
+	int err;
+	struct pt_regs * childregs;
+	struct task_struct *me = current;
+
+	childregs = ((struct pt_regs *) (THREAD_SIZE + (unsigned long) p->thread_info)) - 1;
+
+	*childregs = *regs;
+
+	childregs->rax = 0;
+	childregs->rsp = rsp;
+	if (rsp == ~0UL) {
+		childregs->rsp = (unsigned long)childregs;
+	}
+	p->set_child_tid = p->clear_child_tid = NULL;
+
+	p->thread.rsp = (unsigned long) childregs;
+	p->thread.rsp0 = (unsigned long) (childregs+1);
+	p->thread.userrsp = me->thread.userrsp; 
+
+	set_ti_thread_flag(p->thread_info, TIF_FORK);
+
+	p->thread.fs = me->thread.fs;
+	p->thread.gs = me->thread.gs;
+
+	asm("movl %%gs,%0" : "=m" (p->thread.gsindex));
+	asm("movl %%fs,%0" : "=m" (p->thread.fsindex));
+	asm("movl %%es,%0" : "=m" (p->thread.es));
+	asm("movl %%ds,%0" : "=m" (p->thread.ds));
+
+	if (unlikely(me->thread.io_bitmap_ptr != NULL)) { 
+		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
+		if (!p->thread.io_bitmap_ptr) {
+			p->thread.io_bitmap_max = 0;
+			return -ENOMEM;
+		}
+		memcpy(p->thread.io_bitmap_ptr, me->thread.io_bitmap_ptr, IO_BITMAP_BYTES);
+	} 
+
+	/*
+	 * Set a new TLS for the child thread?
+	 */
+	if (clone_flags & CLONE_SETTLS) {
+#ifdef CONFIG_IA32_EMULATION
+		if (test_thread_flag(TIF_IA32))
+			err = ia32_child_tls(p, childregs); 
+		else 			
+#endif	 
+			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8); 
+		if (err) 
+			goto out;
+	}
+	err = 0;
+out:
+	if (err && p->thread.io_bitmap_ptr) {
+		kfree(p->thread.io_bitmap_ptr);
+		p->thread.io_bitmap_max = 0;
+	}
+	return err;
+}
+
+/*
+ * This special macro can be used to load a debugging register
+ */
+#define loaddebug(thread,r) set_debug(thread->debugreg ## r, r)
+
+/*
+ *	switch_to(x,y) should switch tasks from x to y.
+ *
+ * This could still be optimized: 
+ * - fold all the options into a flag word and test it with a single test.
+ * - could test fs/gs bitsliced
+ */
+struct task_struct *__switch_to(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	struct thread_struct *prev = &prev_p->thread,
+				 *next = &next_p->thread;
+	int cpu = smp_processor_id();  
+	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+
+	unlazy_fpu(prev_p);
+
+	/*
+	 * Reload esp0, LDT and the page table pointer:
+	 */
+	tss->rsp0 = next->rsp0;
+
+	/* 
+	 * Switch DS and ES.
+	 * This won't pick up thread selector changes, but I guess that is ok.
+	 */
+	asm volatile("movl %%es,%0" : "=m" (prev->es)); 
+	if (unlikely(next->es | prev->es))
+		loadsegment(es, next->es); 
+	
+	asm volatile ("movl %%ds,%0" : "=m" (prev->ds)); 
+	if (unlikely(next->ds | prev->ds))
+		loadsegment(ds, next->ds);
+
+	load_TLS(next, cpu);
+
+	/* 
+	 * Switch FS and GS.
+	 */
+	{ 
+		unsigned fsindex;
+		asm volatile("movl %%fs,%0" : "=g" (fsindex)); 
+		/* segment register != 0 always requires a reload. 
+		   also reload when it has changed. 
+		   when prev process used 64bit base always reload
+		   to avoid an information leak. */
+		if (unlikely(fsindex | next->fsindex | prev->fs)) {
+			loadsegment(fs, next->fsindex);
+			/* check if the user used a selector != 0
+	                 * if yes clear 64bit base, since overloaded base
+                         * is always mapped to the Null selector
+                         */
+			if (fsindex)
+			prev->fs = 0;				
+		}
+		/* when next process has a 64bit base use it */
+		if (next->fs) 
+			wrmsrl(MSR_FS_BASE, next->fs); 
+		prev->fsindex = fsindex;
+	}
+	{ 
+		unsigned gsindex;
+		asm volatile("movl %%gs,%0" : "=g" (gsindex)); 
+		if (unlikely(gsindex | next->gsindex | prev->gs)) {
+			load_gs_index(next->gsindex);
+			if (gsindex)
+			prev->gs = 0;				
+		}
+		if (next->gs)
+			wrmsrl(MSR_KERNEL_GS_BASE, next->gs); 
+		prev->gsindex = gsindex;
+	}
+
+	/* 
+	 * Switch the PDA context.
+	 */
+	prev->userrsp = read_pda(oldrsp); 
+	write_pda(oldrsp, next->userrsp); 
+	write_pda(pcurrent, next_p); 
+	write_pda(kernelstack, (unsigned long)next_p->thread_info + THREAD_SIZE - PDA_STACKOFFSET);
+
+	/*
+	 * Now maybe reload the debug registers
+	 */
+	if (unlikely(next->debugreg7)) {
+		loaddebug(next, 0);
+		loaddebug(next, 1);
+		loaddebug(next, 2);
+		loaddebug(next, 3);
+		/* no 4 and 5 */
+		loaddebug(next, 6);
+		loaddebug(next, 7);
+	}
+
+
+	/* 
+	 * Handle the IO bitmap 
+	 */ 
+	if (unlikely(prev->io_bitmap_ptr || next->io_bitmap_ptr)) {
+		if (next->io_bitmap_ptr)
+			/*
+			 * Copy the relevant range of the IO bitmap.
+			 * Normally this is 128 bytes or less:
+ 			 */
+			memcpy(tss->io_bitmap, next->io_bitmap_ptr,
+				max(prev->io_bitmap_max, next->io_bitmap_max));
+		else {
+			/*
+			 * Clear any possible leftover bits:
+			 */
+			memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+		}
+	}
+
+	return prev_p;
+}
+
+/*
+ * sys_execve() executes a new program.
+ */
+asmlinkage 
+long sys_execve(char __user *name, char __user * __user *argv,
+		char __user * __user *envp, struct pt_regs regs)
+{
+	long error;
+	char * filename;
+
+	filename = getname(name);
+	error = PTR_ERR(filename);
+	if (IS_ERR(filename)) 
+		return error;
+	error = do_execve(filename, argv, envp, &regs); 
+	if (error == 0)
+		current->ptrace &= ~PT_DTRACE;
+	putname(filename);
+	return error;
+}
+
+void set_personality_64bit(void)
+{
+	/* inherit personality from parent */
+
+	/* Make sure to be in 64bit mode */
+	clear_thread_flag(TIF_IA32); 
+}
+
+asmlinkage long sys_fork(struct pt_regs *regs)
+{
+	return do_fork(SIGCHLD, regs->rsp, regs, 0, NULL, NULL);
+}
+
+asmlinkage long sys_clone(unsigned long clone_flags, unsigned long newsp, void __user *parent_tid, void __user *child_tid, struct pt_regs *regs)
+{
+	if (!newsp)
+		newsp = regs->rsp;
+	return do_fork(clone_flags, newsp, regs, 0, parent_tid, child_tid);
+}
+
+/*
+ * This is trivial, and on the face of it looks like it
+ * could equally well be done in user mode.
+ *
+ * Not so, for quite unobvious reasons - register pressure.
+ * In user mode vfork() cannot have a stack frame, and if
+ * done by calling the "clone()" system call directly, you
+ * do not have enough call-clobbered registers to hold all
+ * the information you need.
+ */
+asmlinkage long sys_vfork(struct pt_regs *regs)
+{
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->rsp, regs, 0,
+		    NULL, NULL);
+}
+
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long stack;
+	u64 fp,rip;
+	int count = 0;
+
+	if (!p || p == current || p->state==TASK_RUNNING)
+		return 0; 
+	stack = (unsigned long)p->thread_info; 
+	if (p->thread.rsp < stack || p->thread.rsp > stack+THREAD_SIZE)
+		return 0;
+	fp = *(u64 *)(p->thread.rsp);
+	do { 
+		if (fp < (unsigned long)stack || fp > (unsigned long)stack+THREAD_SIZE)
+			return 0; 
+		rip = *(u64 *)(fp+8); 
+		if (!in_sched_functions(rip))
+			return rip; 
+		fp = *(u64 *)fp; 
+	} while (count++ < 16); 
+	return 0;
+}
+
+long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
+{ 
+	int ret = 0; 
+	int doit = task == current;
+	int cpu;
+
+	switch (code) { 
+	case ARCH_SET_GS:
+		if (addr >= TASK_SIZE) 
+			return -EPERM; 
+		cpu = get_cpu();
+		/* handle small bases via the GDT because that's faster to 
+		   switch. */
+		if (addr <= 0xffffffff) {  
+			set_32bit_tls(task, GS_TLS, addr); 
+			if (doit) { 
+				load_TLS(&task->thread, cpu);
+				load_gs_index(GS_TLS_SEL); 
+			}
+			task->thread.gsindex = GS_TLS_SEL; 
+			task->thread.gs = 0;
+		} else { 
+			task->thread.gsindex = 0;
+			task->thread.gs = addr;
+			if (doit) {
+		load_gs_index(0);
+		ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr); 
+			} 
+		}
+		put_cpu();
+		break;
+	case ARCH_SET_FS:
+		/* Not strictly needed for fs, but do it for symmetry
+		   with gs */
+		if (addr >= TASK_SIZE)
+			return -EPERM; 
+		cpu = get_cpu();
+		/* handle small bases via the GDT because that's faster to 
+		   switch. */
+		if (addr <= 0xffffffff) { 
+			set_32bit_tls(task, FS_TLS, addr);
+			if (doit) { 
+				load_TLS(&task->thread, cpu); 
+				asm volatile("movl %0,%%fs" :: "r" (FS_TLS_SEL));
+			}
+			task->thread.fsindex = FS_TLS_SEL;
+			task->thread.fs = 0;
+		} else { 
+			task->thread.fsindex = 0;
+			task->thread.fs = addr;
+			if (doit) {
+				/* set the selector to 0 to not confuse
+				   __switch_to */
+		asm volatile("movl %0,%%fs" :: "r" (0));
+		ret = checking_wrmsrl(MSR_FS_BASE, addr); 
+			}
+		}
+		put_cpu();
+		break;
+	case ARCH_GET_FS: { 
+		unsigned long base; 
+		if (task->thread.fsindex == FS_TLS_SEL)
+			base = read_32bit_tls(task, FS_TLS);
+		else if (doit) {
+			rdmsrl(MSR_FS_BASE, base);
+		} else
+			base = task->thread.fs;
+		ret = put_user(base, (unsigned long __user *)addr); 
+		break; 
+	}
+	case ARCH_GET_GS: { 
+		unsigned long base;
+		if (task->thread.gsindex == GS_TLS_SEL)
+			base = read_32bit_tls(task, GS_TLS);
+		else if (doit) {
+			rdmsrl(MSR_KERNEL_GS_BASE, base);
+		} else
+			base = task->thread.gs;
+		ret = put_user(base, (unsigned long __user *)addr); 
+		break;
+	}
+
+	default:
+		ret = -EINVAL;
+		break;
+	} 
+
+	return ret;	
+} 
+
+long sys_arch_prctl(int code, unsigned long addr)
+{
+	return do_arch_prctl(current, code, addr);
+} 
+
+/* 
+ * Capture the user space registers if the task is not running (in user space)
+ */
+int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
+{
+	struct pt_regs *pp, ptregs;
+
+	pp = (struct pt_regs *)(tsk->thread.rsp0);
+	--pp; 
+
+	ptregs = *pp; 
+	ptregs.cs &= 0xffff;
+	ptregs.ss &= 0xffff;
+
+	elf_core_copy_regs(regs, &ptregs);
+ 
+	return 1;
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_80003es2lan.c linux-2.6.9/drivers/net/e1000/e1000_80003es2lan.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_80003es2lan.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_80003es2lan.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1309 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_80003es2lan
+ */
+
+#include "e1000_api.h"
+#include "e1000_80003es2lan.h"
+
+void e1000_init_function_pointers_80003es2lan(struct e1000_hw *hw);
+
+static s32  e1000_init_phy_params_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_init_nvm_params_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_init_mac_params_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_acquire_phy_80003es2lan(struct e1000_hw *hw);
+static void e1000_release_phy_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_acquire_nvm_80003es2lan(struct e1000_hw *hw);
+static void e1000_release_nvm_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_read_phy_reg_gg82563_80003es2lan(struct e1000_hw *hw,
+                                                   u32 offset,
+                                                   u16 *data);
+static s32  e1000_write_phy_reg_gg82563_80003es2lan(struct e1000_hw *hw,
+                                                    u32 offset,
+                                                    u16 data);
+static s32  e1000_write_nvm_80003es2lan(struct e1000_hw *hw, u16 offset,
+                                        u16 words, u16 *data);
+static s32  e1000_get_cfg_done_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_phy_force_speed_duplex_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_get_cable_length_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_get_link_up_info_80003es2lan(struct e1000_hw *hw, u16 *speed,
+                                               u16 *duplex);
+static s32  e1000_reset_hw_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_init_hw_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_setup_copper_link_80003es2lan(struct e1000_hw *hw);
+static void e1000_clear_hw_cntrs_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_acquire_swfw_sync_80003es2lan(struct e1000_hw *hw, u16 mask);
+static s32  e1000_cfg_kmrn_10_100_80003es2lan(struct e1000_hw *hw, u16 duplex);
+static s32  e1000_cfg_kmrn_1000_80003es2lan(struct e1000_hw *hw);
+static s32  e1000_copper_link_setup_gg82563_80003es2lan(struct e1000_hw *hw);
+static void e1000_initialize_hw_bits_80003es2lan(struct e1000_hw *hw);
+static void e1000_release_swfw_sync_80003es2lan(struct e1000_hw *hw, u16 mask);
+
+/* A table for the GG82563 cable length where the range is defined
+ * with a lower bound at "index" and the upper bound at
+ * "index + 5".
+ */
+static const u16 e1000_gg82563_cable_length_table[] =
+         { 0, 60, 115, 150, 150, 60, 115, 150, 180, 180, 0xFF };
+#define GG82563_CABLE_LENGTH_TABLE_SIZE \
+                (sizeof(e1000_gg82563_cable_length_table) / \
+                 sizeof(e1000_gg82563_cable_length_table[0]))
+
+/**
+ *  e1000_init_phy_params_80003es2lan - Init ESB2 PHY func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_80003es2lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_phy_params_80003es2lan");
+
+	if (hw->media_type != e1000_media_type_copper) {
+		phy->type        = e1000_phy_none;
+		goto out;
+	}
+
+	phy->addr                = 1;
+	phy->autoneg_mask        = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+	phy->reset_delay_us      = 100;
+	phy->type                = e1000_phy_gg82563;
+
+	func->acquire_phy        = e1000_acquire_phy_80003es2lan;
+	func->check_polarity     = e1000_check_polarity_m88;
+	func->check_reset_block  = e1000_check_reset_block_generic;
+	func->commit_phy         = e1000_phy_sw_reset_generic;
+	func->get_cfg_done       = e1000_get_cfg_done_80003es2lan;
+	func->get_phy_info       = e1000_get_phy_info_m88;
+	func->release_phy        = e1000_release_phy_80003es2lan;
+	func->reset_phy          = e1000_phy_hw_reset_generic;
+	func->set_d3_lplu_state  = e1000_set_d3_lplu_state_generic;
+
+	func->force_speed_duplex = e1000_phy_force_speed_duplex_80003es2lan;
+	func->get_cable_length   = e1000_get_cable_length_80003es2lan;
+	func->read_phy_reg       = e1000_read_phy_reg_gg82563_80003es2lan;
+	func->write_phy_reg      = e1000_write_phy_reg_gg82563_80003es2lan;
+
+	/* This can only be done after all function pointers are setup. */
+	ret_val = e1000_get_phy_id(hw);
+
+	/* Verify phy id */
+	if (phy->id != GG82563_E_PHY_ID) {
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_80003es2lan - Init ESB2 NVM func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_80003es2lan(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	u16 size;
+
+	DEBUGFUNC("e1000_init_nvm_params_80003es2lan");
+
+	nvm->opcode_bits        = 8;
+	nvm->delay_usec         = 1;
+	switch (nvm->override) {
+	case e1000_nvm_override_spi_large:
+		nvm->page_size    = 32;
+		nvm->address_bits = 16;
+		break;
+	case e1000_nvm_override_spi_small:
+		nvm->page_size    = 8;
+		nvm->address_bits = 8;
+		break;
+	default:
+		nvm->page_size    = eecd & E1000_EECD_ADDR_BITS ? 32 : 8;
+		nvm->address_bits = eecd & E1000_EECD_ADDR_BITS ? 16 : 8;
+		break;
+	}
+
+	nvm->type               = e1000_nvm_eeprom_spi;
+
+	size = (u16)((eecd & E1000_EECD_SIZE_EX_MASK) >>
+	                  E1000_EECD_SIZE_EX_SHIFT);
+
+	/* Added to a constant, "size" becomes the left-shift value
+	 * for setting word_size.
+	 */
+	size += NVM_WORD_SIZE_BASE_SHIFT;
+	nvm->word_size	= 1 << size;
+
+	/* Function Pointers */
+	func->acquire_nvm       = e1000_acquire_nvm_80003es2lan;
+	func->read_nvm          = e1000_read_nvm_eerd;
+	func->release_nvm       = e1000_release_nvm_80003es2lan;
+	func->update_nvm        = e1000_update_nvm_checksum_generic;
+	func->valid_led_default = e1000_valid_led_default_generic;
+	func->validate_nvm      = e1000_validate_nvm_checksum_generic;
+	func->write_nvm         = e1000_write_nvm_80003es2lan;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_init_mac_params_80003es2lan - Init ESB2 MAC func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_80003es2lan(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_mac_params_80003es2lan");
+
+	/* Set media type */
+	switch (hw->device_id) {
+	case E1000_DEV_ID_80003ES2LAN_SERDES_DPT:
+		hw->media_type = e1000_media_type_internal_serdes;
+		break;
+	default:
+		hw->media_type = e1000_media_type_copper;
+		break;
+	}
+
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+	/* Set if part includes ASF firmware */
+	mac->asf_firmware_present = TRUE;
+	/* Set if manageability features are enabled. */
+	mac->arc_subsystem_valid =
+	        (E1000_READ_REG(hw, E1000_FWSM) & E1000_FWSM_MODE_MASK)
+	                ? TRUE : FALSE;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_pcie_generic;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_80003es2lan;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_80003es2lan;
+	/* link setup */
+	func->setup_link = e1000_setup_link_generic;
+	/* physical interface link setup */
+	func->setup_physical_interface =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_setup_copper_link_80003es2lan
+	                : e1000_setup_fiber_serdes_link_generic;
+	/* check for link */
+	switch (hw->media_type) {
+	case e1000_media_type_copper:
+		func->check_for_link = e1000_check_for_copper_link_generic;
+		break;
+	case e1000_media_type_fiber:
+		func->check_for_link = e1000_check_for_fiber_link_generic;
+		break;
+	case e1000_media_type_internal_serdes:
+		func->check_for_link = e1000_check_for_serdes_link_generic;
+		break;
+	default:
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+		break;
+	}
+	/* check management mode */
+	func->check_mng_mode = e1000_check_mng_mode_generic;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_generic;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* blink LED */
+	func->blink_led = e1000_blink_led_generic;
+	/* setup LED */
+	func->setup_led = e1000_setup_led_generic;
+	/* cleanup LED */
+	func->cleanup_led = e1000_cleanup_led_generic;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_generic;
+	func->led_off = e1000_led_off_generic;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_80003es2lan;
+	/* link info */
+	func->get_link_up_info = e1000_get_link_up_info_80003es2lan;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_80003es2lan - Init ESB2 func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  The only function explicitly called by the api module to initialize
+ *  all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_80003es2lan(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_80003es2lan");
+
+	hw->func.init_mac_params = e1000_init_mac_params_80003es2lan;
+	hw->func.init_nvm_params = e1000_init_nvm_params_80003es2lan;
+	hw->func.init_phy_params = e1000_init_phy_params_80003es2lan;
+}
+
+/**
+ *  e1000_acquire_phy_80003es2lan - Acquire rights to access PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  A wrapper to acquire access rights to the correct PHY.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_acquire_phy_80003es2lan(struct e1000_hw *hw)
+{
+	u16 mask;
+
+	DEBUGFUNC("e1000_acquire_phy_80003es2lan");
+
+	mask = hw->bus.func ? E1000_SWFW_PHY1_SM : E1000_SWFW_PHY0_SM;
+
+	return e1000_acquire_swfw_sync_80003es2lan(hw, mask);
+}
+
+/**
+ *  e1000_release_phy_80003es2lan - Release rights to access PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  A wrapper to release access rights to the correct PHY.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static void e1000_release_phy_80003es2lan(struct e1000_hw *hw)
+{
+	u16 mask;
+
+	DEBUGFUNC("e1000_release_phy_80003es2lan");
+
+	mask = hw->bus.func ? E1000_SWFW_PHY1_SM : E1000_SWFW_PHY0_SM;
+	e1000_release_swfw_sync_80003es2lan(hw, mask);
+}
+
+/**
+ *  e1000_acquire_nvm_80003es2lan - Acquire rights to access NVM
+ *  @hw: pointer to the HW structure
+ *
+ *  Acquire the semaphore to access the EEPROM.  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_acquire_nvm_80003es2lan(struct e1000_hw *hw)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_acquire_nvm_80003es2lan");
+
+	ret_val = e1000_acquire_swfw_sync_80003es2lan(hw, E1000_SWFW_EEP_SM);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_acquire_nvm_generic(hw);
+
+	if (ret_val)
+		e1000_release_swfw_sync_80003es2lan(hw, E1000_SWFW_EEP_SM);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_release_nvm_80003es2lan - Relinquish rights to access NVM
+ *  @hw: pointer to the HW structure
+ *
+ *  Release the semaphore used to access the EEPROM.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static void e1000_release_nvm_80003es2lan(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_release_nvm_80003es2lan");
+
+	e1000_release_nvm_generic(hw);
+	e1000_release_swfw_sync_80003es2lan(hw, E1000_SWFW_EEP_SM);
+}
+
+/**
+ *  e1000_acquire_swfw_sync_80003es2lan - Acquire SW/FW semaphore
+ *  @hw: pointer to the HW structure
+ *  @mask: specifies which semaphore to acquire
+ *
+ *  Acquire the SW/FW semaphore to access the PHY or NVM.  The mask
+ *  will also specify which port we're acquiring the lock for.
+ **/
+static s32 e1000_acquire_swfw_sync_80003es2lan(struct e1000_hw *hw, u16 mask)
+{
+	u32 swfw_sync;
+	u32 swmask = mask;
+	u32 fwmask = mask << 16;
+	s32 ret_val = E1000_SUCCESS;
+	s32 i = 0, timeout = 200;
+
+	DEBUGFUNC("e1000_acquire_swfw_sync_80003es2lan");
+
+	while (i < timeout) {
+		if (e1000_get_hw_semaphore_generic(hw)) {
+			ret_val = -E1000_ERR_SWFW_SYNC;
+			goto out;
+		}
+
+		swfw_sync = E1000_READ_REG(hw, E1000_SW_FW_SYNC);
+		if (!(swfw_sync & (fwmask | swmask)))
+			break;
+
+		/* Firmware currently using resource (fwmask)
+		 * or other software thread using resource (swmask) */
+		e1000_put_hw_semaphore_generic(hw);
+		msec_delay_irq(5);
+		i++;
+	}
+
+	if (i == timeout) {
+		DEBUGOUT("Driver can't access resource, SW_FW_SYNC timeout.\n");
+		ret_val = -E1000_ERR_SWFW_SYNC;
+		goto out;
+	}
+
+	swfw_sync |= swmask;
+	E1000_WRITE_REG(hw, E1000_SW_FW_SYNC, swfw_sync);
+
+	e1000_put_hw_semaphore_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_release_swfw_sync_80003es2lan - Release SW/FW semaphore
+ *  @hw: pointer to the HW structure
+ *  @mask: specifies which semaphore to acquire
+ *
+ *  Release the SW/FW semaphore used to access the PHY or NVM.  The mask
+ *  will also specify which port we're releasing the lock for.
+ **/
+static void e1000_release_swfw_sync_80003es2lan(struct e1000_hw *hw, u16 mask)
+{
+	u32 swfw_sync;
+
+	DEBUGFUNC("e1000_release_swfw_sync_80003es2lan");
+
+	while (e1000_get_hw_semaphore_generic(hw) != E1000_SUCCESS);
+	/* Empty */
+
+	swfw_sync = E1000_READ_REG(hw, E1000_SW_FW_SYNC);
+	swfw_sync &= ~mask;
+	E1000_WRITE_REG(hw, E1000_SW_FW_SYNC, swfw_sync);
+
+	e1000_put_hw_semaphore_generic(hw);
+}
+
+/**
+ *  e1000_read_phy_reg_gg82563_80003es2lan - Read GG82563 PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of the register to read
+ *  @data: pointer to the data returned from the operation
+ *
+ *  Read the GG82563 PHY register.  This is a function pointer entry
+ *  point called by the api module.
+ **/
+static s32 e1000_read_phy_reg_gg82563_80003es2lan(struct e1000_hw *hw,
+                                                  u32 offset, u16 *data)
+{
+	s32 ret_val;
+	u32 page_select;
+	u16 temp;
+
+	DEBUGFUNC("e1000_read_phy_reg_gg82563_80003es2lan");
+
+	/* Select Configuration Page */
+	if ((offset & MAX_PHY_REG_ADDRESS) < GG82563_MIN_ALT_REG)
+		page_select = GG82563_PHY_PAGE_SELECT;
+	else {
+		/* Use Alternative Page Select register to access
+		 * registers 30 and 31
+		 */
+		page_select = GG82563_PHY_PAGE_SELECT_ALT;
+	}
+
+	temp = (u16)((u16)offset >> GG82563_PAGE_SHIFT);
+	ret_val = e1000_write_phy_reg_m88(hw, page_select, temp);
+	if (ret_val)
+		goto out;
+
+	/* The "ready" bit in the MDIC register may be incorrectly set
+	 * before the device has completed the "Page Select" MDI
+	 * transaction.  So we wait 200us after each MDI command...
+	 */
+	usec_delay(200);
+
+	/* ...and verify the command was successful. */
+	ret_val = e1000_read_phy_reg_m88(hw, page_select, &temp);
+
+	if (((u16)offset >> GG82563_PAGE_SHIFT) != temp) {
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+	usec_delay(200);
+
+	ret_val = e1000_read_phy_reg_m88(hw,
+	                                 MAX_PHY_REG_ADDRESS & offset,
+	                                 data);
+
+	usec_delay(200);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_phy_reg_gg82563_80003es2lan - Write GG82563 PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of the register to read
+ *  @data: value to write to the register
+ *
+ *  Write to the GG82563 PHY register.  This is a function pointer entry
+ *  point called by the api module.
+ **/
+static s32 e1000_write_phy_reg_gg82563_80003es2lan(struct e1000_hw *hw,
+                                                   u32 offset, u16 data)
+{
+	s32 ret_val;
+	u32 page_select;
+	u16 temp;
+
+	DEBUGFUNC("e1000_write_phy_reg_gg82563_80003es2lan");
+
+	/* Select Configuration Page */
+	if ((offset & MAX_PHY_REG_ADDRESS) < GG82563_MIN_ALT_REG)
+		page_select = GG82563_PHY_PAGE_SELECT;
+	else {
+		/* Use Alternative Page Select register to access
+		 * registers 30 and 31
+		 */
+		page_select = GG82563_PHY_PAGE_SELECT_ALT;
+	}
+
+	temp = (u16)((u16)offset >> GG82563_PAGE_SHIFT);
+	ret_val = e1000_write_phy_reg_m88(hw, page_select, temp);
+	if (ret_val)
+		goto out;
+
+
+	/* The "ready" bit in the MDIC register may be incorrectly set
+	 * before the device has completed the "Page Select" MDI
+	 * transaction.  So we wait 200us after each MDI command...
+	 */
+	usec_delay(200);
+
+	/* ...and verify the command was successful. */
+	ret_val = e1000_read_phy_reg_m88(hw, page_select, &temp);
+
+	if (((u16)offset >> GG82563_PAGE_SHIFT) != temp) {
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+	usec_delay(200);
+
+	ret_val = e1000_write_phy_reg_m88(hw,
+	                                  MAX_PHY_REG_ADDRESS & offset,
+	                                  data);
+
+	usec_delay(200);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_nvm_80003es2lan - Write to ESB2 NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of the register to read
+ *  @words: number of words to write
+ *  @data: buffer of data to write to the NVM
+ *
+ *  Write "words" of data to the ESB2 NVM.  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_write_nvm_80003es2lan(struct e1000_hw *hw, u16 offset,
+                            u16 words, u16 *data)
+{
+	DEBUGFUNC("e1000_write_nvm_80003es2lan");
+
+	return e1000_write_nvm_spi(hw, offset, words, data);
+}
+
+/**
+ *  e1000_get_cfg_done_80003es2lan - Wait for configuration to complete
+ *  @hw: pointer to the HW structure
+ *
+ *  Wait a specific amount of time for manageability processes to complete.
+ *  This is a function pointer entry point called by the phy module.
+ **/
+static s32 e1000_get_cfg_done_80003es2lan(struct e1000_hw *hw)
+{
+	s32 timeout = PHY_CFG_TIMEOUT;
+	s32 ret_val = E1000_SUCCESS;
+	u32 mask = E1000_NVM_CFG_DONE_PORT_0;
+
+	DEBUGFUNC("e1000_get_cfg_done_80003es2lan");
+
+	if (hw->bus.func == 1)
+		mask = E1000_NVM_CFG_DONE_PORT_1;
+
+	while (timeout) {
+		if (E1000_READ_REG(hw, E1000_EEMNGCTL) & mask)
+			break;
+		msec_delay(1);
+		timeout--;
+	}
+	if (!timeout) {
+		DEBUGOUT("MNG configuration cycle has not completed.\n");
+		ret_val = -E1000_ERR_RESET;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_80003es2lan - Force PHY speed and duplex
+ *  @hw: pointer to the HW structure
+ *
+ *  Force the speed and duplex settings onto the PHY.  This is a
+ *  function pointer entry point called by the phy module.
+ **/
+static s32 e1000_phy_force_speed_duplex_80003es2lan(struct e1000_hw *hw)
+{
+	s32 ret_val;
+	u16 phy_data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_80003es2lan");
+
+	/* Clear Auto-Crossover to force MDI manually.  M88E1000 requires MDI
+	 * forced whenever speed and duplex are forced.
+	 */
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data &= ~GG82563_PSCR_CROSSOVER_MODE_AUTO;
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_SPEC_CTRL, phy_data);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT1("GG82563 PSCR: %X\n", phy_data);
+
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	e1000_phy_force_speed_duplex_setup(hw, &phy_data);
+
+	/* Reset the phy to commit changes. */
+	phy_data |= MII_CR_RESET;
+
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, phy_data);
+	if (ret_val)
+		goto out;
+
+	usec_delay(1);
+
+	if (hw->phy.wait_for_link) {
+		DEBUGOUT("Waiting for forced speed/duplex link "
+		         "on GG82563 phy.\n");
+
+		ret_val = e1000_phy_has_link_generic(hw, PHY_FORCE_LIMIT,
+		                                     100000, &link);
+		if (ret_val)
+			goto out;
+
+		if (!link) {
+			/* We didn't get link.
+			 * Reset the DSP and cross our fingers.
+			 */
+			ret_val = e1000_phy_reset_dsp_generic(hw);
+			if (ret_val)
+				goto out;
+		}
+
+		/* Try once more */
+		ret_val = e1000_phy_has_link_generic(hw, PHY_FORCE_LIMIT,
+		                                     100000, &link);
+		if (ret_val)
+			goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	/* Resetting the phy means we need to verify the TX_CLK corresponds
+	 * to the link speed.  10Mbps -> 2.5MHz, else 25MHz.
+	 */
+	phy_data &= ~GG82563_MSCR_TX_CLK_MASK;
+	if (hw->mac.forced_speed_duplex & E1000_ALL_10_SPEED)
+		phy_data |= GG82563_MSCR_TX_CLK_10MBPS_2_5;
+	else
+		phy_data |= GG82563_MSCR_TX_CLK_100MBPS_25;
+
+	/* In addition, we must re-enable CRS on Tx for both half and full
+	 * duplex.
+	 */
+	phy_data |= GG82563_MSCR_ASSERT_CRS_ON_TX;
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL, phy_data);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cable_length_80003es2lan - Set approximate cable length
+ *  @hw: pointer to the HW structure
+ *
+ *  Find the approximate cable length as measured by the GG82563 PHY.
+ *  This is a function pointer entry point called by the phy module.
+ **/
+static s32 e1000_get_cable_length_80003es2lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data, index;
+
+	DEBUGFUNC("e1000_get_cable_length_80003es2lan");
+
+	ret_val = e1000_read_phy_reg(hw, GG82563_PHY_DSP_DISTANCE, &phy_data);
+	if (ret_val)
+		goto out;
+
+	index = phy_data & GG82563_DSPD_CABLE_LENGTH;
+	phy->min_cable_length = e1000_gg82563_cable_length_table[index];
+	phy->max_cable_length = e1000_gg82563_cable_length_table[index+5];
+
+	phy->cable_length = (phy->min_cable_length + phy->max_cable_length) / 2;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_link_up_info_80003es2lan - Report speed and duplex
+ *  @hw: pointer to the HW structure
+ *  @speed: pointer to speed buffer
+ *  @duplex: pointer to duplex buffer
+ *
+ *  Retrieve the current speed and duplex configuration.
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_get_link_up_info_80003es2lan(struct e1000_hw *hw, u16 *speed,
+                                              u16 *duplex)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_get_link_up_info_80003es2lan");
+
+	if (hw->media_type == e1000_media_type_copper) {
+		ret_val = e1000_get_speed_and_duplex_copper_generic(hw,
+		                                                    speed,
+		                                                    duplex);
+		if (ret_val)
+			goto out;
+		if (*speed == SPEED_1000)
+			ret_val = e1000_cfg_kmrn_1000_80003es2lan(hw);
+		else
+			ret_val = e1000_cfg_kmrn_10_100_80003es2lan(hw,
+			                                      *duplex);
+	} else {
+		ret_val = e1000_get_speed_and_duplex_fiber_serdes_generic(hw,
+		                                                  speed,
+		                                                  duplex);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_reset_hw_80003es2lan - Reset the ESB2 controller
+ *  @hw: pointer to the HW structure
+ *
+ *  Perform a global reset to the ESB2 controller.
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_80003es2lan(struct e1000_hw *hw)
+{
+	u32 ctrl, icr;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_reset_hw_80003es2lan");
+
+	/* Prevent the PCI-E bus from sticking if there is no TLP connection
+	 * on the last TLP read/write transaction when MAC is reset.
+	 */
+	ret_val = e1000_disable_pcie_master_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("PCI-E Master disable polling has failed.\n");
+	}
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	msec_delay(10);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGOUT("Issuing a global reset to MAC\n");
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+
+	ret_val = e1000_get_auto_rd_done_generic(hw);
+	if (ret_val)
+		/* We don't want to continue accessing MAC registers. */
+		goto out;
+
+	/* Clear any pending interrupt events. */
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_80003es2lan - Initialize the ESB2 controller
+ *  @hw: pointer to the HW structure
+ *
+ *  Initialize the hw bits, LED, VFTA, MTA, link and hw counters.
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_hw_80003es2lan(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 reg_data;
+	s32 ret_val;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_hw_80003es2lan");
+
+	e1000_initialize_hw_bits_80003es2lan(hw);
+
+	/* Initialize identification LED */
+	ret_val = e1000_id_led_init_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error initializing identification LED\n");
+		goto out;
+	}
+
+	/* Disabling VLAN filtering */
+	DEBUGOUT("Initializing the IEEE VLAN\n");
+	e1000_clear_vfta(hw);
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++)
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	/* Set the transmit descriptor write-back policy */
+	reg_data = E1000_READ_REG(hw, E1000_TXDCTL);
+	reg_data = (reg_data & ~E1000_TXDCTL_WTHRESH) |
+	           E1000_TXDCTL_FULL_TX_DESC_WB | E1000_TXDCTL_COUNT_DESC;
+	E1000_WRITE_REG(hw, E1000_TXDCTL, reg_data);
+
+	/* ...for both queues. */
+	reg_data = E1000_READ_REG(hw, E1000_TXDCTL1);
+	reg_data = (reg_data & ~E1000_TXDCTL_WTHRESH) |
+	           E1000_TXDCTL_FULL_TX_DESC_WB | E1000_TXDCTL_COUNT_DESC;
+	E1000_WRITE_REG(hw, E1000_TXDCTL1, reg_data);
+
+	/* Enable retransmit on late collisions */
+	reg_data = E1000_READ_REG(hw, E1000_TCTL);
+	reg_data |= E1000_TCTL_RTLC;
+	E1000_WRITE_REG(hw, E1000_TCTL, reg_data);
+
+	/* Configure Gigabit Carry Extend Padding */
+	reg_data = E1000_READ_REG(hw, E1000_TCTL_EXT);
+	reg_data &= ~E1000_TCTL_EXT_GCEX_MASK;
+	reg_data |= DEFAULT_TCTL_EXT_GCEX_80003ES2LAN;
+	E1000_WRITE_REG(hw, E1000_TCTL_EXT, reg_data);
+
+	/* Configure Transmit Inter-Packet Gap */
+	reg_data = E1000_READ_REG(hw, E1000_TIPG);
+	reg_data &= ~E1000_TIPG_IPGT_MASK;
+	reg_data |= DEFAULT_TIPG_IPGT_1000_80003ES2LAN;
+	E1000_WRITE_REG(hw, E1000_TIPG, reg_data);
+
+	reg_data = E1000_READ_REG_ARRAY(hw, E1000_FFLT, 0x0001);
+	reg_data &= ~0x00100000;
+	E1000_WRITE_REG_ARRAY(hw, E1000_FFLT, 0x0001, reg_data);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_80003es2lan(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_initialize_hw_bits_80003es2lan - Init hw bits of ESB2
+ *  @hw: pointer to the HW structure
+ *
+ *  Initializes required hardware-dependent bits needed for normal operation.
+ **/
+static void e1000_initialize_hw_bits_80003es2lan(struct e1000_hw *hw)
+{
+	u32 reg;
+
+	DEBUGFUNC("e1000_initialize_hw_bits_80003es2lan");
+
+	if (hw->mac.disable_hw_init_bits)
+		goto out;
+
+	/* Transmit Descriptor Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL, reg);
+
+	/* Transmit Descriptor Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL1);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL1, reg);
+
+	/* Transmit Arbitration Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TARC0);
+	reg &= ~(0xF << 27); /* 30:27 */
+	if (hw->media_type != e1000_media_type_copper)
+		reg &= ~(1 << 20);
+	E1000_WRITE_REG(hw, E1000_TARC0, reg);
+
+	/* Transmit Arbitration Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TARC1);
+	if (E1000_READ_REG(hw, E1000_TCTL) & E1000_TCTL_MULR)
+		reg &= ~(1 << 28);
+	else
+		reg |= (1 << 28);
+	E1000_WRITE_REG(hw, E1000_TARC1, reg);
+
+out:
+	return;
+}
+
+/**
+ *  e1000_copper_link_setup_gg82563_80003es2lan - Configure GG82563 Link
+ *  @hw: pointer to the HW structure
+ *
+ *  Setup some GG82563 PHY registers for obtaining link
+ **/
+static s32 e1000_copper_link_setup_gg82563_80003es2lan(struct e1000_hw *hw)
+{
+	struct   e1000_phy_info *phy = &hw->phy;
+	s32  ret_val;
+	u32 ctrl_ext;
+	u16 data;
+
+	DEBUGFUNC("e1000_copper_link_setup_gg82563_80003es2lan");
+
+	if (!phy->reset_disable) {
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL,
+		                             &data);
+		if (ret_val)
+			goto out;
+
+		data |= GG82563_MSCR_ASSERT_CRS_ON_TX;
+		/* Use 25MHz for both link down and 1000Base-T for Tx clock. */
+		data |= GG82563_MSCR_TX_CLK_1000MBPS_25;
+
+		ret_val = e1000_write_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL,
+		                              data);
+		if (ret_val)
+			goto out;
+
+		/* Options:
+		 *   MDI/MDI-X = 0 (default)
+		 *   0 - Auto for all speeds
+		 *   1 - MDI mode
+		 *   2 - MDI-X mode
+		 *   3 - Auto for 1000Base-T only (MDI-X for 10/100Base-T modes)
+		 */
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_SPEC_CTRL, &data);
+		if (ret_val)
+			goto out;
+
+		data &= ~GG82563_PSCR_CROSSOVER_MODE_MASK;
+
+		switch (phy->mdix) {
+		case 1:
+			data |= GG82563_PSCR_CROSSOVER_MODE_MDI;
+			break;
+		case 2:
+			data |= GG82563_PSCR_CROSSOVER_MODE_MDIX;
+			break;
+		case 0:
+		default:
+			data |= GG82563_PSCR_CROSSOVER_MODE_AUTO;
+			break;
+		}
+
+		/* Options:
+		 *   disable_polarity_correction = 0 (default)
+		 *       Automatic Correction for Reversed Cable Polarity
+		 *   0 - Disabled
+		 *   1 - Enabled
+		 */
+		data &= ~GG82563_PSCR_POLARITY_REVERSAL_DISABLE;
+		if (phy->disable_polarity_correction == TRUE)
+			data |= GG82563_PSCR_POLARITY_REVERSAL_DISABLE;
+
+		ret_val = e1000_write_phy_reg(hw, GG82563_PHY_SPEC_CTRL, data);
+		if (ret_val)
+			goto out;
+
+		/* SW Reset the PHY so all changes take effect */
+		ret_val = e1000_phy_commit(hw);
+		if (ret_val) {
+			DEBUGOUT("Error Resetting the PHY\n");
+			goto out;
+		}
+
+	}
+
+	/* Bypass RX and TX FIFO's */
+	ret_val = e1000_write_kmrn_reg(hw,
+	                        E1000_KMRNCTRLSTA_OFFSET_FIFO_CTRL,
+	                        E1000_KMRNCTRLSTA_FIFO_CTRL_RX_BYPASS |
+	                                E1000_KMRNCTRLSTA_FIFO_CTRL_TX_BYPASS);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg(hw, GG82563_PHY_SPEC_CTRL_2, &data);
+	if (ret_val)
+		goto out;
+
+	data &= ~GG82563_PSCR2_REVERSE_AUTO_NEG;
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_SPEC_CTRL_2, data);
+	if (ret_val)
+		goto out;
+
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	ctrl_ext &= ~(E1000_CTRL_EXT_LINK_MODE_MASK);
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+
+	ret_val = e1000_read_phy_reg(hw, GG82563_PHY_PWR_MGMT_CTRL, &data);
+	if (ret_val)
+		goto out;
+
+	/* Do not init these registers when the HW is in IAMT mode, since the
+	 * firmware will have already initialized them.  We only initialize
+	 * them if the HW is not in IAMT mode.
+	 */
+	if (e1000_check_mng_mode(hw) == FALSE) {
+		/* Enable Electrical Idle on the PHY */
+		data |= GG82563_PMCR_ENABLE_ELECTRICAL_IDLE;
+		ret_val = e1000_write_phy_reg(hw,
+		                             GG82563_PHY_PWR_MGMT_CTRL,
+		                             data);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw,
+		                            GG82563_PHY_KMRN_MODE_CTRL,
+		                            &data);
+		if (ret_val)
+			goto out;
+
+		data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+		ret_val = e1000_write_phy_reg(hw,
+		                             GG82563_PHY_KMRN_MODE_CTRL,
+		                             data);
+
+		if (ret_val)
+			goto out;
+	}
+
+	/* Workaround: Disable padding in Kumeran interface in the MAC
+	 * and in the PHY to avoid CRC errors.
+	 */
+	ret_val = e1000_read_phy_reg(hw, GG82563_PHY_INBAND_CTRL, &data);
+	if (ret_val)
+		goto out;
+
+	data |= GG82563_ICR_DIS_PADDING;
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_INBAND_CTRL, data);
+	if (ret_val)
+		goto out;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_80003es2lan - Setup Copper Link for ESB2
+ *  @hw: pointer to the HW structure
+ *
+ *  Essentially a wrapper for setting up all things "copper" related.
+ *  This is a function pointer entry point called by the mac module.
+ **/
+static s32 e1000_setup_copper_link_80003es2lan(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32  ret_val;
+	u16 reg_data;
+
+	DEBUGFUNC("e1000_setup_copper_link_80003es2lan");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_SLU;
+	ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	/* Set the mac to wait the maximum time between each
+	 * iteration and increase the max iterations when
+	 * polling the phy; this fixes erroneous timeouts at 10Mbps. */
+	ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 4), 0xFFFF);
+	if (ret_val)
+		goto out;
+	ret_val = e1000_read_kmrn_reg(hw, GG82563_REG(0x34, 9), &reg_data);
+	if (ret_val)
+		goto out;
+	reg_data |= 0x3F;
+	ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 9), reg_data);
+	if (ret_val)
+		goto out;
+	ret_val = e1000_read_kmrn_reg(hw,
+	                              E1000_KMRNCTRLSTA_OFFSET_INB_CTRL,
+	                              &reg_data);
+	if (ret_val)
+		goto out;
+	reg_data |= E1000_KMRNCTRLSTA_INB_CTRL_DIS_PADDING;
+	ret_val = e1000_write_kmrn_reg(hw,
+	                               E1000_KMRNCTRLSTA_OFFSET_INB_CTRL,
+	                               reg_data);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_copper_link_setup_gg82563_80003es2lan(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_setup_copper_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_cfg_kmrn_10_100_80003es2lan - Apply "quirks" for 10/100 operation
+ *  @hw: pointer to the HW structure
+ *  @duplex: current duplex setting
+ *
+ *  Configure the KMRN interface by applying last minute quirks for
+ *  10/100 operation.
+ **/
+static s32 e1000_cfg_kmrn_10_100_80003es2lan(struct e1000_hw *hw, u16 duplex)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u32 tipg;
+	u32 i = 0;
+	u16 reg_data, reg_data2;
+
+	DEBUGFUNC("e1000_configure_kmrn_for_10_100");
+
+	reg_data = E1000_KMRNCTRLSTA_HD_CTRL_10_100_DEFAULT;
+	ret_val = e1000_write_kmrn_reg(hw,
+	                               E1000_KMRNCTRLSTA_OFFSET_HD_CTRL,
+	                               reg_data);
+	if (ret_val)
+		goto out;
+
+	/* Configure Transmit Inter-Packet Gap */
+	tipg = E1000_READ_REG(hw, E1000_TIPG);
+	tipg &= ~E1000_TIPG_IPGT_MASK;
+	tipg |= DEFAULT_TIPG_IPGT_10_100_80003ES2LAN;
+	E1000_WRITE_REG(hw, E1000_TIPG, tipg);
+
+
+	do {
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+		                             &reg_data);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+		                             &reg_data2);
+		if (ret_val)
+			goto out;
+		i++;
+	} while ((reg_data != reg_data2) && (i < GG82563_MAX_KMRN_RETRY));
+
+	if (duplex == HALF_DUPLEX)
+		reg_data |= GG82563_KMCR_PASS_FALSE_CARRIER;
+	else
+		reg_data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, reg_data);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_cfg_kmrn_1000_80003es2lan - Apply "quirks" for gigabit operation
+ *  @hw: pointer to the HW structure
+ *
+ *  Configure the KMRN interface by applying last minute quirks for
+ *  gigabit operation.
+ **/
+static s32 e1000_cfg_kmrn_1000_80003es2lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 reg_data, reg_data2;
+	u32 tipg;
+	u32 i = 0;
+
+	DEBUGFUNC("e1000_configure_kmrn_for_1000");
+
+	reg_data = E1000_KMRNCTRLSTA_HD_CTRL_1000_DEFAULT;
+	ret_val = e1000_write_kmrn_reg(hw,
+	                               E1000_KMRNCTRLSTA_OFFSET_HD_CTRL,
+	                               reg_data);
+	if (ret_val)
+		goto out;
+
+	/* Configure Transmit Inter-Packet Gap */
+	tipg = E1000_READ_REG(hw, E1000_TIPG);
+	tipg &= ~E1000_TIPG_IPGT_MASK;
+	tipg |= DEFAULT_TIPG_IPGT_1000_80003ES2LAN;
+	E1000_WRITE_REG(hw, E1000_TIPG, tipg);
+
+
+	do {
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+		                             &reg_data);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+		                             &reg_data2);
+		if (ret_val)
+			goto out;
+		i++;
+	} while ((reg_data != reg_data2) && (i < GG82563_MAX_KMRN_RETRY));
+
+	reg_data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+	ret_val = e1000_write_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, reg_data);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_80003es2lan - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_80003es2lan(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_80003es2lan");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+
+	temp = E1000_READ_REG(hw, E1000_MGTPRC);
+	temp = E1000_READ_REG(hw, E1000_MGTPDC);
+	temp = E1000_READ_REG(hw, E1000_MGTPTC);
+
+	temp = E1000_READ_REG(hw, E1000_IAC);
+	temp = E1000_READ_REG(hw, E1000_ICRXOC);
+
+	temp = E1000_READ_REG(hw, E1000_ICRXPTC);
+	temp = E1000_READ_REG(hw, E1000_ICRXATC);
+	temp = E1000_READ_REG(hw, E1000_ICTXPTC);
+	temp = E1000_READ_REG(hw, E1000_ICTXATC);
+	temp = E1000_READ_REG(hw, E1000_ICTXQEC);
+	temp = E1000_READ_REG(hw, E1000_ICTXQMTC);
+	temp = E1000_READ_REG(hw, E1000_ICRXDMTC);
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_80003es2lan.h linux-2.6.9/drivers/net/e1000/e1000_80003es2lan.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_80003es2lan.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_80003es2lan.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,90 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_80003ES2LAN_H_
+#define _E1000_80003ES2LAN_H_
+
+#define E1000_KMRNCTRLSTA_OFFSET_FIFO_CTRL       0x00
+#define E1000_KMRNCTRLSTA_OFFSET_INB_CTRL        0x02
+#define E1000_KMRNCTRLSTA_OFFSET_HD_CTRL         0x10
+
+#define E1000_KMRNCTRLSTA_FIFO_CTRL_RX_BYPASS    0x0008
+#define E1000_KMRNCTRLSTA_FIFO_CTRL_TX_BYPASS    0x0800
+#define E1000_KMRNCTRLSTA_INB_CTRL_DIS_PADDING   0x0010
+
+#define E1000_KMRNCTRLSTA_HD_CTRL_10_100_DEFAULT 0x0004
+#define E1000_KMRNCTRLSTA_HD_CTRL_1000_DEFAULT   0x0000
+
+#define E1000_TCTL_EXT_GCEX_MASK 0x000FFC00 /* Gigabit Carry Extend Padding */
+#define DEFAULT_TCTL_EXT_GCEX_80003ES2LAN 0x00010000
+
+#define DEFAULT_TIPG_IPGT_1000_80003ES2LAN   0x8
+#define DEFAULT_TIPG_IPGT_10_100_80003ES2LAN 0x9
+
+/* GG82563 PHY Specific Status Register (Page 0, Register 16 */
+#define GG82563_PSCR_POLARITY_REVERSAL_DISABLE  0x0002 /* 1=Reversal Disabled */
+#define GG82563_PSCR_CROSSOVER_MODE_MASK        0x0060
+#define GG82563_PSCR_CROSSOVER_MODE_MDI         0x0000 /* 00=Manual MDI */
+#define GG82563_PSCR_CROSSOVER_MODE_MDIX        0x0020 /* 01=Manual MDIX */
+#define GG82563_PSCR_CROSSOVER_MODE_AUTO        0x0060 /* 11=Auto crossover */
+
+/* PHY Specific Control Register 2 (Page 0, Register 26) */
+#define GG82563_PSCR2_REVERSE_AUTO_NEG              0x2000
+                                                /* 1=Reverse Auto-Negotiation */
+
+/* MAC Specific Control Register (Page 2, Register 21) */
+/* Tx clock speed for Link Down and 1000BASE-T for the following speeds */
+#define GG82563_MSCR_TX_CLK_MASK                    0x0007
+#define GG82563_MSCR_TX_CLK_10MBPS_2_5              0x0004
+#define GG82563_MSCR_TX_CLK_100MBPS_25              0x0005
+#define GG82563_MSCR_TX_CLK_1000MBPS_2_5            0x0006
+#define GG82563_MSCR_TX_CLK_1000MBPS_25             0x0007
+
+#define GG82563_MSCR_ASSERT_CRS_ON_TX               0x0010 /* 1=Assert */
+
+/* DSP Distance Register (Page 5, Register 26) */
+#define GG82563_DSPD_CABLE_LENGTH               0x0007 /* 0 = <50M;
+                                                          1 = 50-80M;
+                                                          2 = 80-110M;
+                                                          3 = 110-140M;
+                                                          4 = >140M */
+
+/* Kumeran Mode Control Register (Page 193, Register 16) */
+#define GG82563_KMCR_PASS_FALSE_CARRIER             0x0800
+
+/* Max number of times Kumeran read/write should be validated */
+#define GG82563_MAX_KMRN_RETRY  0x5
+
+/* Power Management Control Register (Page 193, Register 20) */
+#define GG82563_PMCR_ENABLE_ELECTRICAL_IDLE         0x0001
+                                           /* 1=Enable SERDES Electrical Idle */
+
+/* In-Band Control Register (Page 194, Register 18) */
+#define GG82563_ICR_DIS_PADDING                     0x0010 /* Disable Padding */
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82540.c linux-2.6.9/drivers/net/e1000/e1000_82540.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_82540.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82540.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,658 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_82540
+ * e1000_82545
+ * e1000_82546
+ * e1000_82545_rev_3
+ * e1000_82546_rev_3
+ */
+
+#include "e1000_api.h"
+
+void e1000_init_function_pointers_82540(struct e1000_hw *hw);
+
+static s32  e1000_init_phy_params_82540(struct e1000_hw *hw);
+static s32  e1000_init_nvm_params_82540(struct e1000_hw *hw);
+static s32  e1000_init_mac_params_82540(struct e1000_hw *hw);
+static s32  e1000_adjust_serdes_amplitude_82540(struct e1000_hw *hw);
+static void e1000_clear_hw_cntrs_82540(struct e1000_hw *hw);
+static s32  e1000_init_hw_82540(struct e1000_hw *hw);
+static s32  e1000_reset_hw_82540(struct e1000_hw *hw);
+static s32  e1000_set_phy_mode_82540(struct e1000_hw *hw);
+static s32  e1000_set_vco_speed_82540(struct e1000_hw *hw);
+static s32  e1000_setup_copper_link_82540(struct e1000_hw *hw);
+static s32  e1000_setup_fiber_serdes_link_82540(struct e1000_hw *hw);
+
+/**
+ * e1000_init_phy_params_82540 - Init PHY func ptrs.
+ * @hw: pointer to the HW structure
+ *
+ * This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_82540(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	phy->addr                       = 1;
+	phy->autoneg_mask               = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+	phy->reset_delay_us             = 10000;
+	phy->type                       = e1000_phy_m88;
+
+	/* Function Pointers */
+	func->check_polarity            = e1000_check_polarity_m88;
+	func->commit_phy                = e1000_phy_sw_reset_generic;
+	func->force_speed_duplex        = e1000_phy_force_speed_duplex_m88;
+	func->get_cable_length          = e1000_get_cable_length_m88;
+	func->get_cfg_done              = e1000_get_cfg_done_generic;
+	func->read_phy_reg              = e1000_read_phy_reg_m88;
+	func->reset_phy                 = e1000_phy_hw_reset_generic;
+	func->write_phy_reg             = e1000_write_phy_reg_m88;
+	func->get_phy_info              = e1000_get_phy_info_m88;
+
+	ret_val = e1000_get_phy_id(hw);
+	if (ret_val)
+		goto out;
+
+	/* Verify phy id */
+	switch (hw->mac.type) {
+	case e1000_82540:
+	case e1000_82545:
+	case e1000_82545_rev_3:
+	case e1000_82546:
+	case e1000_82546_rev_3:
+		if (phy->id == M88E1011_I_PHY_ID)
+			break;
+		/* Fall Through */
+	default:
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+		break;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ * e1000_init_nvm_params_82540 - Init NVM func ptrs.
+ * @hw: pointer to the HW structure
+ *
+ * This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_82540(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+
+	DEBUGFUNC("e1000_init_nvm_params_82540");
+
+	nvm->type               = e1000_nvm_eeprom_microwire;
+	nvm->delay_usec         = 50;
+	nvm->opcode_bits        = 3;
+	switch (nvm->override) {
+	case e1000_nvm_override_microwire_large:
+		nvm->address_bits       = 8;
+		nvm->word_size          = 256;
+		break;
+	case e1000_nvm_override_microwire_small:
+		nvm->address_bits       = 6;
+		nvm->word_size          = 64;
+		break;
+	default:
+		nvm->address_bits       = eecd & E1000_EECD_SIZE ? 8 : 6;
+		nvm->word_size          = eecd & E1000_EECD_SIZE ? 256 : 64;
+		break;
+	}
+
+	/* Function Pointers */
+	func->acquire_nvm        = e1000_acquire_nvm_generic;
+	func->read_nvm           = e1000_read_nvm_microwire;
+	func->release_nvm        = e1000_release_nvm_generic;
+	func->update_nvm         = e1000_update_nvm_checksum_generic;
+	func->valid_led_default  = e1000_valid_led_default_generic;
+	func->validate_nvm       = e1000_validate_nvm_checksum_generic;
+	func->write_nvm          = e1000_write_nvm_microwire;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ * e1000_init_mac_params_82540 - Init MAC func ptrs.
+ * @hw: pointer to the HW structure
+ *
+ * This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_82540(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_mac_params_82540");
+
+	/* Set media type */
+	switch (hw->device_id) {
+	case E1000_DEV_ID_82545EM_FIBER:
+	case E1000_DEV_ID_82545GM_FIBER:
+	case E1000_DEV_ID_82546EB_FIBER:
+	case E1000_DEV_ID_82546GB_FIBER:
+		hw->media_type = e1000_media_type_fiber;
+		break;
+	case E1000_DEV_ID_82545GM_SERDES:
+	case E1000_DEV_ID_82546GB_SERDES:
+		hw->media_type = e1000_media_type_internal_serdes;
+		break;
+	default:
+		hw->media_type = e1000_media_type_copper;
+		break;
+	}
+
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_pci_generic;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_82540;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_82540;
+	/* link setup */
+	func->setup_link = e1000_setup_link_generic;
+	/* physical interface setup */
+	func->setup_physical_interface =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_setup_copper_link_82540
+	                : e1000_setup_fiber_serdes_link_82540;
+	/* check for link */
+	switch (hw->media_type) {
+	case e1000_media_type_copper:
+		func->check_for_link = e1000_check_for_copper_link_generic;
+		break;
+	case e1000_media_type_fiber:
+		func->check_for_link = e1000_check_for_fiber_link_generic;
+		break;
+	case e1000_media_type_internal_serdes:
+		func->check_for_link = e1000_check_for_serdes_link_generic;
+		break;
+	default:
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+		break;
+	}
+	/* link info */
+	func->get_link_up_info =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_get_speed_and_duplex_copper_generic
+	                : e1000_get_speed_and_duplex_fiber_serdes_generic;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_generic;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* setup LED */
+	func->setup_led = e1000_setup_led_generic;
+	/* cleanup LED */
+	func->cleanup_led = e1000_cleanup_led_generic;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_generic;
+	func->led_off = e1000_led_off_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_82540;
+
+out:
+	return ret_val;
+}
+
+/**
+ * e1000_init_function_pointers_82540 - Init func ptrs.
+ * @hw: pointer to the HW structure
+ *
+ * The only function explicitly called by the api module to initialize
+ * all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_82540(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_82540");
+
+	hw->func.init_mac_params = e1000_init_mac_params_82540;
+	hw->func.init_nvm_params = e1000_init_nvm_params_82540;
+	hw->func.init_phy_params = e1000_init_phy_params_82540;
+}
+
+/**
+ *  e1000_reset_hw_82540 - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_82540(struct e1000_hw *hw)
+{
+	u32 ctrl, icr, manc;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_reset_hw_82540");
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xFFFFFFFF);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	/* Delay to allow any outstanding PCI transactions to complete
+	 * before resetting the device.
+	 */
+	msec_delay(10);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGOUT("Issuing a global reset to 82540/82545/82546 MAC\n");
+	switch (hw->mac.type) {
+	case e1000_82545_rev_3:
+	case e1000_82546_rev_3:
+		E1000_WRITE_REG(hw, E1000_CTRL_DUP, ctrl | E1000_CTRL_RST);
+		break;
+	default:
+		/* These controllers can't ack the 64-bit write when
+		 * issuing the reset, so we use IO-mapping as a
+		 * workaround to issue the reset.
+		 */
+		E1000_WRITE_REG_IO(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+		break;
+	}
+
+	/* Wait for EEPROM reload */
+	msec_delay(5);
+
+	/* Disable HW ARPs on ASF enabled adapters */
+	manc = E1000_READ_REG(hw, E1000_MANC);
+	manc &= ~E1000_MANC_ARP_EN;
+	E1000_WRITE_REG(hw, E1000_MANC, manc);
+
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_82540 - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_hw_82540(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 txdctl, ctrl_ext;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_hw_82540");
+
+	/* Initialize identification LED */
+	ret_val = e1000_id_led_init_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error initializing identification LED\n");
+		goto out;
+	}
+
+	/* Disabling VLAN filtering */
+	DEBUGOUT("Initializing the IEEE VLAN\n");
+	if (mac->type < e1000_82545_rev_3) {
+		E1000_WRITE_REG(hw, E1000_VET, 0);
+	}
+	e1000_clear_vfta(hw);
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+		/* Avoid back to back register writes by adding the register
+		 * read (flush).  This is to protect against some strange
+		 * bridge configurations that may issue Memory Write Block
+		 * (MWB) to our register space.  The *_rev_3 hardware at
+		 * least doesn't respond correctly to every other dword in an
+		 * MWB to our register space.
+		 */
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	if (mac->type < e1000_82545_rev_3)
+		e1000_pcix_mmrbc_workaround_generic(hw);
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	txdctl = E1000_READ_REG(hw, E1000_TXDCTL);
+	txdctl = (txdctl & ~E1000_TXDCTL_WTHRESH) |
+	         E1000_TXDCTL_FULL_TX_DESC_WB;
+	E1000_WRITE_REG(hw, E1000_TXDCTL, txdctl);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_82540(hw);
+
+	if ((hw->device_id == E1000_DEV_ID_82546GB_QUAD_COPPER) ||
+	    (hw->device_id == E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3)) {
+		ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		/* Relaxed ordering must be disabled to avoid a parity
+		 * error crash in a PCI slot. */
+		ctrl_ext |= E1000_CTRL_EXT_RO_DIS;
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_82540 - Configure copper link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the appropriate function to configure the link for auto-neg or forced
+ *  speed and duplex.  Then we check for link, once link is established calls
+ *  to configure collision distance and flow control are called.  If link is
+ *  not established, we return -E1000_ERR_PHY (-2).  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_setup_copper_link_82540(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val = E1000_SUCCESS;
+	u16 data;
+
+	DEBUGFUNC("e1000_setup_copper_link_82540");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_SLU;
+	ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	ret_val = e1000_set_phy_mode_82540(hw);
+	if (ret_val)
+		goto out;
+
+	if (hw->mac.type == e1000_82545_rev_3 ||
+	    hw->mac.type == e1000_82546_rev_3) {
+		ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &data);
+		if (ret_val)
+			goto out;
+		data |= 0x00000008;
+		ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, data);
+		if (ret_val)
+			goto out;
+	}
+
+	ret_val = e1000_copper_link_setup_m88(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_setup_copper_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_fiber_serdes_link_82540 - Setup link for fiber/serdes
+ *  @hw: pointer to the HW structure
+ *
+ *  Set the output amplitude to the value in the EEPROM and adjust the VCO
+ *  speed to improve Bit Error Rate (BER) performance.  Configures collision
+ *  distance and flow control for fiber and serdes links.  Upon successful
+ *  setup, poll for link.  This is a function pointer entry point called by
+ *  the api module.
+ **/
+static s32 e1000_setup_fiber_serdes_link_82540(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_fiber_serdes_link_82540");
+
+	switch (mac->type) {
+	case e1000_82545_rev_3:
+	case e1000_82546_rev_3:
+		if (hw->media_type == e1000_media_type_internal_serdes) {
+			/* If we're on serdes media, adjust the output
+			 * amplitude to value set in the EEPROM.
+			 */
+			ret_val = e1000_adjust_serdes_amplitude_82540(hw);
+			if (ret_val)
+				goto out;
+		}
+		/* Adjust VCO speed to improve BER performance */
+		ret_val = e1000_set_vco_speed_82540(hw);
+		if (ret_val)
+			goto out;
+	default:
+		break;
+	}
+
+	ret_val = e1000_setup_fiber_serdes_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_adjust_serdes_amplitude_82540 - Adjust amplitude based on EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Adjust the SERDES ouput amplitude based on the EEPROM settings.
+ **/
+static s32 e1000_adjust_serdes_amplitude_82540(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 nvm_data;
+
+	DEBUGFUNC("e1000_adjust_serdes_amplitude_82540");
+
+	ret_val = e1000_read_nvm(hw, NVM_SERDES_AMPLITUDE, 1, &nvm_data);
+	if (ret_val) {
+		goto out;
+	}
+
+	if (nvm_data != NVM_RESERVED_WORD) {
+		/* Adjust serdes output amplitude only. */
+		nvm_data &= NVM_SERDES_AMPLITUDE_MASK;
+		ret_val = e1000_write_phy_reg(hw,
+		                             M88E1000_PHY_EXT_CTRL,
+		                             nvm_data);
+		if (ret_val)
+			goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_vco_speed_82540 - Set VCO speed for better performance
+ *  @hw: pointer to the HW structure
+ *
+ *  Set the VCO speed to improve Bit Error Rate (BER) performance.
+ **/
+static s32 e1000_set_vco_speed_82540(struct e1000_hw *hw)
+{
+	s32  ret_val = E1000_SUCCESS;
+	u16 default_page = 0;
+	u16 phy_data;
+
+	DEBUGFUNC("e1000_set_vco_speed_82540");
+
+	/* Set PHY register 30, page 5, bit 8 to 0 */
+
+	ret_val = e1000_read_phy_reg(hw,
+	                            M88E1000_PHY_PAGE_SELECT,
+	                            &default_page);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0005);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data &= ~M88E1000_PHY_VCO_REG_BIT8;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, phy_data);
+	if (ret_val)
+		goto out;
+
+	/* Set PHY register 30, page 4, bit 11 to 1 */
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0004);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data |= M88E1000_PHY_VCO_REG_BIT11;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, phy_data);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT,
+	                              default_page);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_phy_mode_82540 - Set PHY to class A mode
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets the PHY to class A mode and assumes the following operations will
+ *  follow to enable the new class mode:
+ *    1.  Do a PHY soft reset.
+ *    2.  Restart auto-negotiation or force link.
+ **/
+static s32 e1000_set_phy_mode_82540(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val = E1000_SUCCESS;
+	u16 nvm_data;
+
+	DEBUGFUNC("e1000_set_phy_mode_82540");
+
+	if (hw->mac.type != e1000_82545_rev_3)
+		goto out;
+
+	ret_val = e1000_read_nvm(hw, NVM_PHY_CLASS_WORD, 1, &nvm_data);
+	if (ret_val) {
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+	if ((nvm_data != NVM_RESERVED_WORD) && (nvm_data & NVM_PHY_CLASS_A)) {
+		ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT,
+					      0x000B);
+		if (ret_val) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+		ret_val = e1000_write_phy_reg(hw,
+					     M88E1000_PHY_GEN_CONTROL,
+					     0x8104);
+		if (ret_val) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+
+		phy->reset_disable = FALSE;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_82540 - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_82540(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_82540");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+
+	temp = E1000_READ_REG(hw, E1000_MGTPRC);
+	temp = E1000_READ_REG(hw, E1000_MGTPDC);
+	temp = E1000_READ_REG(hw, E1000_MGTPTC);
+}
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82541.c linux-2.6.9/drivers/net/e1000/e1000_82541.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_82541.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82541.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1290 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_82541
+ * e1000_82547
+ * e1000_82541_rev_2
+ * e1000_82547_rev_2
+ */
+
+#include "e1000_api.h"
+#include "e1000_82541.h"
+
+void e1000_init_function_pointers_82541(struct e1000_hw *hw);
+
+static s32  e1000_init_phy_params_82541(struct e1000_hw *hw);
+static s32  e1000_init_nvm_params_82541(struct e1000_hw *hw);
+static s32  e1000_init_mac_params_82541(struct e1000_hw *hw);
+static s32  e1000_reset_hw_82541(struct e1000_hw *hw);
+static s32  e1000_init_hw_82541(struct e1000_hw *hw);
+static s32  e1000_get_link_up_info_82541(struct e1000_hw *hw, u16 *speed,
+                                         u16 *duplex);
+static s32  e1000_phy_hw_reset_82541(struct e1000_hw *hw);
+static s32  e1000_setup_copper_link_82541(struct e1000_hw *hw);
+static s32  e1000_check_for_link_82541(struct e1000_hw *hw);
+static s32  e1000_get_cable_length_igp_82541(struct e1000_hw *hw);
+static s32  e1000_set_d3_lplu_state_82541(struct e1000_hw *hw,
+                                          boolean_t active);
+static s32  e1000_setup_led_82541(struct e1000_hw *hw);
+static s32  e1000_cleanup_led_82541(struct e1000_hw *hw);
+static void e1000_clear_hw_cntrs_82541(struct e1000_hw *hw);
+static s32  e1000_config_dsp_after_link_change_82541(struct e1000_hw *hw,
+                                                     boolean_t link_up);
+static s32  e1000_phy_init_script_82541(struct e1000_hw *hw);
+
+static const u16 e1000_igp_cable_length_table[] =
+    { 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
+      5, 10, 10, 10, 10, 10, 10, 10, 20, 20, 20, 20, 20, 25, 25, 25,
+      25, 25, 25, 25, 30, 30, 30, 30, 40, 40, 40, 40, 40, 40, 40, 40,
+      40, 50, 50, 50, 50, 50, 50, 50, 60, 60, 60, 60, 60, 60, 60, 60,
+      60, 70, 70, 70, 70, 70, 70, 80, 80, 80, 80, 80, 80, 90, 90, 90,
+      90, 90, 90, 90, 90, 90, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
+      100, 100, 100, 100, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,
+      110, 110, 110, 110, 110, 110, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120};
+#define IGP01E1000_AGC_LENGTH_TABLE_SIZE \
+                (sizeof(e1000_igp_cable_length_table) / \
+                 sizeof(e1000_igp_cable_length_table[0]))
+
+struct e1000_dev_spec_82541 {
+	e1000_dsp_config dsp_config;
+	e1000_ffe_config ffe_config;
+	u16 spd_default;
+	boolean_t phy_init_script;
+};
+
+/**
+ *  e1000_init_phy_params_82541 - Init PHY func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_82541(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_phy_params_82541");
+
+	phy->addr                       = 1;
+	phy->autoneg_mask               = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+	phy->reset_delay_us             = 10000;
+	phy->type                       = e1000_phy_igp;
+
+	/* Function Pointers */
+	func->check_polarity            = e1000_check_polarity_igp;
+	func->force_speed_duplex        = e1000_phy_force_speed_duplex_igp;
+	func->get_cable_length          = e1000_get_cable_length_igp_82541;
+	func->get_cfg_done              = e1000_get_cfg_done_generic;
+	func->get_phy_info              = e1000_get_phy_info_igp;
+	func->read_phy_reg              = e1000_read_phy_reg_igp;
+	func->reset_phy                 = e1000_phy_hw_reset_82541;
+	func->set_d3_lplu_state         = e1000_set_d3_lplu_state_82541;
+	func->write_phy_reg             = e1000_write_phy_reg_igp;
+
+	ret_val = e1000_get_phy_id(hw);
+	if (ret_val)
+		goto out;
+
+	/* Verify phy id */
+	if (phy->id != IGP01E1000_I_PHY_ID) {
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_82541 - Init NVM func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_82541(struct e1000_hw *hw)
+{
+	struct   e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+	s32  ret_val = E1000_SUCCESS;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	u16 size;
+
+	DEBUGFUNC("e1000_init_nvm_params_82541");
+
+	switch (nvm->override) {
+	case e1000_nvm_override_spi_large:
+		nvm->type = e1000_nvm_eeprom_spi;
+		eecd |= E1000_EECD_ADDR_BITS;
+		break;
+	case e1000_nvm_override_spi_small:
+		nvm->type = e1000_nvm_eeprom_spi;
+		eecd &= ~E1000_EECD_ADDR_BITS;
+		break;
+	case e1000_nvm_override_microwire_large:
+		nvm->type = e1000_nvm_eeprom_microwire;
+		eecd |= E1000_EECD_SIZE;
+		break;
+	case e1000_nvm_override_microwire_small:
+		nvm->type = e1000_nvm_eeprom_microwire;
+		eecd &= ~E1000_EECD_SIZE;
+		break;
+	default:
+		nvm->type = eecd & E1000_EECD_TYPE
+		            ? e1000_nvm_eeprom_spi
+		            : e1000_nvm_eeprom_microwire;
+		break;
+	}
+
+	if (nvm->type == e1000_nvm_eeprom_spi) {
+		nvm->address_bits       = (eecd & E1000_EECD_ADDR_BITS)
+		                          ? 16 : 8;
+		nvm->delay_usec         = 1;
+		nvm->opcode_bits        = 8;
+		nvm->page_size          = (eecd & E1000_EECD_ADDR_BITS)
+		                          ? 32 : 8;
+
+		/* Function Pointers */
+		func->acquire_nvm       = e1000_acquire_nvm_generic;
+		func->read_nvm          = e1000_read_nvm_spi;
+		func->release_nvm       = e1000_release_nvm_generic;
+		func->update_nvm        = e1000_update_nvm_checksum_generic;
+		func->valid_led_default = e1000_valid_led_default_generic;
+		func->validate_nvm      = e1000_validate_nvm_checksum_generic;
+		func->write_nvm         = e1000_write_nvm_spi;
+
+		/* nvm->word_size must be discovered after the pointers
+		 * are set so we can verify the size from the nvm image
+		 * itself.  Temporarily set it to a dummy value so the
+		 * read will work.
+		 */
+		nvm->word_size = 64;
+		ret_val = e1000_read_nvm(hw, NVM_CFG, 1, &size);
+		if (ret_val)
+			goto out;
+		size = (size & NVM_SIZE_MASK) >> NVM_SIZE_SHIFT;
+		/* if size != 0, it can be added to a constant and become
+		 * the left-shift value to set the word_size.  Otherwise,
+		 * word_size stays at 64.
+		 */
+		if (size) {
+			size += NVM_WORD_SIZE_BASE_SHIFT_82541;
+			nvm->word_size = 1 << size;
+		}
+	} else {
+		nvm->address_bits       = (eecd & E1000_EECD_ADDR_BITS)
+		                          ? 8 : 6;
+		nvm->delay_usec         = 50;
+		nvm->opcode_bits        = 3;
+		nvm->word_size          = (eecd & E1000_EECD_ADDR_BITS)
+		                          ? 256 : 64;
+
+		/* Function Pointers */
+		func->acquire_nvm       = e1000_acquire_nvm_generic;
+		func->read_nvm          = e1000_read_nvm_microwire;
+		func->release_nvm       = e1000_release_nvm_generic;
+		func->update_nvm        = e1000_update_nvm_checksum_generic;
+		func->valid_led_default = e1000_valid_led_default_generic;
+		func->validate_nvm      = e1000_validate_nvm_checksum_generic;
+		func->write_nvm         = e1000_write_nvm_microwire;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_mac_params_82541 - Init MAC func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_82541(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_init_mac_params_82541");
+
+	/* Set media type */
+	hw->media_type = e1000_media_type_copper;
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+	/* Set if part includes ASF firmware */
+	mac->asf_firmware_present = TRUE;
+
+	/* Function Pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_pci_generic;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_82541;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_82541;
+	/* link setup */
+	func->setup_link = e1000_setup_link_generic;
+	/* physical interface link setup */
+	func->setup_physical_interface = e1000_setup_copper_link_82541;
+	/* check for link */
+	func->check_for_link = e1000_check_for_link_82541;
+	/* link info */
+	func->get_link_up_info = e1000_get_link_up_info_82541;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_generic;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* setup LED */
+	func->setup_led = e1000_setup_led_82541;
+	/* cleanup LED */
+	func->cleanup_led = e1000_cleanup_led_82541;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_generic;
+	func->led_off = e1000_led_off_generic;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_82541;
+
+	hw->dev_spec_size = sizeof(struct e1000_dev_spec_82541);
+
+	/* Device-specific structure allocation */
+	ret_val = e1000_alloc_zeroed_dev_spec_struct(hw, hw->dev_spec_size);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_82541 - Init func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  The only function explicitly called by the api module to initialize
+ *  all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_82541(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_82541");
+
+	hw->func.init_mac_params = e1000_init_mac_params_82541;
+	hw->func.init_nvm_params = e1000_init_nvm_params_82541;
+	hw->func.init_phy_params = e1000_init_phy_params_82541;
+}
+
+/**
+ *  e1000_reset_hw_82541 - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_82541(struct e1000_hw *hw)
+{
+	u32 ledctl, ctrl, icr, manc;
+
+	DEBUGFUNC("e1000_reset_hw_82541");
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xFFFFFFFF);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	/* Delay to allow any outstanding PCI transactions to complete
+	 * before resetting the device.
+	 */
+	msec_delay(10);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Must reset the Phy before resetting the MAC */
+	if ((hw->mac.type == e1000_82541) || (hw->mac.type == e1000_82547)) {
+		E1000_WRITE_REG(hw, E1000_CTRL, (ctrl | E1000_CTRL_PHY_RST));
+		msec_delay(5);
+	}
+
+	DEBUGOUT("Issuing a global reset to 82541/82547 MAC\n");
+	switch (hw->mac.type) {
+	case e1000_82541:
+	case e1000_82541_rev_2:
+		/* These controllers can't ack the 64-bit write when
+		 * issuing the reset, so we use IO-mapping as a
+		 * workaround to issue the reset.
+		 */
+		E1000_WRITE_REG_IO(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+		break;
+	default:
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+		break;
+	}
+
+	/* Wait for NVM reload */
+	msec_delay(20);
+
+	/* Disable HW ARPs on ASF enabled adapters */
+	manc = E1000_READ_REG(hw, E1000_MANC);
+	manc &= ~E1000_MANC_ARP_EN;
+	E1000_WRITE_REG(hw, E1000_MANC, manc);
+
+	if ((hw->mac.type == e1000_82541) || (hw->mac.type == e1000_82547)) {
+		e1000_phy_init_script_82541(hw);
+
+		/* Configure activity LED after Phy reset */
+		ledctl = E1000_READ_REG(hw, E1000_LEDCTL);
+		ledctl &= IGP_ACTIVITY_LED_MASK;
+		ledctl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
+		E1000_WRITE_REG(hw, E1000_LEDCTL, ledctl);
+	}
+
+	/* Once again, mask the interrupts */
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xFFFFFFFF);
+
+	/* Clear any pending interrupt events. */
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_init_hw_82541 - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_hw_82541(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 i, txdctl;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_init_hw_82541");
+
+	/* Initialize identification LED */
+	ret_val = e1000_id_led_init_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error initializing identification LED\n");
+		goto out;
+	}
+
+	/* Disabling VLAN filtering */
+	DEBUGOUT("Initializing the IEEE VLAN\n");
+	e1000_clear_vfta(hw);
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+		/* Avoid back to back register writes by adding the register
+		 * read (flush).  This is to protect against some strange
+		 * bridge configurations that may issue Memory Write Block
+		 * (MWB) to our register space.
+		 */
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	txdctl = E1000_READ_REG(hw, E1000_TXDCTL);
+	txdctl = (txdctl & ~E1000_TXDCTL_WTHRESH) |
+	         E1000_TXDCTL_FULL_TX_DESC_WB;
+	E1000_WRITE_REG(hw, E1000_TXDCTL, txdctl);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_82541(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ * e1000_get_link_up_info_82541 - Report speed and duplex
+ * @hw: pointer to the HW structure
+ * @speed: pointer to speed buffer
+ * @duplex: pointer to duplex buffer
+ *
+ * Retrieve the current speed and duplex configuration.
+ * This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_get_link_up_info_82541(struct e1000_hw *hw, u16 *speed,
+                                        u16 *duplex)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_get_link_up_info_82541");
+
+	ret_val = e1000_get_speed_and_duplex_copper_generic(hw, speed, duplex);
+	if (ret_val)
+		goto out;
+
+	if (!phy->speed_downgraded)
+		goto out;
+
+	/* IGP01 PHY may advertise full duplex operation after speed
+	 * downgrade even if it is operating at half duplex.
+	 * Here we set the duplex settings to match the duplex in the
+	 * link partner's capabilities.
+	 */
+	ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_EXP, &data);
+	if (ret_val)
+		goto out;
+
+	if (!(data & NWAY_ER_LP_NWAY_CAPS))
+		*duplex = HALF_DUPLEX;
+	else {
+		ret_val = e1000_read_phy_reg(hw, PHY_LP_ABILITY, &data);
+		if (ret_val)
+			goto out;
+
+		if (*speed == SPEED_100) {
+			if (!(data & NWAY_LPAR_100TX_FD_CAPS))
+				*duplex = HALF_DUPLEX;
+		} else if (*speed == SPEED_10) {
+			if (!(data & NWAY_LPAR_10T_FD_CAPS))
+				*duplex = HALF_DUPLEX;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_hw_reset_82541 - PHY hardware reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Verify the reset block is not blocking us from resetting.  Acquire
+ *  semaphore (if necessary) and read/set/write the device control reset
+ *  bit in the PHY.  Wait the appropriate delay time for the device to
+ *  reset and relase the semaphore (if necessary).
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_phy_hw_reset_82541(struct e1000_hw *hw)
+{
+	s32 ret_val;
+	u32 ledctl;
+
+	DEBUGFUNC("e1000_phy_hw_reset_82541");
+
+	ret_val = e1000_phy_hw_reset_generic(hw);
+	if (ret_val)
+		goto out;
+
+	e1000_phy_init_script_82541(hw);
+
+	if ((hw->mac.type == e1000_82541) || (hw->mac.type == e1000_82547)) {
+		/* Configure activity LED after PHY reset */
+		ledctl = E1000_READ_REG(hw, E1000_LEDCTL);
+		ledctl &= IGP_ACTIVITY_LED_MASK;
+		ledctl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
+		E1000_WRITE_REG(hw, E1000_LEDCTL, ledctl);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_82541 - Configure copper link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the appropriate function to configure the link for auto-neg or forced
+ *  speed and duplex.  Then we check for link, once link is established calls
+ *  to configure collision distance and flow control are called.  If link is
+ *  not established, we return -E1000_ERR_PHY (-2).  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_setup_copper_link_82541(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_dev_spec_82541 *dev_spec;
+	s32  ret_val;
+	u32 ctrl, ledctl;
+
+	DEBUGFUNC("e1000_setup_copper_link_82541");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_SLU;
+	ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	hw->phy.reset_disable = FALSE;
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	/* Earlier revs of the IGP phy require us to force MDI. */
+	if (hw->mac.type == e1000_82541 || hw->mac.type == e1000_82547) {
+		dev_spec->dsp_config = e1000_dsp_config_disabled;
+		phy->mdix = 1;
+	} else {
+		dev_spec->dsp_config = e1000_dsp_config_enabled;
+	}
+
+	ret_val = e1000_copper_link_setup_igp(hw);
+	if (ret_val)
+		goto out;
+
+	if (hw->mac.autoneg) {
+		if (dev_spec->ffe_config == e1000_ffe_config_active)
+			dev_spec->ffe_config = e1000_ffe_config_enabled;
+	}
+
+	/* Configure activity LED after Phy reset */
+	ledctl = E1000_READ_REG(hw, E1000_LEDCTL);
+	ledctl &= IGP_ACTIVITY_LED_MASK;
+	ledctl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
+	E1000_WRITE_REG(hw, E1000_LEDCTL, ledctl);
+
+	ret_val = e1000_setup_copper_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_for_link_82541 - Check/Store link connection
+ *  @hw: pointer to the HW structure
+ *
+ *  This checks the link condition of the adapter and stores the
+ *  results in the hw->mac structure. This is a function pointer entry
+ *  point called by the api module.
+ **/
+static s32 e1000_check_for_link_82541(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_check_for_link_82541");
+
+	/* We only want to go out to the PHY registers to see if Auto-Neg
+	 * has completed and/or if our link status has changed.  The
+	 * get_link_status flag is set upon receiving a Link Status
+	 * Change or Rx Sequence Error interrupt.
+	 */
+	if (!mac->get_link_status) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	/* First we want to see if the MII Status Register reports
+	 * link.  If so, then we want to get the current speed/duplex
+	 * of the PHY.
+	 */
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link) {
+		ret_val = e1000_config_dsp_after_link_change_82541(hw, FALSE);
+		goto out; /* No link detected */
+	}
+
+	mac->get_link_status = FALSE;
+
+	/* Check if there was DownShift, must be checked
+	 * immediately after link-up */
+	e1000_check_downshift_generic(hw);
+
+	/* If we are forcing speed/duplex, then we simply return since
+	 * we have already determined whether we have link or not.
+	 */
+	if (!mac->autoneg) {
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	ret_val = e1000_config_dsp_after_link_change_82541(hw, TRUE);
+
+	/* Auto-Neg is enabled.  Auto Speed Detection takes care
+	 * of MAC speed/duplex configuration.  So we only need to
+	 * configure Collision Distance in the MAC.
+	 */
+	e1000_config_collision_dist_generic(hw);
+
+	/* Configure Flow Control now that Auto-Neg has completed.
+	 * First, we need to restore the desired flow control
+	 * settings because we may have had to re-autoneg with a
+	 * different link partner.
+	 */
+	ret_val = e1000_config_fc_after_link_up_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error configuring flow control\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_config_dsp_after_link_change_82541 - Config DSP after link
+ *  @hw: pointer to the HW structure
+ *  @link_up: boolean flag for link up status
+ *
+ *  Return E1000_ERR_PHY when failing to read/write the PHY, else E1000_SUCCESS
+ *  at any other case.
+ *
+ *  82541_rev_2 & 82547_rev_2 have the capability to configure the DSP when a
+ *  gigabit link is achieved to improve link quality.
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_config_dsp_after_link_change_82541(struct e1000_hw *hw,
+                                                    boolean_t link_up)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_dev_spec_82541 *dev_spec;
+	s32 ret_val;
+	u32 idle_errs = 0;
+	u16 phy_data, phy_saved_data, speed, duplex, i;
+	u16 ffe_idle_err_timeout = FFE_IDLE_ERR_COUNT_TIMEOUT_20;
+	u16 dsp_reg_array[IGP01E1000_PHY_CHANNEL_NUM] =
+	                                           {IGP01E1000_PHY_AGC_PARAM_A,
+	                                            IGP01E1000_PHY_AGC_PARAM_B,
+	                                            IGP01E1000_PHY_AGC_PARAM_C,
+	                                            IGP01E1000_PHY_AGC_PARAM_D};
+
+	DEBUGFUNC("e1000_config_dsp_after_link_change_82541");
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	if (link_up) {
+		ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
+		if (ret_val) {
+			DEBUGOUT("Error getting link speed and duplex\n");
+			goto out;
+		}
+
+		if (speed != SPEED_1000) {
+			ret_val = E1000_SUCCESS;
+			goto out;
+		}
+
+		ret_val = e1000_get_cable_length(hw);
+		if (ret_val)
+			goto out;
+
+		if ((dev_spec->dsp_config == e1000_dsp_config_enabled) &&
+		    phy->min_cable_length >= 50) {
+
+			for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+				ret_val = e1000_read_phy_reg(hw,
+				                            dsp_reg_array[i],
+				                            &phy_data);
+				if (ret_val)
+					goto out;
+
+				phy_data &= ~IGP01E1000_PHY_EDAC_MU_INDEX;
+
+				ret_val = e1000_write_phy_reg(hw,
+				                             dsp_reg_array[i],
+				                             phy_data);
+				if (ret_val)
+					goto out;
+			}
+			dev_spec->dsp_config = e1000_dsp_config_activated;
+		}
+
+		if ((dev_spec->ffe_config != e1000_ffe_config_enabled) ||
+		    (phy->min_cable_length >= 50)) {
+			ret_val = E1000_SUCCESS;
+			goto out;
+		}
+
+		/* clear previous idle error counts */
+		ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_data);
+		if (ret_val)
+			goto out;
+
+		for (i = 0; i < ffe_idle_err_timeout; i++) {
+			usec_delay(1000);
+			ret_val = e1000_read_phy_reg(hw,
+			                            PHY_1000T_STATUS,
+			                            &phy_data);
+			if (ret_val)
+				goto out;
+
+			idle_errs += (phy_data & SR_1000T_IDLE_ERROR_CNT);
+			if (idle_errs > SR_1000T_PHY_EXCESSIVE_IDLE_ERR_COUNT) {
+				dev_spec->ffe_config = e1000_ffe_config_active;
+
+				ret_val = e1000_write_phy_reg(hw,
+				                  IGP01E1000_PHY_DSP_FFE,
+				                  IGP01E1000_PHY_DSP_FFE_CM_CP);
+				if (ret_val)
+					goto out;
+				break;
+			}
+
+			if (idle_errs)
+				ffe_idle_err_timeout =
+				                 FFE_IDLE_ERR_COUNT_TIMEOUT_100;
+		}
+	} else {
+		if (dev_spec->dsp_config == e1000_dsp_config_activated) {
+			/* Save off the current value of register 0x2F5B
+			 * to be restored at the end of the routines. */
+			ret_val = e1000_read_phy_reg(hw,
+			                            0x2F5B,
+			                            &phy_saved_data);
+			if (ret_val)
+				goto out;
+
+			/* Disable the PHY transmitter */
+			ret_val = e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+			if (ret_val)
+				goto out;
+
+			msec_delay_irq(20);
+
+			ret_val = e1000_write_phy_reg(hw,
+			                             0x0000,
+			                             IGP01E1000_IEEE_FORCE_GIG);
+			if (ret_val)
+				goto out;
+			for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+				ret_val = e1000_read_phy_reg(hw,
+				                            dsp_reg_array[i],
+				                            &phy_data);
+				if (ret_val)
+					goto out;
+
+				phy_data &= ~IGP01E1000_PHY_EDAC_MU_INDEX;
+				phy_data |= IGP01E1000_PHY_EDAC_SIGN_EXT_9_BITS;
+
+				ret_val = e1000_write_phy_reg(hw,
+				                             dsp_reg_array[i],
+				                             phy_data);
+				if (ret_val)
+					goto out;
+			}
+
+			ret_val = e1000_write_phy_reg(hw,
+			                       0x0000,
+			                       IGP01E1000_IEEE_RESTART_AUTONEG);
+			if (ret_val)
+				goto out;
+
+			msec_delay_irq(20);
+
+			/* Now enable the transmitter */
+			ret_val = e1000_write_phy_reg(hw,
+			                             0x2F5B,
+			                             phy_saved_data);
+			if (ret_val)
+				goto out;
+
+			dev_spec->dsp_config = e1000_dsp_config_enabled;
+		}
+
+		if (dev_spec->ffe_config != e1000_ffe_config_active) {
+			ret_val = E1000_SUCCESS;
+			goto out;
+		}
+
+		/* Save off the current value of register 0x2F5B
+		 * to be restored at the end of the routines. */
+		ret_val = e1000_read_phy_reg(hw, 0x2F5B, &phy_saved_data);
+		if (ret_val)
+			goto out;
+
+		/* Disable the PHY transmitter */
+		ret_val = e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+		if (ret_val)
+			goto out;
+
+		msec_delay_irq(20);
+
+		ret_val = e1000_write_phy_reg(hw,
+		                             0x0000,
+		                             IGP01E1000_IEEE_FORCE_GIG);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_write_phy_reg(hw,
+		                             IGP01E1000_PHY_DSP_FFE,
+		                             IGP01E1000_PHY_DSP_FFE_DEFAULT);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_write_phy_reg(hw,
+		                             0x0000,
+		                             IGP01E1000_IEEE_RESTART_AUTONEG);
+		if (ret_val)
+			goto out;
+
+		msec_delay_irq(20);
+
+		/* Now enable the transmitter */
+		ret_val = e1000_write_phy_reg(hw, 0x2F5B, phy_saved_data);
+
+		if (ret_val)
+			goto out;
+
+		dev_spec->ffe_config = e1000_ffe_config_enabled;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cable_length_igp_82541 - Determine cable length for igp PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  The automatic gain control (agc) normalizes the amplitude of the
+ *  received signal, adjusting for the attenuation produced by the
+ *  cable.  By reading the AGC registers, which reperesent the
+ *  cobination of course and fine gain value, the value can be put
+ *  into a lookup table to obtain the approximate cable length
+ *  for each channel.  This is a function pointer entry point called by the
+ *  api module.
+ **/
+static s32 e1000_get_cable_length_igp_82541(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i, data;
+	u16 cur_agc_value, agc_value = 0;
+	u16 min_agc_value = IGP01E1000_AGC_LENGTH_TABLE_SIZE;
+	u16 agc_reg_array[IGP01E1000_PHY_CHANNEL_NUM] =
+	                                                 {IGP01E1000_PHY_AGC_A,
+	                                                  IGP01E1000_PHY_AGC_B,
+	                                                  IGP01E1000_PHY_AGC_C,
+	                                                  IGP01E1000_PHY_AGC_D};
+
+	DEBUGFUNC("e1000_get_cable_length_igp_82541");
+
+	/* Read the AGC registers for all channels */
+	for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+		ret_val = e1000_read_phy_reg(hw, agc_reg_array[i], &data);
+		if (ret_val)
+			goto out;
+
+		cur_agc_value = data >> IGP01E1000_AGC_LENGTH_SHIFT;
+
+		/* Bounds checking */
+		if ((cur_agc_value >= IGP01E1000_AGC_LENGTH_TABLE_SIZE - 1) ||
+		    (cur_agc_value == 0)) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+
+		agc_value += cur_agc_value;
+
+		if (min_agc_value > cur_agc_value)
+			min_agc_value = cur_agc_value;
+	}
+
+	/* Remove the minimal AGC result for length < 50m */
+	if (agc_value < IGP01E1000_PHY_CHANNEL_NUM * 50) {
+		agc_value -= min_agc_value;
+		/* Average the three remaining channels for the length. */
+		agc_value /= (IGP01E1000_PHY_CHANNEL_NUM - 1);
+	} else {
+		/* Average the channels for the length. */
+		agc_value /= IGP01E1000_PHY_CHANNEL_NUM;
+	}
+
+	phy->min_cable_length = (e1000_igp_cable_length_table[agc_value] >
+	                         IGP01E1000_AGC_RANGE)
+	                        ? (e1000_igp_cable_length_table[agc_value] -
+	                           IGP01E1000_AGC_RANGE)
+	                        : 0;
+	phy->max_cable_length = e1000_igp_cable_length_table[agc_value] +
+	                        IGP01E1000_AGC_RANGE;
+
+	phy->cable_length = (phy->min_cable_length + phy->max_cable_length) / 2;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_d3_lplu_state_82541 - Sets low power link up state for D3
+ *  @hw: pointer to the HW structure
+ *  @active: boolean used to enable/disable lplu
+ *
+ *  Success returns 0, Failure returns 1
+ *
+ *  The low power link up (lplu) state is set to the power management level D3
+ *  and SmartSpeed is disabled when active is true, else clear lplu for D3
+ *  and enable Smartspeed.  LPLU and Smartspeed are mutually exclusive.  LPLU
+ *  is used during Dx states where the power conservation is most important.
+ *  During driver activity, SmartSpeed should be enabled so performance is
+ *  maintained.  This is a function pointer entry point called by the
+ *  api module.
+ **/
+static s32 e1000_set_d3_lplu_state_82541(struct e1000_hw *hw, boolean_t active)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_set_d3_lplu_state_82541");
+
+	switch (hw->mac.type) {
+	case e1000_82541_rev_2:
+	case e1000_82547_rev_2:
+		break;
+	default:
+		ret_val = e1000_set_d3_lplu_state_generic(hw, active);
+		goto out;
+		break;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, IGP01E1000_GMII_FIFO, &data);
+	if (ret_val)
+		goto out;
+
+	if (!active) {
+		data &= ~IGP01E1000_GMII_FLEX_SPD;
+		ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, data);
+		if (ret_val)
+			goto out;
+
+		/* LPLU and SmartSpeed are mutually exclusive.  LPLU is used
+		 * during Dx states where the power conservation is most
+		 * important.  During driver activity we should enable
+		 * SmartSpeed, so performance is maintained. */
+		if (phy->smart_speed == e1000_smart_speed_on) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data |= IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		} else if (phy->smart_speed == e1000_smart_speed_off) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		}
+	} else if ((phy->autoneg_advertised == E1000_ALL_SPEED_DUPLEX) ||
+	           (phy->autoneg_advertised == E1000_ALL_NOT_GIG) ||
+	           (phy->autoneg_advertised == E1000_ALL_10_SPEED)) {
+		data |= IGP01E1000_GMII_FLEX_SPD;
+		ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, data);
+		if (ret_val)
+			goto out;
+
+		/* When LPLU is enabled, we should disable SmartSpeed */
+		ret_val = e1000_read_phy_reg(hw,
+		                            IGP01E1000_PHY_PORT_CONFIG,
+		                            &data);
+		if (ret_val)
+			goto out;
+
+		data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+		ret_val = e1000_write_phy_reg(hw,
+		                             IGP01E1000_PHY_PORT_CONFIG,
+		                             data);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_led_82541 - Configures SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  This prepares the SW controllable LED for use and saves the current state
+ *  of the LED so it can be later restored.  This is a function pointer entry
+ *  point called by the api module.
+ **/
+static s32 e1000_setup_led_82541(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82541 *dev_spec;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_setup_led_82541");
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	ret_val = e1000_read_phy_reg(hw,
+	                            IGP01E1000_GMII_FIFO,
+	                            &dev_spec->spd_default);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw,
+	                             IGP01E1000_GMII_FIFO,
+	                             (u16)(dev_spec->spd_default &
+	                                        ~IGP01E1000_GMII_SPD));
+	if (ret_val)
+		goto out;
+
+	E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode1);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_cleanup_led_82541 - Set LED config to default operation
+ *  @hw: pointer to the HW structure
+ *
+ *  Remove the current LED configuration and set the LED configuration
+ *  to the default value, saved from the EEPROM.  This is a function pointer
+ *  entry point called by the api module.
+ **/
+static s32 e1000_cleanup_led_82541(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82541 *dev_spec;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_cleanup_led_82541");
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	ret_val = e1000_write_phy_reg(hw,
+	                             IGP01E1000_GMII_FIFO,
+	                             dev_spec->spd_default);
+	if (ret_val)
+		goto out;
+
+	E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_default);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_init_script_82541 - Initialize GbE PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Initializes the IGP PHY.
+ **/
+static s32 e1000_phy_init_script_82541(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82541 *dev_spec;
+	u32 ret_val;
+	u16 phy_saved_data;
+
+	DEBUGFUNC("e1000_phy_init_script_82541");
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	if (!dev_spec->phy_init_script) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	/* Delay after phy reset to enable NVM configuration to load */
+	msec_delay(20);
+
+	/* Save off the current value of register 0x2F5B to be restored at
+	 * the end of this routine. */
+	ret_val = e1000_read_phy_reg(hw, 0x2F5B, &phy_saved_data);
+
+	/* Disabled the PHY transmitter */
+	e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+
+	msec_delay(20);
+
+	e1000_write_phy_reg(hw, 0x0000, 0x0140);
+
+	msec_delay(5);
+
+	switch (hw->mac.type) {
+	case e1000_82541:
+	case e1000_82547:
+		e1000_write_phy_reg(hw, 0x1F95, 0x0001);
+
+		e1000_write_phy_reg(hw, 0x1F71, 0xBD21);
+
+		e1000_write_phy_reg(hw, 0x1F79, 0x0018);
+
+		e1000_write_phy_reg(hw, 0x1F30, 0x1600);
+
+		e1000_write_phy_reg(hw, 0x1F31, 0x0014);
+
+		e1000_write_phy_reg(hw, 0x1F32, 0x161C);
+
+		e1000_write_phy_reg(hw, 0x1F94, 0x0003);
+
+		e1000_write_phy_reg(hw, 0x1F96, 0x003F);
+
+		e1000_write_phy_reg(hw, 0x2010, 0x0008);
+		break;
+	case e1000_82541_rev_2:
+	case e1000_82547_rev_2:
+		e1000_write_phy_reg(hw, 0x1F73, 0x0099);
+		break;
+	default:
+		break;
+	}
+
+	e1000_write_phy_reg(hw, 0x0000, 0x3300);
+
+	msec_delay(20);
+
+	/* Now enable the transmitter */
+	e1000_write_phy_reg(hw, 0x2F5B, phy_saved_data);
+
+	if (hw->mac.type == e1000_82547) {
+		u16 fused, fine, coarse;
+
+		/* Move to analog registers page */
+		e1000_read_phy_reg(hw,
+		                  IGP01E1000_ANALOG_SPARE_FUSE_STATUS,
+		                  &fused);
+
+		if (!(fused & IGP01E1000_ANALOG_SPARE_FUSE_ENABLED)) {
+			e1000_read_phy_reg(hw,
+			                  IGP01E1000_ANALOG_FUSE_STATUS,
+			                  &fused);
+
+			fine = fused & IGP01E1000_ANALOG_FUSE_FINE_MASK;
+			coarse = fused & IGP01E1000_ANALOG_FUSE_COARSE_MASK;
+
+			if (coarse > IGP01E1000_ANALOG_FUSE_COARSE_THRESH) {
+				coarse -= IGP01E1000_ANALOG_FUSE_COARSE_10;
+				fine -= IGP01E1000_ANALOG_FUSE_FINE_1;
+			} else if (coarse ==
+			           IGP01E1000_ANALOG_FUSE_COARSE_THRESH)
+				fine -= IGP01E1000_ANALOG_FUSE_FINE_10;
+
+			fused = (fused & IGP01E1000_ANALOG_FUSE_POLY_MASK) |
+			        (fine & IGP01E1000_ANALOG_FUSE_FINE_MASK) |
+			        (coarse & IGP01E1000_ANALOG_FUSE_COARSE_MASK);
+
+			e1000_write_phy_reg(hw,
+			                   IGP01E1000_ANALOG_FUSE_CONTROL,
+			                   fused);
+			e1000_write_phy_reg(hw,
+			              IGP01E1000_ANALOG_FUSE_BYPASS,
+			              IGP01E1000_ANALOG_FUSE_ENABLE_SW_CONTROL);
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_script_state_82541 - Enable/Disable PHY init script
+ *  @hw: pointer to the HW structure
+ *  @state: boolean value used to enable/disable PHY init script
+ *
+ *  Allows the driver to enable/disable the PHY init script, if the PHY is an
+ *  IGP PHY.  This is a function pointer entry point called by the api module.
+ **/
+void e1000_init_script_state_82541(struct e1000_hw *hw, boolean_t state)
+{
+	struct e1000_dev_spec_82541 *dev_spec;
+
+	DEBUGFUNC("e1000_init_script_state_82541");
+
+	if (hw->phy.type != e1000_phy_igp) {
+		DEBUGOUT("Initialization script not necessary.\n");
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_82541 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		goto out;
+	}
+
+	dev_spec->phy_init_script = state;
+
+out:
+	return;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_82541 - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_82541(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_82541");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+
+	temp = E1000_READ_REG(hw, E1000_MGTPRC);
+	temp = E1000_READ_REG(hw, E1000_MGTPDC);
+	temp = E1000_READ_REG(hw, E1000_MGTPTC);
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82541.h linux-2.6.9/drivers/net/e1000/e1000_82541.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_82541.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82541.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,84 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_82541_H_
+#define _E1000_82541_H_
+
+#define NVM_WORD_SIZE_BASE_SHIFT_82541 (NVM_WORD_SIZE_BASE_SHIFT + 1)
+
+#define IGP01E1000_PHY_CHANNEL_NUM                    4
+
+#define IGP01E1000_PHY_AGC_A                     0x1172
+#define IGP01E1000_PHY_AGC_B                     0x1272
+#define IGP01E1000_PHY_AGC_C                     0x1472
+#define IGP01E1000_PHY_AGC_D                     0x1872
+
+#define IGP01E1000_PHY_AGC_PARAM_A               0x1171
+#define IGP01E1000_PHY_AGC_PARAM_B               0x1271
+#define IGP01E1000_PHY_AGC_PARAM_C               0x1471
+#define IGP01E1000_PHY_AGC_PARAM_D               0x1871
+
+#define IGP01E1000_PHY_EDAC_MU_INDEX             0xC000
+#define IGP01E1000_PHY_EDAC_SIGN_EXT_9_BITS      0x8000
+
+#define IGP01E1000_PHY_DSP_RESET                 0x1F33
+
+#define IGP01E1000_PHY_DSP_FFE                   0x1F35
+#define IGP01E1000_PHY_DSP_FFE_CM_CP             0x0069
+#define IGP01E1000_PHY_DSP_FFE_DEFAULT           0x002A
+
+#define IGP01E1000_IEEE_FORCE_GIG                0x0140
+#define IGP01E1000_IEEE_RESTART_AUTONEG          0x3300
+
+#define IGP01E1000_AGC_LENGTH_SHIFT                   7
+#define IGP01E1000_AGC_RANGE                         10
+
+#define FFE_IDLE_ERR_COUNT_TIMEOUT_20                20
+#define FFE_IDLE_ERR_COUNT_TIMEOUT_100              100
+
+#define IGP01E1000_ANALOG_FUSE_STATUS            0x20D0
+#define IGP01E1000_ANALOG_SPARE_FUSE_STATUS      0x20D1
+#define IGP01E1000_ANALOG_FUSE_CONTROL           0x20DC
+#define IGP01E1000_ANALOG_FUSE_BYPASS            0x20DE
+
+#define IGP01E1000_ANALOG_SPARE_FUSE_ENABLED     0x0100
+#define IGP01E1000_ANALOG_FUSE_FINE_MASK         0x0F80
+#define IGP01E1000_ANALOG_FUSE_COARSE_MASK       0x0070
+#define IGP01E1000_ANALOG_FUSE_COARSE_THRESH     0x0040
+#define IGP01E1000_ANALOG_FUSE_COARSE_10         0x0010
+#define IGP01E1000_ANALOG_FUSE_FINE_1            0x0080
+#define IGP01E1000_ANALOG_FUSE_FINE_10           0x0500
+#define IGP01E1000_ANALOG_FUSE_POLY_MASK         0xF000
+#define IGP01E1000_ANALOG_FUSE_ENABLE_SW_CONTROL 0x0002
+
+#define IGP01E1000_MSE_CHANNEL_D                 0x000F
+#define IGP01E1000_MSE_CHANNEL_C                 0x00F0
+#define IGP01E1000_MSE_CHANNEL_B                 0x0F00
+#define IGP01E1000_MSE_CHANNEL_A                 0xF000
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82542.c linux-2.6.9/drivers/net/e1000/e1000_82542.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_82542.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82542.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,539 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_82542 (rev 1 & 2)
+ */
+
+#include "e1000_api.h"
+
+void e1000_init_function_pointers_82542(struct e1000_hw *hw);
+
+static s32  e1000_init_phy_params_82542(struct e1000_hw *hw);
+static s32  e1000_init_nvm_params_82542(struct e1000_hw *hw);
+static s32  e1000_init_mac_params_82542(struct e1000_hw *hw);
+static s32  e1000_get_bus_info_82542(struct e1000_hw *hw);
+static s32  e1000_reset_hw_82542(struct e1000_hw *hw);
+static s32  e1000_init_hw_82542(struct e1000_hw *hw);
+static s32  e1000_setup_link_82542(struct e1000_hw *hw);
+static s32  e1000_led_on_82542(struct e1000_hw *hw);
+static s32  e1000_led_off_82542(struct e1000_hw *hw);
+static void e1000_clear_hw_cntrs_82542(struct e1000_hw *hw);
+
+struct e1000_dev_spec_82542 {
+	boolean_t dma_fairness;
+};
+
+/**
+ *  e1000_init_phy_params_82542 - Init PHY func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_82542(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_phy_params_82542");
+
+	phy->type               = e1000_phy_none;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_82542 - Init NVM func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_82542(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+
+	DEBUGFUNC("e1000_init_nvm_params_82542");
+
+	nvm->address_bits       =  6;
+	nvm->delay_usec         = 50;
+	nvm->opcode_bits        =  3;
+	nvm->type               = e1000_nvm_eeprom_microwire;
+	nvm->word_size          = 64;
+
+	/* Function Pointers */
+	func->read_nvm          = e1000_read_nvm_microwire;
+	func->release_nvm       = e1000_stop_nvm;
+	func->write_nvm         = e1000_write_nvm_microwire;
+	func->update_nvm        = e1000_update_nvm_checksum_generic;
+	func->validate_nvm      = e1000_validate_nvm_checksum_generic;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_init_mac_params_82542 - Init MAC func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_82542(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_mac_params_82542");
+
+	/* Set media type */
+	hw->media_type = e1000_media_type_fiber;
+
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_82542;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_82542;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_82542;
+	/* link setup */
+	func->setup_link = e1000_setup_link_82542;
+	/* phy/fiber/serdes setup */
+	func->setup_physical_interface = e1000_setup_fiber_serdes_link_generic;
+	/* check for link */
+	func->check_for_link = e1000_check_for_fiber_link_generic;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_generic;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_82542;
+	func->led_off = e1000_led_off_82542;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_82542;
+	/* link info */
+	func->get_link_up_info = e1000_get_speed_and_duplex_fiber_serdes_generic;
+
+	hw->dev_spec_size = sizeof(struct e1000_dev_spec_82542);
+
+	/* Device-specific structure allocation */
+	ret_val = e1000_alloc_zeroed_dev_spec_struct(hw, hw->dev_spec_size);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_82542 - Init func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  The only function explicitly called by the api module to initialize
+ *  all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_82542(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_82542");
+
+	hw->func.init_mac_params = e1000_init_mac_params_82542;
+	hw->func.init_nvm_params = e1000_init_nvm_params_82542;
+	hw->func.init_phy_params = e1000_init_phy_params_82542;
+}
+
+/**
+ *  e1000_get_bus_info_82542 - Obtain bus information for adapter
+ *  @hw: pointer to the HW structure
+ *
+ *  This will obtain information about the HW bus for which the
+ *  adaper is attached and stores it in the hw structure.  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_get_bus_info_82542(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_get_bus_info_82542");
+
+	hw->bus.type = e1000_bus_type_pci;
+	hw->bus.speed = e1000_bus_speed_unknown;
+	hw->bus.width = e1000_bus_width_unknown;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_reset_hw_82542 - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_82542(struct e1000_hw *hw)
+{
+	struct e1000_bus_info *bus = &hw->bus;
+	s32 ret_val = E1000_SUCCESS;
+	u32 ctrl, icr;
+
+	DEBUGFUNC("e1000_reset_hw_82542");
+
+	if (hw->revision_id == E1000_REVISION_2) {
+		DEBUGOUT("Disabling MWI on 82542 rev 2\n");
+		e1000_pci_clear_mwi(hw);
+	}
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	/* Delay to allow any outstanding PCI transactions to complete before
+	 * resetting the device
+	 */
+	msec_delay(10);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGOUT("Issuing a global reset to 82542/82543 MAC\n");
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+
+	e1000_reload_nvm(hw);
+	msec_delay(2);
+
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+	if (hw->revision_id == E1000_REVISION_2) {
+		if (bus->pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
+			e1000_pci_set_mwi(hw);
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_82542 - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_hw_82542(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_dev_spec_82542 *dev_spec;
+	s32 ret_val = E1000_SUCCESS;
+	u32 ctrl;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_hw_82542");
+
+	dev_spec = (struct e1000_dev_spec_82542 *)hw->dev_spec;
+
+	/* Disabling VLAN filtering */
+	E1000_WRITE_REG(hw, E1000_VET, 0);
+	e1000_clear_vfta(hw);
+
+	/* For 82542 (rev 2.0), disable MWI and put the receiver into reset */
+	if (hw->revision_id == E1000_REVISION_2) {
+		DEBUGOUT("Disabling MWI on 82542 rev 2.0\n");
+		e1000_pci_clear_mwi(hw);
+		E1000_WRITE_REG(hw, E1000_RCTL, E1000_RCTL_RST);
+		E1000_WRITE_FLUSH(hw);
+		msec_delay(5);
+	}
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* For 82542 (rev 2.0), take the receiver out of reset and enable MWI */
+	if (hw->revision_id == E1000_REVISION_2) {
+		E1000_WRITE_REG(hw, E1000_RCTL, 0);
+		E1000_WRITE_FLUSH(hw);
+		msec_delay(1);
+		if (hw->bus.pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
+			e1000_pci_set_mwi(hw);
+	}
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++)
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+
+	/* Set the PCI priority bit correctly in the CTRL register.  This
+	 * determines if the adapter gives priority to receives, or if it
+	 * gives equal priority to transmits and receives.
+	 */
+	if (dev_spec->dma_fairness) {
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_PRIOR);
+	}
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link_82542(hw);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_82542(hw);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_link_82542 - Setup flow control and link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines which flow control settings to use, then configures flow
+ *  control.  Calls the appropriate media-specific link configuration
+ *  function.  Assuming the adapter has a valid link partner, a valid link
+ *  should be established.  Assumes the hardware has previously been reset
+ *  and the transmitter and receiver are not enabled.  This is a function
+ *  pointer entry point called by the api module.
+ **/
+static s32 e1000_setup_link_82542(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_link_82542");
+
+	ret_val = e1000_set_default_fc_generic(hw);
+	if (ret_val)
+		goto out;
+
+	mac->fc &= ~e1000_fc_tx_pause;
+
+	if (mac->report_tx_early == 1)
+		mac->fc &= ~e1000_fc_rx_pause;
+
+	/* We want to save off the original Flow Control configuration just in
+	 * case we get disconnected and then reconnected into a different hub
+	 * or switch with different Flow Control capabilities.
+	 */
+	mac->original_fc = mac->fc;
+
+	DEBUGOUT1("After fix-ups FlowControl is now = %x\n", mac->fc);
+
+	/* Call the necessary subroutine to configure the link. */
+	ret_val = func->setup_physical_interface(hw);
+	if (ret_val)
+		goto out;
+
+	/* Initialize the flow control address, type, and PAUSE timer
+	 * registers to their default values.  This is done even if flow
+	 * control is disabled, because it does not hurt anything to
+	 * initialize these registers.
+	 */
+	DEBUGOUT("Initializing Flow Control address, type and timer regs\n");
+
+	E1000_WRITE_REG(hw, E1000_FCAL, FLOW_CONTROL_ADDRESS_LOW);
+	E1000_WRITE_REG(hw, E1000_FCAH, FLOW_CONTROL_ADDRESS_HIGH);
+	E1000_WRITE_REG(hw, E1000_FCT, FLOW_CONTROL_TYPE);
+
+	E1000_WRITE_REG(hw, E1000_FCTTV, mac->fc_pause_time);
+
+	ret_val = e1000_set_fc_watermarks_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_led_on_82542 - Turn on SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED on.  This is a function pointer entry point
+ *  called by the api module.
+ **/
+static s32 e1000_led_on_82542(struct e1000_hw *hw)
+{
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGFUNC("e1000_led_on_82542");
+
+	ctrl |= E1000_CTRL_SWDPIN0;
+	ctrl |= E1000_CTRL_SWDPIO0;
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_off_82542 - Turn off SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED off.  This is a function pointer entry point
+ *  called by the api module.
+ **/
+static s32 e1000_led_off_82542(struct e1000_hw *hw)
+{
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGFUNC("e1000_led_off_82542");
+
+	ctrl &= ~E1000_CTRL_SWDPIN0;
+	ctrl |= E1000_CTRL_SWDPIO0;
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_translate_register_82542 - Translate the proper regiser offset
+ *  @reg: e1000 register to be read
+ *
+ *  Registers in 82542 are located in different offsets than other adapters
+ *  even though they function in the same manner.  This function takes in
+ *  the name of the register to read and returns the correct offset for
+ *  82542 silicon.
+ **/
+u32 e1000_translate_register_82542(u32 reg)
+{
+	/* Some of the 82542 registers are located at different
+	 * offsets than they are in newer adapters.
+	 * Despite the difference in location, the registers
+	 * function in the same manner.
+	 */
+	switch (reg) {
+	case E1000_RA:
+		reg = 0x00040;
+		break;
+	case E1000_RDTR:
+		reg = 0x00108;
+		break;
+	case E1000_RDBAL:
+		reg = 0x00110;
+		break;
+	case E1000_RDBAH:
+		reg = 0x00114;
+		break;
+	case E1000_RDLEN:
+		reg = 0x00118;
+		break;
+	case E1000_RDH:
+		reg = 0x00120;
+		break;
+	case E1000_RDT:
+		reg = 0x00128;
+		break;
+	case E1000_RDBAL1:
+		reg = 0x00138;
+		break;
+	case E1000_RDBAH1:
+		reg = 0x0013C;
+		break;
+	case E1000_RDLEN1:
+		reg = 0x00140;
+		break;
+	case E1000_RDH1:
+		reg = 0x00148;
+		break;
+	case E1000_RDT1:
+		reg = 0x00150;
+		break;
+	case E1000_FCRTH:
+		reg = 0x00160;
+		break;
+	case E1000_FCRTL:
+		reg = 0x00168;
+		break;
+	case E1000_MTA:
+		reg = 0x00200;
+		break;
+	case E1000_TDBAL:
+		reg = 0x00420;
+		break;
+	case E1000_TDBAH:
+		reg = 0x00424;
+		break;
+	case E1000_TDLEN:
+		reg = 0x00428;
+		break;
+	case E1000_TDH:
+		reg = 0x00430;
+		break;
+	case E1000_TDT:
+		reg = 0x00438;
+		break;
+	case E1000_TIDV:
+		reg = 0x00440;
+		break;
+	case E1000_VFTA:
+		reg = 0x00600;
+		break;
+	case E1000_TDFH:
+		reg = 0x08010;
+		break;
+	case E1000_TDFT:
+		reg = 0x08018;
+		break;
+	default:
+		break;
+	}
+
+	return reg;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_82542 - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_82542(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_82542");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82543.c linux-2.6.9/drivers/net/e1000/e1000_82543.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_82543.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82543.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1616 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_82543
+ * e1000_82544
+ */
+
+#include "e1000_api.h"
+#include "e1000_82543.h"
+
+void e1000_init_function_pointers_82543(struct e1000_hw *hw);
+
+static s32       e1000_init_phy_params_82543(struct e1000_hw *hw);
+static s32       e1000_init_nvm_params_82543(struct e1000_hw *hw);
+static s32       e1000_init_mac_params_82543(struct e1000_hw *hw);
+static s32       e1000_read_phy_reg_82543(struct e1000_hw *hw, u32 offset,
+                                          u16 *data);
+static s32       e1000_write_phy_reg_82543(struct e1000_hw *hw, u32 offset,
+                                           u16 data);
+static s32       e1000_phy_force_speed_duplex_82543(struct e1000_hw *hw);
+static s32       e1000_phy_hw_reset_82543(struct e1000_hw *hw);
+static s32       e1000_reset_hw_82543(struct e1000_hw *hw);
+static s32       e1000_init_hw_82543(struct e1000_hw *hw);
+static s32       e1000_setup_link_82543(struct e1000_hw *hw);
+static s32       e1000_setup_copper_link_82543(struct e1000_hw *hw);
+static s32       e1000_setup_fiber_link_82543(struct e1000_hw *hw);
+static s32       e1000_check_for_copper_link_82543(struct e1000_hw *hw);
+static s32       e1000_check_for_fiber_link_82543(struct e1000_hw *hw);
+static s32       e1000_led_on_82543(struct e1000_hw *hw);
+static s32       e1000_led_off_82543(struct e1000_hw *hw);
+static void      e1000_write_vfta_82543(struct e1000_hw *hw, u32 offset,
+                                        u32 value);
+static void      e1000_mta_set_82543(struct e1000_hw *hw, u32 hash_value);
+static void      e1000_clear_hw_cntrs_82543(struct e1000_hw *hw);
+static s32       e1000_config_mac_to_phy_82543(struct e1000_hw *hw);
+static boolean_t e1000_init_phy_disabled_82543(struct e1000_hw *hw);
+static void      e1000_lower_mdi_clk_82543(struct e1000_hw *hw, u32 *ctrl);
+static s32       e1000_polarity_reversal_workaround_82543(struct e1000_hw *hw);
+static void      e1000_raise_mdi_clk_82543(struct e1000_hw *hw, u32 *ctrl);
+static u16       e1000_shift_in_mdi_bits_82543(struct e1000_hw *hw);
+static void      e1000_shift_out_mdi_bits_82543(struct e1000_hw *hw, u32 data,
+                                                u16 count);
+static boolean_t e1000_tbi_compatibility_enabled_82543(struct e1000_hw *hw);
+static void      e1000_set_tbi_sbp_82543(struct e1000_hw *hw, boolean_t state);
+
+struct e1000_dev_spec_82543 {
+	u32  tbi_compatibility;
+	boolean_t dma_fairness;
+	boolean_t init_phy_disabled;
+};
+
+/**
+ *  e1000_init_phy_params_82543 - Init PHY func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_82543(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_phy_params_82543");
+
+	if (hw->media_type != e1000_media_type_copper) {
+		phy->type               = e1000_phy_none;
+		goto out;
+	}
+
+	phy->addr                       = 1;
+	phy->autoneg_mask               = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+	phy->reset_delay_us             = 10000;
+	phy->type                       = e1000_phy_m88;
+
+	/* Function Pointers */
+	func->check_polarity            = e1000_check_polarity_m88;
+	func->commit_phy                = e1000_phy_sw_reset_generic;
+	func->force_speed_duplex        = e1000_phy_force_speed_duplex_82543;
+	func->get_cable_length          = e1000_get_cable_length_m88;
+	func->get_cfg_done              = e1000_get_cfg_done_generic;
+	func->read_phy_reg              = (hw->mac.type == e1000_82543)
+	                                  ? e1000_read_phy_reg_82543
+	                                  : e1000_read_phy_reg_m88;
+	func->reset_phy                 = (hw->mac.type == e1000_82543)
+	                                  ? e1000_phy_hw_reset_82543
+	                                  : e1000_phy_hw_reset_generic;
+	func->write_phy_reg             = (hw->mac.type == e1000_82543)
+	                                  ? e1000_write_phy_reg_82543
+	                                  : e1000_write_phy_reg_m88;
+	func->get_phy_info              = e1000_get_phy_info_m88;
+
+	/* The external PHY of the 82543 can be in a funky state.
+	 * Resetting helps us read the PHY registers for acquiring
+	 * the PHY ID.
+	 */
+	if (!e1000_init_phy_disabled_82543(hw)) {
+		ret_val = e1000_phy_hw_reset(hw);
+		if (ret_val) {
+			DEBUGOUT("Resetting PHY during init failed.\n");
+			goto out;
+		}
+		msec_delay(20);
+	}
+
+	ret_val = e1000_get_phy_id(hw);
+	if (ret_val)
+		goto out;
+
+	/* Verify phy id */
+	switch (hw->mac.type) {
+	case e1000_82543:
+		if (phy->id != M88E1000_E_PHY_ID) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+		break;
+	case e1000_82544:
+		if (phy->id != M88E1000_I_PHY_ID) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+		break;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_82543 - Init NVM func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_82543(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+
+	DEBUGFUNC("e1000_init_nvm_params_82543");
+
+	nvm->type               = e1000_nvm_eeprom_microwire;
+	nvm->word_size          = 64;
+	nvm->delay_usec         = 50;
+	nvm->address_bits       =  6;
+	nvm->opcode_bits        =  3;
+
+	/* Function Pointers */
+	func->read_nvm          = e1000_read_nvm_microwire;
+	func->update_nvm        = e1000_update_nvm_checksum_generic;
+	func->valid_led_default = e1000_valid_led_default_generic;
+	func->validate_nvm      = e1000_validate_nvm_checksum_generic;
+	func->write_nvm         = e1000_write_nvm_microwire;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_init_mac_params_82543 - Init MAC func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_82543(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_init_mac_params_82543");
+
+	/* Set media type */
+	switch (hw->device_id) {
+	case E1000_DEV_ID_82543GC_FIBER:
+	case E1000_DEV_ID_82544EI_FIBER:
+		hw->media_type = e1000_media_type_fiber;
+		break;
+	default:
+		hw->media_type = e1000_media_type_copper;
+		break;
+	}
+
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_pci_generic;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_82543;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_82543;
+	/* link setup */
+	func->setup_link = e1000_setup_link_82543;
+	/* physical interface setup */
+	func->setup_physical_interface =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_setup_copper_link_82543
+	                : e1000_setup_fiber_link_82543;
+	/* check for link */
+	func->check_for_link =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_check_for_copper_link_82543
+	                : e1000_check_for_fiber_link_82543;
+	/* link info */
+	func->get_link_up_info =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_get_speed_and_duplex_copper_generic
+	                : e1000_get_speed_and_duplex_fiber_serdes_generic;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_82543;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_82543;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_82543;
+	func->led_off = e1000_led_off_82543;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_82543;
+
+	hw->dev_spec_size = sizeof(struct e1000_dev_spec_82543);
+
+	/* Device-specific structure allocation */
+	ret_val = e1000_alloc_zeroed_dev_spec_struct(hw, hw->dev_spec_size);
+	if (ret_val)
+		goto out;
+
+	/* Set tbi compatibility */
+	if ((hw->mac.type != e1000_82543) ||
+	    (hw->media_type == e1000_media_type_fiber))
+		e1000_set_tbi_compatibility_82543(hw, FALSE);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_82543 - Init func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  The only function explicitly called by the api module to initialize
+ *  all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_82543(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_82543");
+
+	hw->func.init_mac_params = e1000_init_mac_params_82543;
+	hw->func.init_nvm_params = e1000_init_nvm_params_82543;
+	hw->func.init_phy_params = e1000_init_phy_params_82543;
+}
+
+/**
+ *  e1000_tbi_compatibility_enabled_82543 - Returns TBI compat status
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns the curent status of 10-bit Interface (TBI) compatibility
+ *  (enabled/disabled).
+ **/
+static boolean_t e1000_tbi_compatibility_enabled_82543(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82543 *dev_spec;
+	boolean_t state = FALSE;
+
+	DEBUGFUNC("e1000_tbi_compatibility_enabled_82543");
+
+	if (hw->mac.type != e1000_82543) {
+		DEBUGOUT("TBI compatibility workaround for 82543 only.\n");
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		goto out;
+	}
+
+	state = (dev_spec->tbi_compatibility & TBI_COMPAT_ENABLED)
+	        ? TRUE : FALSE;
+
+out:
+	return state;
+}
+
+/**
+ *  e1000_set_tbi_compatibility_82543 - Set TBI compatibility
+ *  @hw: pointer to the HW structure
+ *  @state: enable/disable TBI compatibility
+ *
+ *  Enables or disabled 10-bit Interface (TBI) compatibility.
+ **/
+void e1000_set_tbi_compatibility_82543(struct e1000_hw *hw, boolean_t state)
+{
+	struct e1000_dev_spec_82543 *dev_spec;
+
+	DEBUGFUNC("e1000_set_tbi_compatibility_82543");
+
+	if (hw->mac.type != e1000_82543) {
+		DEBUGOUT("TBI compatibility workaround for 82543 only.\n");
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		goto out;
+	}
+
+	if (state)
+		dev_spec->tbi_compatibility |= TBI_COMPAT_ENABLED;
+	else
+		dev_spec->tbi_compatibility &= ~TBI_COMPAT_ENABLED;
+
+out:
+	return;
+}
+
+/**
+ *  e1000_tbi_sbp_enabled_82543 - Returns TBI SBP status
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns the curent status of 10-bit Interface (TBI) store bad packet (SBP)
+ *  (enabled/disabled).
+ **/
+boolean_t e1000_tbi_sbp_enabled_82543(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82543 *dev_spec;
+	boolean_t state = FALSE;
+
+	DEBUGFUNC("e1000_tbi_sbp_enabled_82543");
+
+	if (hw->mac.type != e1000_82543) {
+		DEBUGOUT("TBI compatibility workaround for 82543 only.\n");
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		goto out;
+	}
+
+	state = (dev_spec->tbi_compatibility & TBI_SBP_ENABLED)
+	        ? TRUE : FALSE;
+
+out:
+	return state;
+}
+
+/**
+ *  e1000_set_tbi_sbp_82543 - Set TBI SBP
+ *  @hw: pointer to the HW structure
+ *  @state: enable/disable TBI store bad packet
+ *
+ *  Enables or disabled 10-bit Interface (TBI) store bad packet (SBP).
+ **/
+static void e1000_set_tbi_sbp_82543(struct e1000_hw *hw, boolean_t state)
+{
+	struct e1000_dev_spec_82543 *dev_spec;
+
+	DEBUGFUNC("e1000_set_tbi_sbp_82543");
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (state && e1000_tbi_compatibility_enabled_82543(hw))
+		dev_spec->tbi_compatibility |= TBI_SBP_ENABLED;
+	else
+		dev_spec->tbi_compatibility &= ~TBI_SBP_ENABLED;
+
+	return;
+}
+
+/**
+ *  e1000_init_phy_disabled_82543 - Returns init PHY status
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns the current status of whether PHY initialization is disabled.
+ *  True if PHY initialization is disabled else false.
+ **/
+static boolean_t e1000_init_phy_disabled_82543(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82543 *dev_spec;
+	boolean_t ret_val;
+
+	DEBUGFUNC("e1000_init_phy_disabled_82543");
+
+	if (hw->mac.type != e1000_82543) {
+		ret_val = FALSE;
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = FALSE;
+		goto out;
+	}
+
+	ret_val = dev_spec->init_phy_disabled;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_tbi_adjust_stats_82543 - Adjust stats when TBI enabled
+ *  @hw: pointer to the HW structure
+ *  @stats: Struct containing statistic register values
+ *  @frame_len: The length of the frame in question
+ *  @mac_addr: The Ethernet destination address of the frame in question
+ *
+ *  Adjusts the statistic counters when a frame is accepted by TBI_ACCEPT
+ **/
+void e1000_tbi_adjust_stats_82543(struct e1000_hw *hw,
+                                  struct e1000_hw_stats *stats, u32 frame_len,
+                                  u8 *mac_addr)
+{
+	u64 carry_bit;
+
+	if (e1000_tbi_sbp_enabled_82543(hw) == FALSE)
+		goto out;
+
+	/* First adjust the frame length. */
+	frame_len--;
+	/* We need to adjust the statistics counters, since the hardware
+	 * counters overcount this packet as a CRC error and undercount
+	 * the packet as a good packet
+	 */
+	/* This packet should not be counted as a CRC error.    */
+	stats->crcerrs--;
+	/* This packet does count as a Good Packet Received.    */
+	stats->gprc++;
+
+	/* Adjust the Good Octets received counters             */
+	carry_bit = 0x80000000 & stats->gorcl;
+	stats->gorcl += frame_len;
+	/* If the high bit of Gorcl (the low 32 bits of the Good Octets
+	 * Received Count) was one before the addition,
+	 * AND it is zero after, then we lost the carry out,
+	 * need to add one to Gorch (Good Octets Received Count High).
+	 * This could be simplified if all environments supported
+	 * 64-bit integers.
+	 */
+	if (carry_bit && ((stats->gorcl & 0x80000000) == 0))
+		stats->gorch++;
+	/* Is this a broadcast or multicast?  Check broadcast first,
+	 * since the test for a multicast frame will test positive on
+	 * a broadcast frame.
+	 */
+	if ((mac_addr[0] == 0xff) && (mac_addr[1] == 0xff))
+		/* Broadcast packet */
+		stats->bprc++;
+	else if (*mac_addr & 0x01)
+		/* Multicast packet */
+		stats->mprc++;
+
+	/* In this case, the hardware has overcounted the number of
+	 * oversize frames.
+	 */
+	if ((frame_len == hw->mac.max_frame_size) && (stats->roc > 0))
+		stats->roc--;
+
+	/* Adjust the bin counters when the extra byte put the frame in the
+	 * wrong bin. Remember that the frame_len was adjusted above.
+	 */
+	if (frame_len == 64) {
+		stats->prc64++;
+		stats->prc127--;
+	} else if (frame_len == 127) {
+		stats->prc127++;
+		stats->prc255--;
+	} else if (frame_len == 255) {
+		stats->prc255++;
+		stats->prc511--;
+	} else if (frame_len == 511) {
+		stats->prc511++;
+		stats->prc1023--;
+	} else if (frame_len == 1023) {
+		stats->prc1023++;
+		stats->prc1522--;
+	} else if (frame_len == 1522) {
+		stats->prc1522++;
+	}
+
+out:
+	return;
+}
+
+/**
+ *  e1000_read_phy_reg_82543 - Read PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be read
+ *  @data: pointer to the read data
+ *
+ *  Reads the PHY at offset and stores the information read to data.
+ **/
+static s32 e1000_read_phy_reg_82543(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	u32 mdic;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_read_phy_reg_82543");
+
+	if (offset > MAX_PHY_REG_ADDRESS) {
+		DEBUGOUT1("PHY Address %d is out of range\n", offset);
+		ret_val = -E1000_ERR_PARAM;
+		goto out;
+	}
+
+	/* We must first send a preamble through the MDIO pin to signal the
+	 * beginning of an MII instruction.  This is done by sending 32
+	 * consecutive "1" bits.
+	 */
+	e1000_shift_out_mdi_bits_82543(hw, PHY_PREAMBLE, PHY_PREAMBLE_SIZE);
+
+	/* Now combine the next few fields that are required for a read
+	 * operation.  We use this method instead of calling the
+	 * e1000_shift_out_mdi_bits routine five different times.  The format
+	 * of an MII read instruction consists of a shift out of 14 bits and
+	 * is defined as follows:
+	 * 	<Preamble><SOF><Op Code><Phy Addr><Offset>
+	 * followed by a shift in of 18 bits.  This first two bits shifted in
+	 * are TurnAround bits used to avoid contention on the MDIO pin when a
+	 * READ operation is performed.  These two bits are thrown away
+	 * followed by a shift in of 16 bits which contains the desired data.
+	 */
+	mdic = (offset | (hw->phy.addr << 5) |
+		(PHY_OP_READ << 10) | (PHY_SOF << 12));
+
+	e1000_shift_out_mdi_bits_82543(hw, mdic, 14);
+
+	/* Now that we've shifted out the read command to the MII, we need to
+	 * "shift in" the 16-bit value (18 total bits) of the requested PHY
+	 * register address.
+	 */
+	*data = e1000_shift_in_mdi_bits_82543(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_phy_reg_82543 - Write PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be written
+ *  @data: pointer to the data to be written at offset
+ *
+ *  Writes data to the PHY at offset.
+ **/
+static s32 e1000_write_phy_reg_82543(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	u32 mdic;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_write_phy_reg_82543");
+
+	if (offset > MAX_PHY_REG_ADDRESS) {
+		DEBUGOUT1("PHY Address %d is out of range\n", offset);
+		ret_val = -E1000_ERR_PARAM;
+		goto out;
+	}
+
+	/* We'll need to use the SW defined pins to shift the write command
+	 * out to the PHY. We first send a preamble to the PHY to signal the
+	 * beginning of the MII instruction.  This is done by sending 32
+	 * consecutive "1" bits.
+	 */
+	e1000_shift_out_mdi_bits_82543(hw, PHY_PREAMBLE, PHY_PREAMBLE_SIZE);
+
+	/* Now combine the remaining required fields that will indicate a
+	 * write operation. We use this method instead of calling the
+	 * e1000_shift_out_mdi_bits routine for each field in the command. The
+	 * format of a MII write instruction is as follows:
+	 * <Preamble><SOF><Op Code><Phy Addr><Reg Addr><Turnaround><Data>.
+	 */
+	mdic = ((PHY_TURNAROUND) | (offset << 2) | (hw->phy.addr << 7) |
+	        (PHY_OP_WRITE << 12) | (PHY_SOF << 14));
+	mdic <<= 16;
+	mdic |= (u32) data;
+
+	e1000_shift_out_mdi_bits_82543(hw, mdic, 32);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_raise_mdi_clk_82543 - Raise Management Data Input clock
+ *  @hw: pointer to the HW structure
+ *  @ctrl: pointer to the control register
+ *
+ *  Raise the management data input clock by setting the MDC bit in the control
+ *  register.
+ **/
+static void e1000_raise_mdi_clk_82543(struct e1000_hw *hw, u32 *ctrl)
+{
+	/* Raise the clock input to the Management Data Clock (by setting the
+	 * MDC bit), and then delay a sufficient amount of time.
+	 */
+	E1000_WRITE_REG(hw, E1000_CTRL, (*ctrl | E1000_CTRL_MDC));
+	E1000_WRITE_FLUSH(hw);
+	usec_delay(10);
+}
+
+/**
+ *  e1000_lower_mdi_clk_82543 - Lower Management Data Input clock
+ *  @hw: pointer to the HW structure
+ *  @ctrl: pointer to the control register
+ *
+ *  Lower the management data input clock by clearing the MDC bit in the control
+ *  register.
+ **/
+static void e1000_lower_mdi_clk_82543(struct e1000_hw *hw, u32 *ctrl)
+{
+	/* Lower the clock input to the Management Data Clock (by clearing the
+	 * MDC bit), and then delay a sufficient amount of time.
+	 */
+	E1000_WRITE_REG(hw, E1000_CTRL, (*ctrl & ~E1000_CTRL_MDC));
+	E1000_WRITE_FLUSH(hw);
+	usec_delay(10);
+}
+
+/**
+ *  e1000_shift_out_mdi_bits_82543 - Shift data bits our to the PHY
+ *  @hw: pointer to the HW structure
+ *  @data: data to send to the PHY
+ *  @count: number of bits to shift out
+ *
+ *  We need to shift 'count' bits out to the PHY.  So, the value in the
+ *  "data" parameter will be shifted out to the PHY one bit at a time.
+ *  In order to do this, "data" must be broken down into bits.
+ **/
+static void e1000_shift_out_mdi_bits_82543(struct e1000_hw *hw, u32 data,
+                                           u16 count)
+{
+	u32 ctrl, mask;
+
+	/* We need to shift "count" number of bits out to the PHY.  So, the
+	 * value in the "data" parameter will be shifted out to the PHY one
+	 * bit at a time.  In order to do this, "data" must be broken down
+	 * into bits.
+	 */
+	mask = 0x01;
+	mask <<= (count -1);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Set MDIO_DIR and MDC_DIR direction bits to be used as output pins. */
+	ctrl |= (E1000_CTRL_MDIO_DIR | E1000_CTRL_MDC_DIR);
+
+	while (mask) {
+		/* A "1" is shifted out to the PHY by setting the MDIO bit to
+		 * "1" and then raising and lowering the Management Data Clock.
+		 * A "0" is shifted out to the PHY by setting the MDIO bit to
+		 * "0" and then raising and lowering the clock.
+		 */
+		if (data & mask) ctrl |= E1000_CTRL_MDIO;
+		else ctrl &= ~E1000_CTRL_MDIO;
+
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+		E1000_WRITE_FLUSH(hw);
+
+		usec_delay(10);
+
+		e1000_raise_mdi_clk_82543(hw, &ctrl);
+		e1000_lower_mdi_clk_82543(hw, &ctrl);
+
+		mask >>= 1;
+	}
+}
+
+/**
+ *  e1000_shift_in_mdi_bits_82543 - Shift data bits in from the PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  In order to read a register from the PHY, we need to shift 18 bits
+ *  in from the PHY.  Bits are "shifted in" by raising the clock input to
+ *  the PHY (setting the MDC bit), and then reading the value of the data out
+ *  MDIO bit.
+ **/
+static u16 e1000_shift_in_mdi_bits_82543(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	u16 data = 0;
+	u8 i;
+
+	/* In order to read a register from the PHY, we need to shift in a
+	 * total of 18 bits from the PHY.  The first two bit (turnaround)
+	 * times are used to avoid contention on the MDIO pin when a read
+	 * operation is performed.  These two bits are ignored by us and
+	 * thrown away.  Bits are "shifted in" by raising the input to the
+	 * Management Data Clock (setting the MDC bit) and then reading the
+	 * value of the MDIO bit.
+	 */
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Clear MDIO_DIR (SWDPIO1) to indicate this bit is to be used as
+	 * input.
+	 */
+	ctrl &= ~E1000_CTRL_MDIO_DIR;
+	ctrl &= ~E1000_CTRL_MDIO;
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	E1000_WRITE_FLUSH(hw);
+
+	/* Raise and lower the clock before reading in the data.  This accounts
+	 * for the turnaround bits.  The first clock occurred when we clocked
+	 * out the last bit of the Register Address.
+	 */
+	e1000_raise_mdi_clk_82543(hw, &ctrl);
+	e1000_lower_mdi_clk_82543(hw, &ctrl);
+
+	for (data = 0, i = 0; i < 16; i++) {
+		data <<= 1;
+		e1000_raise_mdi_clk_82543(hw, &ctrl);
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		/* Check to see if we shifted in a "1". */
+		if (ctrl & E1000_CTRL_MDIO)
+			data |= 1;
+		e1000_lower_mdi_clk_82543(hw, &ctrl);
+	}
+
+	e1000_raise_mdi_clk_82543(hw, &ctrl);
+	e1000_lower_mdi_clk_82543(hw, &ctrl);
+
+	return data;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_82543 - Force speed/duplex for PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the function to force speed and duplex for the m88 PHY, and
+ *  if the PHY is not auto-negotiating and the speed is forced to 10Mbit,
+ *  then call the function for polarity reversal workaround.
+ **/
+static s32 e1000_phy_force_speed_duplex_82543(struct e1000_hw *hw)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_82543");
+
+	ret_val = e1000_phy_force_speed_duplex_m88(hw);
+	if (ret_val)
+		goto out;
+
+	if (!hw->mac.autoneg &&
+	    (hw->mac.forced_speed_duplex & E1000_ALL_10_SPEED))
+		ret_val = e1000_polarity_reversal_workaround_82543(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_polarity_reversal_workaround_82543 - Workaround polarity reversal
+ *  @hw: pointer to the HW structure
+ *
+ *  When forcing link to 10 Full or 10 Half, the PHY can reverse the polarity
+ *  inadvertantly.  To workaround the issue, we disable the transmitter on
+ *  the PHY until we have established the link partner's link parameters.
+ **/
+static s32 e1000_polarity_reversal_workaround_82543(struct e1000_hw *hw)
+{
+	s32 ret_val;
+	u16 mii_status_reg;
+	u16 i;
+	boolean_t link;
+
+	/* Polarity reversal workaround for forced 10F/10H links. */
+
+	/* Disable the transmitter on the PHY */
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0019);
+	if (ret_val)
+		goto out;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFFFF);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0000);
+	if (ret_val)
+		goto out;
+
+	/* This loop will early-out if the NO link condition has been met.
+	 * In other words, DO NOT use e1000_phy_has_link_generic() here.
+	 */
+	for (i = PHY_FORCE_TIME; i > 0; i--) {
+		/* Read the MII Status Register and wait for Link Status bit
+		 * to be clear.
+		 */
+
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+		if (ret_val)
+			goto out;
+
+		if ((mii_status_reg & ~MII_SR_LINK_STATUS) == 0)
+			break;
+		msec_delay_irq(100);
+	}
+
+	/* Recommended delay time after link has been lost */
+	msec_delay_irq(1000);
+
+	/* Now we will re-enable the transmitter on the PHY */
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0019);
+	if (ret_val)
+		goto out;
+	msec_delay_irq(50);
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFFF0);
+	if (ret_val)
+		goto out;
+	msec_delay_irq(50);
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFF00);
+	if (ret_val)
+		goto out;
+	msec_delay_irq(50);
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0x0000);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0000);
+	if (ret_val)
+		goto out;
+
+	/* Read the MII Status Register and wait for Link Status bit
+	 * to be set.
+	 */
+	ret_val = e1000_phy_has_link_generic(hw, PHY_FORCE_TIME, 100000, &link);
+	if (ret_val)
+		goto out;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_hw_reset_82543 - PHY hardware reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets the PHY_RESET_DIR bit in the extended device control register
+ *  to put the PHY into a reset and waits for completion.  Once the reset
+ *  has been accomplished, clear the PHY_RESET_DIR bit to take the PHY out
+ *  of reset.  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_phy_hw_reset_82543(struct e1000_hw *hw)
+{
+	struct e1000_functions *func = &hw->func;
+	u32 ctrl_ext;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_phy_hw_reset_82543");
+
+	/* Read the Extended Device Control Register, assert the PHY_RESET_DIR
+	 * bit to put the PHY into reset...
+	 */
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	ctrl_ext |= E1000_CTRL_EXT_SDP4_DIR;
+	ctrl_ext &= ~E1000_CTRL_EXT_SDP4_DATA;
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	E1000_WRITE_FLUSH(hw);
+
+	msec_delay(10);
+
+	/* ...then take it out of reset. */
+	ctrl_ext |= E1000_CTRL_EXT_SDP4_DATA;
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	E1000_WRITE_FLUSH(hw);
+
+	usec_delay(150);
+
+	ret_val = func->get_cfg_done(hw);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_reset_hw_82543 - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_82543(struct e1000_hw *hw)
+{
+	u32 ctrl, icr;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_reset_hw_82543");
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	e1000_set_tbi_sbp_82543(hw, FALSE);
+
+	/* Delay to allow any outstanding PCI transactions to complete before
+	 * resetting the device
+	 */
+	msec_delay(10);
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGOUT("Issuing a global reset to 82543/82544 MAC\n");
+	if (hw->mac.type == e1000_82543) {
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+	} else {
+		/* The 82544 can't ACK the 64-bit write when issuing the
+		 * reset, so use IO-mapping as a workaround.
+		 */
+		E1000_WRITE_REG_IO(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+	}
+
+	/* After MAC reset, force reload of NVM to restore power-on
+	 * settings to device.
+	 */
+	e1000_reload_nvm(hw);
+	msec_delay(2);
+
+	/* Masking off and clearing any pending interrupts */
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_82543 - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation.
+ **/
+static s32 e1000_init_hw_82543(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_dev_spec_82543 *dev_spec;
+	u32 ctrl;
+	s32 ret_val;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_hw_82543");
+
+	dev_spec = (struct e1000_dev_spec_82543 *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	/* Disabling VLAN filtering */
+	E1000_WRITE_REG(hw, E1000_VET, 0);
+	e1000_clear_vfta(hw);
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	/* Set the PCI priority bit correctly in the CTRL register.  This
+	 * determines if the adapter gives priority to receives, or if it
+	 * gives equal priority to transmits and receives.
+	 */
+	if (hw->mac.type == e1000_82543 && dev_spec->dma_fairness) {
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_PRIOR);
+	}
+
+	e1000_pcix_mmrbc_workaround_generic(hw);
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_82543(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_link_82543 - Setup flow control and link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Read the EEPROM to determine the initial polarity value and write the
+ *  extended device control register with the information before calling
+ *  the generic setup link function, which does the following:
+ *  Determines which flow control settings to use, then configures flow
+ *  control.  Calls the appropriate media-specific link configuration
+ *  function.  Assuming the adapter has a valid link partner, a valid link
+ *  should be established.  Assumes the hardware has previously been reset
+ *  and the transmitter and receiver are not enabled.
+ **/
+static s32 e1000_setup_link_82543(struct e1000_hw *hw)
+{
+	u32 ctrl_ext;
+	s32  ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_setup_link_82543");
+
+	/* Take the 4 bits from NVM word 0xF that determine the initial
+	 * polarity value for the SW controlled pins, and setup the
+	 * Extended Device Control reg with that info.
+	 * This is needed because one of the SW controlled pins is used for
+	 * signal detection.  So this should be done before phy setup.
+	 */
+	if (hw->mac.type == e1000_82543) {
+		ret_val = e1000_read_nvm(hw, NVM_INIT_CONTROL2_REG, 1, &data);
+		if (ret_val) {
+			DEBUGOUT("NVM Read Error\n");
+			ret_val = -E1000_ERR_NVM;
+			goto out;
+		}
+		ctrl_ext = ((data & NVM_WORD0F_SWPDIO_EXT_MASK) <<
+		            NVM_SWDPIO_EXT_SHIFT);
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	}
+
+	ret_val = e1000_setup_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_82543 - Configure copper link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures the link for auto-neg or forced speed and duplex.  Then we check
+ *  for link, once link is established calls to configure collision distance
+ *  and flow control are called.
+ **/
+static s32 e1000_setup_copper_link_82543(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_setup_copper_link_82543");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL) | E1000_CTRL_SLU;
+	/* With 82543, we need to force speed and duplex on the MAC
+	 * equal to what the PHY speed and duplex configuration is.
+	 * In addition, we need to perform a hardware reset on the
+	 * PHY to take it out of reset.
+	 */
+	if (hw->mac.type == e1000_82543) {
+		ctrl |= (E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+		ret_val = e1000_phy_hw_reset(hw);
+		if (ret_val)
+			goto out;
+		hw->phy.reset_disable = FALSE;
+	} else {
+		ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	}
+
+	/* Set MDI/MDI-X, Polarity Reversal, and downshift settings */
+	ret_val = e1000_copper_link_setup_m88(hw);
+	if (ret_val)
+		goto out;
+
+	if (hw->mac.autoneg) {
+		/* Setup autoneg and flow control advertisement and perform
+		 * autonegotiation. */
+		ret_val = e1000_copper_link_autoneg(hw);
+		if (ret_val)
+			goto out;
+	} else {
+		/* PHY will be set to 10H, 10F, 100H or 100F
+		 * depending on user settings. */
+		DEBUGOUT("Forcing Speed and Duplex\n");
+		ret_val = e1000_phy_force_speed_duplex_82543(hw);
+		if (ret_val) {
+			DEBUGOUT("Error Forcing Speed and Duplex\n");
+			goto out;
+		}
+	}
+
+	/* Check link status. Wait up to 100 microseconds for link to become
+	 * valid.
+	 */
+	ret_val = e1000_phy_has_link_generic(hw,
+	                                     COPPER_LINK_UP_LIMIT,
+	                                     10,
+	                                     &link);
+	if (ret_val)
+		goto out;
+
+
+	if (link) {
+		DEBUGOUT("Valid link established!!!\n");
+		/* Config the MAC and PHY after link is up */
+		if (hw->mac.type == e1000_82544)
+			e1000_config_collision_dist_generic(hw);
+		else {
+			ret_val = e1000_config_mac_to_phy_82543(hw);
+			if (ret_val)
+				goto out;
+		}
+		ret_val = e1000_config_fc_after_link_up_generic(hw);
+	} else {
+		DEBUGOUT("Unable to establish link!!!\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_fiber_link_82543 - Setup link for fiber
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures collision distance and flow control for fiber links.  Upon
+ *  successful setup, poll for link.
+ **/
+static s32 e1000_setup_fiber_link_82543(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_setup_fiber_link_82543");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Take the link out of reset */
+	ctrl &= ~E1000_CTRL_LRST;
+
+	e1000_config_collision_dist_generic(hw);
+
+	ret_val = e1000_commit_fc_settings_generic(hw);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT("Auto-negotiation enabled\n");
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	E1000_WRITE_FLUSH(hw);
+	msec_delay(1);
+
+	/* For these adapters, the SW defineable pin 1 is cleared when the
+	 * optics detect a signal.  If we have a signal, then poll for a
+	 * "Link-Up" indication.
+	 */
+	if (!(E1000_READ_REG(hw, E1000_CTRL) & E1000_CTRL_SWDPIN1)) {
+		ret_val = e1000_poll_fiber_serdes_link_generic(hw);
+	} else {
+		DEBUGOUT("No signal detected\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_for_copper_link_82543 - Check for link (Copper)
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks the phy for link, if link exists, do the following:
+ *   - check for downshift
+ *   - do polarity workaround (if necessary)
+ *   - configure collision distance
+ *   - configure flow control after link up
+ *   - configure tbi compatibility
+ **/
+static s32 e1000_check_for_copper_link_82543(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 icr, rctl;
+	s32 ret_val;
+	u16 speed, duplex;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_check_for_copper_link_82543");
+
+	if (!mac->get_link_status) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link)
+		goto out; /* No link detected */
+
+	mac->get_link_status = FALSE;
+
+	e1000_check_downshift_generic(hw);
+
+	/* If we are forcing speed/duplex, then we can return since
+	 * we have already determined whether we have link or not.
+	 */
+	if (!mac->autoneg) {
+		/* If speed and duplex are forced to 10H or 10F, then we will
+		 * implement the polarity reversal workaround.  We disable
+		 * interrupts first, and upon returning, place the devices
+		 * interrupt state to its previous value except for the link
+		 * status change interrupt which will happened due to the
+		 * execution of this workaround.
+		 */
+		if (mac->forced_speed_duplex & E1000_ALL_10_SPEED) {
+			E1000_WRITE_REG(hw, E1000_IMC, 0xFFFFFFFF);
+			ret_val = e1000_polarity_reversal_workaround_82543(hw);
+			icr = E1000_READ_REG(hw, E1000_ICR);
+			E1000_WRITE_REG(hw, E1000_ICS, (icr & ~E1000_ICS_LSC));
+			E1000_WRITE_REG(hw, E1000_IMS, IMS_ENABLE_MASK);
+		}
+
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	/* We have a M88E1000 PHY and Auto-Neg is enabled.  If we
+	 * have Si on board that is 82544 or newer, Auto
+	 * Speed Detection takes care of MAC speed/duplex
+	 * configuration.  So we only need to configure Collision
+	 * Distance in the MAC.  Otherwise, we need to force
+	 * speed/duplex on the MAC to the current PHY speed/duplex
+	 * settings.
+	 */
+	if (mac->type == e1000_82544)
+		e1000_config_collision_dist_generic(hw);
+	else {
+		ret_val = e1000_config_mac_to_phy_82543(hw);
+		if (ret_val) {
+			DEBUGOUT("Error configuring MAC to PHY settings\n");
+			goto out;
+		}
+	}
+
+	/* Configure Flow Control now that Auto-Neg has completed.
+	 * First, we need to restore the desired flow control
+	 * settings because we may have had to re-autoneg with a
+	 * different link partner.
+	 */
+	ret_val = e1000_config_fc_after_link_up_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error configuring flow control\n");
+	}
+
+	/* At this point we know that we are on copper and we have
+	 * auto-negotiated link.  These are conditions for checking the link
+	 * partner capability register.  We use the link speed to determine if
+	 * TBI compatibility needs to be turned on or off.  If the link is not
+	 * at gigabit speed, then TBI compatibility is not needed.  If we are
+	 * at gigabit speed, we turn on TBI compatibility.
+	 */
+	if (e1000_tbi_compatibility_enabled_82543(hw)) {
+		ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
+		if (ret_val) {
+			DEBUGOUT("Error getting link speed and duplex\n");
+			return ret_val;
+		}
+		if (speed != SPEED_1000) {
+			/* If link speed is not set to gigabit speed,
+			 * we do not need to enable TBI compatibility.
+			 */
+			if (e1000_tbi_sbp_enabled_82543(hw)) {
+				/* If we previously were in the mode,
+				 * turn it off.
+				 */
+				e1000_set_tbi_sbp_82543(hw, FALSE);
+				rctl = E1000_READ_REG(hw, E1000_RCTL);
+				rctl &= ~E1000_RCTL_SBP;
+				E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+			}
+		} else {
+			/* If TBI compatibility is was previously off,
+			 * turn it on. For compatibility with a TBI link
+			 * partner, we will store bad packets. Some
+			 * frames have an additional byte on the end and
+			 * will look like CRC errors to to the hardware.
+			 */
+			if (!e1000_tbi_sbp_enabled_82543(hw)) {
+				e1000_set_tbi_sbp_82543(hw, TRUE);
+				rctl = E1000_READ_REG(hw, E1000_RCTL);
+				rctl |= E1000_RCTL_SBP;
+				E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+			}
+		}
+	}
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_for_fiber_link_82543 - Check for link (Fiber)
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks for link up on the hardware.  If link is not up and we have
+ *  a signal, then we need to force link up.
+ **/
+static s32 e1000_check_for_fiber_link_82543(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 rxcw, ctrl, status;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_check_for_fiber_link_82543");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	status = E1000_READ_REG(hw, E1000_CTRL);
+	rxcw = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* If we don't have link (auto-negotiation failed or link partner
+	 * cannot auto-negotiate), the cable is plugged in (we have signal),
+	 * and our link partner is not trying to auto-negotiate with us (we
+	 * are receiving idles or data), we need to force link up. We also
+	 * need to give auto-negotiation time to complete, in case the cable
+	 * was just plugged in. The autoneg_failed flag does this.
+	 */
+	/* (ctrl & E1000_CTRL_SWDPIN1) == 0 == have signal */
+	if ((!(ctrl & E1000_CTRL_SWDPIN1)) &&
+	    (!(status & E1000_STATUS_LU)) &&
+	    (!(rxcw & E1000_RXCW_C))) {
+		if (mac->autoneg_failed == 0) {
+			mac->autoneg_failed = 1;
+			ret_val = 0;
+			goto out;
+		}
+		DEBUGOUT("NOT RXing /C/, disable AutoNeg and force link.\n");
+
+		/* Disable auto-negotiation in the TXCW register */
+		E1000_WRITE_REG(hw, E1000_TXCW, (mac->txcw & ~E1000_TXCW_ANE));
+
+		/* Force link-up and also force full-duplex. */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= (E1000_CTRL_SLU | E1000_CTRL_FD);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+		/* Configure Flow Control after forcing link up. */
+		ret_val = e1000_config_fc_after_link_up_generic(hw);
+		if (ret_val) {
+			DEBUGOUT("Error configuring flow control\n");
+			goto out;
+		}
+	} else if ((ctrl & E1000_CTRL_SLU) && (rxcw & E1000_RXCW_C)) {
+		/* If we are forcing link and we are receiving /C/ ordered
+		 * sets, re-enable auto-negotiation in the TXCW register
+		 * and disable forced link in the Device Control register
+		 * in an attempt to auto-negotiate with our link partner.
+		 */
+		DEBUGOUT("RXing /C/, enable AutoNeg and stop forcing link.\n");
+		E1000_WRITE_REG(hw, E1000_TXCW, mac->txcw);
+		E1000_WRITE_REG(hw, E1000_CTRL, (ctrl & ~E1000_CTRL_SLU));
+
+		mac->serdes_has_link = TRUE;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_config_mac_to_phy_82543 - Configure MAC to PHY settings
+ *  @hw: pointer to the HW structure
+ *
+ *  For the 82543 silicon, we need to set the MAC to match the settings
+ *  of the PHY, even if the PHY is auto-negotiating.
+ **/
+static s32 e1000_config_mac_to_phy_82543(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val;
+	u16 phy_data;
+
+	DEBUGFUNC("e1000_config_mac_to_phy_82543");
+
+	/* Set the bits to force speed and duplex */
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= (E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	ctrl &= ~(E1000_CTRL_SPD_SEL | E1000_CTRL_ILOS);
+
+	/* Set up duplex in the Device Control and Transmit Control
+	 * registers depending on negotiated values.
+	 */
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
+	if (ret_val)
+		goto out;
+
+	ctrl &= ~E1000_CTRL_FD;
+	if (phy_data & M88E1000_PSSR_DPLX)
+		ctrl |= E1000_CTRL_FD;
+
+	e1000_config_collision_dist_generic(hw);
+
+	/* Set up speed in the Device Control register depending on
+	 * negotiated values.
+	 */
+	if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_1000MBS)
+		ctrl |= E1000_CTRL_SPD_1000;
+	else if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_100MBS)
+		ctrl |= E1000_CTRL_SPD_100;
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_vfta_82543 - Write value to VLAN filter table
+ *  @hw: pointer to the HW structure
+ *  @offset: the 32-bit offset in which to write the value to.
+ *  @value: the 32-bit value to write at location offset.
+ *
+ *  This writes a 32-bit value to a 32-bit offset in the VLAN filter
+ *  table.
+ **/
+static void e1000_write_vfta_82543(struct e1000_hw *hw, u32 offset, u32 value)
+{
+	u32 temp;
+
+	DEBUGFUNC("e1000_write_vfta_82543");
+
+	if ((hw->mac.type == e1000_82544) && (offset & 1)) {
+		temp = E1000_READ_REG_ARRAY(hw, E1000_VFTA, offset - 1);
+		E1000_WRITE_REG_ARRAY(hw, E1000_VFTA, offset, value);
+		E1000_WRITE_FLUSH(hw);
+		E1000_WRITE_REG_ARRAY(hw, E1000_VFTA, offset - 1, temp);
+		E1000_WRITE_FLUSH(hw);
+	} else {
+		e1000_write_vfta_generic(hw, offset, value);
+	}
+}
+
+/**
+ *  e1000_mta_set_82543 - Set multicast filter table address
+ *  @hw: pointer to the HW structure
+ *  @hash_value: determines the MTA register and bit to set
+ *
+ *  The multicast table address is a register array of 32-bit registers.
+ *  The hash_value is used to determine what register the bit is in, the
+ *  current value is read, the new bit is OR'd in and the new value is
+ *  written back into the register.
+ **/
+static void e1000_mta_set_82543(struct e1000_hw *hw, u32 hash_value)
+{
+	u32 hash_bit, hash_reg, mta, temp;
+
+	DEBUGFUNC("e1000_mta_set_82543");
+
+	hash_reg = (hash_value >> 5);
+
+	/* If we are on an 82544 and we are trying to write an odd offset
+	 * in the MTA, save off the previous entry before writing and
+	 * restore the old value after writing.
+	 */
+	if ((hw->mac.type == e1000_82544) && (hash_reg & 1)) {
+		hash_reg &= (hw->mac.mta_reg_count - 1);
+		hash_bit = hash_value & 0x1F;
+		mta = E1000_READ_REG_ARRAY(hw, E1000_MTA, hash_reg);
+		mta |= (1 << hash_bit);
+		temp = E1000_READ_REG_ARRAY(hw, E1000_MTA, hash_reg - 1);
+
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, hash_reg, mta);
+		E1000_WRITE_FLUSH(hw);
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, hash_reg - 1, temp);
+		E1000_WRITE_FLUSH(hw);
+	} else {
+		e1000_mta_set_generic(hw, hash_value);
+	}
+}
+
+/**
+ *  e1000_led_on_82543 - Turn on SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED on.  This is a function pointer entry point
+ *  called by the api module.
+ **/
+static s32 e1000_led_on_82543(struct e1000_hw *hw)
+{
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGFUNC("e1000_led_on_82543");
+
+	if (hw->mac.type == e1000_82544 &&
+	    hw->media_type == e1000_media_type_copper) {
+		/* Clear SW-defineable Pin 0 to turn on the LED */
+		ctrl &= ~E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+	} else {
+		/* Fiber 82544 and all 82543 use this method */
+		ctrl |= E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+	}
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_off_82543 - Turn off SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED off.  This is a function pointer entry point
+ *  called by the api module.
+ **/
+static s32 e1000_led_off_82543(struct e1000_hw *hw)
+{
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGFUNC("e1000_led_off_82543");
+
+	if (hw->mac.type == e1000_82544 &&
+	    hw->media_type == e1000_media_type_copper) {
+		/* Set SW-defineable Pin 0 to turn off the LED */
+		ctrl |= E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+	} else {
+		ctrl &= ~E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+	}
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_82543 - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_82543(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_82543");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82543.h linux-2.6.9/drivers/net/e1000/e1000_82543.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_82543.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82543.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,44 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_82543_H_
+#define _E1000_82543_H_
+
+#define PHY_PREAMBLE      0xFFFFFFFF
+#define PHY_PREAMBLE_SIZE 32
+#define PHY_SOF           0x1
+#define PHY_OP_READ       0x2
+#define PHY_OP_WRITE      0x1
+#define PHY_TURNAROUND    0x2
+
+#define TBI_COMPAT_ENABLED 0x1 /* Global "knob" for the workaround */
+/* If TBI_COMPAT_ENABLED, then this is the current state (on/off) */
+#define TBI_SBP_ENABLED    0x2 
+                                
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82571.c linux-2.6.9/drivers/net/e1000/e1000_82571.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_82571.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82571.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1368 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_82571
+ * e1000_82572
+ * e1000_82573
+ */
+
+#include "e1000_api.h"
+#include "e1000_82571.h"
+
+void e1000_init_function_pointers_82571(struct e1000_hw *hw);
+
+static s32  e1000_init_phy_params_82571(struct e1000_hw *hw);
+static s32  e1000_init_nvm_params_82571(struct e1000_hw *hw);
+static s32  e1000_init_mac_params_82571(struct e1000_hw *hw);
+static s32  e1000_acquire_nvm_82571(struct e1000_hw *hw);
+static void e1000_release_nvm_82571(struct e1000_hw *hw);
+static s32  e1000_write_nvm_82571(struct e1000_hw *hw, u16 offset,
+                                  u16 words, u16 *data);
+static s32  e1000_update_nvm_checksum_82571(struct e1000_hw *hw);
+static s32  e1000_validate_nvm_checksum_82571(struct e1000_hw *hw);
+static s32  e1000_get_cfg_done_82571(struct e1000_hw *hw);
+static s32  e1000_set_d0_lplu_state_82571(struct e1000_hw *hw,
+                                          boolean_t active);
+static s32  e1000_reset_hw_82571(struct e1000_hw *hw);
+static s32  e1000_init_hw_82571(struct e1000_hw *hw);
+static void e1000_clear_vfta_82571(struct e1000_hw *hw);
+static void e1000_mc_addr_list_update_82571(struct e1000_hw *hw,
+                                            u8 *mc_addr_list, u32 mc_addr_count,
+                                            u32 rar_used_count, u32 rar_count);
+static s32  e1000_setup_link_82571(struct e1000_hw *hw);
+static s32  e1000_setup_copper_link_82571(struct e1000_hw *hw);
+static s32  e1000_setup_fiber_serdes_link_82571(struct e1000_hw *hw);
+static s32  e1000_valid_led_default_82571(struct e1000_hw *hw, u16 *data);
+static void e1000_clear_hw_cntrs_82571(struct e1000_hw *hw);
+static s32  e1000_get_hw_semaphore_82571(struct e1000_hw *hw);
+static s32  e1000_fix_nvm_checksum_82571(struct e1000_hw *hw);
+static s32  e1000_get_phy_id_82571(struct e1000_hw *hw);
+static void e1000_put_hw_semaphore_82571(struct e1000_hw *hw);
+static void e1000_initialize_hw_bits_82571(struct e1000_hw *hw);
+static s32  e1000_write_nvm_eewr_82571(struct e1000_hw *hw, u16 offset,
+                                       u16 words, u16 *data);
+
+struct e1000_dev_spec_82571 {
+	boolean_t laa_is_present;
+};
+
+/**
+ *  e1000_init_phy_params_82571 - Init PHY func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_phy_params_82571(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_phy_params_82571");
+
+	if (hw->media_type != e1000_media_type_copper) {
+		phy->type        = e1000_phy_none;
+		goto out;
+	}
+
+	phy->addr                        = 1;
+	phy->autoneg_mask                = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+	phy->reset_delay_us              = 100;
+
+	func->acquire_phy                = e1000_get_hw_semaphore_82571;
+	func->check_polarity             = e1000_check_polarity_igp;
+	func->check_reset_block          = e1000_check_reset_block_generic;
+	func->release_phy                = e1000_put_hw_semaphore_82571;
+	func->reset_phy                  = e1000_phy_hw_reset_generic;
+	func->set_d0_lplu_state          = e1000_set_d0_lplu_state_82571;
+	func->set_d3_lplu_state          = e1000_set_d3_lplu_state_generic;
+
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		phy->type                = e1000_phy_igp_2;
+		func->get_cfg_done       = e1000_get_cfg_done_82571;
+		func->get_phy_info       = e1000_get_phy_info_igp;
+		func->force_speed_duplex = e1000_phy_force_speed_duplex_igp;
+		func->get_cable_length   = e1000_get_cable_length_igp_2;
+		func->read_phy_reg       = e1000_read_phy_reg_igp;
+		func->write_phy_reg      = e1000_write_phy_reg_igp;
+		break;
+	case e1000_82573:
+		phy->type                = e1000_phy_m88;
+		func->get_cfg_done       = e1000_get_cfg_done_generic;
+		func->get_phy_info       = e1000_get_phy_info_m88;
+		func->commit_phy         = e1000_phy_sw_reset_generic;
+		func->force_speed_duplex = e1000_phy_force_speed_duplex_m88;
+		func->get_cable_length   = e1000_get_cable_length_m88;
+		func->read_phy_reg       = e1000_read_phy_reg_m88;
+		func->write_phy_reg      = e1000_write_phy_reg_m88;
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+		break;
+	}
+
+	/* This can only be done after all function pointers are setup. */
+	ret_val = e1000_get_phy_id_82571(hw);
+
+	/* Verify phy id */
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		if (phy->id != IGP01E1000_I_PHY_ID) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+		break;
+	case e1000_82573:
+		if (phy->id != M88E1111_I_PHY_ID) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+		break;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_82571 - Init NVM func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_nvm_params_82571(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	u16 size;
+
+	DEBUGFUNC("e1000_init_nvm_params_82571");
+
+	nvm->opcode_bits          = 8;
+	nvm->delay_usec           = 1;
+	switch (nvm->override) {
+	case e1000_nvm_override_spi_large:
+		nvm->page_size    = 32;
+		nvm->address_bits = 16;
+		break;
+	case e1000_nvm_override_spi_small:
+		nvm->page_size    = 8;
+		nvm->address_bits = 8;
+		break;
+	default:
+		nvm->page_size    = eecd & E1000_EECD_ADDR_BITS ? 32 : 8;
+		nvm->address_bits = eecd & E1000_EECD_ADDR_BITS ? 16 : 8;
+		break;
+	}
+
+	switch (hw->mac.type) {
+	case e1000_82573:
+		if (((eecd >> 15) & 0x3) == 0x3) {
+			nvm->type = e1000_nvm_flash_hw;
+			nvm->word_size = 2048;
+			/* Autonomous Flash update bit must be cleared due
+			 * to Flash update issue.
+			 */
+			eecd &= ~E1000_EECD_AUPDEN;
+			E1000_WRITE_REG(hw, E1000_EECD, eecd);
+			break;
+		}
+		/* Fall Through */
+	default:
+		nvm->type	= e1000_nvm_eeprom_spi;
+		size = (u16)((eecd & E1000_EECD_SIZE_EX_MASK) >>
+		                  E1000_EECD_SIZE_EX_SHIFT);
+		/* Added to a constant, "size" becomes the left-shift value
+		 * for setting word_size.
+		 */
+		size += NVM_WORD_SIZE_BASE_SHIFT;
+		nvm->word_size	= 1 << size;
+		break;
+	}
+
+	/* Function Pointers */
+	func->acquire_nvm       = e1000_acquire_nvm_82571;
+	func->read_nvm          = (hw->mac.type == e1000_82573)
+	                          ? e1000_read_nvm_eerd
+	                          : e1000_read_nvm_spi;
+	func->release_nvm       = e1000_release_nvm_82571;
+	func->update_nvm        = e1000_update_nvm_checksum_82571;
+	func->validate_nvm      = e1000_validate_nvm_checksum_82571;
+	func->valid_led_default = e1000_valid_led_default_82571;
+	func->write_nvm         = e1000_write_nvm_82571;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_init_mac_params_82571 - Init MAC func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  This is a function pointer entry point called by the api module.
+ **/
+static s32 e1000_init_mac_params_82571(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_mac_params_82571");
+
+	/* Set media type */
+	switch (hw->device_id) {
+	case E1000_DEV_ID_82571EB_FIBER:
+	case E1000_DEV_ID_82572EI_FIBER:
+	case E1000_DEV_ID_82571EB_QUAD_FIBER:
+		hw->media_type = e1000_media_type_fiber;
+		break;
+	case E1000_DEV_ID_82571EB_SERDES:
+	case E1000_DEV_ID_82572EI_SERDES:
+		hw->media_type = e1000_media_type_internal_serdes;
+		break;
+	default:
+		hw->media_type = e1000_media_type_copper;
+		break;
+	}
+
+	/* Set mta register count */
+	mac->mta_reg_count = 128;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_RAR_ENTRIES;
+	/* Set if part includes ASF firmware */
+	mac->asf_firmware_present = TRUE;
+	/* Set if manageability features are enabled. */
+	mac->arc_subsystem_valid =
+	        (E1000_READ_REG(hw, E1000_FWSM) & E1000_FWSM_MODE_MASK)
+	                ? TRUE : FALSE;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_pcie_generic;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_82571;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_82571;
+	/* link setup */
+	func->setup_link = e1000_setup_link_82571;
+	/* physical interface link setup */
+	func->setup_physical_interface =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_setup_copper_link_82571
+	                : e1000_setup_fiber_serdes_link_82571;
+	/* check for link */
+	switch (hw->media_type) {
+	case e1000_media_type_copper:
+		func->check_for_link = e1000_check_for_copper_link_generic;
+		break;
+	case e1000_media_type_fiber:
+		func->check_for_link = e1000_check_for_fiber_link_generic;
+		break;
+	case e1000_media_type_internal_serdes:
+		func->check_for_link = e1000_check_for_serdes_link_generic;
+		break;
+	default:
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+		break;
+	}
+	/* check management mode */
+	func->check_mng_mode = e1000_check_mng_mode_generic;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_82571;
+	/* writing VFTA */
+	func->write_vfta = e1000_write_vfta_generic;
+	/* clearing VFTA */
+	func->clear_vfta = e1000_clear_vfta_82571;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* blink LED */
+	func->blink_led = e1000_blink_led_generic;
+	/* setup LED */
+	func->setup_led = e1000_setup_led_generic;
+	/* cleanup LED */
+	func->cleanup_led = e1000_cleanup_led_generic;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_generic;
+	func->led_off = e1000_led_off_generic;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_82571;
+	/* link info */
+	func->get_link_up_info =
+	        (hw->media_type == e1000_media_type_copper)
+	                ? e1000_get_speed_and_duplex_copper_generic
+	                : e1000_get_speed_and_duplex_fiber_serdes_generic;
+
+	hw->dev_spec_size = sizeof(struct e1000_dev_spec_82571);
+
+	/* Device-specific structure allocation */
+	ret_val = e1000_alloc_zeroed_dev_spec_struct(hw, hw->dev_spec_size);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_82571 - Init func ptrs.
+ *  @hw: pointer to the HW structure
+ *
+ *  The only function explicitly called by the api module to initialize
+ *  all function pointers and parameters.
+ **/
+void e1000_init_function_pointers_82571(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_82571");
+
+	hw->func.init_mac_params = e1000_init_mac_params_82571;
+	hw->func.init_nvm_params = e1000_init_nvm_params_82571;
+	hw->func.init_phy_params = e1000_init_phy_params_82571;
+}
+
+/**
+ *  e1000_get_phy_id_82571 - Retrieve the PHY ID and revision
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the PHY registers and stores the PHY ID and possibly the PHY
+ *  revision in the hardware structure.
+ **/
+static s32 e1000_get_phy_id_82571(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_get_phy_id_82571");
+
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		/* The 82571 firmware may still be configuring the PHY.
+		 * In this case, we cannot access the PHY until the
+		 * configuration is done.  So we explicitly set the
+		 * PHY ID. */
+		phy->id = IGP01E1000_I_PHY_ID;
+		break;
+	case e1000_82573:
+		ret_val = e1000_get_phy_id(hw);
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		break;
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_get_hw_semaphore_82571 - Acquire hardware semaphore
+ *  @hw: pointer to the HW structure
+ *
+ *  Acquire the HW semaphore to access the PHY or NVM
+ **/
+s32 e1000_get_hw_semaphore_82571(struct e1000_hw *hw)
+{
+	u32 swsm;
+	s32 ret_val = E1000_SUCCESS;
+	s32 timeout = hw->nvm.word_size + 1;
+	s32 i = 0;
+
+	DEBUGFUNC("e1000_get_hw_semaphore_82571");
+
+	/* Get the FW semaphore. */
+	for (i = 0; i < timeout; i++) {
+		swsm = E1000_READ_REG(hw, E1000_SWSM);
+		E1000_WRITE_REG(hw, E1000_SWSM, swsm | E1000_SWSM_SWESMBI);
+
+		/* Semaphore acquired if bit latched */
+		if (E1000_READ_REG(hw, E1000_SWSM) & E1000_SWSM_SWESMBI)
+			break;
+
+		usec_delay(50);
+	}
+
+	if (i == timeout) {
+		/* Release semaphores */
+		e1000_put_hw_semaphore_generic(hw);
+		DEBUGOUT("Driver can't access the NVM\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_put_hw_semaphore_82571 - Release hardware semaphore
+ *  @hw: pointer to the HW structure
+ *
+ *  Release hardware semaphore used to access the PHY or NVM
+ **/
+void e1000_put_hw_semaphore_82571(struct e1000_hw *hw)
+{
+	u32 swsm;
+
+	DEBUGFUNC("e1000_put_hw_semaphore_82571");
+
+	swsm = E1000_READ_REG(hw, E1000_SWSM);
+
+	swsm &= ~E1000_SWSM_SWESMBI;
+
+	E1000_WRITE_REG(hw, E1000_SWSM, swsm);
+}
+
+/**
+ *  e1000_acquire_nvm_82571 - Request for access to the EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  To gain access to the EEPROM, first we must obtain a hardware semaphore.
+ *  Then for non-82573 hardware, set the EEPROM access request bit and wait
+ *  for EEPROM access grant bit.  If the access grant bit is not set, release
+ *  hardware semaphore.
+ **/
+static s32 e1000_acquire_nvm_82571(struct e1000_hw *hw)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_acquire_nvm_82571");
+
+	ret_val = e1000_get_hw_semaphore_82571(hw);
+	if (ret_val)
+		goto out;
+
+	if (hw->mac.type != e1000_82573)
+		ret_val = e1000_acquire_nvm_generic(hw);
+
+	if (ret_val)
+		e1000_put_hw_semaphore_82571(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_release_nvm_82571 - Release exclusive access to EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Stop any current commands to the EEPROM and clear the EEPROM request bit.
+ **/
+static void e1000_release_nvm_82571(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_release_nvm_82571");
+
+	e1000_release_nvm_generic(hw);
+	e1000_put_hw_semaphore_82571(hw);
+}
+
+/**
+ *  e1000_write_nvm_82571 - Write to EEPROM using appropriate interface
+ *  @hw: pointer to the HW structure
+ *  @offset: offset within the EEPROM to be written to
+ *  @words: number of words to write
+ *  @data: 16 bit word(s) to be written to the EEPROM
+ *
+ *  For non-82573 silicon, write data to EEPROM at offset using SPI interface.
+ *
+ *  If e1000_update_nvm_checksum is not called after this function, the
+ *  EEPROM will most likley contain an invalid checksum.
+ **/
+static s32 e1000_write_nvm_82571(struct e1000_hw *hw, u16 offset, u16 words,
+                                 u16 *data)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_write_nvm_82571");
+
+	switch (hw->mac.type) {
+	case e1000_82573:
+		ret_val = e1000_write_nvm_eewr_82571(hw, offset, words, data);
+		break;
+	case e1000_82571:
+	case e1000_82572:
+		ret_val = e1000_write_nvm_spi(hw, offset, words, data);
+		break;
+	default:
+		ret_val = -E1000_ERR_NVM;
+		break;
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_update_nvm_checksum_82571 - Update EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Updates the EEPROM checksum by reading/adding each word of the EEPROM
+ *  up to the checksum.  Then calculates the EEPROM checksum and writes the
+ *  value to the EEPROM.
+ **/
+static s32 e1000_update_nvm_checksum_82571(struct e1000_hw *hw)
+{
+	u32 eecd;
+	s32 ret_val;
+	u16 i;
+
+	DEBUGFUNC("e1000_update_nvm_checksum_82571");
+
+	ret_val = e1000_update_nvm_checksum_generic(hw);
+	if (ret_val)
+		goto out;
+
+	/* If our nvm is an EEPROM, then we're done
+	 * otherwise, commit the checksum to the flash NVM. */
+	if (hw->nvm.type != e1000_nvm_flash_hw)
+		goto out;
+
+	/* Check for pending operations. */
+	for (i = 0; i < E1000_FLASH_UPDATES; i++) {
+		msec_delay(1);
+		if ((E1000_READ_REG(hw, E1000_EECD) & E1000_EECD_FLUPD) == 0)
+			break;
+	}
+
+	if (i == E1000_FLASH_UPDATES) {
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	/* Reset the firmware if using STM opcode. */
+	if ((E1000_READ_REG(hw, E1000_FLOP) & 0xFF00) == E1000_STM_OPCODE) {
+		/* The enabling of and the actual reset must be done
+		 * in two write cycles.
+		 */
+		E1000_WRITE_REG(hw, E1000_HICR, E1000_HICR_FW_RESET_ENABLE);
+		E1000_WRITE_FLUSH(hw);
+		E1000_WRITE_REG(hw, E1000_HICR, E1000_HICR_FW_RESET);
+	}
+
+	/* Commit the write to flash */
+	eecd = E1000_READ_REG(hw, E1000_EECD) | E1000_EECD_FLUPD;
+	E1000_WRITE_REG(hw, E1000_EECD, eecd);
+
+	for (i = 0; i < E1000_FLASH_UPDATES; i++) {
+		msec_delay(1);
+		if ((E1000_READ_REG(hw, E1000_EECD) & E1000_EECD_FLUPD) == 0)
+			break;
+	}
+
+	if (i == E1000_FLASH_UPDATES) {
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_validate_nvm_checksum_82571 - Validate EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Calculates the EEPROM checksum by reading/adding each word of the EEPROM
+ *  and then verifies that the sum of the EEPROM is equal to 0xBABA.
+ **/
+static s32 e1000_validate_nvm_checksum_82571(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_validate_nvm_checksum_82571");
+
+	if (hw->nvm.type == e1000_nvm_flash_hw)
+		e1000_fix_nvm_checksum_82571(hw);
+
+	return e1000_validate_nvm_checksum_generic(hw);
+}
+
+/**
+ *  e1000_write_nvm_eewr_82571 - Write to EEPROM for 82573 silicon
+ *  @hw: pointer to the HW structure
+ *  @offset: offset within the EEPROM to be written to
+ *  @words: number of words to write
+ *  @data: 16 bit word(s) to be written to the EEPROM
+ *
+ *  After checking for invalid values, poll the EEPROM to ensure the previous
+ *  command has completed before trying to write the next word.  After write
+ *  poll for completion.
+ *
+ *  If e1000_update_nvm_checksum is not called after this function, the
+ *  EEPROM will most likley contain an invalid checksum.
+ **/
+static s32 e1000_write_nvm_eewr_82571(struct e1000_hw *hw, u16 offset,
+                                      u16 words, u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 i, eewr = 0;
+	s32 ret_val = 0;
+
+	DEBUGFUNC("e1000_write_nvm_eewr_82571");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	for (i = 0; i < words; i++) {
+		eewr = (data[i] << E1000_NVM_RW_REG_DATA) |
+		       ((offset+i) << E1000_NVM_RW_ADDR_SHIFT) |
+		       E1000_NVM_RW_REG_START;
+
+		ret_val = e1000_poll_eerd_eewr_done(hw, E1000_NVM_POLL_WRITE);
+		if (ret_val)
+			break;
+
+		E1000_WRITE_REG(hw, E1000_EEWR, eewr);
+
+		ret_val = e1000_poll_eerd_eewr_done(hw, E1000_NVM_POLL_WRITE);
+		if (ret_val)
+			break;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cfg_done_82571 - Poll for configuration done
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the management control register for the config done bit to be set.
+ **/
+static s32 e1000_get_cfg_done_82571(struct e1000_hw *hw)
+{
+	s32 timeout = PHY_CFG_TIMEOUT;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_get_cfg_done_82571");
+
+	while (timeout) {
+		if (E1000_READ_REG(hw, E1000_EEMNGCTL) & E1000_NVM_CFG_DONE_PORT_0)
+			break;
+		msec_delay(1);
+		timeout--;
+	}
+	if (!timeout) {
+		DEBUGOUT("MNG configuration cycle has not completed.\n");
+		ret_val = -E1000_ERR_RESET;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_d0_lplu_state_82571 - Set Low Power Linkup D0 state
+ *  @hw: pointer to the HW structure
+ *  @active: TRUE to enable LPLU, FALSE to disable
+ *
+ *  Sets the LPLU D0 state according to the active flag.  When activating LPLU
+ *  this function also disables smart speed and vice versa.  LPLU will not be
+ *  activated unless the device autonegotiation advertisement meets standards
+ *  of either 10 or 10/100 or 10/100/1000 at all duplexes.  This is a function
+ *  pointer entry point only called by PHY setup routines.
+ **/
+static s32 e1000_set_d0_lplu_state_82571(struct e1000_hw *hw, boolean_t active)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_set_d0_lplu_state_82571");
+
+	ret_val = e1000_read_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, &data);
+	if (ret_val)
+		goto out;
+
+	if (active) {
+		data |= IGP02E1000_PM_D0_LPLU;
+		ret_val = e1000_write_phy_reg(hw,
+		                              IGP02E1000_PHY_POWER_MGMT,
+		                              data);
+		if (ret_val)
+			goto out;
+
+		/* When LPLU is enabled, we should disable SmartSpeed */
+		ret_val = e1000_read_phy_reg(hw,
+		                             IGP01E1000_PHY_PORT_CONFIG,
+		                             &data);
+		data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+		ret_val = e1000_write_phy_reg(hw,
+		                              IGP01E1000_PHY_PORT_CONFIG,
+		                              data);
+		if (ret_val)
+			goto out;
+	} else {
+		data &= ~IGP02E1000_PM_D0_LPLU;
+		ret_val = e1000_write_phy_reg(hw,
+		                              IGP02E1000_PHY_POWER_MGMT,
+		                              data);
+		/* LPLU and SmartSpeed are mutually exclusive.  LPLU is used
+		 * during Dx states where the power conservation is most
+		 * important.  During driver activity we should enable
+		 * SmartSpeed, so performance is maintained. */
+		if (phy->smart_speed == e1000_smart_speed_on) {
+			ret_val = e1000_read_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             &data);
+			if (ret_val)
+				goto out;
+
+			data |= IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		} else if (phy->smart_speed == e1000_smart_speed_off) {
+			ret_val = e1000_read_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_reset_hw_82571 - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state.  This is a
+ *  function pointer entry point called by the api module.
+ **/
+static s32 e1000_reset_hw_82571(struct e1000_hw *hw)
+{
+	u32 ctrl, extcnf_ctrl, ctrl_ext, icr;
+	s32 ret_val;
+	u16 i = 0;
+
+	DEBUGFUNC("e1000_reset_hw_82571");
+
+	/* Prevent the PCI-E bus from sticking if there is no TLP connection
+	 * on the last TLP read/write transaction when MAC is reset.
+	 */
+	ret_val = e1000_disable_pcie_master_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("PCI-E Master disable polling has failed.\n");
+	}
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	msec_delay(10);
+
+	/* Must acquire the MDIO ownership before MAC reset.
+	 * Ownership defaults to firmware after a reset. */
+	if (hw->mac.type == e1000_82573) {
+		extcnf_ctrl = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+		extcnf_ctrl |= E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP;
+
+		do {
+			E1000_WRITE_REG(hw, E1000_EXTCNF_CTRL, extcnf_ctrl);
+			extcnf_ctrl = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+
+			if (extcnf_ctrl & E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP)
+				break;
+
+			extcnf_ctrl |= E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP;
+
+			msec_delay(2);
+			i++;
+		} while (i < MDIO_OWNERSHIP_TIMEOUT);
+	}
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	DEBUGOUT("Issuing a global reset to MAC\n");
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_RST);
+
+	if (hw->nvm.type == e1000_nvm_flash_hw) {
+		usec_delay(10);
+		ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		ctrl_ext |= E1000_CTRL_EXT_EE_RST;
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	ret_val = e1000_get_auto_rd_done_generic(hw);
+	if (ret_val)
+		/* We don't want to continue accessing MAC registers. */
+		goto out;
+
+	/* Phy configuration from NVM just starts after EECD_AUTO_RD is set.
+	 * Need to wait for Phy configuration completion before accessing
+	 * NVM and Phy.
+	 */
+	if (hw->mac.type == e1000_82573)
+		msec_delay(25);
+
+	/* Clear any pending interrupt events. */
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_82571 - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation.
+ **/
+static s32 e1000_init_hw_82571(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 reg_data;
+	s32 ret_val;
+	u16 i, rar_count = mac->rar_entry_count;
+
+	DEBUGFUNC("e1000_init_hw_82571");
+
+	e1000_initialize_hw_bits_82571(hw);
+
+	/* Initialize identification LED */
+	ret_val = e1000_id_led_init_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error initializing identification LED\n");
+		goto out;
+	}
+
+	/* Disabling VLAN filtering */
+	DEBUGOUT("Initializing the IEEE VLAN\n");
+	e1000_clear_vfta(hw);
+
+	/* Setup the receive address. */
+	/* If, however, a locally administered address was assigned to the
+	 * 82571, we must reserve a RAR for it to work around an issue where
+	 * resetting one port will reload the MAC on the other port.
+	 */
+	if (e1000_get_laa_state_82571(hw) == TRUE)
+		rar_count--;
+	e1000_init_rx_addrs_generic(hw, rar_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++)
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	/* Set the transmit descriptor write-back policy */
+	reg_data = E1000_READ_REG(hw, E1000_TXDCTL);
+	reg_data = (reg_data & ~E1000_TXDCTL_WTHRESH) |
+	           E1000_TXDCTL_FULL_TX_DESC_WB |
+	           E1000_TXDCTL_COUNT_DESC;
+	E1000_WRITE_REG(hw, E1000_TXDCTL, reg_data);
+
+	/* ...for both queues. */
+	if (mac->type != e1000_82573) {
+		reg_data = E1000_READ_REG(hw, E1000_TXDCTL1);
+		reg_data = (reg_data & ~E1000_TXDCTL_WTHRESH) |
+		           E1000_TXDCTL_FULL_TX_DESC_WB |
+		           E1000_TXDCTL_COUNT_DESC;
+		E1000_WRITE_REG(hw, E1000_TXDCTL1, reg_data);
+	} else {
+		e1000_enable_tx_pkt_filtering(hw);
+		reg_data = E1000_READ_REG(hw, E1000_GCR);
+		reg_data |= E1000_GCR_L1_ACT_WITHOUT_L0S_RX;
+		E1000_WRITE_REG(hw, E1000_GCR, reg_data);
+	}
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_82571(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_initialize_hw_bits_82571 - Initialize hardware-dependent bits
+ *  @hw: pointer to the HW structure
+ *
+ *  Initializes required hardware-dependent bits needed for normal operation.
+ **/
+static void e1000_initialize_hw_bits_82571(struct e1000_hw *hw)
+{
+	u32 reg;
+
+	DEBUGFUNC("e1000_initialize_hw_bits_82571");
+
+	if (hw->mac.disable_hw_init_bits)
+		goto out;
+
+	/* Transmit Descriptor Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL, reg);
+
+	/* Transmit Descriptor Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL1);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL1, reg);
+
+	/* Transmit Arbitration Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TARC0);
+	reg &= ~(0xF << 27); /* 30:27 */
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		reg |= (1 << 23) | (1 << 24) | (1 << 25) | (1 << 26);
+		break;
+	default:
+		break;
+	}
+	E1000_WRITE_REG(hw, E1000_TARC0, reg);
+
+	/* Transmit Arbitration Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TARC1);
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		reg &= ~((1 << 29) | (1 << 30));
+		reg |= (1 << 22) | (1 << 24) | (1 << 25) | (1 << 26);
+		if (E1000_READ_REG(hw, E1000_TCTL) & E1000_TCTL_MULR)
+			reg &= ~(1 << 28);
+		else
+			reg |= (1 << 28);
+		E1000_WRITE_REG(hw, E1000_TARC1, reg);
+		break;
+	default:
+		break;
+	}
+
+	/* Device Control */
+	if (hw->mac.type == e1000_82573) {
+		reg = E1000_READ_REG(hw, E1000_CTRL);
+		reg &= ~(1 << 29);
+		E1000_WRITE_REG(hw, E1000_CTRL, reg);
+	}
+
+	/* Extended Device Control */
+	if (hw->mac.type == e1000_82573) {
+		reg = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		reg &= ~(1 << 23);
+		reg |= (1 << 22);
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, reg);
+	}
+
+out:
+	return;
+}
+
+/**
+ *  e1000_clear_vfta_82571 - Clear VLAN filter table
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the register array which contains the VLAN filter table by
+ *  setting all the values to 0.
+ **/
+static void e1000_clear_vfta_82571(struct e1000_hw *hw)
+{
+	u32 offset;
+	u32 vfta_value = 0;
+	u32 vfta_offset = 0;
+	u32 vfta_bit_in_reg = 0;
+
+	DEBUGFUNC("e1000_clear_vfta_82571");
+
+	if (hw->mac.type == e1000_82573) {
+		if (hw->mng_cookie.vlan_id != 0) {
+			/* The VFTA is a 4096b bit-field, each identifying
+			 * a single VLAN ID.  The following operations
+			 * determine which 32b entry (i.e. offset) into the
+			 * array we want to set the VLAN ID (i.e. bit) of
+			 * the manageability unit.
+			 */
+			vfta_offset = (hw->mng_cookie.vlan_id >>
+			               E1000_VFTA_ENTRY_SHIFT) &
+			              E1000_VFTA_ENTRY_MASK;
+			vfta_bit_in_reg = 1 << (hw->mng_cookie.vlan_id &
+			                       E1000_VFTA_ENTRY_BIT_SHIFT_MASK);
+		}
+	}
+	for (offset = 0; offset < E1000_VLAN_FILTER_TBL_SIZE; offset++) {
+		/* If the offset we want to clear is the same offset of the
+		 * manageability VLAN ID, then clear all bits except that of
+		 * the manageability unit.
+		 */
+		vfta_value = (offset == vfta_offset) ? vfta_bit_in_reg : 0;
+		E1000_WRITE_REG_ARRAY(hw, E1000_VFTA, offset, vfta_value);
+		E1000_WRITE_FLUSH(hw);
+	}
+}
+
+/**
+ *  e1000_mc_addr_list_update_82571 - Update Multicast addresses
+ *  @hw: pointer to the HW structure
+ *  @mc_addr_list: array of multicast addresses to program
+ *  @mc_addr_count: number of multicast addresses to program
+ *  @rar_used_count: the first RAR register free to program
+ *  @rar_count: total number of supported Receive Address Registers
+ *
+ *  Updates the Receive Address Registers and Multicast Table Array.
+ *  The caller must have a packed mc_addr_list of multicast addresses.
+ *  The parameter rar_count will usually be hw->mac.rar_entry_count
+ *  unless there are workarounds that change this.
+ **/
+static void e1000_mc_addr_list_update_82571(struct e1000_hw *hw,
+                                            u8 *mc_addr_list, u32 mc_addr_count,
+                                            u32 rar_used_count, u32 rar_count)
+{
+	DEBUGFUNC("e1000_mc_addr_list_update_82571");
+
+	if (e1000_get_laa_state_82571(hw))
+		rar_count--;
+
+	e1000_mc_addr_list_update_generic(hw, mc_addr_list, mc_addr_count,
+	                                  rar_used_count, rar_count);
+}
+
+/**
+ *  e1000_setup_link_82571 - Setup flow control and link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines which flow control settings to use, then configures flow
+ *  control.  Calls the appropriate media-specific link configuration
+ *  function.  Assuming the adapter has a valid link partner, a valid link
+ *  should be established.  Assumes the hardware has previously been reset
+ *  and the transmitter and receiver are not enabled.
+ **/
+static s32 e1000_setup_link_82571(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_setup_link_82571");
+
+	/* 82573 does not have a word in the NVM to determine
+	 * the default flow control setting, so we explicitly
+	 * set it to full.
+	 */
+	if (hw->mac.type == e1000_82573)
+		hw->mac.fc = e1000_fc_full;
+
+	return e1000_setup_link_generic(hw);
+}
+
+/**
+ *  e1000_setup_copper_link_82571 - Configure copper link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures the link for auto-neg or forced speed and duplex.  Then we check
+ *  for link, once link is established calls to configure collision distance
+ *  and flow control are called.
+ **/
+static s32 e1000_setup_copper_link_82571(struct e1000_hw *hw)
+{
+	u32 ctrl, led_ctrl;
+	s32  ret_val;
+
+	DEBUGFUNC("e1000_setup_copper_link_82571");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_SLU;
+	ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	switch (hw->phy.type) {
+	case e1000_phy_m88:
+		ret_val = e1000_copper_link_setup_m88(hw);
+		break;
+	case e1000_phy_igp_2:
+		ret_val = e1000_copper_link_setup_igp(hw);
+		/* Setup activity LED */
+		led_ctrl = E1000_READ_REG(hw, E1000_LEDCTL);
+		led_ctrl &= IGP_ACTIVITY_LED_MASK;
+		led_ctrl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
+		E1000_WRITE_REG(hw, E1000_LEDCTL, led_ctrl);
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		break;
+	}
+
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_setup_copper_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_fiber_serdes_link_82571 - Setup link for fiber/serdes
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures collision distance and flow control for fiber and serdes links.
+ *  Upon successful setup, poll for link.
+ **/
+static s32 e1000_setup_fiber_serdes_link_82571(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_setup_fiber_serdes_link_82571");
+
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+		/* If SerDes loopback mode is entered, there is no form
+		 * of reset to take the adapter out of that mode.  So we
+		 * have to explicitly take the adapter out of loopback
+		 * mode.  This prevents drivers from twidling their thumbs
+		 * if another tool failed to take it out of loopback mode.
+		 */
+		E1000_WRITE_REG(hw, E1000_SCTL, E1000_SCTL_DISABLE_SERDES_LOOPBACK);
+		break;
+	default:
+		break;
+	}
+
+	return e1000_setup_fiber_serdes_link_generic(hw);
+}
+
+/**
+ *  e1000_valid_led_default_82571 - Verify a valid default LED config
+ *  @hw: pointer to the HW structure
+ *  @data: pointer to the NVM (EEPROM)
+ *
+ *  Read the EEPROM for the current default LED configuration.  If the
+ *  LED configuration is not valid, set to a valid LED configuration.
+ **/
+static s32 e1000_valid_led_default_82571(struct e1000_hw *hw, u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_valid_led_default_82571");
+
+	ret_val = e1000_read_nvm(hw, NVM_ID_LED_SETTINGS, 1, data);
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+
+	if (hw->mac.type == e1000_82573 &&
+	    *data == ID_LED_RESERVED_F746)
+		*data = ID_LED_DEFAULT_82573;
+	else if (*data == ID_LED_RESERVED_0000 ||
+	         *data == ID_LED_RESERVED_FFFF)
+		*data = ID_LED_DEFAULT;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_laa_state_82571 - Get locally administered address state
+ *  @hw: pointer to the HW structure
+ *
+ *  Retrieve and return the current locally administed address state.
+ **/
+boolean_t e1000_get_laa_state_82571(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_82571 *dev_spec;
+	boolean_t state = FALSE;
+
+	DEBUGFUNC("e1000_get_laa_state_82571");
+
+	if (hw->mac.type != e1000_82571)
+		goto out;
+
+	dev_spec = (struct e1000_dev_spec_82571 *)hw->dev_spec;
+
+	state = dev_spec->laa_is_present;
+
+out:
+	return state;
+}
+
+/**
+ *  e1000_set_laa_state_82571 - Set locally administered address state
+ *  @hw: pointer to the HW structure
+ *  @state: enable/disable locally administered address
+ *
+ *  Enable/Disable the current locally administed address state.
+ **/
+void e1000_set_laa_state_82571(struct e1000_hw *hw, boolean_t state)
+{
+	struct e1000_dev_spec_82571 *dev_spec;
+
+	DEBUGFUNC("e1000_set_laa_state_82571");
+
+	if (hw->mac.type != e1000_82571)
+		goto out;
+
+	dev_spec = (struct e1000_dev_spec_82571 *)hw->dev_spec;
+
+	dev_spec->laa_is_present = state;
+
+	/* If workaround is activated... */
+	if (state == TRUE) {
+		/* Hold a copy of the LAA in RAR[14] This is done so that
+		 * between the time RAR[0] gets clobbered and the time it
+		 * gets fixed, the actual LAA is in one of the RARs and no
+		 * incoming packets directed to this port are dropped.
+		 * Eventually the LAA will be in RAR[0] and RAR[14].
+		 */
+		e1000_rar_set_generic(hw, hw->mac.addr,
+		                      hw->mac.rar_entry_count - 1);
+	}
+
+out:
+	return;
+}
+
+/**
+ *  e1000_fix_nvm_checksum_82571 - Fix EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Verifies that the EEPROM has completed the update.  After updating the
+ *  EEPROM, we need to check bit 15 in work 0x23 for the checksum fix.  If
+ *  the checksum fix is not implemented, we need to set the bit and update
+ *  the checksum.  Otherwise, if bit 15 is set and the checksum is incorrect,
+ *  we need to return bad checksum.
+ **/
+static s32 e1000_fix_nvm_checksum_82571(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	s32 ret_val = E1000_SUCCESS;
+	u16 data;
+
+	DEBUGFUNC("e1000_fix_nvm_checksum_82571");
+
+	if (nvm->type != e1000_nvm_flash_hw)
+		goto out;
+
+	/* Check bit 4 of word 10h.  If it is 0, firmware is done updating
+	 * 10h-12h.  Checksum may need to be fixed.
+	 */
+	ret_val = e1000_read_nvm(hw, 0x10, 1, &data);
+	if (ret_val)
+		goto out;
+
+	if (!(data & 0x10)) {
+		/* Read 0x23 and check bit 15.  This bit is a 1
+		 * when the checksum has already been fixed.  If
+		 * the checksum is still wrong and this bit is a
+		 * 1, we need to return bad checksum.  Otherwise,
+		 * we need to set this bit to a 1 and update the
+		 * checksum.
+		 */
+		ret_val = e1000_read_nvm(hw, 0x23, 1, &data);
+		if (ret_val)
+			goto out;
+
+		if (!(data & 0x8000)) {
+			data |= 0x8000;
+			ret_val = e1000_write_nvm(hw, 0x23, 1, &data);
+			if (ret_val)
+				goto out;
+			ret_val = e1000_update_nvm_checksum(hw);
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_82571 - Clear device specific hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the hardware counters by reading the counter registers.
+ **/
+static void e1000_clear_hw_cntrs_82571(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_82571");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+	temp = E1000_READ_REG(hw, E1000_PRC64);
+	temp = E1000_READ_REG(hw, E1000_PRC127);
+	temp = E1000_READ_REG(hw, E1000_PRC255);
+	temp = E1000_READ_REG(hw, E1000_PRC511);
+	temp = E1000_READ_REG(hw, E1000_PRC1023);
+	temp = E1000_READ_REG(hw, E1000_PRC1522);
+	temp = E1000_READ_REG(hw, E1000_PTC64);
+	temp = E1000_READ_REG(hw, E1000_PTC127);
+	temp = E1000_READ_REG(hw, E1000_PTC255);
+	temp = E1000_READ_REG(hw, E1000_PTC511);
+	temp = E1000_READ_REG(hw, E1000_PTC1023);
+	temp = E1000_READ_REG(hw, E1000_PTC1522);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+
+	temp = E1000_READ_REG(hw, E1000_MGTPRC);
+	temp = E1000_READ_REG(hw, E1000_MGTPDC);
+	temp = E1000_READ_REG(hw, E1000_MGTPTC);
+
+	temp = E1000_READ_REG(hw, E1000_IAC);
+	temp = E1000_READ_REG(hw, E1000_ICRXOC);
+
+	temp = E1000_READ_REG(hw, E1000_ICRXPTC);
+	temp = E1000_READ_REG(hw, E1000_ICRXATC);
+	temp = E1000_READ_REG(hw, E1000_ICTXPTC);
+	temp = E1000_READ_REG(hw, E1000_ICTXATC);
+	temp = E1000_READ_REG(hw, E1000_ICTXQEC);
+	temp = E1000_READ_REG(hw, E1000_ICTXQMTC);
+	temp = E1000_READ_REG(hw, E1000_ICRXDMTC);
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_82571.h linux-2.6.9/drivers/net/e1000/e1000_82571.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_82571.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_82571.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,40 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_82571_H_
+#define _E1000_82571_H_
+
+#define ID_LED_RESERVED_F746 0xF746
+#define ID_LED_DEFAULT_82573 ((ID_LED_DEF1_DEF2 << 12) | \
+                              (ID_LED_OFF1_ON2  <<  8) | \
+                              (ID_LED_DEF1_DEF2 <<  4) | \
+                              (ID_LED_DEF1_DEF2))
+
+#define E1000_GCR_L1_ACT_WITHOUT_L0S_RX 0x08000000
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_api.c linux-2.6.9/drivers/net/e1000/e1000_api.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_api.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_api.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1118 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "e1000_api.h"
+#include "e1000_mac.h"
+#include "e1000_nvm.h"
+#include "e1000_phy.h"
+
+extern void    e1000_init_function_pointers_82542(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_82543(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_82540(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_82571(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_82541(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_80003es2lan(struct e1000_hw *hw);
+extern void    e1000_init_function_pointers_ich8lan(struct e1000_hw *hw);
+
+/**
+ *  e1000_init_mac_params - Initialize MAC function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  This function initializes the function pointers for the MAC
+ *  set of functions.  Called by drivers or by e1000_setup_init_funcs.
+ **/
+s32 e1000_init_mac_params(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	if (hw->func.init_mac_params != NULL) {
+		ret_val = hw->func.init_mac_params(hw);
+		if (ret_val) {
+			DEBUGOUT("MAC Initialization Error\n");
+			goto out;
+		}
+	} else {
+		DEBUGOUT("mac.init_mac_params was NULL\n");
+		ret_val = -E1000_ERR_CONFIG;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params - Initialize NVM function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  This function initializes the function pointers for the NVM
+ *  set of functions.  Called by drivers or by e1000_setup_init_funcs.
+ **/
+s32 e1000_init_nvm_params(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	if (hw->func.init_nvm_params != NULL) {
+		ret_val = hw->func.init_nvm_params(hw);
+		if (ret_val) {
+			DEBUGOUT("NVM Initialization Error\n");
+			goto out;
+		}
+	} else {
+		DEBUGOUT("nvm.init_nvm_params was NULL\n");
+		ret_val = -E1000_ERR_CONFIG;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_phy_params - Initialize PHY function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  This function initializes the function pointers for the PHY
+ *  set of functions.  Called by drivers or by e1000_setup_init_funcs.
+ **/
+s32 e1000_init_phy_params(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	if (hw->func.init_phy_params != NULL) {
+		ret_val = hw->func.init_phy_params(hw);
+		if (ret_val) {
+			DEBUGOUT("PHY Initialization Error\n");
+			goto out;
+		}
+	} else {
+		DEBUGOUT("phy.init_phy_params was NULL\n");
+		ret_val =  -E1000_ERR_CONFIG;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_mac_type - Sets MAC type
+ *  @hw: pointer to the HW structure
+ *
+ *  This function sets the mac type of the adapter based on the
+ *  device ID stored in the hw structure.
+ *  MUST BE FIRST FUNCTION CALLED (explicitly or through
+ *  e1000_setup_init_funcs()).
+ **/
+s32 e1000_set_mac_type(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_set_mac_type");
+
+	switch (hw->device_id) {
+	case E1000_DEV_ID_82542:
+		mac->type = e1000_82542;
+		break;
+	case E1000_DEV_ID_82543GC_FIBER:
+	case E1000_DEV_ID_82543GC_COPPER:
+		mac->type = e1000_82543;
+		break;
+	case E1000_DEV_ID_82544EI_COPPER:
+	case E1000_DEV_ID_82544EI_FIBER:
+	case E1000_DEV_ID_82544GC_COPPER:
+	case E1000_DEV_ID_82544GC_LOM:
+		mac->type = e1000_82544;
+		break;
+	case E1000_DEV_ID_82540EM:
+	case E1000_DEV_ID_82540EM_LOM:
+	case E1000_DEV_ID_82540EP:
+	case E1000_DEV_ID_82540EP_LOM:
+	case E1000_DEV_ID_82540EP_LP:
+		mac->type = e1000_82540;
+		break;
+	case E1000_DEV_ID_82545EM_COPPER:
+	case E1000_DEV_ID_82545EM_FIBER:
+		mac->type = e1000_82545;
+		break;
+	case E1000_DEV_ID_82545GM_COPPER:
+	case E1000_DEV_ID_82545GM_FIBER:
+	case E1000_DEV_ID_82545GM_SERDES:
+		mac->type = e1000_82545_rev_3;
+		break;
+	case E1000_DEV_ID_82546EB_COPPER:
+	case E1000_DEV_ID_82546EB_FIBER:
+	case E1000_DEV_ID_82546EB_QUAD_COPPER:
+		mac->type = e1000_82546;
+		break;
+	case E1000_DEV_ID_82546GB_COPPER:
+	case E1000_DEV_ID_82546GB_FIBER:
+	case E1000_DEV_ID_82546GB_SERDES:
+	case E1000_DEV_ID_82546GB_PCIE:
+	case E1000_DEV_ID_82546GB_QUAD_COPPER:
+	case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
+		mac->type = e1000_82546_rev_3;
+		break;
+	case E1000_DEV_ID_82541EI:
+	case E1000_DEV_ID_82541EI_MOBILE:
+	case E1000_DEV_ID_82541ER_LOM:
+		mac->type = e1000_82541;
+		break;
+	case E1000_DEV_ID_82541ER:
+	case E1000_DEV_ID_82541GI:
+	case E1000_DEV_ID_82541GI_LF:
+	case E1000_DEV_ID_82541GI_MOBILE:
+		mac->type = e1000_82541_rev_2;
+		break;
+	case E1000_DEV_ID_82547EI:
+	case E1000_DEV_ID_82547EI_MOBILE:
+		mac->type = e1000_82547;
+		break;
+	case E1000_DEV_ID_82547GI:
+		mac->type = e1000_82547_rev_2;
+		break;
+	case E1000_DEV_ID_82571EB_COPPER:
+	case E1000_DEV_ID_82571EB_FIBER:
+	case E1000_DEV_ID_82571EB_SERDES:
+	case E1000_DEV_ID_82571EB_QUAD_COPPER:
+	case E1000_DEV_ID_82571EB_QUAD_FIBER:
+	case E1000_DEV_ID_82571EB_QUAD_COPPER_LP:
+		mac->type = e1000_82571;
+		break;
+	case E1000_DEV_ID_82572EI:
+	case E1000_DEV_ID_82572EI_COPPER:
+	case E1000_DEV_ID_82572EI_FIBER:
+	case E1000_DEV_ID_82572EI_SERDES:
+		mac->type = e1000_82572;
+		break;
+	case E1000_DEV_ID_82573E:
+	case E1000_DEV_ID_82573E_IAMT:
+	case E1000_DEV_ID_82573L:
+		mac->type = e1000_82573;
+		break;
+	case E1000_DEV_ID_80003ES2LAN_COPPER_DPT:
+	case E1000_DEV_ID_80003ES2LAN_SERDES_DPT:
+	case E1000_DEV_ID_80003ES2LAN_COPPER_SPT:
+	case E1000_DEV_ID_80003ES2LAN_SERDES_SPT:
+		mac->type = e1000_80003es2lan;
+		break;
+	case E1000_DEV_ID_ICH8_IFE:
+	case E1000_DEV_ID_ICH8_IFE_GT:
+	case E1000_DEV_ID_ICH8_IFE_G:
+	case E1000_DEV_ID_ICH8_IGP_M:
+	case E1000_DEV_ID_ICH8_IGP_M_AMT:
+	case E1000_DEV_ID_ICH8_IGP_AMT:
+	case E1000_DEV_ID_ICH8_IGP_C:
+		mac->type = e1000_ich8lan;
+		break;
+	case E1000_DEV_ID_ICH9_IFE:
+	case E1000_DEV_ID_ICH9_IFE_GT:
+	case E1000_DEV_ID_ICH9_IFE_G:
+	case E1000_DEV_ID_ICH9_IGP_AMT:
+	case E1000_DEV_ID_ICH9_IGP_C:
+		mac->type = e1000_ich9lan;
+		break;
+	default:
+		/* Should never have loaded on this device */
+		ret_val = -E1000_ERR_MAC_INIT;
+		break;
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_init_funcs - Initializes function pointers
+ *  @hw: pointer to the HW structure
+ *  @init_device: TRUE will initialize the rest of the function pointers
+ *                 getting the device ready for use.  FALSE will only set
+ *                 MAC type and the function pointers for the other init
+ *                 functions.  Passing FALSE will not generate any hardware
+ *                 reads or writes.
+ *
+ *  This function must be called by a driver in order to use the rest
+ *  of the 'shared' code files. Called by drivers only.
+ **/
+s32 e1000_setup_init_funcs(struct e1000_hw *hw, boolean_t init_device)
+{
+	s32 ret_val;
+
+	/* Can't do much good without knowing the MAC type.
+	 */
+	ret_val = e1000_set_mac_type(hw);
+	if (ret_val) {
+		DEBUGOUT("ERROR: MAC type could not be set properly.\n");
+		goto out;
+	}
+
+	if (!hw->hw_addr) {
+		DEBUGOUT("ERROR: Registers not mapped\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	/* Init some generic function pointers that are currently all pointing
+	 * to generic implementations. We do this first allowing a driver
+	 * module to override it afterwards.
+	 */
+	hw->func.config_collision_dist = e1000_config_collision_dist_generic;
+	hw->func.rar_set = e1000_rar_set_generic;
+	hw->func.validate_mdi_setting = e1000_validate_mdi_setting_generic;
+	hw->func.mng_host_if_write = e1000_mng_host_if_write_generic;
+	hw->func.mng_write_cmd_header = e1000_mng_write_cmd_header_generic;
+	hw->func.mng_enable_host_if = e1000_mng_enable_host_if_generic;
+	hw->func.wait_autoneg = e1000_wait_autoneg_generic;
+	hw->func.reload_nvm = e1000_reload_nvm_generic;
+
+	/* Set up the init function pointers. These are functions within the
+	 * adapter family file that sets up function pointers for the rest of
+	 * the functions in that family.
+	 */
+	switch (hw->mac.type) {
+	case e1000_82542:
+		e1000_init_function_pointers_82542(hw);
+		break;
+	case e1000_82543:
+	case e1000_82544:
+		e1000_init_function_pointers_82543(hw);
+		break;
+	case e1000_82540:
+	case e1000_82545:
+	case e1000_82545_rev_3:
+	case e1000_82546:
+	case e1000_82546_rev_3:
+		e1000_init_function_pointers_82540(hw);
+		break;
+	case e1000_82541:
+	case e1000_82541_rev_2:
+	case e1000_82547:
+	case e1000_82547_rev_2:
+		e1000_init_function_pointers_82541(hw);
+		break;
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_82573:
+		e1000_init_function_pointers_82571(hw);
+		break;
+	case e1000_80003es2lan:
+		e1000_init_function_pointers_80003es2lan(hw);
+		break;
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		e1000_init_function_pointers_ich8lan(hw);
+		break;
+	default:
+		DEBUGOUT("Hardware not supported\n");
+		ret_val = -E1000_ERR_CONFIG;
+		break;
+	}
+
+	/* Initialize the rest of the function pointers. These require some
+	 * register reads/writes in some cases.
+	 */
+	if ((ret_val == E1000_SUCCESS) && (init_device == TRUE)) {
+		ret_val = e1000_init_mac_params(hw);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_init_nvm_params(hw);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_init_phy_params(hw);
+		if (ret_val)
+			goto out;
+
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_remove_device - Free device specific structure
+ *  @hw: pointer to the HW structure
+ *
+ *  If a device specific structure was allocated, this function will
+ *  free it. This is a function pointer entry point called by drivers.
+ **/
+void e1000_remove_device(struct e1000_hw *hw)
+{
+	if (hw->func.remove_device != NULL)
+		hw->func.remove_device(hw);
+}
+
+/**
+ *  e1000_get_bus_info - Obtain bus information for adapter
+ *  @hw: pointer to the HW structure
+ *
+ *  This will obtain information about the HW bus for which the
+ *  adaper is attached and stores it in the hw structure. This is a
+ *  function pointer entry point called by drivers.
+ **/
+s32 e1000_get_bus_info(struct e1000_hw *hw)
+{
+	if (hw->func.get_bus_info != NULL)
+		return hw->func.get_bus_info(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_clear_vfta - Clear VLAN filter table
+ *  @hw: pointer to the HW structure
+ *
+ *  This clears the VLAN filter table on the adapter. This is a function
+ *  pointer entry point called by drivers.
+ **/
+void e1000_clear_vfta(struct e1000_hw *hw)
+{
+	if (hw->func.clear_vfta != NULL)
+		hw->func.clear_vfta (hw);
+}
+
+/**
+ *  e1000_write_vfta - Write value to VLAN filter table
+ *  @hw: pointer to the HW structure
+ *  @offset: the 32-bit offset in which to write the value to.
+ *  @value: the 32-bit value to write at location offset.
+ *
+ *  This writes a 32-bit value to a 32-bit offset in the VLAN filter
+ *  table. This is a function pointer entry point called by drivers.
+ **/
+void e1000_write_vfta(struct e1000_hw *hw, u32 offset, u32 value)
+{
+	if (hw->func.write_vfta != NULL)
+		hw->func.write_vfta(hw, offset, value);
+}
+
+/**
+ *  e1000_mc_addr_list_update - Update Multicast addresses
+ *  @hw: pointer to the HW structure
+ *  @mc_addr_list: array of multicast addresses to program
+ *  @mc_addr_count: number of multicast addresses to program
+ *  @rar_used_count: the first RAR register free to program
+ *  @rar_count: total number of supported Receive Address Registers
+ *
+ *  Updates the Receive Address Registers and Multicast Table Array.
+ *  The caller must have a packed mc_addr_list of multicast addresses.
+ *  The parameter rar_count will usually be hw->mac.rar_entry_count
+ *  unless there are workarounds that change this.  Currently no func pointer
+ *  exists and all implementations are handled in the generic version of this
+ *  function.
+ **/
+void e1000_mc_addr_list_update(struct e1000_hw *hw, u8 *mc_addr_list,
+                               u32 mc_addr_count, u32 rar_used_count,
+                               u32 rar_count)
+{
+	if (hw->func.mc_addr_list_update != NULL)
+		hw->func.mc_addr_list_update(hw,
+		                             mc_addr_list,
+		                             mc_addr_count,
+		                             rar_used_count,
+		                             rar_count);
+}
+
+/**
+ *  e1000_force_mac_fc - Force MAC flow control
+ *  @hw: pointer to the HW structure
+ *
+ *  Force the MAC's flow control settings. Currently no func pointer exists
+ *  and all implementations are handled in the generic version of this
+ *  function.
+ **/
+s32 e1000_force_mac_fc(struct e1000_hw *hw)
+{
+	return e1000_force_mac_fc_generic(hw);
+}
+
+/**
+ *  e1000_check_for_link - Check/Store link connection
+ *  @hw: pointer to the HW structure
+ *
+ *  This checks the link condition of the adapter and stores the
+ *  results in the hw->mac structure. This is a function pointer entry
+ *  point called by drivers.
+ **/
+s32 e1000_check_for_link(struct e1000_hw *hw)
+{
+	if (hw->func.check_for_link != NULL)
+		return hw->func.check_for_link(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_check_mng_mode - Check management mode
+ *  @hw: pointer to the HW structure
+ *
+ *  This checks if the adapter has manageability enabled.
+ *  This is a function pointer entry point called by drivers.
+ **/
+boolean_t e1000_check_mng_mode(struct e1000_hw *hw)
+{
+	if (hw->func.check_mng_mode != NULL)
+		return hw->func.check_mng_mode(hw);
+	else
+		return FALSE;
+}
+
+/**
+ *  e1000_mng_write_dhcp_info - Writes DHCP info to host interface
+ *  @hw: pointer to the HW structure
+ *  @buffer: pointer to the host interface
+ *  @length: size of the buffer
+ *
+ *  Writes the DHCP information to the host interface.
+ **/
+s32 e1000_mng_write_dhcp_info(struct e1000_hw *hw, u8 *buffer, u16 length)
+{
+	return e1000_mng_write_dhcp_info_generic(hw, buffer, length);
+}
+
+/**
+ *  e1000_reset_hw - Reset hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This resets the hardware into a known state. This is a function pointer
+ *  entry point called by drivers.
+ **/
+s32 e1000_reset_hw(struct e1000_hw *hw)
+{
+	if (hw->func.reset_hw != NULL)
+		return hw->func.reset_hw(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_init_hw - Initialize hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  This inits the hardware readying it for operation. This is a function
+ *  pointer entry point called by drivers.
+ **/
+s32 e1000_init_hw(struct e1000_hw *hw)
+{
+	if (hw->func.init_hw != NULL)
+		return hw->func.init_hw(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_setup_link - Configures link and flow control
+ *  @hw: pointer to the HW structure
+ *
+ *  This configures link and flow control settings for the adapter. This
+ *  is a function pointer entry point called by drivers. While modules can
+ *  also call this, they probably call their own version of this function.
+ **/
+s32 e1000_setup_link(struct e1000_hw *hw)
+{
+	if (hw->func.setup_link != NULL)
+		return hw->func.setup_link(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_get_speed_and_duplex - Returns current speed and duplex
+ *  @hw: pointer to the HW structure
+ *  @speed: pointer to a 16-bit value to store the speed
+ *  @duplex: pointer to a 16-bit value to store the duplex.
+ *
+ *  This returns the speed and duplex of the adapter in the two 'out'
+ *  variables passed in. This is a function pointer entry point called
+ *  by drivers.
+ **/
+s32 e1000_get_speed_and_duplex(struct e1000_hw *hw, u16 *speed, u16 *duplex)
+{
+	if (hw->func.get_link_up_info != NULL)
+		return hw->func.get_link_up_info(hw, speed, duplex);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_setup_led - Configures SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  This prepares the SW controllable LED for use and saves the current state
+ *  of the LED so it can be later restored. This is a function pointer entry
+ *  point called by drivers.
+ **/
+s32 e1000_setup_led(struct e1000_hw *hw)
+{
+	if (hw->func.setup_led != NULL)
+		return hw->func.setup_led(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_cleanup_led - Restores SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  This restores the SW controllable LED to the value saved off by
+ *  e1000_setup_led. This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_cleanup_led(struct e1000_hw *hw)
+{
+	if (hw->func.cleanup_led != NULL)
+		return hw->func.cleanup_led(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_blink_led - Blink SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  This starts the adapter LED blinking. Request the LED to be setup first
+ *  and cleaned up after. This is a function pointer entry point called by
+ *  drivers.
+ **/
+s32 e1000_blink_led(struct e1000_hw *hw)
+{
+	if (hw->func.blink_led != NULL)
+		return hw->func.blink_led(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_on - Turn on SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED on. This is a function pointer entry point
+ *  called by drivers.
+ **/
+s32 e1000_led_on(struct e1000_hw *hw)
+{
+	if (hw->func.led_on != NULL)
+		return hw->func.led_on(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_off - Turn off SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Turns the SW defined LED off. This is a function pointer entry point
+ *  called by drivers.
+ **/
+s32 e1000_led_off(struct e1000_hw *hw)
+{
+	if (hw->func.led_off != NULL)
+		return hw->func.led_off(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_reset_adaptive - Reset adaptive IFS
+ *  @hw: pointer to the HW structure
+ *
+ *  Resets the adaptive IFS. Currently no func pointer exists and all
+ *  implementations are handled in the generic version of this function.
+ **/
+void e1000_reset_adaptive(struct e1000_hw *hw)
+{
+	e1000_reset_adaptive_generic(hw);
+}
+
+/**
+ *  e1000_update_adaptive - Update adaptive IFS
+ *  @hw: pointer to the HW structure
+ *
+ *  Updates adapter IFS. Currently no func pointer exists and all
+ *  implementations are handled in the generic version of this function.
+ **/
+void e1000_update_adaptive(struct e1000_hw *hw)
+{
+	e1000_update_adaptive_generic(hw);
+}
+
+/**
+ *  e1000_disable_pcie_master - Disable PCI-Express master access
+ *  @hw: pointer to the HW structure
+ *
+ *  Disables PCI-Express master access and verifies there are no pending
+ *  requests. Currently no func pointer exists and all implementations are
+ *  handled in the generic version of this function.
+ **/
+s32 e1000_disable_pcie_master(struct e1000_hw *hw)
+{
+	return e1000_disable_pcie_master_generic(hw);
+}
+
+/**
+ *  e1000_config_collision_dist - Configure collision distance
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures the collision distance to the default value and is used
+ *  during link setup.
+ **/
+void e1000_config_collision_dist(struct e1000_hw *hw)
+{
+	if (hw->func.config_collision_dist != NULL)
+		hw->func.config_collision_dist(hw);
+}
+
+/**
+ *  e1000_rar_set - Sets a receive address register
+ *  @hw: pointer to the HW structure
+ *  @addr: address to set the RAR to
+ *  @index: the RAR to set
+ *
+ *  Sets a Receive Address Register (RAR) to the specified address.
+ **/
+void e1000_rar_set(struct e1000_hw *hw, u8 *addr, u32 index)
+{
+	if (hw->func.rar_set != NULL)
+		hw->func.rar_set(hw, addr, index);
+}
+
+/**
+ *  e1000_validate_mdi_setting - Ensures valid MDI/MDIX SW state
+ *  @hw: pointer to the HW structure
+ *
+ *  Ensures that the MDI/MDIX SW state is valid.
+ **/
+s32 e1000_validate_mdi_setting(struct e1000_hw *hw)
+{
+	if (hw->func.validate_mdi_setting != NULL)
+		return hw->func.validate_mdi_setting(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_mta_set - Sets multicast table bit
+ *  @hw: pointer to the HW structure
+ *  @hash_value: Multicast hash value.
+ *
+ *  This sets the bit in the multicast table corresponding to the
+ *  hash value.  This is a function pointer entry point called by drivers.
+ **/
+void e1000_mta_set(struct e1000_hw *hw, u32 hash_value)
+{
+	if (hw->func.mta_set != NULL)
+		hw->func.mta_set(hw, hash_value);
+}
+
+/**
+ *  e1000_hash_mc_addr - Determines address location in multicast table
+ *  @hw: pointer to the HW structure
+ *  @mc_addr: Multicast address to hash.
+ *
+ *  This hashes an address to determine its location in the multicast
+ *  table. Currently no func pointer exists and all implementations
+ *  are handled in the generic version of this function.
+ **/
+u32 e1000_hash_mc_addr(struct e1000_hw *hw, u8 *mc_addr)
+{
+	return e1000_hash_mc_addr_generic(hw, mc_addr);
+}
+
+/**
+ *  e1000_enable_tx_pkt_filtering - Enable packet filtering on TX
+ *  @hw: pointer to the HW structure
+ *
+ *  Enables packet filtering on transmit packets if manageability is enabled
+ *  and host interface is enabled.
+ *  Currently no func pointer exists and all implementations are handled in the
+ *  generic version of this function.
+ **/
+boolean_t e1000_enable_tx_pkt_filtering(struct e1000_hw *hw)
+{
+	return e1000_enable_tx_pkt_filtering_generic(hw);
+}
+
+/**
+ *  e1000_mng_host_if_write - Writes to the manageability host interface
+ *  @hw: pointer to the HW structure
+ *  @buffer: pointer to the host interface buffer
+ *  @length: size of the buffer
+ *  @offset: location in the buffer to write to
+ *  @sum: sum of the data (not checksum)
+ *
+ *  This function writes the buffer content at the offset given on the host if.
+ *  It also does alignment considerations to do the writes in most efficient
+ *  way.  Also fills up the sum of the buffer in *buffer parameter.
+ **/
+s32 e1000_mng_host_if_write(struct e1000_hw * hw, u8 *buffer, u16 length,
+                            u16 offset, u8 *sum)
+{
+	if (hw->func.mng_host_if_write != NULL)
+		return hw->func.mng_host_if_write(hw, buffer, length, offset,
+		                                  sum);
+	else
+		return E1000_NOT_IMPLEMENTED;
+}
+
+/**
+ *  e1000_mng_write_cmd_header - Writes manageability command header
+ *  @hw: pointer to the HW structure
+ *  @hdr: pointer to the host interface command header
+ *
+ *  Writes the command header after does the checksum calculation.
+ **/
+s32 e1000_mng_write_cmd_header(struct e1000_hw *hw,
+                               struct e1000_host_mng_command_header *hdr)
+{
+	if (hw->func.mng_write_cmd_header != NULL)
+		return hw->func.mng_write_cmd_header(hw, hdr);
+	else
+		return E1000_NOT_IMPLEMENTED;
+}
+
+/**
+ *  e1000_mng_enable_host_if - Checks host interface is enabled
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns E1000_success upon success, else E1000_ERR_HOST_INTERFACE_COMMAND
+ *
+ *  This function checks whether the HOST IF is enabled for command operaton
+ *  and also checks whether the previous command is completed.  It busy waits
+ *  in case of previous command is not completed.
+ **/
+s32 e1000_mng_enable_host_if(struct e1000_hw * hw)
+{
+	if (hw->func.mng_enable_host_if != NULL)
+		return hw->func.mng_enable_host_if(hw);
+	else
+		return E1000_NOT_IMPLEMENTED;
+}
+
+/**
+ *  e1000_wait_autoneg - Waits for autonegotiation completion
+ *  @hw: pointer to the HW structure
+ *
+ *  Waits for autoneg to complete. Currently no func pointer exists and all
+ *  implementations are handled in the generic version of this function.
+ **/
+s32 e1000_wait_autoneg(struct e1000_hw *hw)
+{
+	if (hw->func.wait_autoneg != NULL)
+		return hw->func.wait_autoneg(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_check_reset_block - Verifies PHY can be reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks if the PHY is in a state that can be reset or if manageability
+ *  has it tied up. This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_check_reset_block(struct e1000_hw *hw)
+{
+	if (hw->func.check_reset_block != NULL)
+		return hw->func.check_reset_block(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_read_phy_reg - Reads PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: the register to read
+ *  @data: the buffer to store the 16-bit read.
+ *
+ *  Reads the PHY register and returns the value in data.
+ *  This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_read_phy_reg(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	if (hw->func.read_phy_reg != NULL)
+		return hw->func.read_phy_reg(hw, offset, data);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_write_phy_reg - Writes PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: the register to write
+ *  @data: the value to write.
+ *
+ *  Writes the PHY register at offset with the value in data.
+ *  This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_write_phy_reg(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	if (hw->func.write_phy_reg != NULL)
+		return hw->func.write_phy_reg(hw, offset, data);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_read_kmrn_reg - Reads register using Kumeran interface
+ *  @hw: pointer to the HW structure
+ *  @offset: the register to read
+ *  @data: the location to store the 16-bit value read.
+ *
+ *  Reads a register out of the Kumeran interface. Currently no func pointer
+ *  exists and all implementations are handled in the generic version of
+ *  this function.
+ **/
+s32 e1000_read_kmrn_reg(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	return e1000_read_kmrn_reg_generic(hw, offset, data);
+}
+
+/**
+ *  e1000_write_kmrn_reg - Writes register using Kumeran interface
+ *  @hw: pointer to the HW structure
+ *  @offset: the register to write
+ *  @data: the value to write.
+ *
+ *  Writes a register to the Kumeran interface. Currently no func pointer
+ *  exists and all implementations are handled in the generic version of
+ *  this function.
+ **/
+s32 e1000_write_kmrn_reg(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	return e1000_write_kmrn_reg_generic(hw, offset, data);
+}
+
+/**
+ *  e1000_get_cable_length - Retrieves cable length estimation
+ *  @hw: pointer to the HW structure
+ *
+ *  This function estimates the cable length and stores them in
+ *  hw->phy.min_length and hw->phy.max_length. This is a function pointer
+ *  entry point called by drivers.
+ **/
+s32 e1000_get_cable_length(struct e1000_hw *hw)
+{
+	if (hw->func.get_cable_length != NULL)
+		return hw->func.get_cable_length(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_get_phy_info - Retrieves PHY information from registers
+ *  @hw: pointer to the HW structure
+ *
+ *  This function gets some information from various PHY registers and
+ *  populates hw->phy values with it. This is a function pointer entry
+ *  point called by drivers.
+ **/
+s32 e1000_get_phy_info(struct e1000_hw *hw)
+{
+	if (hw->func.get_phy_info != NULL)
+		return hw->func.get_phy_info(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_phy_hw_reset - Hard PHY reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Performs a hard PHY reset. This is a function pointer entry point called
+ *  by drivers.
+ **/
+s32 e1000_phy_hw_reset(struct e1000_hw *hw)
+{
+	if (hw->func.reset_phy != NULL)
+		return hw->func.reset_phy(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_phy_commit - Soft PHY reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Performs a soft PHY reset on those that apply. This is a function pointer
+ *  entry point called by drivers.
+ **/
+s32 e1000_phy_commit(struct e1000_hw *hw)
+{
+	if (hw->func.commit_phy != NULL)
+		return hw->func.commit_phy(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_set_d3_lplu_state - Sets low power link up state for D0
+ *  @hw: pointer to the HW structure
+ *  @active: boolean used to enable/disable lplu
+ *
+ *  Success returns 0, Failure returns 1
+ *
+ *  The low power link up (lplu) state is set to the power management level D0
+ *  and SmartSpeed is disabled when active is true, else clear lplu for D0
+ *  and enable Smartspeed.  LPLU and Smartspeed are mutually exclusive.  LPLU
+ *  is used during Dx states where the power conservation is most important.
+ *  During driver activity, SmartSpeed should be enabled so performance is
+ *  maintained.  This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_set_d0_lplu_state(struct e1000_hw *hw, boolean_t active)
+{
+	if (hw->func.set_d0_lplu_state != NULL)
+		return hw->func.set_d0_lplu_state(hw, active);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_set_d3_lplu_state - Sets low power link up state for D3
+ *  @hw: pointer to the HW structure
+ *  @active: boolean used to enable/disable lplu
+ *
+ *  Success returns 0, Failure returns 1
+ *
+ *  The low power link up (lplu) state is set to the power management level D3
+ *  and SmartSpeed is disabled when active is true, else clear lplu for D3
+ *  and enable Smartspeed.  LPLU and Smartspeed are mutually exclusive.  LPLU
+ *  is used during Dx states where the power conservation is most important.
+ *  During driver activity, SmartSpeed should be enabled so performance is
+ *  maintained.  This is a function pointer entry point called by drivers.
+ **/
+s32 e1000_set_d3_lplu_state(struct e1000_hw *hw, boolean_t active)
+{
+	if (hw->func.set_d3_lplu_state != NULL)
+		return hw->func.set_d3_lplu_state(hw, active);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_read_mac_addr - Reads MAC address
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the MAC address out of the adapter and stores it in the HW structure.
+ *  Currently no func pointer exists and all implementations are handled in the
+ *  generic version of this function.
+ **/
+s32 e1000_read_mac_addr(struct e1000_hw *hw)
+{
+	return e1000_read_mac_addr_generic(hw);
+}
+
+/**
+ *  e1000_read_part_num - Read device part number
+ *  @hw: pointer to the HW structure
+ *  @part_num: pointer to device part number
+ *
+ *  Reads the product board assembly (PBA) number from the EEPROM and stores
+ *  the value in part_num.
+ *  Currently no func pointer exists and all implementations are handled in the
+ *  generic version of this function.
+ **/
+s32 e1000_read_part_num(struct e1000_hw *hw, u32 *part_num)
+{
+	return e1000_read_part_num_generic(hw, part_num);
+}
+
+/**
+ *  e1000_validate_nvm_checksum - Verifies NVM (EEPROM) checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Validates the NVM checksum is correct. This is a function pointer entry
+ *  point called by drivers.
+ **/
+s32 e1000_validate_nvm_checksum(struct e1000_hw *hw)
+{
+	if (hw->func.validate_nvm != NULL)
+		return hw->func.validate_nvm(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_update_nvm_checksum - Updates NVM (EEPROM) checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Updates the NVM checksum. Currently no func pointer exists and all
+ *  implementations are handled in the generic version of this function.
+ **/
+s32 e1000_update_nvm_checksum(struct e1000_hw *hw)
+{
+	if (hw->func.update_nvm != NULL)
+		return hw->func.update_nvm(hw);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_reload_nvm - Reloads EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Reloads the EEPROM by setting the "Reinitialize from EEPROM" bit in the
+ *  extended control register.
+ **/
+void e1000_reload_nvm(struct e1000_hw *hw)
+{
+	if (hw->func.reload_nvm != NULL)
+		hw->func.reload_nvm(hw);
+}
+
+/**
+ *  e1000_read_nvm - Reads NVM (EEPROM)
+ *  @hw: pointer to the HW structure
+ *  @offset: the word offset to read
+ *  @words: number of 16-bit words to read
+ *  @data: pointer to the properly sized buffer for the data.
+ *
+ *  Reads 16-bit chunks of data from the NVM (EEPROM). This is a function
+ *  pointer entry point called by drivers.
+ **/
+s32 e1000_read_nvm(struct e1000_hw *hw, u16 offset, u16 words, u16 *data)
+{
+	if (hw->func.read_nvm != NULL)
+		return hw->func.read_nvm(hw, offset, words, data);
+	else
+		return -E1000_ERR_CONFIG;
+}
+
+/**
+ *  e1000_write_nvm - Writes to NVM (EEPROM)
+ *  @hw: pointer to the HW structure
+ *  @offset: the word offset to read
+ *  @words: number of 16-bit words to write
+ *  @data: pointer to the properly sized buffer for the data.
+ *
+ *  Writes 16-bit chunks of data to the NVM (EEPROM). This is a function
+ *  pointer entry point called by drivers.
+ **/
+s32 e1000_write_nvm(struct e1000_hw *hw, u16 offset, u16 words, u16 *data)
+{
+	if (hw->func.write_nvm != NULL)
+		return hw->func.write_nvm(hw, offset, words, data);
+	else
+		return E1000_SUCCESS;
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_api.h linux-2.6.9/drivers/net/e1000/e1000_api.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_api.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_api.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,152 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_API_H_
+#define _E1000_API_H_
+
+#include "e1000_hw.h"
+
+s32       e1000_set_mac_type(struct e1000_hw *hw);
+s32       e1000_setup_init_funcs(struct e1000_hw *hw, boolean_t init_device);
+s32       e1000_init_mac_params(struct e1000_hw *hw);
+s32       e1000_init_nvm_params(struct e1000_hw *hw);
+s32       e1000_init_phy_params(struct e1000_hw *hw);
+void      e1000_remove_device(struct e1000_hw *hw);
+s32       e1000_get_bus_info(struct e1000_hw *hw);
+void      e1000_clear_vfta(struct e1000_hw *hw);
+void      e1000_write_vfta(struct e1000_hw *hw, u32 offset, u32 value);
+s32       e1000_force_mac_fc(struct e1000_hw *hw);
+s32       e1000_check_for_link(struct e1000_hw *hw);
+s32       e1000_reset_hw(struct e1000_hw *hw);
+s32       e1000_init_hw(struct e1000_hw *hw);
+s32       e1000_setup_link(struct e1000_hw *hw);
+s32       e1000_get_speed_and_duplex(struct e1000_hw *hw, u16 *speed,
+                                     u16 *duplex);
+s32       e1000_disable_pcie_master(struct e1000_hw *hw);
+void      e1000_config_collision_dist(struct e1000_hw *hw);
+void      e1000_rar_set(struct e1000_hw *hw, u8 *addr, u32 index);
+void      e1000_mta_set(struct e1000_hw *hw, u32 hash_value);
+u32       e1000_hash_mc_addr(struct e1000_hw *hw, u8 *mc_addr);
+void      e1000_mc_addr_list_update(struct e1000_hw *hw,
+                                    u8 *mc_addr_list, u32 mc_addr_count,
+                                    u32 rar_used_count, u32 rar_count);
+s32       e1000_setup_led(struct e1000_hw *hw);
+s32       e1000_cleanup_led(struct e1000_hw *hw);
+s32       e1000_check_reset_block(struct e1000_hw *hw);
+s32       e1000_blink_led(struct e1000_hw *hw);
+s32       e1000_led_on(struct e1000_hw *hw);
+s32       e1000_led_off(struct e1000_hw *hw);
+void      e1000_reset_adaptive(struct e1000_hw *hw);
+void      e1000_update_adaptive(struct e1000_hw *hw);
+s32       e1000_get_cable_length(struct e1000_hw *hw);
+s32       e1000_validate_mdi_setting(struct e1000_hw *hw);
+s32       e1000_read_phy_reg(struct e1000_hw *hw, u32 offset, u16 *data);
+s32       e1000_write_phy_reg(struct e1000_hw *hw, u32 offset, u16 data);
+s32       e1000_get_phy_info(struct e1000_hw *hw);
+s32       e1000_phy_hw_reset(struct e1000_hw *hw);
+s32       e1000_phy_commit(struct e1000_hw *hw);
+s32       e1000_read_mac_addr(struct e1000_hw *hw);
+s32       e1000_read_part_num(struct e1000_hw *hw, u32 *part_num);
+void      e1000_reload_nvm(struct e1000_hw *hw);
+s32       e1000_update_nvm_checksum(struct e1000_hw *hw);
+s32       e1000_validate_nvm_checksum(struct e1000_hw *hw);
+s32       e1000_read_nvm(struct e1000_hw *hw, u16 offset, u16 words, u16 *data);
+s32       e1000_read_kmrn_reg(struct e1000_hw *hw, u32 offset, u16 *data);
+s32       e1000_write_kmrn_reg(struct e1000_hw *hw, u32 offset, u16 data);
+s32       e1000_write_nvm(struct e1000_hw *hw, u16 offset, u16 words,
+                          u16 *data);
+s32       e1000_wait_autoneg(struct e1000_hw *hw);
+s32       e1000_set_d3_lplu_state(struct e1000_hw *hw, boolean_t active);
+s32       e1000_set_d0_lplu_state(struct e1000_hw *hw, boolean_t active);
+boolean_t e1000_check_mng_mode(struct e1000_hw *hw);
+boolean_t e1000_enable_mng_pass_thru(struct e1000_hw *hw);
+boolean_t e1000_enable_tx_pkt_filtering(struct e1000_hw *hw);
+s32       e1000_mng_enable_host_if(struct e1000_hw *hw);
+s32       e1000_mng_host_if_write(struct e1000_hw *hw,
+                                  u8 *buffer, u16 length, u16 offset, u8 *sum);
+s32       e1000_mng_write_cmd_header(struct e1000_hw *hw,
+                                     struct e1000_host_mng_command_header *hdr);
+s32       e1000_mng_write_dhcp_info(struct e1000_hw * hw,
+                                    u8 *buffer, u16 length);
+void      e1000_tbi_adjust_stats_82543(struct e1000_hw *hw,
+                                       struct e1000_hw_stats *stats,
+                                       u32 frame_len, u8 *mac_addr);
+void      e1000_set_tbi_compatibility_82543(struct e1000_hw *hw,
+                                            boolean_t state);
+boolean_t e1000_tbi_sbp_enabled_82543(struct e1000_hw *hw);
+u32       e1000_translate_register_82542(u32 reg);
+void      e1000_init_script_state_82541(struct e1000_hw *hw, boolean_t state);
+boolean_t e1000_get_laa_state_82571(struct e1000_hw *hw);
+void      e1000_set_laa_state_82571(struct e1000_hw *hw, boolean_t state);
+void      e1000_set_kmrn_lock_loss_workaround_ich8lan(struct e1000_hw *hw,
+                                                      boolean_t state);
+void      e1000_igp3_phy_powerdown_workaround_ich8lan(struct e1000_hw *hw);
+void      e1000_gig_downshift_workaround_ich8lan(struct e1000_hw *hw);
+
+
+/* TBI_ACCEPT macro definition:
+ *
+ * This macro requires:
+ *      adapter = a pointer to struct e1000_hw
+ *      status = the 8 bit status field of the RX descriptor with EOP set
+ *      error = the 8 bit error field of the RX descriptor with EOP set
+ *      length = the sum of all the length fields of the RX descriptors that
+ *               make up the current frame
+ *      last_byte = the last byte of the frame DMAed by the hardware
+ *      max_frame_length = the maximum frame length we want to accept.
+ *      min_frame_length = the minimum frame length we want to accept.
+ *
+ * This macro is a conditional that should be used in the interrupt
+ * handler's Rx processing routine when RxErrors have been detected.
+ *
+ * Typical use:
+ *  ...
+ *  if (TBI_ACCEPT) {
+ *      accept_frame = TRUE;
+ *      e1000_tbi_adjust_stats(adapter, MacAddress);
+ *      frame_length--;
+ *  } else {
+ *      accept_frame = FALSE;
+ *  }
+ *  ...
+ */
+
+/* The carrier extension symbol, as received by the NIC. */
+#define CARRIER_EXTENSION   0x0F
+
+#define TBI_ACCEPT(a, status, errors, length, last_byte) \
+    (e1000_tbi_sbp_enabled_82543(a) && \
+     (((errors) & E1000_RXD_ERR_FRAME_ERR_MASK) == E1000_RXD_ERR_CE) && \
+     ((last_byte) == CARRIER_EXTENSION) && \
+     (((status) & E1000_RXD_STAT_VP) ? \
+          (((length) > ((a)->mac.min_frame_size - VLAN_TAG_SIZE)) && \
+           ((length) <= ((a)->mac.max_frame_size + 1))) : \
+          (((length) > (a)->mac.min_frame_size) && \
+           ((length) <= ((a)->mac.max_frame_size + VLAN_TAG_SIZE + 1)))))
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_defines.h linux-2.6.9/drivers/net/e1000/e1000_defines.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_defines.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_defines.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1279 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_DEFINES_H_
+#define _E1000_DEFINES_H_
+
+/* Number of Transmit and Receive Descriptors must be a multiple of 8 */
+#define REQ_TX_DESCRIPTOR_MULTIPLE  8
+#define REQ_RX_DESCRIPTOR_MULTIPLE  8
+
+/* Definitions for power management and wakeup registers */
+/* Wake Up Control */
+#define E1000_WUC_APME       0x00000001 /* APM Enable */
+#define E1000_WUC_PME_EN     0x00000002 /* PME Enable */
+#define E1000_WUC_PME_STATUS 0x00000004 /* PME Status */
+#define E1000_WUC_APMPME     0x00000008 /* Assert PME on APM Wakeup */
+#define E1000_WUC_SPM        0x80000000 /* Enable SPM */
+
+/* Wake Up Filter Control */
+#define E1000_WUFC_LNKC 0x00000001 /* Link Status Change Wakeup Enable */
+#define E1000_WUFC_MAG  0x00000002 /* Magic Packet Wakeup Enable */
+#define E1000_WUFC_EX   0x00000004 /* Directed Exact Wakeup Enable */
+#define E1000_WUFC_MC   0x00000008 /* Directed Multicast Wakeup Enable */
+#define E1000_WUFC_BC   0x00000010 /* Broadcast Wakeup Enable */
+#define E1000_WUFC_ARP  0x00000020 /* ARP Request Packet Wakeup Enable */
+#define E1000_WUFC_IPV4 0x00000040 /* Directed IPv4 Packet Wakeup Enable */
+#define E1000_WUFC_IPV6 0x00000080 /* Directed IPv6 Packet Wakeup Enable */
+#define E1000_WUFC_IGNORE_TCO      0x00008000 /* Ignore WakeOn TCO packets */
+#define E1000_WUFC_FLX0 0x00010000 /* Flexible Filter 0 Enable */
+#define E1000_WUFC_FLX1 0x00020000 /* Flexible Filter 1 Enable */
+#define E1000_WUFC_FLX2 0x00040000 /* Flexible Filter 2 Enable */
+#define E1000_WUFC_FLX3 0x00080000 /* Flexible Filter 3 Enable */
+#define E1000_WUFC_ALL_FILTERS 0x000F00FF /* Mask for all wakeup filters */
+#define E1000_WUFC_FLX_OFFSET 16       /* Offset to the Flexible Filters bits */
+#define E1000_WUFC_FLX_FILTERS 0x000F0000 /* Mask for the 4 flexible filters */
+
+/* Wake Up Status */
+#define E1000_WUS_LNKC         E1000_WUFC_LNKC
+#define E1000_WUS_MAG          E1000_WUFC_MAG
+#define E1000_WUS_EX           E1000_WUFC_EX
+#define E1000_WUS_MC           E1000_WUFC_MC
+#define E1000_WUS_BC           E1000_WUFC_BC
+#define E1000_WUS_ARP          E1000_WUFC_ARP
+#define E1000_WUS_IPV4         E1000_WUFC_IPV4
+#define E1000_WUS_IPV6         E1000_WUFC_IPV6
+#define E1000_WUS_FLX0         E1000_WUFC_FLX0
+#define E1000_WUS_FLX1         E1000_WUFC_FLX1
+#define E1000_WUS_FLX2         E1000_WUFC_FLX2
+#define E1000_WUS_FLX3         E1000_WUFC_FLX3
+#define E1000_WUS_FLX_FILTERS  E1000_WUFC_FLX_FILTERS
+
+/* Wake Up Packet Length */
+#define E1000_WUPL_LENGTH_MASK 0x0FFF   /* Only the lower 12 bits are valid */
+
+/* Four Flexible Filters are supported */
+#define E1000_FLEXIBLE_FILTER_COUNT_MAX 4
+
+/* Each Flexible Filter is at most 128 (0x80) bytes in length */
+#define E1000_FLEXIBLE_FILTER_SIZE_MAX  128
+
+#define E1000_FFLT_SIZE E1000_FLEXIBLE_FILTER_COUNT_MAX
+#define E1000_FFMT_SIZE E1000_FLEXIBLE_FILTER_SIZE_MAX
+#define E1000_FFVT_SIZE E1000_FLEXIBLE_FILTER_SIZE_MAX
+
+/* Extended Device Control */
+#define E1000_CTRL_EXT_GPI0_EN   0x00000001 /* Maps SDP4 to GPI0 */
+#define E1000_CTRL_EXT_GPI1_EN   0x00000002 /* Maps SDP5 to GPI1 */
+#define E1000_CTRL_EXT_PHYINT_EN E1000_CTRL_EXT_GPI1_EN
+#define E1000_CTRL_EXT_GPI2_EN   0x00000004 /* Maps SDP6 to GPI2 */
+#define E1000_CTRL_EXT_GPI3_EN   0x00000008 /* Maps SDP7 to GPI3 */
+#define E1000_CTRL_EXT_SDP4_DATA 0x00000010 /* Value of SW Defineable Pin 4 */
+#define E1000_CTRL_EXT_SDP5_DATA 0x00000020 /* Value of SW Defineable Pin 5 */
+#define E1000_CTRL_EXT_PHY_INT   E1000_CTRL_EXT_SDP5_DATA
+#define E1000_CTRL_EXT_SDP6_DATA 0x00000040 /* Value of SW Defineable Pin 6 */
+#define E1000_CTRL_EXT_SDP7_DATA 0x00000080 /* Value of SW Defineable Pin 7 */
+#define E1000_CTRL_EXT_SDP4_DIR  0x00000100 /* Direction of SDP4 0=in 1=out */
+#define E1000_CTRL_EXT_SDP5_DIR  0x00000200 /* Direction of SDP5 0=in 1=out */
+#define E1000_CTRL_EXT_SDP6_DIR  0x00000400 /* Direction of SDP6 0=in 1=out */
+#define E1000_CTRL_EXT_SDP7_DIR  0x00000800 /* Direction of SDP7 0=in 1=out */
+#define E1000_CTRL_EXT_ASDCHK    0x00001000 /* Initiate an ASD sequence */
+#define E1000_CTRL_EXT_EE_RST    0x00002000 /* Reinitialize from EEPROM */
+#define E1000_CTRL_EXT_IPS       0x00004000 /* Invert Power State */
+#define E1000_CTRL_EXT_SPD_BYPS  0x00008000 /* Speed Select Bypass */
+#define E1000_CTRL_EXT_RO_DIS    0x00020000 /* Relaxed Ordering disable */
+#define E1000_CTRL_EXT_LINK_MODE_MASK 0x00C00000
+#define E1000_CTRL_EXT_LINK_MODE_GMII 0x00000000
+#define E1000_CTRL_EXT_LINK_MODE_TBI  0x00C00000
+#define E1000_CTRL_EXT_LINK_MODE_KMRN    0x00000000
+#define E1000_CTRL_EXT_LINK_MODE_PCIE_SERDES  0x00C00000
+#define E1000_CTRL_EXT_LINK_MODE_PCIX_SERDES  0x00800000
+#define E1000_CTRL_EXT_LINK_MODE_SGMII   0x00800000
+#define E1000_CTRL_EXT_WR_WMARK_MASK  0x03000000
+#define E1000_CTRL_EXT_WR_WMARK_256   0x00000000
+#define E1000_CTRL_EXT_WR_WMARK_320   0x01000000
+#define E1000_CTRL_EXT_WR_WMARK_384   0x02000000
+#define E1000_CTRL_EXT_WR_WMARK_448   0x03000000
+#define E1000_CTRL_EXT_CANC           0x04000000 /* Interrupt delay cancellation */
+#define E1000_CTRL_EXT_DRV_LOAD       0x10000000 /* Driver loaded bit for FW */
+#define E1000_CTRL_EXT_IAME           0x08000000 /* Interrupt acknowledge Auto-mask */
+#define E1000_CTRL_EXT_INT_TIMER_CLR  0x20000000 /* Clear Interrupt timers after IMS clear */
+#define E1000_CRTL_EXT_PB_PAREN       0x01000000 /* packet buffer parity error detection enabled */
+#define E1000_CTRL_EXT_DF_PAREN       0x02000000 /* descriptor FIFO parity error detection enable */
+#define E1000_CTRL_EXT_GHOST_PAREN    0x40000000
+
+/* Receive Decriptor bit definitions */
+#define E1000_RXD_STAT_DD       0x01    /* Descriptor Done */
+#define E1000_RXD_STAT_EOP      0x02    /* End of Packet */
+#define E1000_RXD_STAT_IXSM     0x04    /* Ignore checksum */
+#define E1000_RXD_STAT_VP       0x08    /* IEEE VLAN Packet */
+#define E1000_RXD_STAT_UDPCS    0x10    /* UDP xsum caculated */
+#define E1000_RXD_STAT_TCPCS    0x20    /* TCP xsum calculated */
+#define E1000_RXD_STAT_IPCS     0x40    /* IP xsum calculated */
+#define E1000_RXD_STAT_PIF      0x80    /* passed in-exact filter */
+#define E1000_RXD_STAT_IPIDV    0x200   /* IP identification valid */
+#define E1000_RXD_STAT_UDPV     0x400   /* Valid UDP checksum */
+#define E1000_RXD_STAT_ACK      0x8000  /* ACK Packet indication */
+#define E1000_RXD_ERR_CE        0x01    /* CRC Error */
+#define E1000_RXD_ERR_SE        0x02    /* Symbol Error */
+#define E1000_RXD_ERR_SEQ       0x04    /* Sequence Error */
+#define E1000_RXD_ERR_CXE       0x10    /* Carrier Extension Error */
+#define E1000_RXD_ERR_TCPE      0x20    /* TCP/UDP Checksum Error */
+#define E1000_RXD_ERR_IPE       0x40    /* IP Checksum Error */
+#define E1000_RXD_ERR_RXE       0x80    /* Rx Data Error */
+#define E1000_RXD_SPC_VLAN_MASK 0x0FFF  /* VLAN ID is in lower 12 bits */
+#define E1000_RXD_SPC_PRI_MASK  0xE000  /* Priority is in upper 3 bits */
+#define E1000_RXD_SPC_PRI_SHIFT 13
+#define E1000_RXD_SPC_CFI_MASK  0x1000  /* CFI is bit 12 */
+#define E1000_RXD_SPC_CFI_SHIFT 12
+
+#define E1000_RXDEXT_STATERR_CE    0x01000000
+#define E1000_RXDEXT_STATERR_SE    0x02000000
+#define E1000_RXDEXT_STATERR_SEQ   0x04000000
+#define E1000_RXDEXT_STATERR_CXE   0x10000000
+#define E1000_RXDEXT_STATERR_TCPE  0x20000000
+#define E1000_RXDEXT_STATERR_IPE   0x40000000
+#define E1000_RXDEXT_STATERR_RXE   0x80000000
+
+/* mask to determine if packets should be dropped due to frame errors */
+#define E1000_RXD_ERR_FRAME_ERR_MASK ( \
+    E1000_RXD_ERR_CE  |                \
+    E1000_RXD_ERR_SE  |                \
+    E1000_RXD_ERR_SEQ |                \
+    E1000_RXD_ERR_CXE |                \
+    E1000_RXD_ERR_RXE)
+
+/* Same mask, but for extended and packet split descriptors */
+#define E1000_RXDEXT_ERR_FRAME_ERR_MASK ( \
+    E1000_RXDEXT_STATERR_CE  |            \
+    E1000_RXDEXT_STATERR_SE  |            \
+    E1000_RXDEXT_STATERR_SEQ |            \
+    E1000_RXDEXT_STATERR_CXE |            \
+    E1000_RXDEXT_STATERR_RXE)
+
+#define E1000_MRQC_ENABLE_MASK                 0x00000007
+#define E1000_MRQC_ENABLE_RSS_2Q               0x00000001
+#define E1000_MRQC_ENABLE_RSS_INT              0x00000004
+#define E1000_MRQC_RSS_FIELD_MASK              0xFFFF0000
+#define E1000_MRQC_RSS_FIELD_IPV4_TCP          0x00010000
+#define E1000_MRQC_RSS_FIELD_IPV4              0x00020000
+#define E1000_MRQC_RSS_FIELD_IPV6_TCP_EX       0x00040000
+#define E1000_MRQC_RSS_FIELD_IPV6_EX           0x00080000
+#define E1000_MRQC_RSS_FIELD_IPV6              0x00100000
+#define E1000_MRQC_RSS_FIELD_IPV6_TCP          0x00200000
+
+#define E1000_RXDPS_HDRSTAT_HDRSP              0x00008000
+#define E1000_RXDPS_HDRSTAT_HDRLEN_MASK        0x000003FF
+
+/* Management Control */
+#define E1000_MANC_SMBUS_EN      0x00000001 /* SMBus Enabled - RO */
+#define E1000_MANC_ASF_EN        0x00000002 /* ASF Enabled - RO */
+#define E1000_MANC_R_ON_FORCE    0x00000004 /* Reset on Force TCO - RO */
+#define E1000_MANC_RMCP_EN       0x00000100 /* Enable RCMP 026Fh Filtering */
+#define E1000_MANC_0298_EN       0x00000200 /* Enable RCMP 0298h Filtering */
+#define E1000_MANC_IPV4_EN       0x00000400 /* Enable IPv4 */
+#define E1000_MANC_IPV6_EN       0x00000800 /* Enable IPv6 */
+#define E1000_MANC_SNAP_EN       0x00001000 /* Accept LLC/SNAP */
+#define E1000_MANC_ARP_EN        0x00002000 /* Enable ARP Request Filtering */
+#define E1000_MANC_NEIGHBOR_EN   0x00004000 /* Enable Neighbor Discovery
+                                             * Filtering */
+#define E1000_MANC_ARP_RES_EN    0x00008000 /* Enable ARP response Filtering */
+#define E1000_MANC_TCO_RESET     0x00010000 /* TCO Reset Occurred */
+#define E1000_MANC_RCV_TCO_EN    0x00020000 /* Receive TCO Packets Enabled */
+#define E1000_MANC_REPORT_STATUS 0x00040000 /* Status Reporting Enabled */
+#define E1000_MANC_RCV_ALL       0x00080000 /* Receive All Enabled */
+#define E1000_MANC_BLK_PHY_RST_ON_IDE   0x00040000 /* Block phy resets */
+#define E1000_MANC_EN_MAC_ADDR_FILTER   0x00100000 /* Enable MAC address
+                                                    * filtering */
+#define E1000_MANC_EN_MNG2HOST   0x00200000 /* Enable MNG packets to host
+                                             * memory */
+#define E1000_MANC_EN_IP_ADDR_FILTER    0x00400000 /* Enable IP address
+                                                    * filtering */
+#define E1000_MANC_EN_XSUM_FILTER   0x00800000 /* Enable checksum filtering */
+#define E1000_MANC_BR_EN            0x01000000 /* Enable broadcast filtering */
+#define E1000_MANC_SMB_REQ       0x01000000 /* SMBus Request */
+#define E1000_MANC_SMB_GNT       0x02000000 /* SMBus Grant */
+#define E1000_MANC_SMB_CLK_IN    0x04000000 /* SMBus Clock In */
+#define E1000_MANC_SMB_DATA_IN   0x08000000 /* SMBus Data In */
+#define E1000_MANC_SMB_DATA_OUT  0x10000000 /* SMBus Data Out */
+#define E1000_MANC_SMB_CLK_OUT   0x20000000 /* SMBus Clock Out */
+
+#define E1000_MANC_SMB_DATA_OUT_SHIFT  28 /* SMBus Data Out Shift */
+#define E1000_MANC_SMB_CLK_OUT_SHIFT   29 /* SMBus Clock Out Shift */
+
+/* Receive Control */
+#define E1000_RCTL_RST            0x00000001    /* Software reset */
+#define E1000_RCTL_EN             0x00000002    /* enable */
+#define E1000_RCTL_SBP            0x00000004    /* store bad packet */
+#define E1000_RCTL_UPE            0x00000008    /* unicast promiscuous enable */
+#define E1000_RCTL_MPE            0x00000010    /* multicast promiscuous enab */
+#define E1000_RCTL_LPE            0x00000020    /* long packet enable */
+#define E1000_RCTL_LBM_NO         0x00000000    /* no loopback mode */
+#define E1000_RCTL_LBM_MAC        0x00000040    /* MAC loopback mode */
+#define E1000_RCTL_LBM_SLP        0x00000080    /* serial link loopback mode */
+#define E1000_RCTL_LBM_TCVR       0x000000C0    /* tcvr loopback mode */
+#define E1000_RCTL_DTYP_MASK      0x00000C00    /* Descriptor type mask */
+#define E1000_RCTL_DTYP_PS        0x00000400    /* Packet Split descriptor */
+#define E1000_RCTL_RDMTS_HALF     0x00000000    /* rx desc min threshold size */
+#define E1000_RCTL_RDMTS_QUAT     0x00000100    /* rx desc min threshold size */
+#define E1000_RCTL_RDMTS_EIGTH    0x00000200    /* rx desc min threshold size */
+#define E1000_RCTL_MO_SHIFT       12            /* multicast offset shift */
+#define E1000_RCTL_MO_0           0x00000000    /* multicast offset 11:0 */
+#define E1000_RCTL_MO_1           0x00001000    /* multicast offset 12:1 */
+#define E1000_RCTL_MO_2           0x00002000    /* multicast offset 13:2 */
+#define E1000_RCTL_MO_3           0x00003000    /* multicast offset 15:4 */
+#define E1000_RCTL_MDR            0x00004000    /* multicast desc ring 0 */
+#define E1000_RCTL_BAM            0x00008000    /* broadcast enable */
+/* these buffer sizes are valid if E1000_RCTL_BSEX is 0 */
+#define E1000_RCTL_SZ_2048        0x00000000    /* rx buffer size 2048 */
+#define E1000_RCTL_SZ_1024        0x00010000    /* rx buffer size 1024 */
+#define E1000_RCTL_SZ_512         0x00020000    /* rx buffer size 512 */
+#define E1000_RCTL_SZ_256         0x00030000    /* rx buffer size 256 */
+/* these buffer sizes are valid if E1000_RCTL_BSEX is 1 */
+#define E1000_RCTL_SZ_16384       0x00010000    /* rx buffer size 16384 */
+#define E1000_RCTL_SZ_8192        0x00020000    /* rx buffer size 8192 */
+#define E1000_RCTL_SZ_4096        0x00030000    /* rx buffer size 4096 */
+#define E1000_RCTL_VFE            0x00040000    /* vlan filter enable */
+#define E1000_RCTL_CFIEN          0x00080000    /* canonical form enable */
+#define E1000_RCTL_CFI            0x00100000    /* canonical form indicator */
+#define E1000_RCTL_DPF            0x00400000    /* discard pause frames */
+#define E1000_RCTL_PMCF           0x00800000    /* pass MAC control frames */
+#define E1000_RCTL_BSEX           0x02000000    /* Buffer size extension */
+#define E1000_RCTL_SECRC          0x04000000    /* Strip Ethernet CRC */
+#define E1000_RCTL_FLXBUF_MASK    0x78000000    /* Flexible buffer size */
+#define E1000_RCTL_FLXBUF_SHIFT   27            /* Flexible buffer shift */
+
+/* Use byte values for the following shift parameters
+ * Usage:
+ *     psrctl |= (((ROUNDUP(value0, 128) >> E1000_PSRCTL_BSIZE0_SHIFT) &
+ *                  E1000_PSRCTL_BSIZE0_MASK) |
+ *                ((ROUNDUP(value1, 1024) >> E1000_PSRCTL_BSIZE1_SHIFT) &
+ *                  E1000_PSRCTL_BSIZE1_MASK) |
+ *                ((ROUNDUP(value2, 1024) << E1000_PSRCTL_BSIZE2_SHIFT) &
+ *                  E1000_PSRCTL_BSIZE2_MASK) |
+ *                ((ROUNDUP(value3, 1024) << E1000_PSRCTL_BSIZE3_SHIFT) |;
+ *                  E1000_PSRCTL_BSIZE3_MASK))
+ * where value0 = [128..16256],  default=256
+ *       value1 = [1024..64512], default=4096
+ *       value2 = [0..64512],    default=4096
+ *       value3 = [0..64512],    default=0
+ */
+
+#define E1000_PSRCTL_BSIZE0_MASK   0x0000007F
+#define E1000_PSRCTL_BSIZE1_MASK   0x00003F00
+#define E1000_PSRCTL_BSIZE2_MASK   0x003F0000
+#define E1000_PSRCTL_BSIZE3_MASK   0x3F000000
+
+#define E1000_PSRCTL_BSIZE0_SHIFT  7            /* Shift _right_ 7 */
+#define E1000_PSRCTL_BSIZE1_SHIFT  2            /* Shift _right_ 2 */
+#define E1000_PSRCTL_BSIZE2_SHIFT  6            /* Shift _left_ 6 */
+#define E1000_PSRCTL_BSIZE3_SHIFT 14            /* Shift _left_ 14 */
+
+/* SWFW_SYNC Definitions */
+#define E1000_SWFW_EEP_SM   0x1
+#define E1000_SWFW_PHY0_SM  0x2
+#define E1000_SWFW_PHY1_SM  0x4
+
+/* Device Control */
+#define E1000_CTRL_FD       0x00000001  /* Full duplex.0=half; 1=full */
+#define E1000_CTRL_BEM      0x00000002  /* Endian Mode.0=little,1=big */
+#define E1000_CTRL_PRIOR    0x00000004  /* Priority on PCI. 0=rx,1=fair */
+#define E1000_CTRL_GIO_MASTER_DISABLE 0x00000004 /*Blocks new Master requests */
+#define E1000_CTRL_LRST     0x00000008  /* Link reset. 0=normal,1=reset */
+#define E1000_CTRL_TME      0x00000010  /* Test mode. 0=normal,1=test */
+#define E1000_CTRL_SLE      0x00000020  /* Serial Link on 0=dis,1=en */
+#define E1000_CTRL_ASDE     0x00000020  /* Auto-speed detect enable */
+#define E1000_CTRL_SLU      0x00000040  /* Set link up (Force Link) */
+#define E1000_CTRL_ILOS     0x00000080  /* Invert Loss-Of Signal */
+#define E1000_CTRL_SPD_SEL  0x00000300  /* Speed Select Mask */
+#define E1000_CTRL_SPD_10   0x00000000  /* Force 10Mb */
+#define E1000_CTRL_SPD_100  0x00000100  /* Force 100Mb */
+#define E1000_CTRL_SPD_1000 0x00000200  /* Force 1Gb */
+#define E1000_CTRL_BEM32    0x00000400  /* Big Endian 32 mode */
+#define E1000_CTRL_FRCSPD   0x00000800  /* Force Speed */
+#define E1000_CTRL_FRCDPX   0x00001000  /* Force Duplex */
+#define E1000_CTRL_D_UD_EN  0x00002000  /* Dock/Undock enable */
+#define E1000_CTRL_D_UD_POLARITY 0x00004000 /* Defined polarity of Dock/Undock indication in SDP[0] */
+#define E1000_CTRL_FORCE_PHY_RESET 0x00008000 /* Reset both PHY ports, through PHYRST_N pin */
+#define E1000_CTRL_EXT_LINK_EN 0x00010000 /* enable link status from external LINK_0 and LINK_1 pins */
+#define E1000_CTRL_SWDPIN0  0x00040000  /* SWDPIN 0 value */
+#define E1000_CTRL_SWDPIN1  0x00080000  /* SWDPIN 1 value */
+#define E1000_CTRL_SWDPIN2  0x00100000  /* SWDPIN 2 value */
+#define E1000_CTRL_SWDPIN3  0x00200000  /* SWDPIN 3 value */
+#define E1000_CTRL_SWDPIO0  0x00400000  /* SWDPIN 0 Input or output */
+#define E1000_CTRL_SWDPIO1  0x00800000  /* SWDPIN 1 input or output */
+#define E1000_CTRL_SWDPIO2  0x01000000  /* SWDPIN 2 input or output */
+#define E1000_CTRL_SWDPIO3  0x02000000  /* SWDPIN 3 input or output */
+#define E1000_CTRL_RST      0x04000000  /* Global reset */
+#define E1000_CTRL_RFCE     0x08000000  /* Receive Flow Control enable */
+#define E1000_CTRL_TFCE     0x10000000  /* Transmit flow control enable */
+#define E1000_CTRL_RTE      0x20000000  /* Routing tag enable */
+#define E1000_CTRL_VME      0x40000000  /* IEEE VLAN mode enable */
+#define E1000_CTRL_PHY_RST  0x80000000  /* PHY Reset */
+#define E1000_CTRL_SW2FW_INT 0x02000000  /* Initiate an interrupt to manageability engine */
+
+/* Bit definitions for the Management Data IO (MDIO) and Management Data
+ * Clock (MDC) pins in the Device Control Register.
+ */
+#define E1000_CTRL_PHY_RESET_DIR  E1000_CTRL_SWDPIO0
+#define E1000_CTRL_PHY_RESET      E1000_CTRL_SWDPIN0
+#define E1000_CTRL_MDIO_DIR       E1000_CTRL_SWDPIO2
+#define E1000_CTRL_MDIO           E1000_CTRL_SWDPIN2
+#define E1000_CTRL_MDC_DIR        E1000_CTRL_SWDPIO3
+#define E1000_CTRL_MDC            E1000_CTRL_SWDPIN3
+#define E1000_CTRL_PHY_RESET_DIR4 E1000_CTRL_EXT_SDP4_DIR
+#define E1000_CTRL_PHY_RESET4     E1000_CTRL_EXT_SDP4_DATA
+
+/* Device Status */
+#define E1000_STATUS_FD         0x00000001      /* Full duplex.0=half,1=full */
+#define E1000_STATUS_LU         0x00000002      /* Link up.0=no,1=link */
+#define E1000_STATUS_FUNC_MASK  0x0000000C      /* PCI Function Mask */
+#define E1000_STATUS_FUNC_SHIFT 2
+#define E1000_STATUS_FUNC_0     0x00000000      /* Function 0 */
+#define E1000_STATUS_FUNC_1     0x00000004      /* Function 1 */
+#define E1000_STATUS_TXOFF      0x00000010      /* transmission paused */
+#define E1000_STATUS_TBIMODE    0x00000020      /* TBI mode */
+#define E1000_STATUS_SPEED_MASK 0x000000C0
+#define E1000_STATUS_SPEED_10   0x00000000      /* Speed 10Mb/s */
+#define E1000_STATUS_SPEED_100  0x00000040      /* Speed 100Mb/s */
+#define E1000_STATUS_SPEED_1000 0x00000080      /* Speed 1000Mb/s */
+#define E1000_STATUS_LAN_INIT_DONE 0x00000200   /* Lan Init Completion by NVM */
+#define E1000_STATUS_ASDV       0x00000300      /* Auto speed detect value */
+#define E1000_STATUS_DOCK_CI    0x00000800      /* Change in Dock/Undock state. Clear on write '0'. */
+#define E1000_STATUS_GIO_MASTER_ENABLE 0x00080000 /* Status of Master requests. */
+#define E1000_STATUS_MTXCKOK    0x00000400      /* MTX clock running OK */
+#define E1000_STATUS_PCI66      0x00000800      /* In 66Mhz slot */
+#define E1000_STATUS_BUS64      0x00001000      /* In 64 bit slot */
+#define E1000_STATUS_PCIX_MODE  0x00002000      /* PCI-X mode */
+#define E1000_STATUS_PCIX_SPEED 0x0000C000      /* PCI-X bus speed */
+#define E1000_STATUS_BMC_SKU_0  0x00100000 /* BMC USB redirect disabled */
+#define E1000_STATUS_BMC_SKU_1  0x00200000 /* BMC SRAM disabled */
+#define E1000_STATUS_BMC_SKU_2  0x00400000 /* BMC SDRAM disabled */
+#define E1000_STATUS_BMC_CRYPTO 0x00800000 /* BMC crypto disabled */
+#define E1000_STATUS_BMC_LITE   0x01000000 /* BMC external code execution disabled */
+#define E1000_STATUS_RGMII_ENABLE 0x02000000 /* RGMII disabled */
+#define E1000_STATUS_FUSE_8       0x04000000
+#define E1000_STATUS_FUSE_9       0x08000000
+#define E1000_STATUS_SERDES0_DIS  0x10000000 /* SERDES disabled on port 0 */
+#define E1000_STATUS_SERDES1_DIS  0x20000000 /* SERDES disabled on port 1 */
+
+/* Constants used to intrepret the masked PCI-X bus speed. */
+#define E1000_STATUS_PCIX_SPEED_66  0x00000000 /* PCI-X bus speed  50-66 MHz */
+#define E1000_STATUS_PCIX_SPEED_100 0x00004000 /* PCI-X bus speed  66-100 MHz */
+#define E1000_STATUS_PCIX_SPEED_133 0x00008000 /* PCI-X bus speed 100-133 MHz */
+
+#define SPEED_10    10
+#define SPEED_100   100
+#define SPEED_1000  1000
+#define HALF_DUPLEX 1
+#define FULL_DUPLEX 2
+
+#define PHY_FORCE_TIME   20
+
+#define ADVERTISE_10_HALF                 0x0001
+#define ADVERTISE_10_FULL                 0x0002
+#define ADVERTISE_100_HALF                0x0004
+#define ADVERTISE_100_FULL                0x0008
+#define ADVERTISE_1000_HALF               0x0010 /* Not used, just FYI */
+#define ADVERTISE_1000_FULL               0x0020
+
+/* 1000/H is not supported, nor spec-compliant. */
+#define E1000_ALL_SPEED_DUPLEX ( ADVERTISE_10_HALF |   ADVERTISE_10_FULL | \
+                                ADVERTISE_100_HALF |  ADVERTISE_100_FULL | \
+                                                     ADVERTISE_1000_FULL)
+#define E1000_ALL_NOT_GIG      ( ADVERTISE_10_HALF |   ADVERTISE_10_FULL | \
+                                ADVERTISE_100_HALF |  ADVERTISE_100_FULL)
+#define E1000_ALL_100_SPEED    (ADVERTISE_100_HALF |  ADVERTISE_100_FULL)
+#define E1000_ALL_10_SPEED      (ADVERTISE_10_HALF |   ADVERTISE_10_FULL)
+#define E1000_ALL_FULL_DUPLEX   (ADVERTISE_10_FULL |  ADVERTISE_100_FULL | \
+                                                     ADVERTISE_1000_FULL)
+#define E1000_ALL_HALF_DUPLEX   (ADVERTISE_10_HALF |  ADVERTISE_100_HALF)
+
+#define AUTONEG_ADVERTISE_SPEED_DEFAULT   E1000_ALL_SPEED_DUPLEX
+
+/* LED Control */
+#define E1000_LEDCTL_LED0_MODE_MASK       0x0000000F
+#define E1000_LEDCTL_LED0_MODE_SHIFT      0
+#define E1000_LEDCTL_LED0_BLINK_RATE      0x00000020
+#define E1000_LEDCTL_LED0_IVRT            0x00000040
+#define E1000_LEDCTL_LED0_BLINK           0x00000080
+#define E1000_LEDCTL_LED1_MODE_MASK       0x00000F00
+#define E1000_LEDCTL_LED1_MODE_SHIFT      8
+#define E1000_LEDCTL_LED1_BLINK_RATE      0x00002000
+#define E1000_LEDCTL_LED1_IVRT            0x00004000
+#define E1000_LEDCTL_LED1_BLINK           0x00008000
+#define E1000_LEDCTL_LED2_MODE_MASK       0x000F0000
+#define E1000_LEDCTL_LED2_MODE_SHIFT      16
+#define E1000_LEDCTL_LED2_BLINK_RATE      0x00200000
+#define E1000_LEDCTL_LED2_IVRT            0x00400000
+#define E1000_LEDCTL_LED2_BLINK           0x00800000
+#define E1000_LEDCTL_LED3_MODE_MASK       0x0F000000
+#define E1000_LEDCTL_LED3_MODE_SHIFT      24
+#define E1000_LEDCTL_LED3_BLINK_RATE      0x20000000
+#define E1000_LEDCTL_LED3_IVRT            0x40000000
+#define E1000_LEDCTL_LED3_BLINK           0x80000000
+
+#define E1000_LEDCTL_MODE_LINK_10_1000  0x0
+#define E1000_LEDCTL_MODE_LINK_100_1000 0x1
+#define E1000_LEDCTL_MODE_LINK_UP       0x2
+#define E1000_LEDCTL_MODE_ACTIVITY      0x3
+#define E1000_LEDCTL_MODE_LINK_ACTIVITY 0x4
+#define E1000_LEDCTL_MODE_LINK_10       0x5
+#define E1000_LEDCTL_MODE_LINK_100      0x6
+#define E1000_LEDCTL_MODE_LINK_1000     0x7
+#define E1000_LEDCTL_MODE_PCIX_MODE     0x8
+#define E1000_LEDCTL_MODE_FULL_DUPLEX   0x9
+#define E1000_LEDCTL_MODE_COLLISION     0xA
+#define E1000_LEDCTL_MODE_BUS_SPEED     0xB
+#define E1000_LEDCTL_MODE_BUS_SIZE      0xC
+#define E1000_LEDCTL_MODE_PAUSED        0xD
+#define E1000_LEDCTL_MODE_LED_ON        0xE
+#define E1000_LEDCTL_MODE_LED_OFF       0xF
+
+/* Transmit Descriptor bit definitions */
+#define E1000_TXD_DTYP_D     0x00100000 /* Data Descriptor */
+#define E1000_TXD_DTYP_C     0x00000000 /* Context Descriptor */
+#define E1000_TXD_POPTS_IXSM 0x01       /* Insert IP checksum */
+#define E1000_TXD_POPTS_TXSM 0x02       /* Insert TCP/UDP checksum */
+#define E1000_TXD_CMD_EOP    0x01000000 /* End of Packet */
+#define E1000_TXD_CMD_IFCS   0x02000000 /* Insert FCS (Ethernet CRC) */
+#define E1000_TXD_CMD_IC     0x04000000 /* Insert Checksum */
+#define E1000_TXD_CMD_RS     0x08000000 /* Report Status */
+#define E1000_TXD_CMD_RPS    0x10000000 /* Report Packet Sent */
+#define E1000_TXD_CMD_DEXT   0x20000000 /* Descriptor extension (0 = legacy) */
+#define E1000_TXD_CMD_VLE    0x40000000 /* Add VLAN tag */
+#define E1000_TXD_CMD_IDE    0x80000000 /* Enable Tidv register */
+#define E1000_TXD_STAT_DD    0x00000001 /* Descriptor Done */
+#define E1000_TXD_STAT_EC    0x00000002 /* Excess Collisions */
+#define E1000_TXD_STAT_LC    0x00000004 /* Late Collisions */
+#define E1000_TXD_STAT_TU    0x00000008 /* Transmit underrun */
+#define E1000_TXD_CMD_TCP    0x01000000 /* TCP packet */
+#define E1000_TXD_CMD_IP     0x02000000 /* IP packet */
+#define E1000_TXD_CMD_TSE    0x04000000 /* TCP Seg enable */
+#define E1000_TXD_STAT_TC    0x00000004 /* Tx Underrun */
+/* Extended desc bits for Linksec and timesync */
+
+/* Transmit Control */
+#define E1000_TCTL_RST    0x00000001    /* software reset */
+#define E1000_TCTL_EN     0x00000002    /* enable tx */
+#define E1000_TCTL_BCE    0x00000004    /* busy check enable */
+#define E1000_TCTL_PSP    0x00000008    /* pad short packets */
+#define E1000_TCTL_CT     0x00000ff0    /* collision threshold */
+#define E1000_TCTL_COLD   0x003ff000    /* collision distance */
+#define E1000_TCTL_SWXOFF 0x00400000    /* SW Xoff transmission */
+#define E1000_TCTL_PBE    0x00800000    /* Packet Burst Enable */
+#define E1000_TCTL_RTLC   0x01000000    /* Re-transmit on late collision */
+#define E1000_TCTL_NRTU   0x02000000    /* No Re-transmit on underrun */
+#define E1000_TCTL_MULR   0x10000000    /* Multiple request support */
+
+/* Transmit Arbitration Count */
+#define E1000_TARC0_ENABLE     0x00000400   /* Enable Tx Queue 0 */
+
+/* SerDes Control */
+#define E1000_SCTL_DISABLE_SERDES_LOOPBACK 0x0400
+
+/* Receive Checksum Control */
+#define E1000_RXCSUM_PCSS_MASK 0x000000FF   /* Packet Checksum Start */
+#define E1000_RXCSUM_IPOFL     0x00000100   /* IPv4 checksum offload */
+#define E1000_RXCSUM_TUOFL     0x00000200   /* TCP / UDP checksum offload */
+#define E1000_RXCSUM_IPV6OFL   0x00000400   /* IPv6 checksum offload */
+#define E1000_RXCSUM_IPPCSE    0x00001000   /* IP payload checksum enable */
+#define E1000_RXCSUM_PCSD      0x00002000   /* packet checksum disabled */
+
+/* Header split receive */
+#define E1000_RFCTL_ISCSI_DIS           0x00000001
+#define E1000_RFCTL_ISCSI_DWC_MASK      0x0000003E
+#define E1000_RFCTL_ISCSI_DWC_SHIFT     1
+#define E1000_RFCTL_NFSW_DIS            0x00000040
+#define E1000_RFCTL_NFSR_DIS            0x00000080
+#define E1000_RFCTL_NFS_VER_MASK        0x00000300
+#define E1000_RFCTL_NFS_VER_SHIFT       8
+#define E1000_RFCTL_IPV6_DIS            0x00000400
+#define E1000_RFCTL_IPV6_XSUM_DIS       0x00000800
+#define E1000_RFCTL_ACK_DIS             0x00001000
+#define E1000_RFCTL_ACKD_DIS            0x00002000
+#define E1000_RFCTL_IPFRSP_DIS          0x00004000
+#define E1000_RFCTL_EXTEN               0x00008000
+#define E1000_RFCTL_IPV6_EX_DIS         0x00010000
+#define E1000_RFCTL_NEW_IPV6_EXT_DIS    0x00020000
+
+/* Collision related configuration parameters */
+#define E1000_COLLISION_THRESHOLD       15
+#define E1000_CT_SHIFT                  4
+#define E1000_COLLISION_DISTANCE        63
+#define E1000_COLD_SHIFT                12
+
+/* Default values for the transmit IPG register */
+#define DEFAULT_82542_TIPG_IPGT        10
+#define DEFAULT_82543_TIPG_IPGT_FIBER  9
+#define DEFAULT_82543_TIPG_IPGT_COPPER 8
+
+#define E1000_TIPG_IPGT_MASK  0x000003FF
+#define E1000_TIPG_IPGR1_MASK 0x000FFC00
+#define E1000_TIPG_IPGR2_MASK 0x3FF00000
+
+#define DEFAULT_82542_TIPG_IPGR1 2
+#define DEFAULT_82543_TIPG_IPGR1 8
+#define E1000_TIPG_IPGR1_SHIFT  10
+
+#define DEFAULT_82542_TIPG_IPGR2 10
+#define DEFAULT_82543_TIPG_IPGR2 6
+#define DEFAULT_80003ES2LAN_TIPG_IPGR2 7
+#define E1000_TIPG_IPGR2_SHIFT  20
+
+/* Ethertype field values */
+#define ETHERNET_IEEE_VLAN_TYPE 0x8100  /* 802.3ac packet */
+
+#define ETHERNET_FCS_SIZE       4
+#define MAX_JUMBO_FRAME_SIZE    0x3F00
+
+/* Extended Configuration Control and Size */
+#define E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP      0x00000020
+#define E1000_EXTCNF_CTRL_LCD_WRITE_ENABLE       0x00000001
+#define E1000_EXTCNF_CTRL_SWFLAG                 0x00000020
+#define E1000_EXTCNF_SIZE_EXT_PCIE_LENGTH_MASK   0x00FF0000
+#define E1000_EXTCNF_SIZE_EXT_PCIE_LENGTH_SHIFT          16
+#define E1000_EXTCNF_CTRL_EXT_CNF_POINTER_MASK   0x0FFF0000
+#define E1000_EXTCNF_CTRL_EXT_CNF_POINTER_SHIFT          16
+
+#define E1000_PHY_CTRL_SPD_EN             0x00000001
+#define E1000_PHY_CTRL_D0A_LPLU           0x00000002
+#define E1000_PHY_CTRL_NOND0A_LPLU        0x00000004
+#define E1000_PHY_CTRL_NOND0A_GBE_DISABLE 0x00000008
+#define E1000_PHY_CTRL_GBE_DISABLE        0x00000040
+
+#define E1000_KABGTXD_BGSQLBIAS           0x00050000
+
+/* PBA constants */
+#define E1000_PBA_8K  0x0008    /* 8KB, default Rx allocation */
+#define E1000_PBA_12K 0x000C    /* 12KB, default Rx allocation */
+#define E1000_PBA_16K 0x0010    /* 16KB, default TX allocation */
+#define E1000_PBA_20K 0x0014
+#define E1000_PBA_22K 0x0016
+#define E1000_PBA_24K 0x0018
+#define E1000_PBA_30K 0x001E
+#define E1000_PBA_32K 0x0020
+#define E1000_PBA_34K 0x0022
+#define E1000_PBA_38K 0x0026
+#define E1000_PBA_40K 0x0028
+#define E1000_PBA_48K 0x0030    /* 48KB, default RX allocation */
+
+#define E1000_PBS_16K E1000_PBA_16K
+#define E1000_PBS_24K E1000_PBA_24K
+
+#define IFS_MAX       80
+#define IFS_MIN       40
+#define IFS_RATIO     4
+#define IFS_STEP      10
+#define MIN_NUM_XMITS 1000
+
+/* SW Semaphore Register */
+#define E1000_SWSM_SMBI         0x00000001 /* Driver Semaphore bit */
+#define E1000_SWSM_SWESMBI      0x00000002 /* FW Semaphore bit */
+#define E1000_SWSM_WMNG         0x00000004 /* Wake MNG Clock */
+#define E1000_SWSM_DRV_LOAD     0x00000008 /* Driver Loaded Bit */
+
+/* Interrupt Cause Read */
+#define E1000_ICR_TXDW          0x00000001 /* Transmit desc written back */
+#define E1000_ICR_TXQE          0x00000002 /* Transmit Queue empty */
+#define E1000_ICR_LSC           0x00000004 /* Link Status Change */
+#define E1000_ICR_RXSEQ         0x00000008 /* rx sequence error */
+#define E1000_ICR_RXDMT0        0x00000010 /* rx desc min. threshold (0) */
+#define E1000_ICR_RXO           0x00000040 /* rx overrun */
+#define E1000_ICR_RXT0          0x00000080 /* rx timer intr (ring 0) */
+#define E1000_ICR_MDAC          0x00000200 /* MDIO access complete */
+#define E1000_ICR_RXCFG         0x00000400 /* RX /c/ ordered set */
+#define E1000_ICR_GPI_EN0       0x00000800 /* GP Int 0 */
+#define E1000_ICR_GPI_EN1       0x00001000 /* GP Int 1 */
+#define E1000_ICR_GPI_EN2       0x00002000 /* GP Int 2 */
+#define E1000_ICR_GPI_EN3       0x00004000 /* GP Int 3 */
+#define E1000_ICR_TXD_LOW       0x00008000
+#define E1000_ICR_SRPD          0x00010000
+#define E1000_ICR_ACK           0x00020000 /* Receive Ack frame */
+#define E1000_ICR_MNG           0x00040000 /* Manageability event */
+#define E1000_ICR_DOCK          0x00080000 /* Dock/Undock */
+#define E1000_ICR_INT_ASSERTED  0x80000000 /* If this bit asserted, the driver should claim the interrupt */
+#define E1000_ICR_RXD_FIFO_PAR0 0x00100000 /* queue 0 Rx descriptor FIFO parity error */
+#define E1000_ICR_TXD_FIFO_PAR0 0x00200000 /* queue 0 Tx descriptor FIFO parity error */
+#define E1000_ICR_HOST_ARB_PAR  0x00400000 /* host arb read buffer parity error */
+#define E1000_ICR_PB_PAR        0x00800000 /* packet buffer parity error */
+#define E1000_ICR_RXD_FIFO_PAR1 0x01000000 /* queue 1 Rx descriptor FIFO parity error */
+#define E1000_ICR_TXD_FIFO_PAR1 0x02000000 /* queue 1 Tx descriptor FIFO parity error */
+#define E1000_ICR_ALL_PARITY    0x03F00000 /* all parity error bits */
+#define E1000_ICR_DSW           0x00000020 /* FW changed the status of DISSW bit in the FWSM */
+#define E1000_ICR_PHYINT        0x00001000 /* LAN connected device generates an interrupt */
+#define E1000_ICR_EPRST         0x00100000 /* ME handware reset occurs */
+
+/* This defines the bits that are set in the Interrupt Mask
+ * Set/Read Register.  Each bit is documented below:
+ *   o RXDMT0 = Receive Descriptor Minimum Threshold hit (ring 0)
+ *   o RXSEQ  = Receive Sequence Error
+ */
+#define POLL_IMS_ENABLE_MASK ( \
+    E1000_IMS_RXDMT0 |    \
+    E1000_IMS_RXSEQ)
+
+/* This defines the bits that are set in the Interrupt Mask
+ * Set/Read Register.  Each bit is documented below:
+ *   o RXT0   = Receiver Timer Interrupt (ring 0)
+ *   o TXDW   = Transmit Descriptor Written Back
+ *   o RXDMT0 = Receive Descriptor Minimum Threshold hit (ring 0)
+ *   o RXSEQ  = Receive Sequence Error
+ *   o LSC    = Link Status Change
+ */
+#define IMS_ENABLE_MASK ( \
+    E1000_IMS_RXT0   |    \
+    E1000_IMS_TXDW   |    \
+    E1000_IMS_RXDMT0 |    \
+    E1000_IMS_RXSEQ  |    \
+    E1000_IMS_LSC)
+
+/* Interrupt Mask Set */
+#define E1000_IMS_TXDW      E1000_ICR_TXDW      /* Transmit desc written back */
+#define E1000_IMS_TXQE      E1000_ICR_TXQE      /* Transmit Queue empty */
+#define E1000_IMS_LSC       E1000_ICR_LSC       /* Link Status Change */
+#define E1000_IMS_RXSEQ     E1000_ICR_RXSEQ     /* rx sequence error */
+#define E1000_IMS_RXDMT0    E1000_ICR_RXDMT0    /* rx desc min. threshold */
+#define E1000_IMS_RXO       E1000_ICR_RXO       /* rx overrun */
+#define E1000_IMS_RXT0      E1000_ICR_RXT0      /* rx timer intr */
+#define E1000_IMS_MDAC      E1000_ICR_MDAC      /* MDIO access complete */
+#define E1000_IMS_RXCFG     E1000_ICR_RXCFG     /* RX /c/ ordered set */
+#define E1000_IMS_GPI_EN0   E1000_ICR_GPI_EN0   /* GP Int 0 */
+#define E1000_IMS_GPI_EN1   E1000_ICR_GPI_EN1   /* GP Int 1 */
+#define E1000_IMS_GPI_EN2   E1000_ICR_GPI_EN2   /* GP Int 2 */
+#define E1000_IMS_GPI_EN3   E1000_ICR_GPI_EN3   /* GP Int 3 */
+#define E1000_IMS_TXD_LOW   E1000_ICR_TXD_LOW
+#define E1000_IMS_SRPD      E1000_ICR_SRPD
+#define E1000_IMS_ACK       E1000_ICR_ACK       /* Receive Ack frame */
+#define E1000_IMS_MNG       E1000_ICR_MNG       /* Manageability event */
+#define E1000_IMS_DOCK      E1000_ICR_DOCK      /* Dock/Undock */
+#define E1000_IMS_RXD_FIFO_PAR0 E1000_ICR_RXD_FIFO_PAR0 /* queue 0 Rx descriptor FIFO parity error */
+#define E1000_IMS_TXD_FIFO_PAR0 E1000_ICR_TXD_FIFO_PAR0 /* queue 0 Tx descriptor FIFO parity error */
+#define E1000_IMS_HOST_ARB_PAR  E1000_ICR_HOST_ARB_PAR  /* host arb read buffer parity error */
+#define E1000_IMS_PB_PAR        E1000_ICR_PB_PAR        /* packet buffer parity error */
+#define E1000_IMS_RXD_FIFO_PAR1 E1000_ICR_RXD_FIFO_PAR1 /* queue 1 Rx descriptor FIFO parity error */
+#define E1000_IMS_TXD_FIFO_PAR1 E1000_ICR_TXD_FIFO_PAR1 /* queue 1 Tx descriptor FIFO parity error */
+#define E1000_IMS_DSW       E1000_ICR_DSW
+#define E1000_IMS_PHYINT    E1000_ICR_PHYINT
+#define E1000_IMS_EPRST     E1000_ICR_EPRST
+
+/* Interrupt Cause Set */
+#define E1000_ICS_TXDW      E1000_ICR_TXDW      /* Transmit desc written back */
+#define E1000_ICS_TXQE      E1000_ICR_TXQE      /* Transmit Queue empty */
+#define E1000_ICS_LSC       E1000_ICR_LSC       /* Link Status Change */
+#define E1000_ICS_RXSEQ     E1000_ICR_RXSEQ     /* rx sequence error */
+#define E1000_ICS_RXDMT0    E1000_ICR_RXDMT0    /* rx desc min. threshold */
+#define E1000_ICS_RXO       E1000_ICR_RXO       /* rx overrun */
+#define E1000_ICS_RXT0      E1000_ICR_RXT0      /* rx timer intr */
+#define E1000_ICS_MDAC      E1000_ICR_MDAC      /* MDIO access complete */
+#define E1000_ICS_RXCFG     E1000_ICR_RXCFG     /* RX /c/ ordered set */
+#define E1000_ICS_GPI_EN0   E1000_ICR_GPI_EN0   /* GP Int 0 */
+#define E1000_ICS_GPI_EN1   E1000_ICR_GPI_EN1   /* GP Int 1 */
+#define E1000_ICS_GPI_EN2   E1000_ICR_GPI_EN2   /* GP Int 2 */
+#define E1000_ICS_GPI_EN3   E1000_ICR_GPI_EN3   /* GP Int 3 */
+#define E1000_ICS_TXD_LOW   E1000_ICR_TXD_LOW
+#define E1000_ICS_SRPD      E1000_ICR_SRPD
+#define E1000_ICS_ACK       E1000_ICR_ACK       /* Receive Ack frame */
+#define E1000_ICS_MNG       E1000_ICR_MNG       /* Manageability event */
+#define E1000_ICS_DOCK      E1000_ICR_DOCK      /* Dock/Undock */
+#define E1000_ICS_RXD_FIFO_PAR0 E1000_ICR_RXD_FIFO_PAR0 /* queue 0 Rx descriptor FIFO parity error */
+#define E1000_ICS_TXD_FIFO_PAR0 E1000_ICR_TXD_FIFO_PAR0 /* queue 0 Tx descriptor FIFO parity error */
+#define E1000_ICS_HOST_ARB_PAR  E1000_ICR_HOST_ARB_PAR  /* host arb read buffer parity error */
+#define E1000_ICS_PB_PAR        E1000_ICR_PB_PAR        /* packet buffer parity error */
+#define E1000_ICS_RXD_FIFO_PAR1 E1000_ICR_RXD_FIFO_PAR1 /* queue 1 Rx descriptor FIFO parity error */
+#define E1000_ICS_TXD_FIFO_PAR1 E1000_ICR_TXD_FIFO_PAR1 /* queue 1 Tx descriptor FIFO parity error */
+#define E1000_ICS_DSW       E1000_ICR_DSW
+#define E1000_ICS_PHYINT    E1000_ICR_PHYINT
+#define E1000_ICS_EPRST     E1000_ICR_EPRST
+
+/* Transmit Descriptor Control */
+#define E1000_TXDCTL_PTHRESH 0x0000003F /* TXDCTL Prefetch Threshold */
+#define E1000_TXDCTL_HTHRESH 0x00003F00 /* TXDCTL Host Threshold */
+#define E1000_TXDCTL_WTHRESH 0x003F0000 /* TXDCTL Writeback Threshold */
+#define E1000_TXDCTL_GRAN    0x01000000 /* TXDCTL Granularity */
+#define E1000_TXDCTL_LWTHRESH 0xFE000000 /* TXDCTL Low Threshold */
+#define E1000_TXDCTL_FULL_TX_DESC_WB 0x01010000 /* GRAN=1, WTHRESH=1 */
+#define E1000_TXDCTL_MAX_TX_DESC_PREFETCH 0x0100001F /* GRAN=1, PTHRESH=31 */
+#define E1000_TXDCTL_COUNT_DESC 0x00400000 /* Enable the counting of desc.
+                                              still to be processed. */
+
+/* Flow Control Constants */
+#define FLOW_CONTROL_ADDRESS_LOW  0x00C28001
+#define FLOW_CONTROL_ADDRESS_HIGH 0x00000100
+#define FLOW_CONTROL_TYPE         0x8808
+
+/* 802.1q VLAN Packet Size */
+#define VLAN_TAG_SIZE              4    /* 802.3ac tag (not DMA'd) */
+#define E1000_VLAN_FILTER_TBL_SIZE 128  /* VLAN Filter Table (4096 bits) */
+
+/* Receive Address */
+/* Number of high/low register pairs in the RAR. The RAR (Receive Address
+ * Registers) holds the directed and multicast addresses that we monitor.
+ * Technically, we have 16 spots.  However, we reserve one of these spots
+ * (RAR[15]) for our directed address used by controllers with
+ * manageability enabled, allowing us room for 15 multicast addresses.
+ */
+#define E1000_RAR_ENTRIES     15
+#define E1000_RAH_AV  0x80000000        /* Receive descriptor valid */
+
+/* Error Codes */
+#define E1000_SUCCESS      0
+#define E1000_ERR_NVM      1
+#define E1000_ERR_PHY      2
+#define E1000_ERR_CONFIG   3
+#define E1000_ERR_PARAM    4
+#define E1000_ERR_MAC_INIT 5
+#define E1000_ERR_PHY_TYPE 6
+#define E1000_ERR_RESET   9
+#define E1000_ERR_MASTER_REQUESTS_PENDING 10
+#define E1000_ERR_HOST_INTERFACE_COMMAND 11
+#define E1000_BLK_PHY_RESET   12
+#define E1000_ERR_SWFW_SYNC 13
+#define E1000_NOT_IMPLEMENTED 14
+
+/* Loop limit on how long we wait for auto-negotiation to complete */
+#define FIBER_LINK_UP_LIMIT               50
+#define COPPER_LINK_UP_LIMIT              10
+#define PHY_AUTO_NEG_LIMIT                45
+#define PHY_FORCE_LIMIT                   20
+/* Number of 100 microseconds we wait for PCI Express master disable */
+#define MASTER_DISABLE_TIMEOUT      800
+/* Number of milliseconds we wait for PHY configuration done after MAC reset */
+#define PHY_CFG_TIMEOUT             100
+/* Number of 2 milliseconds we wait for acquiring MDIO ownership. */
+#define MDIO_OWNERSHIP_TIMEOUT      10
+/* Number of milliseconds for NVM auto read done after MAC reset. */
+#define AUTO_READ_DONE_TIMEOUT      10
+
+/* Flow Control */
+#define E1000_FCRTH_RTH  0x0000FFF8     /* Mask Bits[15:3] for RTH */
+#define E1000_FCRTH_XFCE 0x80000000     /* External Flow Control Enable */
+#define E1000_FCRTL_RTL  0x0000FFF8     /* Mask Bits[15:3] for RTL */
+#define E1000_FCRTL_XONE 0x80000000     /* Enable XON frame transmission */
+
+/* Transmit Configuration Word */
+#define E1000_TXCW_FD         0x00000020        /* TXCW full duplex */
+#define E1000_TXCW_HD         0x00000040        /* TXCW half duplex */
+#define E1000_TXCW_PAUSE      0x00000080        /* TXCW sym pause request */
+#define E1000_TXCW_ASM_DIR    0x00000100        /* TXCW astm pause direction */
+#define E1000_TXCW_PAUSE_MASK 0x00000180        /* TXCW pause request mask */
+#define E1000_TXCW_RF         0x00003000        /* TXCW remote fault */
+#define E1000_TXCW_NP         0x00008000        /* TXCW next page */
+#define E1000_TXCW_CW         0x0000ffff        /* TxConfigWord mask */
+#define E1000_TXCW_TXC        0x40000000        /* Transmit Config control */
+#define E1000_TXCW_ANE        0x80000000        /* Auto-neg enable */
+
+/* Receive Configuration Word */
+#define E1000_RXCW_CW         0x0000ffff        /* RxConfigWord mask */
+#define E1000_RXCW_NC         0x04000000        /* Receive config no carrier */
+#define E1000_RXCW_IV         0x08000000        /* Receive config invalid */
+#define E1000_RXCW_CC         0x10000000        /* Receive config change */
+#define E1000_RXCW_C          0x20000000        /* Receive config */
+#define E1000_RXCW_SYNCH      0x40000000        /* Receive config synch */
+#define E1000_RXCW_ANC        0x80000000        /* Auto-neg complete */
+
+/* PCI Express Control */
+#define E1000_GCR_RXD_NO_SNOOP          0x00000001
+#define E1000_GCR_RXDSCW_NO_SNOOP       0x00000002
+#define E1000_GCR_RXDSCR_NO_SNOOP       0x00000004
+#define E1000_GCR_TXD_NO_SNOOP          0x00000008
+#define E1000_GCR_TXDSCW_NO_SNOOP       0x00000010
+#define E1000_GCR_TXDSCR_NO_SNOOP       0x00000020
+
+#define PCIE_NO_SNOOP_ALL (E1000_GCR_RXD_NO_SNOOP         | \
+                           E1000_GCR_RXDSCW_NO_SNOOP      | \
+                           E1000_GCR_RXDSCR_NO_SNOOP      | \
+                           E1000_GCR_TXD_NO_SNOOP         | \
+                           E1000_GCR_TXDSCW_NO_SNOOP      | \
+                           E1000_GCR_TXDSCR_NO_SNOOP)
+
+/* PHY Control Register */
+#define MII_CR_SPEED_SELECT_MSB 0x0040  /* bits 6,13: 10=1000, 01=100, 00=10 */
+#define MII_CR_COLL_TEST_ENABLE 0x0080  /* Collision test enable */
+#define MII_CR_FULL_DUPLEX      0x0100  /* FDX =1, half duplex =0 */
+#define MII_CR_RESTART_AUTO_NEG 0x0200  /* Restart auto negotiation */
+#define MII_CR_ISOLATE          0x0400  /* Isolate PHY from MII */
+#define MII_CR_POWER_DOWN       0x0800  /* Power down */
+#define MII_CR_AUTO_NEG_EN      0x1000  /* Auto Neg Enable */
+#define MII_CR_SPEED_SELECT_LSB 0x2000  /* bits 6,13: 10=1000, 01=100, 00=10 */
+#define MII_CR_LOOPBACK         0x4000  /* 0 = normal, 1 = loopback */
+#define MII_CR_RESET            0x8000  /* 0 = normal, 1 = PHY reset */
+#define MII_CR_SPEED_1000       0x0040
+#define MII_CR_SPEED_100        0x2000
+#define MII_CR_SPEED_10         0x0000
+
+/* PHY Status Register */
+#define MII_SR_EXTENDED_CAPS     0x0001 /* Extended register capabilities */
+#define MII_SR_JABBER_DETECT     0x0002 /* Jabber Detected */
+#define MII_SR_LINK_STATUS       0x0004 /* Link Status 1 = link */
+#define MII_SR_AUTONEG_CAPS      0x0008 /* Auto Neg Capable */
+#define MII_SR_REMOTE_FAULT      0x0010 /* Remote Fault Detect */
+#define MII_SR_AUTONEG_COMPLETE  0x0020 /* Auto Neg Complete */
+#define MII_SR_PREAMBLE_SUPPRESS 0x0040 /* Preamble may be suppressed */
+#define MII_SR_EXTENDED_STATUS   0x0100 /* Ext. status info in Reg 0x0F */
+#define MII_SR_100T2_HD_CAPS     0x0200 /* 100T2 Half Duplex Capable */
+#define MII_SR_100T2_FD_CAPS     0x0400 /* 100T2 Full Duplex Capable */
+#define MII_SR_10T_HD_CAPS       0x0800 /* 10T   Half Duplex Capable */
+#define MII_SR_10T_FD_CAPS       0x1000 /* 10T   Full Duplex Capable */
+#define MII_SR_100X_HD_CAPS      0x2000 /* 100X  Half Duplex Capable */
+#define MII_SR_100X_FD_CAPS      0x4000 /* 100X  Full Duplex Capable */
+#define MII_SR_100T4_CAPS        0x8000 /* 100T4 Capable */
+
+/* Autoneg Advertisement Register */
+#define NWAY_AR_SELECTOR_FIELD   0x0001   /* indicates IEEE 802.3 CSMA/CD */
+#define NWAY_AR_10T_HD_CAPS      0x0020   /* 10T   Half Duplex Capable */
+#define NWAY_AR_10T_FD_CAPS      0x0040   /* 10T   Full Duplex Capable */
+#define NWAY_AR_100TX_HD_CAPS    0x0080   /* 100TX Half Duplex Capable */
+#define NWAY_AR_100TX_FD_CAPS    0x0100   /* 100TX Full Duplex Capable */
+#define NWAY_AR_100T4_CAPS       0x0200   /* 100T4 Capable */
+#define NWAY_AR_PAUSE            0x0400   /* Pause operation desired */
+#define NWAY_AR_ASM_DIR          0x0800   /* Asymmetric Pause Direction bit */
+#define NWAY_AR_REMOTE_FAULT     0x2000   /* Remote Fault detected */
+#define NWAY_AR_NEXT_PAGE        0x8000   /* Next Page ability supported */
+
+/* Link Partner Ability Register (Base Page) */
+#define NWAY_LPAR_SELECTOR_FIELD 0x0000 /* LP protocol selector field */
+#define NWAY_LPAR_10T_HD_CAPS    0x0020 /* LP is 10T   Half Duplex Capable */
+#define NWAY_LPAR_10T_FD_CAPS    0x0040 /* LP is 10T   Full Duplex Capable */
+#define NWAY_LPAR_100TX_HD_CAPS  0x0080 /* LP is 100TX Half Duplex Capable */
+#define NWAY_LPAR_100TX_FD_CAPS  0x0100 /* LP is 100TX Full Duplex Capable */
+#define NWAY_LPAR_100T4_CAPS     0x0200 /* LP is 100T4 Capable */
+#define NWAY_LPAR_PAUSE          0x0400 /* LP Pause operation desired */
+#define NWAY_LPAR_ASM_DIR        0x0800 /* LP Asymmetric Pause Direction bit */
+#define NWAY_LPAR_REMOTE_FAULT   0x2000 /* LP has detected Remote Fault */
+#define NWAY_LPAR_ACKNOWLEDGE    0x4000 /* LP has rx'd link code word */
+#define NWAY_LPAR_NEXT_PAGE      0x8000 /* Next Page ability supported */
+
+/* Autoneg Expansion Register */
+#define NWAY_ER_LP_NWAY_CAPS      0x0001 /* LP has Auto Neg Capability */
+#define NWAY_ER_PAGE_RXD          0x0002 /* LP is 10T   Half Duplex Capable */
+#define NWAY_ER_NEXT_PAGE_CAPS    0x0004 /* LP is 10T   Full Duplex Capable */
+#define NWAY_ER_LP_NEXT_PAGE_CAPS 0x0008 /* LP is 100TX Half Duplex Capable */
+#define NWAY_ER_PAR_DETECT_FAULT  0x0010 /* LP is 100TX Full Duplex Capable */
+
+/* 1000BASE-T Control Register */
+#define CR_1000T_ASYM_PAUSE      0x0080 /* Advertise asymmetric pause bit */
+#define CR_1000T_HD_CAPS         0x0100 /* Advertise 1000T HD capability */
+#define CR_1000T_FD_CAPS         0x0200 /* Advertise 1000T FD capability  */
+#define CR_1000T_REPEATER_DTE    0x0400 /* 1=Repeater/switch device port */
+                                        /* 0=DTE device */
+#define CR_1000T_MS_VALUE        0x0800 /* 1=Configure PHY as Master */
+                                        /* 0=Configure PHY as Slave */
+#define CR_1000T_MS_ENABLE       0x1000 /* 1=Master/Slave manual config value */
+                                        /* 0=Automatic Master/Slave config */
+#define CR_1000T_TEST_MODE_NORMAL 0x0000 /* Normal Operation */
+#define CR_1000T_TEST_MODE_1     0x2000 /* Transmit Waveform test */
+#define CR_1000T_TEST_MODE_2     0x4000 /* Master Transmit Jitter test */
+#define CR_1000T_TEST_MODE_3     0x6000 /* Slave Transmit Jitter test */
+#define CR_1000T_TEST_MODE_4     0x8000 /* Transmitter Distortion test */
+
+/* 1000BASE-T Status Register */
+#define SR_1000T_IDLE_ERROR_CNT   0x00FF /* Num idle errors since last read */
+#define SR_1000T_ASYM_PAUSE_DIR   0x0100 /* LP asymmetric pause direction bit */
+#define SR_1000T_LP_HD_CAPS       0x0400 /* LP is 1000T HD capable */
+#define SR_1000T_LP_FD_CAPS       0x0800 /* LP is 1000T FD capable */
+#define SR_1000T_REMOTE_RX_STATUS 0x1000 /* Remote receiver OK */
+#define SR_1000T_LOCAL_RX_STATUS  0x2000 /* Local receiver OK */
+#define SR_1000T_MS_CONFIG_RES    0x4000 /* 1=Local TX is Master, 0=Slave */
+#define SR_1000T_MS_CONFIG_FAULT  0x8000 /* Master/Slave config fault */
+
+#define SR_1000T_PHY_EXCESSIVE_IDLE_ERR_COUNT 5
+
+/* PHY 1000 MII Register/Bit Definitions */
+/* PHY Registers defined by IEEE */
+#define PHY_CONTROL      0x00 /* Control Register */
+#define PHY_STATUS       0x01 /* Status Regiser */
+#define PHY_ID1          0x02 /* Phy Id Reg (word 1) */
+#define PHY_ID2          0x03 /* Phy Id Reg (word 2) */
+#define PHY_AUTONEG_ADV  0x04 /* Autoneg Advertisement */
+#define PHY_LP_ABILITY   0x05 /* Link Partner Ability (Base Page) */
+#define PHY_AUTONEG_EXP  0x06 /* Autoneg Expansion Reg */
+#define PHY_NEXT_PAGE_TX 0x07 /* Next Page TX */
+#define PHY_LP_NEXT_PAGE 0x08 /* Link Partner Next Page */
+#define PHY_1000T_CTRL   0x09 /* 1000Base-T Control Reg */
+#define PHY_1000T_STATUS 0x0A /* 1000Base-T Status Reg */
+#define PHY_EXT_STATUS   0x0F /* Extended Status Reg */
+
+/* NVM Control */
+#define E1000_EECD_SK        0x00000001 /* NVM Clock */
+#define E1000_EECD_CS        0x00000002 /* NVM Chip Select */
+#define E1000_EECD_DI        0x00000004 /* NVM Data In */
+#define E1000_EECD_DO        0x00000008 /* NVM Data Out */
+#define E1000_EECD_FWE_MASK  0x00000030
+#define E1000_EECD_FWE_DIS   0x00000010 /* Disable FLASH writes */
+#define E1000_EECD_FWE_EN    0x00000020 /* Enable FLASH writes */
+#define E1000_EECD_FWE_SHIFT 4
+#define E1000_EECD_REQ       0x00000040 /* NVM Access Request */
+#define E1000_EECD_GNT       0x00000080 /* NVM Access Grant */
+#define E1000_EECD_PRES      0x00000100 /* NVM Present */
+#define E1000_EECD_SIZE      0x00000200 /* NVM Size (0=64 word 1=256 word) */
+#define E1000_EECD_ADDR_BITS 0x00000400 /* NVM Addressing bits based on type
+                                         * (0-small, 1-large) */
+#define E1000_EECD_TYPE      0x00002000 /* NVM Type (1-SPI, 0-Microwire) */
+#define E1000_NVM_GRANT_ATTEMPTS   1000 /* NVM # attempts to gain grant */
+#define E1000_EECD_AUTO_RD          0x00000200  /* NVM Auto Read done */
+#define E1000_EECD_SIZE_EX_MASK     0x00007800  /* NVM Size */
+#define E1000_EECD_SIZE_EX_SHIFT     11
+#define E1000_EECD_NVADDS    0x00018000 /* NVM Address Size */
+#define E1000_EECD_SELSHAD   0x00020000 /* Select Shadow RAM */
+#define E1000_EECD_INITSRAM  0x00040000 /* Initialize Shadow RAM */
+#define E1000_EECD_FLUPD     0x00080000 /* Update FLASH */
+#define E1000_EECD_AUPDEN    0x00100000 /* Enable Autonomous FLASH update */
+#define E1000_EECD_SHADV     0x00200000 /* Shadow RAM Data Valid */
+#define E1000_EECD_SEC1VAL   0x00400000 /* Sector One Valid */
+#define E1000_EECD_SECVAL_SHIFT      22
+
+#define E1000_NVM_SWDPIN0   0x0001   /* SWDPIN 0 NVM Value */
+#define E1000_NVM_LED_LOGIC 0x0020   /* Led Logic Word */
+#define E1000_NVM_RW_REG_DATA   16   /* Offset to data in NVM read/write registers */
+#define E1000_NVM_RW_REG_DONE   2    /* Offset to READ/WRITE done bit */
+#define E1000_NVM_RW_REG_START  1    /* Start operation */
+#define E1000_NVM_RW_ADDR_SHIFT 2    /* Shift to the address bits */
+#define E1000_NVM_POLL_WRITE    1    /* Flag for polling for write complete */
+#define E1000_NVM_POLL_READ     0    /* Flag for polling for read complete */
+#define E1000_FLASH_UPDATES  2000
+
+/* NVM Word Offsets */
+#define NVM_COMPAT                 0x0003
+#define NVM_ID_LED_SETTINGS        0x0004
+#define NVM_VERSION                0x0005
+#define NVM_SERDES_AMPLITUDE       0x0006 /* For SERDES output amplitude adjustment. */
+#define NVM_PHY_CLASS_WORD         0x0007
+#define NVM_INIT_CONTROL1_REG      0x000A
+#define NVM_INIT_CONTROL2_REG      0x000F
+#define NVM_SWDEF_PINS_CTRL_PORT_1 0x0010
+#define NVM_INIT_CONTROL3_PORT_B   0x0014
+#define NVM_INIT_3GIO_3            0x001A
+#define NVM_SWDEF_PINS_CTRL_PORT_0 0x0020
+#define NVM_INIT_CONTROL3_PORT_A   0x0024
+#define NVM_CFG                    0x0012
+#define NVM_FLASH_VERSION          0x0032
+#define NVM_CHECKSUM_REG           0x003F
+
+#define E1000_NVM_CFG_DONE_PORT_0  0x40000 /* MNG config cycle done */
+#define E1000_NVM_CFG_DONE_PORT_1  0x80000 /* ...for second port */
+
+/* Mask bits for fields in Word 0x0f of the NVM */
+#define NVM_WORD0F_PAUSE_MASK       0x3000
+#define NVM_WORD0F_PAUSE            0x1000
+#define NVM_WORD0F_ASM_DIR          0x2000
+#define NVM_WORD0F_ANE              0x0800
+#define NVM_WORD0F_SWPDIO_EXT_MASK  0x00F0
+#define NVM_WORD0F_LPLU             0x0001
+
+/* Mask bits for fields in Word 0x1a of the NVM */
+#define NVM_WORD1A_ASPM_MASK  0x000C
+
+/* For checksumming, the sum of all words in the NVM should equal 0xBABA. */
+#define NVM_SUM                    0xBABA
+
+#define NVM_MAC_ADDR_OFFSET        0
+#define NVM_PBA_OFFSET_0           8
+#define NVM_PBA_OFFSET_1           9
+#define NVM_RESERVED_WORD          0xFFFF
+#define NVM_PHY_CLASS_A            0x8000
+#define NVM_SERDES_AMPLITUDE_MASK  0x000F
+#define NVM_SIZE_MASK              0x1C00
+#define NVM_SIZE_SHIFT             10
+#define NVM_WORD_SIZE_BASE_SHIFT   6
+#define NVM_SWDPIO_EXT_SHIFT       4
+
+/* NVM Commands - Microwire */
+#define NVM_READ_OPCODE_MICROWIRE  0x6  /* NVM read opcode */
+#define NVM_WRITE_OPCODE_MICROWIRE 0x5  /* NVM write opcode */
+#define NVM_ERASE_OPCODE_MICROWIRE 0x7  /* NVM erase opcode */
+#define NVM_EWEN_OPCODE_MICROWIRE  0x13 /* NVM erase/write enable */
+#define NVM_EWDS_OPCODE_MICROWIRE  0x10 /* NVM erast/write disable */
+
+/* NVM Commands - SPI */
+#define NVM_MAX_RETRY_SPI          5000 /* Max wait of 5ms, for RDY signal */
+#define NVM_READ_OPCODE_SPI        0x03 /* NVM read opcode */
+#define NVM_WRITE_OPCODE_SPI       0x02 /* NVM write opcode */
+#define NVM_A8_OPCODE_SPI          0x08 /* opcode bit-3 = address bit-8 */
+#define NVM_WREN_OPCODE_SPI        0x06 /* NVM set Write Enable latch */
+#define NVM_WRDI_OPCODE_SPI        0x04 /* NVM reset Write Enable latch */
+#define NVM_RDSR_OPCODE_SPI        0x05 /* NVM read Status register */
+#define NVM_WRSR_OPCODE_SPI        0x01 /* NVM write Status register */
+
+/* SPI NVM Status Register */
+#define NVM_STATUS_RDY_SPI         0x01
+#define NVM_STATUS_WEN_SPI         0x02
+#define NVM_STATUS_BP0_SPI         0x04
+#define NVM_STATUS_BP1_SPI         0x08
+#define NVM_STATUS_WPEN_SPI        0x80
+
+/* Word definitions for ID LED Settings */
+#define ID_LED_RESERVED_0000 0x0000
+#define ID_LED_RESERVED_FFFF 0xFFFF
+#define ID_LED_DEFAULT       ((ID_LED_OFF1_ON2  << 12) | \
+                              (ID_LED_OFF1_OFF2 <<  8) | \
+                              (ID_LED_DEF1_DEF2 <<  4) | \
+                              (ID_LED_DEF1_DEF2))
+#define ID_LED_DEF1_DEF2     0x1
+#define ID_LED_DEF1_ON2      0x2
+#define ID_LED_DEF1_OFF2     0x3
+#define ID_LED_ON1_DEF2      0x4
+#define ID_LED_ON1_ON2       0x5
+#define ID_LED_ON1_OFF2      0x6
+#define ID_LED_OFF1_DEF2     0x7
+#define ID_LED_OFF1_ON2      0x8
+#define ID_LED_OFF1_OFF2     0x9
+
+#define IGP_ACTIVITY_LED_MASK   0xFFFFF0FF
+#define IGP_ACTIVITY_LED_ENABLE 0x0300
+#define IGP_LED3_MODE           0x07000000
+
+/* PCI/PCI-X/PCI-EX Config space */
+#define PCIX_COMMAND_REGISTER        0xE6
+#define PCIX_STATUS_REGISTER_LO      0xE8
+#define PCIX_STATUS_REGISTER_HI      0xEA
+#define PCI_HEADER_TYPE_REGISTER     0x0E
+#define PCIE_LINK_STATUS             0x12
+
+#define PCIX_COMMAND_MMRBC_MASK      0x000C
+#define PCIX_COMMAND_MMRBC_SHIFT     0x2
+#define PCIX_STATUS_HI_MMRBC_MASK    0x0060
+#define PCIX_STATUS_HI_MMRBC_SHIFT   0x5
+#define PCIX_STATUS_HI_MMRBC_4K      0x3
+#define PCIX_STATUS_HI_MMRBC_2K      0x2
+#define PCIX_STATUS_LO_FUNC_MASK     0x7
+#define PCI_HEADER_TYPE_MULTIFUNC    0x80
+#define PCIE_LINK_WIDTH_MASK         0x3F0
+#define PCIE_LINK_WIDTH_SHIFT        4
+
+#ifndef ETH_ADDR_LEN
+#define ETH_ADDR_LEN                 6
+#endif
+
+#define PHY_REVISION_MASK      0xFFFFFFF0
+#define MAX_PHY_REG_ADDRESS    0x1F  /* 5 bit address bus (0-0x1F) */
+#define MAX_PHY_MULTI_PAGE_REG 0xF
+
+/* Bit definitions for valid PHY IDs. */
+/* I = Integrated
+ * E = External
+ */
+#define M88E1000_E_PHY_ID    0x01410C50
+#define M88E1000_I_PHY_ID    0x01410C30
+#define M88E1011_I_PHY_ID    0x01410C20
+#define IGP01E1000_I_PHY_ID  0x02A80380
+#define M88E1011_I_REV_4     0x04
+#define M88E1111_I_PHY_ID    0x01410CC0
+#define GG82563_E_PHY_ID     0x01410CA0
+#define IGP03E1000_E_PHY_ID  0x02A80390
+#define IFE_E_PHY_ID         0x02A80330
+#define IFE_PLUS_E_PHY_ID    0x02A80320
+#define IFE_C_E_PHY_ID       0x02A80310
+#define M88_VENDOR           0x0141
+
+/* M88E1000 Specific Registers */
+#define M88E1000_PHY_SPEC_CTRL     0x10  /* PHY Specific Control Register */
+#define M88E1000_PHY_SPEC_STATUS   0x11  /* PHY Specific Status Register */
+#define M88E1000_INT_ENABLE        0x12  /* Interrupt Enable Register */
+#define M88E1000_INT_STATUS        0x13  /* Interrupt Status Register */
+#define M88E1000_EXT_PHY_SPEC_CTRL 0x14  /* Extended PHY Specific Control */
+#define M88E1000_RX_ERR_CNTR       0x15  /* Receive Error Counter */
+
+#define M88E1000_PHY_EXT_CTRL      0x1A  /* PHY extend control register */
+#define M88E1000_PHY_PAGE_SELECT   0x1D  /* Reg 29 for page number setting */
+#define M88E1000_PHY_GEN_CONTROL   0x1E  /* Its meaning depends on reg 29 */
+#define M88E1000_PHY_VCO_REG_BIT8  0x100 /* Bits 8 & 11 are adjusted for */
+#define M88E1000_PHY_VCO_REG_BIT11 0x800    /* improved BER performance */
+
+/* M88E1000 PHY Specific Control Register */
+#define M88E1000_PSCR_JABBER_DISABLE    0x0001 /* 1=Jabber Function disabled */
+#define M88E1000_PSCR_POLARITY_REVERSAL 0x0002 /* 1=Polarity Reversal enabled */
+#define M88E1000_PSCR_SQE_TEST          0x0004 /* 1=SQE Test enabled */
+#define M88E1000_PSCR_CLK125_DISABLE    0x0010 /* 1=CLK125 low,
+                                                * 0=CLK125 toggling
+                                                */
+#define M88E1000_PSCR_MDI_MANUAL_MODE  0x0000  /* MDI Crossover Mode bits 6:5 */
+                                               /* Manual MDI configuration */
+#define M88E1000_PSCR_MDIX_MANUAL_MODE 0x0020  /* Manual MDIX configuration */
+#define M88E1000_PSCR_AUTO_X_1000T     0x0040  /* 1000BASE-T: Auto crossover,
+                                                *  100BASE-TX/10BASE-T:
+                                                *  MDI Mode
+                                                */
+#define M88E1000_PSCR_AUTO_X_MODE      0x0060  /* Auto crossover enabled
+                                                * all speeds.
+                                                */
+#define M88E1000_PSCR_EN_10BT_EXT_DIST 0x0080
+                                        /* 1=Enable Extended 10BASE-T distance
+                                         * (Lower 10BASE-T RX Threshold)
+                                         * 0=Normal 10BASE-T RX Threshold */
+#define M88E1000_PSCR_MII_5BIT_ENABLE      0x0100
+                                        /* 1=5-Bit interface in 100BASE-TX
+                                         * 0=MII interface in 100BASE-TX */
+#define M88E1000_PSCR_SCRAMBLER_DISABLE    0x0200 /* 1=Scrambler disable */
+#define M88E1000_PSCR_FORCE_LINK_GOOD      0x0400 /* 1=Force link good */
+#define M88E1000_PSCR_ASSERT_CRS_ON_TX     0x0800 /* 1=Assert CRS on Transmit */
+
+/* M88E1000 PHY Specific Status Register */
+#define M88E1000_PSSR_JABBER             0x0001 /* 1=Jabber */
+#define M88E1000_PSSR_REV_POLARITY       0x0002 /* 1=Polarity reversed */
+#define M88E1000_PSSR_DOWNSHIFT          0x0020 /* 1=Downshifted */
+#define M88E1000_PSSR_MDIX               0x0040 /* 1=MDIX; 0=MDI */
+#define M88E1000_PSSR_CABLE_LENGTH       0x0380 /* 0=<50M;1=50-80M;2=80-110M;
+                                            * 3=110-140M;4=>140M */
+#define M88E1000_PSSR_LINK               0x0400 /* 1=Link up, 0=Link down */
+#define M88E1000_PSSR_SPD_DPLX_RESOLVED  0x0800 /* 1=Speed & Duplex resolved */
+#define M88E1000_PSSR_PAGE_RCVD          0x1000 /* 1=Page received */
+#define M88E1000_PSSR_DPLX               0x2000 /* 1=Duplex 0=Half Duplex */
+#define M88E1000_PSSR_SPEED              0xC000 /* Speed, bits 14:15 */
+#define M88E1000_PSSR_10MBS              0x0000 /* 00=10Mbs */
+#define M88E1000_PSSR_100MBS             0x4000 /* 01=100Mbs */
+#define M88E1000_PSSR_1000MBS            0x8000 /* 10=1000Mbs */
+
+#define M88E1000_PSSR_CABLE_LENGTH_SHIFT 7
+
+/* M88E1000 Extended PHY Specific Control Register */
+#define M88E1000_EPSCR_FIBER_LOOPBACK 0x4000 /* 1=Fiber loopback */
+#define M88E1000_EPSCR_DOWN_NO_IDLE   0x8000 /* 1=Lost lock detect enabled.
+                                              * Will assert lost lock and bring
+                                              * link down if idle not seen
+                                              * within 1ms in 1000BASE-T
+                                              */
+/* Number of times we will attempt to autonegotiate before downshifting if we
+ * are the master */
+#define M88E1000_EPSCR_MASTER_DOWNSHIFT_MASK 0x0C00
+#define M88E1000_EPSCR_MASTER_DOWNSHIFT_1X   0x0000
+#define M88E1000_EPSCR_MASTER_DOWNSHIFT_2X   0x0400
+#define M88E1000_EPSCR_MASTER_DOWNSHIFT_3X   0x0800
+#define M88E1000_EPSCR_MASTER_DOWNSHIFT_4X   0x0C00
+/* Number of times we will attempt to autonegotiate before downshifting if we
+ * are the slave */
+#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_MASK  0x0300
+#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_DIS   0x0000
+#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_1X    0x0100
+#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_2X    0x0200
+#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_3X    0x0300
+#define M88E1000_EPSCR_TX_CLK_2_5     0x0060 /* 2.5 MHz TX_CLK */
+#define M88E1000_EPSCR_TX_CLK_25      0x0070 /* 25  MHz TX_CLK */
+#define M88E1000_EPSCR_TX_CLK_0       0x0000 /* NO  TX_CLK */
+
+/* M88EC018 Rev 2 specific DownShift settings */
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_MASK  0x0E00
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_1X    0x0000
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_2X    0x0200
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_3X    0x0400
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_4X    0x0600
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_5X    0x0800
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_6X    0x0A00
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_7X    0x0C00
+#define M88EC018_EPSCR_DOWNSHIFT_COUNTER_8X    0x0E00
+
+/* Bits...
+ * 15-5: page
+ * 4-0: register offset
+ */
+#define GG82563_PAGE_SHIFT        5
+#define GG82563_REG(page, reg)    \
+        (((page) << GG82563_PAGE_SHIFT) | ((reg) & MAX_PHY_REG_ADDRESS))
+#define GG82563_MIN_ALT_REG       30
+
+/* GG82563 Specific Registers */
+#define GG82563_PHY_SPEC_CTRL           \
+        GG82563_REG(0, 16) /* PHY Specific Control */
+#define GG82563_PHY_SPEC_STATUS         \
+        GG82563_REG(0, 17) /* PHY Specific Status */
+#define GG82563_PHY_INT_ENABLE          \
+        GG82563_REG(0, 18) /* Interrupt Enable */
+#define GG82563_PHY_SPEC_STATUS_2       \
+        GG82563_REG(0, 19) /* PHY Specific Status 2 */
+#define GG82563_PHY_RX_ERR_CNTR         \
+        GG82563_REG(0, 21) /* Receive Error Counter */
+#define GG82563_PHY_PAGE_SELECT         \
+        GG82563_REG(0, 22) /* Page Select */
+#define GG82563_PHY_SPEC_CTRL_2         \
+        GG82563_REG(0, 26) /* PHY Specific Control 2 */
+#define GG82563_PHY_PAGE_SELECT_ALT     \
+        GG82563_REG(0, 29) /* Alternate Page Select */
+#define GG82563_PHY_TEST_CLK_CTRL       \
+        GG82563_REG(0, 30) /* Test Clock Control (use reg. 29 to select) */
+
+#define GG82563_PHY_MAC_SPEC_CTRL       \
+        GG82563_REG(2, 21) /* MAC Specific Control Register */
+#define GG82563_PHY_MAC_SPEC_CTRL_2     \
+        GG82563_REG(2, 26) /* MAC Specific Control 2 */
+
+#define GG82563_PHY_DSP_DISTANCE    \
+        GG82563_REG(5, 26) /* DSP Distance */
+
+/* Page 193 - Port Control Registers */
+#define GG82563_PHY_KMRN_MODE_CTRL   \
+        GG82563_REG(193, 16) /* Kumeran Mode Control */
+#define GG82563_PHY_PORT_RESET          \
+        GG82563_REG(193, 17) /* Port Reset */
+#define GG82563_PHY_REVISION_ID         \
+        GG82563_REG(193, 18) /* Revision ID */
+#define GG82563_PHY_DEVICE_ID           \
+        GG82563_REG(193, 19) /* Device ID */
+#define GG82563_PHY_PWR_MGMT_CTRL       \
+        GG82563_REG(193, 20) /* Power Management Control */
+#define GG82563_PHY_RATE_ADAPT_CTRL     \
+        GG82563_REG(193, 25) /* Rate Adaptation Control */
+
+/* Page 194 - KMRN Registers */
+#define GG82563_PHY_KMRN_FIFO_CTRL_STAT \
+        GG82563_REG(194, 16) /* FIFO's Control/Status */
+#define GG82563_PHY_KMRN_CTRL           \
+        GG82563_REG(194, 17) /* Control */
+#define GG82563_PHY_INBAND_CTRL         \
+        GG82563_REG(194, 18) /* Inband Control */
+#define GG82563_PHY_KMRN_DIAGNOSTIC     \
+        GG82563_REG(194, 19) /* Diagnostic */
+#define GG82563_PHY_ACK_TIMEOUTS        \
+        GG82563_REG(194, 20) /* Acknowledge Timeouts */
+#define GG82563_PHY_ADV_ABILITY         \
+        GG82563_REG(194, 21) /* Advertised Ability */
+#define GG82563_PHY_LINK_PARTNER_ADV_ABILITY \
+        GG82563_REG(194, 23) /* Link Partner Advertised Ability */
+#define GG82563_PHY_ADV_NEXT_PAGE       \
+        GG82563_REG(194, 24) /* Advertised Next Page */
+#define GG82563_PHY_LINK_PARTNER_ADV_NEXT_PAGE \
+        GG82563_REG(194, 25) /* Link Partner Advertised Next page */
+#define GG82563_PHY_KMRN_MISC           \
+        GG82563_REG(194, 26) /* Misc. */
+
+/* MDI Control */
+#define E1000_MDIC_DATA_MASK 0x0000FFFF
+#define E1000_MDIC_REG_MASK  0x001F0000
+#define E1000_MDIC_REG_SHIFT 16
+#define E1000_MDIC_PHY_MASK  0x03E00000
+#define E1000_MDIC_PHY_SHIFT 21
+#define E1000_MDIC_OP_WRITE  0x04000000
+#define E1000_MDIC_OP_READ   0x08000000
+#define E1000_MDIC_READY     0x10000000
+#define E1000_MDIC_INT_EN    0x20000000
+#define E1000_MDIC_ERROR     0x40000000
+
+/* SerDes Control */
+#define E1000_GEN_CTL_READY             0x80000000
+#define E1000_GEN_CTL_ADDRESS_SHIFT     8
+#define E1000_GEN_POLL_TIMEOUT          640
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_ethtool.c linux-2.6.9/drivers/net/e1000/e1000_ethtool.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_ethtool.c	2004-10-18 23:54:07.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_ethtool.c	2007-07-16 13:33:15.000000000 +0200
@@ -1,50 +1,60 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
 
 /* ethtool support for e1000 */
 
+#include <linux/netdevice.h>
+
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+
 #include "e1000.h"
+#include "e1000_82541.h"
 
-#include <asm/uaccess.h>
+#ifdef ETHTOOL_OPS_COMPAT
+#include "kcompat_ethtool.c"
+#endif
 
 extern char e1000_driver_name[];
 extern char e1000_driver_version[];
 
 extern int e1000_up(struct e1000_adapter *adapter);
 extern void e1000_down(struct e1000_adapter *adapter);
+extern void e1000_reinit_locked(struct e1000_adapter *adapter);
 extern void e1000_reset(struct e1000_adapter *adapter);
-extern int e1000_set_spd_dplx(struct e1000_adapter *adapter, uint16_t spddplx);
-extern int e1000_setup_rx_resources(struct e1000_adapter *adapter);
-extern int e1000_setup_tx_resources(struct e1000_adapter *adapter);
-extern void e1000_free_rx_resources(struct e1000_adapter *adapter);
-extern void e1000_free_tx_resources(struct e1000_adapter *adapter);
+extern int e1000_set_spd_dplx(struct e1000_adapter *adapter, u16 spddplx);
+extern int e1000_setup_all_rx_resources(struct e1000_adapter *adapter);
+extern int e1000_setup_all_tx_resources(struct e1000_adapter *adapter);
+extern void e1000_free_all_rx_resources(struct e1000_adapter *adapter);
+extern void e1000_free_all_tx_resources(struct e1000_adapter *adapter);
 extern void e1000_update_stats(struct e1000_adapter *adapter);
 
+#ifdef ETHTOOL_GSTATS
 struct e1000_stats {
 	char stat_string[ETH_GSTRING_LEN];
 	int sizeof_stat;
@@ -54,31 +64,36 @@
 #define E1000_STAT(m) sizeof(((struct e1000_adapter *)0)->m), \
 		      offsetof(struct e1000_adapter, m)
 static const struct e1000_stats e1000_gstrings_stats[] = {
-	{ "rx_packets", E1000_STAT(net_stats.rx_packets) },
-	{ "tx_packets", E1000_STAT(net_stats.tx_packets) },
-	{ "rx_bytes", E1000_STAT(net_stats.rx_bytes) },
-	{ "tx_bytes", E1000_STAT(net_stats.tx_bytes) },
+	{ "rx_packets", E1000_STAT(stats.gprc) },
+	{ "tx_packets", E1000_STAT(stats.gptc) },
+	{ "rx_bytes", E1000_STAT(stats.gorcl) },
+	{ "tx_bytes", E1000_STAT(stats.gotcl) },
+	{ "rx_broadcast", E1000_STAT(stats.bprc) },
+	{ "tx_broadcast", E1000_STAT(stats.bptc) },
+	{ "rx_multicast", E1000_STAT(stats.mprc) },
+	{ "tx_multicast", E1000_STAT(stats.mptc) },
 	{ "rx_errors", E1000_STAT(net_stats.rx_errors) },
 	{ "tx_errors", E1000_STAT(net_stats.tx_errors) },
-	{ "rx_dropped", E1000_STAT(net_stats.rx_dropped) },
 	{ "tx_dropped", E1000_STAT(net_stats.tx_dropped) },
-	{ "multicast", E1000_STAT(net_stats.multicast) },
-	{ "collisions", E1000_STAT(net_stats.collisions) },
+	{ "multicast", E1000_STAT(stats.mprc) },
+	{ "collisions", E1000_STAT(stats.colc) },
 	{ "rx_length_errors", E1000_STAT(net_stats.rx_length_errors) },
 	{ "rx_over_errors", E1000_STAT(net_stats.rx_over_errors) },
-	{ "rx_crc_errors", E1000_STAT(net_stats.rx_crc_errors) },
+	{ "rx_crc_errors", E1000_STAT(stats.crcerrs) },
 	{ "rx_frame_errors", E1000_STAT(net_stats.rx_frame_errors) },
-	{ "rx_fifo_errors", E1000_STAT(net_stats.rx_fifo_errors) },
-	{ "rx_missed_errors", E1000_STAT(net_stats.rx_missed_errors) },
-	{ "tx_aborted_errors", E1000_STAT(net_stats.tx_aborted_errors) },
-	{ "tx_carrier_errors", E1000_STAT(net_stats.tx_carrier_errors) },
+	{ "rx_no_buffer_count", E1000_STAT(stats.rnbc) },
+	{ "rx_missed_errors", E1000_STAT(stats.mpc) },
+	{ "tx_aborted_errors", E1000_STAT(stats.ecol) },
+	{ "tx_carrier_errors", E1000_STAT(stats.tncrs) },
 	{ "tx_fifo_errors", E1000_STAT(net_stats.tx_fifo_errors) },
 	{ "tx_heartbeat_errors", E1000_STAT(net_stats.tx_heartbeat_errors) },
-	{ "tx_window_errors", E1000_STAT(net_stats.tx_window_errors) },
+	{ "tx_window_errors", E1000_STAT(stats.latecol) },
 	{ "tx_abort_late_coll", E1000_STAT(stats.latecol) },
 	{ "tx_deferred_ok", E1000_STAT(stats.dc) },
 	{ "tx_single_coll_ok", E1000_STAT(stats.scc) },
 	{ "tx_multi_coll_ok", E1000_STAT(stats.mcc) },
+	{ "tx_timeout_count", E1000_STAT(tx_timeout_count) },
+	{ "tx_restart_queue", E1000_STAT(restart_queue) },
 	{ "rx_long_length_errors", E1000_STAT(stats.roc) },
 	{ "rx_short_length_errors", E1000_STAT(stats.ruc) },
 	{ "rx_align_errors", E1000_STAT(stats.algnerrc) },
@@ -90,24 +105,44 @@
 	{ "tx_flow_control_xoff", E1000_STAT(stats.xofftxc) },
 	{ "rx_long_byte_count", E1000_STAT(stats.gorcl) },
 	{ "rx_csum_offload_good", E1000_STAT(hw_csum_good) },
-	{ "rx_csum_offload_errors", E1000_STAT(hw_csum_err) }
+	{ "rx_csum_offload_errors", E1000_STAT(hw_csum_err) },
+	{ "rx_header_split", E1000_STAT(rx_hdr_split) },
+	{ "alloc_rx_buff_failed", E1000_STAT(alloc_rx_buff_failed) },
+	{ "tx_smbus", E1000_STAT(stats.mgptc) },
+	{ "rx_smbus", E1000_STAT(stats.mgprc) },
+	{ "dropped_smbus", E1000_STAT(stats.mgpdc) },
 };
-#define E1000_STATS_LEN	\
+
+#ifdef CONFIG_E1000_MQ
+#define E1000_QUEUE_STATS_LEN \
+	((((((struct e1000_adapter *)netdev->priv)->num_rx_queues > 1) ? \
+	  ((struct e1000_adapter *)netdev->priv)->num_rx_queues : 0 ) + \
+	 (((((struct e1000_adapter *)netdev->priv)->num_tx_queues > 1) ? \
+	  ((struct e1000_adapter *)netdev->priv)->num_tx_queues : 0 ))) * \
+	(sizeof(struct e1000_queue_stats) / sizeof(u64)))
+#else
+#define E1000_QUEUE_STATS_LEN 0
+#endif
+#define E1000_GLOBAL_STATS_LEN	\
 	sizeof(e1000_gstrings_stats) / sizeof(struct e1000_stats)
+#define E1000_STATS_LEN (E1000_GLOBAL_STATS_LEN + E1000_QUEUE_STATS_LEN)
+#endif /* ETHTOOL_GSTATS */
+#ifdef ETHTOOL_TEST
 static const char e1000_gstrings_test[][ETH_GSTRING_LEN] = {
 	"Register test  (offline)", "Eeprom test    (offline)",
 	"Interrupt test (offline)", "Loopback test  (offline)",
 	"Link test   (on/offline)"
 };
 #define E1000_TEST_LEN sizeof(e1000_gstrings_test) / ETH_GSTRING_LEN
+#endif /* ETHTOOL_TEST */
 
-static int
-e1000_get_settings(struct net_device *netdev, struct ethtool_cmd *ecmd)
+static int e1000_get_settings(struct net_device *netdev,
+                              struct ethtool_cmd *ecmd)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
 
-	if(hw->media_type == e1000_media_type_copper) {
+	if (hw->media_type == e1000_media_type_copper) {
 
 		ecmd->supported = (SUPPORTED_10baseT_Half |
 		                   SUPPORTED_10baseT_Full |
@@ -116,21 +151,20 @@
 		                   SUPPORTED_1000baseT_Full|
 		                   SUPPORTED_Autoneg |
 		                   SUPPORTED_TP);
-
+		if (hw->phy.type == e1000_phy_ife)
+			ecmd->supported &= ~SUPPORTED_1000baseT_Full;
 		ecmd->advertising = ADVERTISED_TP;
 
-		if(hw->autoneg == 1) {
+		if (hw->mac.autoneg == 1) {
 			ecmd->advertising |= ADVERTISED_Autoneg;
-
 			/* the e1000 autoneg seems to match ethtool nicely */
-
-			ecmd->advertising |= hw->autoneg_advertised;
+			ecmd->advertising |= hw->phy.autoneg_advertised;
 		}
 
 		ecmd->port = PORT_TP;
-		ecmd->phy_address = hw->phy_addr;
+		ecmd->phy_address = hw->phy.addr;
 
-		if(hw->mac_type == e1000_82543)
+		if (hw->mac.type == e1000_82543)
 			ecmd->transceiver = XCVR_EXTERNAL;
 		else
 			ecmd->transceiver = XCVR_INTERNAL;
@@ -140,28 +174,28 @@
 				     SUPPORTED_FIBRE |
 				     SUPPORTED_Autoneg);
 
-		ecmd->advertising = (SUPPORTED_1000baseT_Full |
-				     SUPPORTED_FIBRE |
-				     SUPPORTED_Autoneg);
+		ecmd->advertising = (ADVERTISED_1000baseT_Full |
+				     ADVERTISED_FIBRE |
+				     ADVERTISED_Autoneg);
 
 		ecmd->port = PORT_FIBRE;
 
-		if(hw->mac_type >= e1000_82545)
+		if (hw->mac.type >= e1000_82545)
 			ecmd->transceiver = XCVR_INTERNAL;
 		else
 			ecmd->transceiver = XCVR_EXTERNAL;
 	}
 
-	if(netif_carrier_ok(adapter->netdev)) {
+	if (E1000_READ_REG(&adapter->hw, E1000_STATUS) & E1000_STATUS_LU) {
 
 		e1000_get_speed_and_duplex(hw, &adapter->link_speed,
 		                                   &adapter->link_duplex);
 		ecmd->speed = adapter->link_speed;
 
-		/* unfortunatly FULL_DUPLEX != DUPLEX_FULL
+		/* unfortunately FULL_DUPLEX != DUPLEX_FULL
 		 *          and HALF_DUPLEX != DUPLEX_HALF */
 
-		if(adapter->link_duplex == FULL_DUPLEX)
+		if (adapter->link_duplex == FULL_DUPLEX)
 			ecmd->duplex = DUPLEX_FULL;
 		else
 			ecmd->duplex = DUPLEX_HALF;
@@ -171,122 +205,144 @@
 	}
 
 	ecmd->autoneg = ((hw->media_type == e1000_media_type_fiber) ||
-			 hw->autoneg) ? AUTONEG_ENABLE : AUTONEG_DISABLE;
+			 hw->mac.autoneg) ? AUTONEG_ENABLE : AUTONEG_DISABLE;
 	return 0;
 }
 
-static int
-e1000_set_settings(struct net_device *netdev, struct ethtool_cmd *ecmd)
+static int e1000_set_settings(struct net_device *netdev,
+                              struct ethtool_cmd *ecmd)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
 
-	if(ecmd->autoneg == AUTONEG_ENABLE) {
-		hw->autoneg = 1;
-		hw->autoneg_advertised = 0x002F;
-		ecmd->advertising = 0x002F;
-	} else
-		if(e1000_set_spd_dplx(adapter, ecmd->speed + ecmd->duplex))
+	/* When SoL/IDER sessions are active, autoneg/speed/duplex
+	 * cannot be changed */
+	if (e1000_check_reset_block(hw)) {
+		DPRINTK(DRV, ERR, "Cannot change link characteristics "
+		        "when SoL/IDER is active.\n");
+		return -EINVAL;
+	}
+
+	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
+		msleep(1);
+
+	if (ecmd->autoneg == AUTONEG_ENABLE) {
+		hw->mac.autoneg = 1;
+		if (hw->media_type == e1000_media_type_fiber)
+			hw->phy.autoneg_advertised = ADVERTISED_1000baseT_Full |
+			                             ADVERTISED_FIBRE |
+			                             ADVERTISED_Autoneg;
+		else
+			hw->phy.autoneg_advertised = ecmd->advertising |
+			                             ADVERTISED_TP |
+			                             ADVERTISED_Autoneg;
+		ecmd->advertising = hw->phy.autoneg_advertised;
+	} else {
+		if (e1000_set_spd_dplx(adapter, ecmd->speed + ecmd->duplex)) {
+			clear_bit(__E1000_RESETTING, &adapter->state);
 			return -EINVAL;
+		}
+	}
 
 	/* reset the link */
 
-	if(netif_running(adapter->netdev)) {
+	if (netif_running(adapter->netdev)) {
 		e1000_down(adapter);
-		e1000_reset(adapter);
 		e1000_up(adapter);
-	} else
+	} else {
 		e1000_reset(adapter);
+	}
 
+	clear_bit(__E1000_RESETTING, &adapter->state);
 	return 0;
 }
 
-static void
-e1000_get_pauseparam(struct net_device *netdev,
-                     struct ethtool_pauseparam *pause)
+static void e1000_get_pauseparam(struct net_device *netdev,
+                                 struct ethtool_pauseparam *pause)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
 
-	pause->autoneg = 
+	pause->autoneg =
 		(adapter->fc_autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE);
-	
-	if(hw->fc == e1000_fc_rx_pause)
+
+	if (hw->mac.fc == e1000_fc_rx_pause)
 		pause->rx_pause = 1;
-	else if(hw->fc == e1000_fc_tx_pause)
+	else if (hw->mac.fc == e1000_fc_tx_pause)
 		pause->tx_pause = 1;
-	else if(hw->fc == e1000_fc_full) {
+	else if (hw->mac.fc == e1000_fc_full) {
 		pause->rx_pause = 1;
 		pause->tx_pause = 1;
 	}
 }
 
-static int
-e1000_set_pauseparam(struct net_device *netdev,
-                     struct ethtool_pauseparam *pause)
+static int e1000_set_pauseparam(struct net_device *netdev,
+                                struct ethtool_pauseparam *pause)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
-	
+	int retval = 0;
+
 	adapter->fc_autoneg = pause->autoneg;
 
-	if(pause->rx_pause && pause->tx_pause)
-		hw->fc = e1000_fc_full;
-	else if(pause->rx_pause && !pause->tx_pause)
-		hw->fc = e1000_fc_rx_pause;
-	else if(!pause->rx_pause && pause->tx_pause)
-		hw->fc = e1000_fc_tx_pause;
-	else if(!pause->rx_pause && !pause->tx_pause)
-		hw->fc = e1000_fc_none;
+	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
+		msleep(1);
 
-	hw->original_fc = hw->fc;
+	if (pause->rx_pause && pause->tx_pause)
+		hw->mac.fc = e1000_fc_full;
+	else if (pause->rx_pause && !pause->tx_pause)
+		hw->mac.fc = e1000_fc_rx_pause;
+	else if (!pause->rx_pause && pause->tx_pause)
+		hw->mac.fc = e1000_fc_tx_pause;
+	else if (!pause->rx_pause && !pause->tx_pause)
+		hw->mac.fc = e1000_fc_none;
 
-	if(adapter->fc_autoneg == AUTONEG_ENABLE) {
-		if(netif_running(adapter->netdev)) {
+	hw->mac.original_fc = hw->mac.fc;
+
+	if (adapter->fc_autoneg == AUTONEG_ENABLE) {
+		if (netif_running(adapter->netdev)) {
 			e1000_down(adapter);
 			e1000_up(adapter);
-		} else
+		} else {
 			e1000_reset(adapter);
+		}
+	} else {
+		retval = ((hw->media_type == e1000_media_type_fiber) ?
+			  e1000_setup_link(hw) : e1000_force_mac_fc(hw));
 	}
-	else
-		return e1000_force_mac_fc(hw);
-	
-	return 0;
+
+	clear_bit(__E1000_RESETTING, &adapter->state);
+	return retval;
 }
 
-static uint32_t
-e1000_get_rx_csum(struct net_device *netdev)
+static u32 e1000_get_rx_csum(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	return adapter->rx_csum;
 }
 
-static int
-e1000_set_rx_csum(struct net_device *netdev, uint32_t data)
+static int e1000_set_rx_csum(struct net_device *netdev, u32 data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	adapter->rx_csum = data;
 
-	if(netif_running(netdev)) {
-		e1000_down(adapter);
-		e1000_up(adapter);
-	} else
+	if (netif_running(netdev))
+		e1000_reinit_locked(adapter);
+	else
 		e1000_reset(adapter);
 	return 0;
 }
-	
-static uint32_t
-e1000_get_tx_csum(struct net_device *netdev)
+
+static u32 e1000_get_tx_csum(struct net_device *netdev)
 {
 	return (netdev->features & NETIF_F_HW_CSUM) != 0;
 }
 
-static int
-e1000_set_tx_csum(struct net_device *netdev, uint32_t data)
+static int e1000_set_tx_csum(struct net_device *netdev, u32 data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	if(adapter->hw.mac_type < e1000_82543) {
+	if (adapter->hw.mac.type < e1000_82543) {
 		if (!data)
 			return -EINVAL;
 		return 0;
@@ -301,115 +357,121 @@
 }
 
 #ifdef NETIF_F_TSO
-static int
-e1000_set_tso(struct net_device *netdev, uint32_t data)
+static int e1000_set_tso(struct net_device *netdev, u32 data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	if ((adapter->hw.mac_type < e1000_82544) ||
-	    (adapter->hw.mac_type == e1000_82547)) 
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	if (!adapter->flags.has_tso)
 		return data ? -EINVAL : 0;
 
 	if (data)
 		netdev->features |= NETIF_F_TSO;
 	else
 		netdev->features &= ~NETIF_F_TSO;
+
+#ifdef NETIF_F_TSO6
+	if (adapter->flags.has_tso6) {
+		if (data)
+			netdev->features |= NETIF_F_TSO6;
+		else
+			netdev->features &= ~NETIF_F_TSO6;
+	}
+#endif
+
+	DPRINTK(PROBE, INFO, "TSO is %s\n", data ? "Enabled" : "Disabled");
+	adapter->flags.tso_force = 1;
 	return 0;
-} 
+}
 #endif /* NETIF_F_TSO */
 
-static uint32_t
-e1000_get_msglevel(struct net_device *netdev)
+static u32 e1000_get_msglevel(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	return adapter->msg_enable;
 }
 
-static void
-e1000_set_msglevel(struct net_device *netdev, uint32_t data)
+static void e1000_set_msglevel(struct net_device *netdev, u32 data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	adapter->msg_enable = data;
 }
 
-static int 
-e1000_get_regs_len(struct net_device *netdev)
+static int e1000_get_regs_len(struct net_device *netdev)
 {
 #define E1000_REGS_LEN 32
-	return E1000_REGS_LEN * sizeof(uint32_t);
+	return E1000_REGS_LEN * sizeof(u32);
 }
 
-static void
-e1000_get_regs(struct net_device *netdev,
-	       struct ethtool_regs *regs, void *p)
+static void e1000_get_regs(struct net_device *netdev,
+                           struct ethtool_regs *regs, void *p)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
-	uint32_t *regs_buff = p;
-	uint16_t phy_data;
+	u32 *regs_buff = p;
+	u16 phy_data;
 
-	memset(p, 0, E1000_REGS_LEN * sizeof(uint32_t));
+	memset(p, 0, E1000_REGS_LEN * sizeof(u32));
 
 	regs->version = (1 << 24) | (hw->revision_id << 16) | hw->device_id;
 
-	regs_buff[0]  = E1000_READ_REG(hw, CTRL);
-	regs_buff[1]  = E1000_READ_REG(hw, STATUS);
+	regs_buff[0]  = E1000_READ_REG(hw, E1000_CTRL);
+	regs_buff[1]  = E1000_READ_REG(hw, E1000_STATUS);
 
-	regs_buff[2]  = E1000_READ_REG(hw, RCTL);
-	regs_buff[3]  = E1000_READ_REG(hw, RDLEN);
-	regs_buff[4]  = E1000_READ_REG(hw, RDH);
-	regs_buff[5]  = E1000_READ_REG(hw, RDT);
-	regs_buff[6]  = E1000_READ_REG(hw, RDTR);
-
-	regs_buff[7]  = E1000_READ_REG(hw, TCTL);
-	regs_buff[8]  = E1000_READ_REG(hw, TDLEN);
-	regs_buff[9]  = E1000_READ_REG(hw, TDH);
-	regs_buff[10] = E1000_READ_REG(hw, TDT);
-	regs_buff[11] = E1000_READ_REG(hw, TIDV);
+	regs_buff[2]  = E1000_READ_REG(hw, E1000_RCTL);
+	regs_buff[3]  = E1000_READ_REG(hw, E1000_RDLEN);
+	regs_buff[4]  = E1000_READ_REG(hw, E1000_RDH);
+	regs_buff[5]  = E1000_READ_REG(hw, E1000_RDT);
+	regs_buff[6]  = E1000_READ_REG(hw, E1000_RDTR);
+
+	regs_buff[7]  = E1000_READ_REG(hw, E1000_TCTL);
+	regs_buff[8]  = E1000_READ_REG(hw, E1000_TDLEN);
+	regs_buff[9]  = E1000_READ_REG(hw, E1000_TDH);
+	regs_buff[10] = E1000_READ_REG(hw, E1000_TDT);
+	regs_buff[11] = E1000_READ_REG(hw, E1000_TIDV);
 
-	regs_buff[12] = adapter->hw.phy_type;  /* PHY type (IGP=1, M88=0) */
-	if(hw->phy_type == e1000_phy_igp) {
+	regs_buff[12] = adapter->hw.phy.type;  /* PHY type (IGP=1, M88=0) */
+	if (hw->phy.type == e1000_phy_igp) {
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT,
 				    IGP01E1000_PHY_AGC_A);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_AGC_A &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[13] = (uint32_t)phy_data; /* cable length */
+		regs_buff[13] = (u32)phy_data; /* cable length */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT,
 				    IGP01E1000_PHY_AGC_B);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_AGC_B &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[14] = (uint32_t)phy_data; /* cable length */
+		regs_buff[14] = (u32)phy_data; /* cable length */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT,
 				    IGP01E1000_PHY_AGC_C);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_AGC_C &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[15] = (uint32_t)phy_data; /* cable length */
+		regs_buff[15] = (u32)phy_data; /* cable length */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT,
 				    IGP01E1000_PHY_AGC_D);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_AGC_D &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[16] = (uint32_t)phy_data; /* cable length */
+		regs_buff[16] = (u32)phy_data; /* cable length */
 		regs_buff[17] = 0; /* extended 10bt distance (not needed) */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT, 0x0);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[18] = (uint32_t)phy_data; /* cable polarity */
+		regs_buff[18] = (u32)phy_data; /* cable polarity */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT,
 				    IGP01E1000_PHY_PCS_INIT_REG);
 		e1000_read_phy_reg(hw, IGP01E1000_PHY_PCS_INIT_REG &
 				   IGP01E1000_PHY_PAGE_SELECT, &phy_data);
-		regs_buff[19] = (uint32_t)phy_data; /* cable polarity */
+		regs_buff[19] = (u32)phy_data; /* cable polarity */
 		regs_buff[20] = 0; /* polarity correction enabled (always) */
 		regs_buff[22] = 0; /* phy receive errors (unavailable) */
 		regs_buff[23] = regs_buff[18]; /* mdix mode */
 		e1000_write_phy_reg(hw, IGP01E1000_PHY_PAGE_SELECT, 0x0);
 	} else {
-        	e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
-		regs_buff[13] = (uint32_t)phy_data; /* cable length */
+		e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
+		regs_buff[13] = (u32)phy_data; /* cable length */
 		regs_buff[14] = 0;  /* Dummy (to align w/ IGP phy reg dump) */
 		regs_buff[15] = 0;  /* Dummy (to align w/ IGP phy reg dump) */
 		regs_buff[16] = 0;  /* Dummy (to align w/ IGP phy reg dump) */
-        	e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
-		regs_buff[17] = (uint32_t)phy_data; /* extended 10bt distance */
+		e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+		regs_buff[17] = (u32)phy_data; /* extended 10bt distance */
 		regs_buff[18] = regs_buff[13]; /* cable polarity */
 		regs_buff[19] = 0;  /* Dummy (to align w/ IGP phy reg dump) */
 		regs_buff[20] = regs_buff[17]; /* polarity correction */
@@ -419,33 +481,32 @@
 	}
 	regs_buff[21] = adapter->phy_stats.idle_errors;  /* phy idle errors */
 	e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_data);
-	regs_buff[24] = (uint32_t)phy_data;  /* phy local receiver status */
+	regs_buff[24] = (u32)phy_data;  /* phy local receiver status */
 	regs_buff[25] = regs_buff[24];  /* phy remote receiver status */
-	if(hw->mac_type >= e1000_82540 &&
-	   hw->media_type == e1000_media_type_copper) {
-		regs_buff[26] = E1000_READ_REG(hw, MANC);
+	if (hw->mac.type >= e1000_82540 &&
+	    hw->mac.type < e1000_82571 &&
+	    hw->media_type == e1000_media_type_copper) {
+		regs_buff[26] = E1000_READ_REG(hw, E1000_MANC);
 	}
 }
 
-static int
-e1000_get_eeprom_len(struct net_device *netdev)
+static int e1000_get_eeprom_len(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	return adapter->hw.eeprom.word_size * 2;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	return adapter->hw.nvm.word_size * 2;
 }
 
-static int
-e1000_get_eeprom(struct net_device *netdev,
-                      struct ethtool_eeprom *eeprom, uint8_t *bytes)
+static int e1000_get_eeprom(struct net_device *netdev,
+                            struct ethtool_eeprom *eeprom, u8 *bytes)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
-	uint16_t *eeprom_buff;
+	u16 *eeprom_buff;
 	int first_word, last_word;
 	int ret_val = 0;
-	uint16_t i;
+	u16 i;
 
-	if(eeprom->len == 0)
+	if (eeprom->len == 0)
 		return -EINVAL;
 
 	eeprom->magic = hw->vendor_id | (hw->device_id << 16);
@@ -453,19 +514,19 @@
 	first_word = eeprom->offset >> 1;
 	last_word = (eeprom->offset + eeprom->len - 1) >> 1;
 
-	eeprom_buff = kmalloc(sizeof(uint16_t) *
+	eeprom_buff = kmalloc(sizeof(u16) *
 			(last_word - first_word + 1), GFP_KERNEL);
-	if(!eeprom_buff)
+	if (!eeprom_buff)
 		return -ENOMEM;
 
-	if(hw->eeprom.type == e1000_eeprom_spi)
-		ret_val = e1000_read_eeprom(hw, first_word,
-					    last_word - first_word + 1,
-					    eeprom_buff);
+	if (hw->nvm.type == e1000_nvm_eeprom_spi)
+		ret_val = e1000_read_nvm(hw, first_word,
+		                         last_word - first_word + 1,
+		                         eeprom_buff);
 	else {
 		for (i = 0; i < last_word - first_word + 1; i++)
-			if((ret_val = e1000_read_eeprom(hw, first_word + i, 1,
-							&eeprom_buff[i])))
+			if ((ret_val = e1000_read_nvm(hw, first_word + i, 1,
+			                              &eeprom_buff[i])))
 				break;
 	}
 
@@ -473,51 +534,50 @@
 	for (i = 0; i < last_word - first_word + 1; i++)
 		le16_to_cpus(&eeprom_buff[i]);
 
-	memcpy(bytes, (uint8_t *)eeprom_buff + (eeprom->offset & 1),
+	memcpy(bytes, (u8 *)eeprom_buff + (eeprom->offset & 1),
 			eeprom->len);
 	kfree(eeprom_buff);
 
 	return ret_val;
 }
 
-static int
-e1000_set_eeprom(struct net_device *netdev,
-                      struct ethtool_eeprom *eeprom, uint8_t *bytes)
+static int e1000_set_eeprom(struct net_device *netdev,
+                            struct ethtool_eeprom *eeprom, u8 *bytes)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
-	uint16_t *eeprom_buff;
+	u16 *eeprom_buff;
 	void *ptr;
 	int max_len, first_word, last_word, ret_val = 0;
-	uint16_t i;
+	u16 i;
 
-	if(eeprom->len == 0)
+	if (eeprom->len == 0)
 		return -EOPNOTSUPP;
 
-	if(eeprom->magic != (hw->vendor_id | (hw->device_id << 16)))
+	if (eeprom->magic != (hw->vendor_id | (hw->device_id << 16)))
 		return -EFAULT;
 
-	max_len = hw->eeprom.word_size * 2;
+	max_len = hw->nvm.word_size * 2;
 
 	first_word = eeprom->offset >> 1;
 	last_word = (eeprom->offset + eeprom->len - 1) >> 1;
 	eeprom_buff = kmalloc(max_len, GFP_KERNEL);
-	if(!eeprom_buff)
+	if (!eeprom_buff)
 		return -ENOMEM;
 
 	ptr = (void *)eeprom_buff;
 
-	if(eeprom->offset & 1) {
+	if (eeprom->offset & 1) {
 		/* need read/modify/write of first changed EEPROM word */
 		/* only the second byte of the word is being modified */
-		ret_val = e1000_read_eeprom(hw, first_word, 1,
+		ret_val = e1000_read_nvm(hw, first_word, 1,
 					    &eeprom_buff[0]);
 		ptr++;
 	}
-	if(((eeprom->offset + eeprom->len) & 1) && (ret_val == 0)) {
+	if (((eeprom->offset + eeprom->len) & 1) && (ret_val == 0)) {
 		/* need read/modify/write of last changed EEPROM word */
 		/* only the first byte of the word is being modified */
-		ret_val = e1000_read_eeprom(hw, last_word, 1,
+		ret_val = e1000_read_nvm(hw, last_word, 1,
 		                  &eeprom_buff[last_word - first_word]);
 	}
 
@@ -530,26 +590,49 @@
 	for (i = 0; i < last_word - first_word + 1; i++)
 		eeprom_buff[i] = cpu_to_le16(eeprom_buff[i]);
 
-	ret_val = e1000_write_eeprom(hw, first_word,
-				     last_word - first_word + 1, eeprom_buff);
+	ret_val = e1000_write_nvm(hw, first_word,
+	                          last_word - first_word + 1, eeprom_buff);
 
-	/* Update the checksum over the first part of the EEPROM if needed */
-	if((ret_val == 0) && first_word <= EEPROM_CHECKSUM_REG)
-		e1000_update_eeprom_checksum(hw);
+	/* Update the checksum over the first part of the EEPROM if needed
+	 * and flush shadow RAM for 82573 controllers */
+	if ((ret_val == 0) && ((first_word <= NVM_CHECKSUM_REG) ||
+				(hw->mac.type == e1000_82573)))
+		e1000_update_nvm_checksum(hw);
 
 	kfree(eeprom_buff);
 	return ret_val;
 }
 
-static void
-e1000_get_drvinfo(struct net_device *netdev,
-                       struct ethtool_drvinfo *drvinfo)
+static void e1000_get_drvinfo(struct net_device *netdev,
+                              struct ethtool_drvinfo *drvinfo)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	char firmware_version[32];
+	u16 eeprom_data;
 
 	strncpy(drvinfo->driver,  e1000_driver_name, 32);
 	strncpy(drvinfo->version, e1000_driver_version, 32);
-	strncpy(drvinfo->fw_version, "N/A", 32);
+
+	/* EEPROM image version # is reported as firmware version # for
+	 * 8257{1|2|3} controllers */
+	e1000_read_nvm(&adapter->hw, 5, 1, &eeprom_data);
+	switch (adapter->hw.mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_82573:
+	case e1000_80003es2lan:
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		sprintf(firmware_version, "%d.%d-%d",
+			(eeprom_data & 0xF000) >> 12,
+			(eeprom_data & 0x0FF0) >> 4,
+			eeprom_data & 0x000F);
+		break;
+	default:
+		sprintf(firmware_version, "N/A");
+	}
+
+	strncpy(drvinfo->fw_version, firmware_version, 32);
 	strncpy(drvinfo->bus_info, pci_name(adapter->pdev), 32);
 	drvinfo->n_stats = E1000_STATS_LEN;
 	drvinfo->testinfo_len = E1000_TEST_LEN;
@@ -557,14 +640,13 @@
 	drvinfo->eedump_len = e1000_get_eeprom_len(netdev);
 }
 
-static void
-e1000_get_ringparam(struct net_device *netdev,
-                    struct ethtool_ringparam *ring)
-{
-	struct e1000_adapter *adapter = netdev->priv;
-	e1000_mac_type mac_type = adapter->hw.mac_type;
-	struct e1000_desc_ring *txdr = &adapter->tx_ring;
-	struct e1000_desc_ring *rxdr = &adapter->rx_ring;
+static void e1000_get_ringparam(struct net_device *netdev,
+                                struct ethtool_ringparam *ring)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	e1000_mac_type mac_type = adapter->hw.mac.type;
+	struct e1000_tx_ring *tx_ring = adapter->tx_ring;
+	struct e1000_rx_ring *rx_ring = adapter->rx_ring;
 
 	ring->rx_max_pending = (mac_type < e1000_82544) ? E1000_MAX_RXD :
 		E1000_MAX_82544_RXD;
@@ -572,83 +654,121 @@
 		E1000_MAX_82544_TXD;
 	ring->rx_mini_max_pending = 0;
 	ring->rx_jumbo_max_pending = 0;
-	ring->rx_pending = rxdr->count;
-	ring->tx_pending = txdr->count;
+	ring->rx_pending = rx_ring->count;
+	ring->tx_pending = tx_ring->count;
 	ring->rx_mini_pending = 0;
 	ring->rx_jumbo_pending = 0;
 }
 
-static int 
-e1000_set_ringparam(struct net_device *netdev,
-                    struct ethtool_ringparam *ring)
-{
-	struct e1000_adapter *adapter = netdev->priv;
-	e1000_mac_type mac_type = adapter->hw.mac_type;
-	struct e1000_desc_ring *txdr = &adapter->tx_ring;
-	struct e1000_desc_ring *rxdr = &adapter->rx_ring;
-	struct e1000_desc_ring tx_old, tx_new, rx_old, rx_new;
-	int err;
+static int e1000_set_ringparam(struct net_device *netdev,
+                               struct ethtool_ringparam *ring)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	e1000_mac_type mac_type = adapter->hw.mac.type;
+	struct e1000_tx_ring *tx_ring, *tx_old;
+	struct e1000_rx_ring *rx_ring, *rx_old;
+	int i, err, tx_ring_size, rx_ring_size;
+
+	if ((ring->rx_mini_pending) || (ring->rx_jumbo_pending))
+		return -EINVAL;
+
+	tx_ring_size = sizeof(struct e1000_tx_ring) * adapter->num_tx_queues;
+	rx_ring_size = sizeof(struct e1000_rx_ring) * adapter->num_rx_queues;
+
+	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
+		msleep(1);
+
+	if (netif_running(adapter->netdev))
+		e1000_down(adapter);
 
 	tx_old = adapter->tx_ring;
 	rx_old = adapter->rx_ring;
 
-	if(netif_running(adapter->netdev))
-		e1000_down(adapter);
+	err = -ENOMEM;
+	tx_ring = kzalloc(tx_ring_size, GFP_KERNEL);
+	if (!tx_ring)
+		goto err_alloc_tx;
+
+	rx_ring = kzalloc(rx_ring_size, GFP_KERNEL);
+	if (!rx_ring)
+		goto err_alloc_rx;
 
-	rxdr->count = max(ring->rx_pending,(uint32_t)E1000_MIN_RXD);
-	rxdr->count = min(rxdr->count,(uint32_t)(mac_type < e1000_82544 ?
+	adapter->tx_ring = tx_ring;
+	adapter->rx_ring = rx_ring;
+
+	rx_ring->count = max(ring->rx_pending,(u32)E1000_MIN_RXD);
+	rx_ring->count = min(rx_ring->count,(u32)(mac_type < e1000_82544 ?
 		E1000_MAX_RXD : E1000_MAX_82544_RXD));
-	E1000_ROUNDUP(rxdr->count, REQ_RX_DESCRIPTOR_MULTIPLE); 
+	rx_ring->count = ALIGN(rx_ring->count, REQ_RX_DESCRIPTOR_MULTIPLE);
 
-	txdr->count = max(ring->tx_pending,(uint32_t)E1000_MIN_TXD);
-	txdr->count = min(txdr->count,(uint32_t)(mac_type < e1000_82544 ?
+	tx_ring->count = max(ring->tx_pending,(u32)E1000_MIN_TXD);
+	tx_ring->count = min(tx_ring->count,(u32)(mac_type < e1000_82544 ?
 		E1000_MAX_TXD : E1000_MAX_82544_TXD));
-	E1000_ROUNDUP(txdr->count, REQ_TX_DESCRIPTOR_MULTIPLE); 
+	tx_ring->count = ALIGN(tx_ring->count, REQ_TX_DESCRIPTOR_MULTIPLE);
+
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		tx_ring[i].count = tx_ring->count;
+#ifdef CONFIG_E1000_MQ
+		spin_lock_init(&adapter->tx_ring[i].tx_queue_lock);
+#endif
+	}
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		rx_ring[i].count = rx_ring->count;
 
-	if(netif_running(adapter->netdev)) {
+	if (netif_running(adapter->netdev)) {
 		/* Try to get new resources before deleting old */
-		if((err = e1000_setup_rx_resources(adapter)))
+		if ((err = e1000_setup_all_rx_resources(adapter)))
 			goto err_setup_rx;
-		if((err = e1000_setup_tx_resources(adapter)))
+		if ((err = e1000_setup_all_tx_resources(adapter)))
 			goto err_setup_tx;
 
 		/* save the new, restore the old in order to free it,
 		 * then restore the new back again */
 
-		rx_new = adapter->rx_ring;
-		tx_new = adapter->tx_ring;
 		adapter->rx_ring = rx_old;
 		adapter->tx_ring = tx_old;
-		e1000_free_rx_resources(adapter);
-		e1000_free_tx_resources(adapter);
-		adapter->rx_ring = rx_new;
-		adapter->tx_ring = tx_new;
-		if((err = e1000_up(adapter)))
-			return err;
+		e1000_free_all_rx_resources(adapter);
+		e1000_free_all_tx_resources(adapter);
+		kfree(tx_old);
+		kfree(rx_old);
+		adapter->rx_ring = rx_ring;
+		adapter->tx_ring = tx_ring;
+		if ((err = e1000_up(adapter)))
+			goto err_setup;
 	}
 
+	clear_bit(__E1000_RESETTING, &adapter->state);
 	return 0;
 err_setup_tx:
-	e1000_free_rx_resources(adapter);
+	e1000_free_all_rx_resources(adapter);
 err_setup_rx:
 	adapter->rx_ring = rx_old;
 	adapter->tx_ring = tx_old;
+	kfree(rx_ring);
+err_alloc_rx:
+	kfree(tx_ring);
+err_alloc_tx:
 	e1000_up(adapter);
+err_setup:
+	clear_bit(__E1000_RESETTING, &adapter->state);
 	return err;
 }
 
-
-#define REG_PATTERN_TEST(R, M, W)                                              \
+#define REG_PATTERN_TEST(R, M, W) REG_PATTERN_TEST_ARRAY(R, 0, M, W)
+#define REG_PATTERN_TEST_ARRAY(reg, offset, mask, writeable)                   \
 {                                                                              \
-	uint32_t pat, value;                                                   \
-	uint32_t test[] =                                                      \
-		{0x5A5A5A5A, 0xA5A5A5A5, 0x00000000, 0xFFFFFFFF};              \
-	for(pat = 0; pat < sizeof(test)/sizeof(test[0]); pat++) {              \
-		E1000_WRITE_REG(&adapter->hw, R, (test[pat] & W));             \
-		value = E1000_READ_REG(&adapter->hw, R);                       \
-		if(value != (test[pat] & W & M)) {                             \
-			*data = (adapter->hw.mac_type < e1000_82543) ?         \
-				E1000_82542_##R : E1000_##R;                   \
+	u32 pat, value;                                                        \
+	u32 test[] = {0x5A5A5A5A, 0xA5A5A5A5, 0x00000000, 0xFFFFFFFF};         \
+	for (pat = 0; pat < ARRAY_SIZE(test); pat++) {                         \
+		E1000_WRITE_REG_ARRAY(&adapter->hw, reg, offset,               \
+		                      (test[pat] & writeable));                \
+		value = E1000_READ_REG_ARRAY(&adapter->hw, reg, offset);       \
+		if (value != (test[pat] & writeable & mask)) {                 \
+			DPRINTK(DRV, ERR, "pattern test reg %04X failed: got " \
+			        "0x%08X expected 0x%08X\n",                    \
+			        E1000_REGISTER(&adapter->hw, reg) + offset,    \
+			        value, (test[pat] & writeable & mask));        \
+			*data = E1000_REGISTER(&adapter->hw, reg);             \
 			return 1;                                              \
 		}                                                              \
 	}                                                                      \
@@ -656,93 +776,122 @@
 
 #define REG_SET_AND_CHECK(R, M, W)                                             \
 {                                                                              \
-	uint32_t value;                                                        \
+	u32 value;                                                             \
 	E1000_WRITE_REG(&adapter->hw, R, W & M);                               \
 	value = E1000_READ_REG(&adapter->hw, R);                               \
 	if ((W & M) != (value & M)) {                                          \
-		*data = (adapter->hw.mac_type < e1000_82543) ?                 \
-			E1000_82542_##R : E1000_##R;                           \
+		DPRINTK(DRV, ERR, "set/check reg %04X test failed: got 0x%08X "\
+		        "expected 0x%08X\n", R, (value & M), (W & M));         \
+		*data = E1000_REGISTER(&adapter->hw, R);                       \
 		return 1;                                                      \
 	}                                                                      \
 }
 
-static int
-e1000_reg_test(struct e1000_adapter *adapter, uint64_t *data)
+static int e1000_reg_test(struct e1000_adapter *adapter, u64 *data)
 {
-	uint32_t value;
-	uint32_t i;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+	u32 value, before, after;
+	u32 i, toggle;
 
 	/* The status register is Read Only, so a write should fail.
 	 * Some bits that get toggled are ignored.
 	 */
-	value = (E1000_READ_REG(&adapter->hw, STATUS) & (0xFFFFF833));
-	E1000_WRITE_REG(&adapter->hw, STATUS, (0xFFFFFFFF));
-	if(value != (E1000_READ_REG(&adapter->hw, STATUS) & (0xFFFFF833))) {
+	switch (mac->type) {
+	/* there are several bits on newer hardware that are r/w */
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_80003es2lan:
+		toggle = 0x7FFFF3FF;
+		break;
+	case e1000_82573:
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		toggle = 0x7FFFF033;
+		break;
+	default:
+		toggle = 0xFFFFF833;
+		break;
+	}
+
+	before = E1000_READ_REG(&adapter->hw, E1000_STATUS);
+	value = (E1000_READ_REG(&adapter->hw, E1000_STATUS) & toggle);
+	E1000_WRITE_REG(&adapter->hw, E1000_STATUS, toggle);
+	after = E1000_READ_REG(&adapter->hw, E1000_STATUS) & toggle;
+	if (value != after) {
+		DPRINTK(DRV, ERR, "failed STATUS register test got: "
+		        "0x%08X expected: 0x%08X\n", after, value);
 		*data = 1;
 		return 1;
 	}
+	/* restore previous status */
+	E1000_WRITE_REG(&adapter->hw, E1000_STATUS, before);
 
-	REG_PATTERN_TEST(FCAL, 0xFFFFFFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(FCAH, 0x0000FFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(FCT, 0x0000FFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(VET, 0x0000FFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(RDTR, 0x0000FFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(RDBAH, 0xFFFFFFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(RDLEN, 0x000FFF80, 0x000FFFFF);
-	REG_PATTERN_TEST(RDH, 0x0000FFFF, 0x0000FFFF);
-	REG_PATTERN_TEST(RDT, 0x0000FFFF, 0x0000FFFF);
-	REG_PATTERN_TEST(FCRTH, 0x0000FFF8, 0x0000FFF8);
-	REG_PATTERN_TEST(FCTTV, 0x0000FFFF, 0x0000FFFF);
-	REG_PATTERN_TEST(TIPG, 0x3FFFFFFF, 0x3FFFFFFF);
-	REG_PATTERN_TEST(TDBAH, 0xFFFFFFFF, 0xFFFFFFFF);
-	REG_PATTERN_TEST(TDLEN, 0x000FFF80, 0x000FFFFF);
-
-	REG_SET_AND_CHECK(RCTL, 0xFFFFFFFF, 0x00000000);
-	REG_SET_AND_CHECK(RCTL, 0x06DFB3FE, 0x003FFFFB);
-	REG_SET_AND_CHECK(TCTL, 0xFFFFFFFF, 0x00000000);
-
-	if(adapter->hw.mac_type >= e1000_82543) {
-
-		REG_SET_AND_CHECK(RCTL, 0x06DFB3FE, 0xFFFFFFFF);
-		REG_PATTERN_TEST(RDBAL, 0xFFFFFFF0, 0xFFFFFFFF);
-		REG_PATTERN_TEST(TXCW, 0xC000FFFF, 0x0000FFFF);
-		REG_PATTERN_TEST(TDBAL, 0xFFFFFFF0, 0xFFFFFFFF);
-		REG_PATTERN_TEST(TIDV, 0x0000FFFF, 0x0000FFFF);
-
-		for(i = 0; i < E1000_RAR_ENTRIES; i++) {
-			REG_PATTERN_TEST(RA + ((i << 1) << 2), 0xFFFFFFFF,
-					 0xFFFFFFFF);
-			REG_PATTERN_TEST(RA + (((i << 1) + 1) << 2), 0x8003FFFF,
-					 0xFFFFFFFF);
+	if ((mac->type != e1000_ich8lan) &&
+	    (mac->type != e1000_ich9lan)) {
+		REG_PATTERN_TEST(E1000_FCAL, 0xFFFFFFFF, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_FCAH, 0x0000FFFF, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_FCT, 0x0000FFFF, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_VET, 0x0000FFFF, 0xFFFFFFFF);
+	}
+
+	REG_PATTERN_TEST(E1000_RDTR, 0x0000FFFF, 0xFFFFFFFF);
+	REG_PATTERN_TEST(E1000_RDBAH, 0xFFFFFFFF, 0xFFFFFFFF);
+	REG_PATTERN_TEST(E1000_RDLEN, 0x000FFF80, 0x000FFFFF);
+	REG_PATTERN_TEST(E1000_RDH, 0x0000FFFF, 0x0000FFFF);
+	REG_PATTERN_TEST(E1000_RDT, 0x0000FFFF, 0x0000FFFF);
+	REG_PATTERN_TEST(E1000_FCRTH, 0x0000FFF8, 0x0000FFF8);
+	REG_PATTERN_TEST(E1000_FCTTV, 0x0000FFFF, 0x0000FFFF);
+	REG_PATTERN_TEST(E1000_TIPG, 0x3FFFFFFF, 0x3FFFFFFF);
+	REG_PATTERN_TEST(E1000_TDBAH, 0xFFFFFFFF, 0xFFFFFFFF);
+	REG_PATTERN_TEST(E1000_TDLEN, 0x000FFF80, 0x000FFFFF);
+
+	REG_SET_AND_CHECK(E1000_RCTL, 0xFFFFFFFF, 0x00000000);
+
+	before = (((mac->type == e1000_ich8lan) ||
+		   (mac->type == e1000_ich9lan)) ? 0x06C3B33E : 0x06DFB3FE);
+	REG_SET_AND_CHECK(E1000_RCTL, before, 0x003FFFFB);
+	REG_SET_AND_CHECK(E1000_TCTL, 0xFFFFFFFF, 0x00000000);
+
+	if (mac->type >= e1000_82543) {
+
+		REG_SET_AND_CHECK(E1000_RCTL, before, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_RDBAL, 0xFFFFFFF0, 0xFFFFFFFF);
+		if ((mac->type != e1000_ich8lan) &&
+		    (mac->type != e1000_ich9lan))
+			REG_PATTERN_TEST(E1000_TXCW, 0xC000FFFF, 0x0000FFFF);
+		REG_PATTERN_TEST(E1000_TDBAL, 0xFFFFFFF0, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_TIDV, 0x0000FFFF, 0x0000FFFF);
+		for (i = 0; i < mac->rar_entry_count; i++) {
+			REG_PATTERN_TEST_ARRAY(E1000_RA, ((i << 1) + 1),
+			                       0x8003FFFF, 0xFFFFFFFF);
 		}
 
 	} else {
 
-		REG_SET_AND_CHECK(RCTL, 0xFFFFFFFF, 0x01FFFFFF);
-		REG_PATTERN_TEST(RDBAL, 0xFFFFF000, 0xFFFFFFFF);
-		REG_PATTERN_TEST(TXCW, 0x0000FFFF, 0x0000FFFF);
-		REG_PATTERN_TEST(TDBAL, 0xFFFFF000, 0xFFFFFFFF);
+		REG_SET_AND_CHECK(E1000_RCTL, 0xFFFFFFFF, 0x01FFFFFF);
+		REG_PATTERN_TEST(E1000_RDBAL, 0xFFFFF000, 0xFFFFFFFF);
+		REG_PATTERN_TEST(E1000_TXCW, 0x0000FFFF, 0x0000FFFF);
+		REG_PATTERN_TEST(E1000_TDBAL, 0xFFFFF000, 0xFFFFFFFF);
 
 	}
 
-	for(i = 0; i < E1000_MC_TBL_SIZE; i++)
-		REG_PATTERN_TEST(MTA + (i << 2), 0xFFFFFFFF, 0xFFFFFFFF);
+	for (i = 0; i < mac->mta_reg_count; i++)
+		REG_PATTERN_TEST_ARRAY(E1000_MTA, i, 0xFFFFFFFF, 0xFFFFFFFF);
 
 	*data = 0;
 	return 0;
 }
 
-static int
-e1000_eeprom_test(struct e1000_adapter *adapter, uint64_t *data)
+static int e1000_eeprom_test(struct e1000_adapter *adapter, u64 *data)
 {
-	uint16_t temp;
-	uint16_t checksum = 0;
-	uint16_t i;
+	u16 temp;
+	u16 checksum = 0;
+	u16 i;
 
 	*data = 0;
 	/* Read and add up the contents of the EEPROM */
-	for(i = 0; i < (EEPROM_CHECKSUM_REG + 1); i++) {
-		if((e1000_read_eeprom(&adapter->hw, i, 1, &temp)) < 0) {
+	for (i = 0; i < (NVM_CHECKSUM_REG + 1); i++) {
+		if ((e1000_read_nvm(&adapter->hw, i, 1, &temp)) < 0) {
 			*data = 1;
 			break;
 		}
@@ -750,83 +899,73 @@
 	}
 
 	/* If Checksum is not Correct return error else test passed */
-	if((checksum != (uint16_t) EEPROM_SUM) && !(*data))
+	if ((checksum != (u16) NVM_SUM) && !(*data))
 		*data = 2;
 
 	return *data;
 }
 
-static irqreturn_t
-e1000_test_intr(int irq,
-		void *data,
-		struct pt_regs *regs)
+static irqreturn_t e1000_test_intr(int irq, void *data)
 {
 	struct net_device *netdev = (struct net_device *) data;
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	adapter->test_icr |= E1000_READ_REG(&adapter->hw, ICR);
+	adapter->test_icr |= E1000_READ_REG(&adapter->hw, E1000_ICR);
 
 	return IRQ_HANDLED;
 }
 
-static int
-e1000_intr_test(struct e1000_adapter *adapter, uint64_t *data)
+static int e1000_intr_test(struct e1000_adapter *adapter, u64 *data)
 {
 	struct net_device *netdev = adapter->netdev;
- 	uint32_t icr, mask, i=0, shared_int = TRUE;
- 	uint32_t irq = adapter->pdev->irq;
+	u32 mask, i=0, shared_int = TRUE;
+	u32 irq = adapter->pdev->irq;
 
 	*data = 0;
 
+	/* NOTE: we don't test MSI interrupts here, yet */
 	/* Hook up test interrupt handler just for this test */
- 	if(!request_irq(irq, &e1000_test_intr, 0, netdev->name, netdev)) {
- 		shared_int = FALSE;
- 	} else if(request_irq(irq, &e1000_test_intr, SA_SHIRQ, netdev->name, netdev)){
+	if (!request_irq(irq, &e1000_test_intr, IRQF_PROBE_SHARED, netdev->name,
+	                 netdev))
+		shared_int = FALSE;
+	else if (request_irq(irq, &e1000_test_intr, IRQF_SHARED,
+	         netdev->name, netdev)) {
 		*data = 1;
 		return -1;
 	}
+	DPRINTK(HW, INFO, "testing %s interrupt\n",
+	        (shared_int ? "shared" : "unshared"));
 
 	/* Disable all the interrupts */
-	E1000_WRITE_REG(&adapter->hw, IMC, 0xFFFFFFFF);
-	msec_delay(10);
-
-	/* Interrupts are disabled, so read interrupt cause
-	 * register (icr) twice to verify that there are no interrupts
-	 * pending.  icr is clear on read.
-	 */
-	icr = E1000_READ_REG(&adapter->hw, ICR);
-	icr = E1000_READ_REG(&adapter->hw, ICR);
-
-	if(icr != 0) {
-		/* if icr is non-zero, there is no point
-		 * running other interrupt tests.
-		 */
-		*data = 2;
-		i = 10;
-	}
+	E1000_WRITE_REG(&adapter->hw, E1000_IMC, 0xFFFFFFFF);
+	msleep(10);
 
 	/* Test each interrupt */
-	for(; i < 10; i++) {
+	for (; i < 10; i++) {
+
+		if (((adapter->hw.mac.type == e1000_ich8lan) ||
+		     (adapter->hw.mac.type == e1000_ich9lan)) && i == 8)
+			continue;
 
 		/* Interrupt to test */
 		mask = 1 << i;
 
- 		if(!shared_int) {
- 			/* Disable the interrupt to be reported in
- 			 * the cause register and then force the same
- 			 * interrupt and see if one gets posted.  If
- 			 * an interrupt was posted to the bus, the
- 			 * test failed.
- 			 */
- 			adapter->test_icr = 0;
- 			E1000_WRITE_REG(&adapter->hw, IMC, mask);
- 			E1000_WRITE_REG(&adapter->hw, ICS, mask);
- 			msec_delay(10);
- 
- 			if(adapter->test_icr & mask) {
- 				*data = 3;
- 				break;
- 			}
+		if (!shared_int) {
+			/* Disable the interrupt to be reported in
+			 * the cause register and then force the same
+			 * interrupt and see if one gets posted.  If
+			 * an interrupt was posted to the bus, the
+			 * test failed.
+			 */
+			adapter->test_icr = 0;
+			E1000_WRITE_REG(&adapter->hw, E1000_IMC, mask);
+			E1000_WRITE_REG(&adapter->hw, E1000_ICS, mask);
+			msleep(10);
+
+			if (adapter->test_icr & mask) {
+				*data = 3;
+				break;
+			}
 		}
 
 		/* Enable the interrupt to be reported in
@@ -836,16 +975,16 @@
 		 * test failed.
 		 */
 		adapter->test_icr = 0;
-		E1000_WRITE_REG(&adapter->hw, IMS, mask);
-		E1000_WRITE_REG(&adapter->hw, ICS, mask);
-		msec_delay(10);
+		E1000_WRITE_REG(&adapter->hw, E1000_IMS, mask);
+		E1000_WRITE_REG(&adapter->hw, E1000_ICS, mask);
+		msleep(10);
 
-		if(!(adapter->test_icr & mask)) {
+		if (!(adapter->test_icr & mask)) {
 			*data = 4;
 			break;
 		}
 
- 		if(!shared_int) {
+		if (!shared_int) {
 			/* Disable the other interrupts to be reported in
 			 * the cause register and then force the other
 			 * interrupts and see if any get posted.  If
@@ -853,11 +992,13 @@
 			 * test failed.
 			 */
 			adapter->test_icr = 0;
-			E1000_WRITE_REG(&adapter->hw, IMC, ~mask);
-			E1000_WRITE_REG(&adapter->hw, ICS, ~mask);
-			msec_delay(10);
+			E1000_WRITE_REG(&adapter->hw, E1000_IMC,
+			                ~mask & 0x00007FFF);
+			E1000_WRITE_REG(&adapter->hw, E1000_ICS,
+			                ~mask & 0x00007FFF);
+			msleep(10);
 
-			if(adapter->test_icr) {
+			if (adapter->test_icr) {
 				*data = 5;
 				break;
 			}
@@ -865,8 +1006,8 @@
 	}
 
 	/* Disable all the interrupts */
-	E1000_WRITE_REG(&adapter->hw, IMC, 0xFFFFFFFF);
-	msec_delay(10);
+	E1000_WRITE_REG(&adapter->hw, E1000_IMC, 0xFFFFFFFF);
+	msleep(10);
 
 	/* Unhook test interrupt handler */
 	free_irq(irq, netdev);
@@ -874,106 +1015,110 @@
 	return *data;
 }
 
-static void
-e1000_free_desc_rings(struct e1000_adapter *adapter)
+static void e1000_free_desc_rings(struct e1000_adapter *adapter)
 {
-	struct e1000_desc_ring *txdr = &adapter->test_tx_ring;
-	struct e1000_desc_ring *rxdr = &adapter->test_rx_ring;
+	struct e1000_tx_ring *tx_ring = &adapter->test_tx_ring;
+	struct e1000_rx_ring *rx_ring = &adapter->test_rx_ring;
 	struct pci_dev *pdev = adapter->pdev;
 	int i;
 
-	if(txdr->desc && txdr->buffer_info) {
-		for(i = 0; i < txdr->count; i++) {
-			if(txdr->buffer_info[i].dma)
-				pci_unmap_single(pdev, txdr->buffer_info[i].dma,
-						 txdr->buffer_info[i].length,
+	if (tx_ring->desc && tx_ring->buffer_info) {
+		for (i = 0; i < tx_ring->count; i++) {
+			if (tx_ring->buffer_info[i].dma)
+				pci_unmap_single(pdev, tx_ring->buffer_info[i].dma,
+						 tx_ring->buffer_info[i].length,
 						 PCI_DMA_TODEVICE);
-			if(txdr->buffer_info[i].skb)
-				dev_kfree_skb(txdr->buffer_info[i].skb);
+			if (tx_ring->buffer_info[i].skb)
+				dev_kfree_skb(tx_ring->buffer_info[i].skb);
 		}
 	}
 
-	if(rxdr->desc && rxdr->buffer_info) {
-		for(i = 0; i < rxdr->count; i++) {
-			if(rxdr->buffer_info[i].dma)
-				pci_unmap_single(pdev, rxdr->buffer_info[i].dma,
-						 rxdr->buffer_info[i].length,
+	if (rx_ring->desc && rx_ring->buffer_info) {
+		for (i = 0; i < rx_ring->count; i++) {
+			if (rx_ring->buffer_info[i].dma)
+				pci_unmap_single(pdev, rx_ring->buffer_info[i].dma,
+						 E1000_RXBUFFER_2048,
 						 PCI_DMA_FROMDEVICE);
-			if(rxdr->buffer_info[i].skb)
-				dev_kfree_skb(rxdr->buffer_info[i].skb);
+			if (rx_ring->buffer_info[i].skb)
+				dev_kfree_skb(rx_ring->buffer_info[i].skb);
 		}
 	}
 
-	if(txdr->desc)
-		pci_free_consistent(pdev, txdr->size, txdr->desc, txdr->dma);
-	if(rxdr->desc)
-		pci_free_consistent(pdev, rxdr->size, rxdr->desc, rxdr->dma);
+	if (tx_ring->desc) {
+		pci_free_consistent(pdev, tx_ring->size, tx_ring->desc, tx_ring->dma);
+		tx_ring->desc = NULL;
+	}
+	if (rx_ring->desc) {
+		pci_free_consistent(pdev, rx_ring->size, rx_ring->desc, rx_ring->dma);
+		rx_ring->desc = NULL;
+	}
 
-	if(txdr->buffer_info)
-		kfree(txdr->buffer_info);
-	if(rxdr->buffer_info)
-		kfree(rxdr->buffer_info);
+	kfree(tx_ring->buffer_info);
+	tx_ring->buffer_info = NULL;
+	kfree(rx_ring->buffer_info);
+	rx_ring->buffer_info = NULL;
 
 	return;
 }
 
-static int
-e1000_setup_desc_rings(struct e1000_adapter *adapter)
+static int e1000_setup_desc_rings(struct e1000_adapter *adapter)
 {
-	struct e1000_desc_ring *txdr = &adapter->test_tx_ring;
-	struct e1000_desc_ring *rxdr = &adapter->test_rx_ring;
+	struct e1000_tx_ring *tx_ring = &adapter->test_tx_ring;
+	struct e1000_rx_ring *rx_ring = &adapter->test_rx_ring;
 	struct pci_dev *pdev = adapter->pdev;
-	uint32_t rctl;
+	u32 rctl;
 	int size, i, ret_val;
 
 	/* Setup Tx descriptor ring and Tx buffers */
 
-	txdr->count = 80;
+	if (!tx_ring->count)
+		tx_ring->count = E1000_DEFAULT_TXD;
 
-	size = txdr->count * sizeof(struct e1000_buffer);
-	if(!(txdr->buffer_info = kmalloc(size, GFP_KERNEL))) {
+	size = tx_ring->count * sizeof(struct e1000_buffer);
+	if (!(tx_ring->buffer_info = kmalloc(size, GFP_KERNEL))) {
 		ret_val = 1;
 		goto err_nomem;
 	}
-	memset(txdr->buffer_info, 0, size);
+	memset(tx_ring->buffer_info, 0, size);
 
-	txdr->size = txdr->count * sizeof(struct e1000_tx_desc);
-	E1000_ROUNDUP(txdr->size, 4096);
-	if(!(txdr->desc = pci_alloc_consistent(pdev, txdr->size, &txdr->dma))) {
+	tx_ring->size = tx_ring->count * sizeof(struct e1000_tx_desc);
+	tx_ring->size = ALIGN(tx_ring->size, 4096);
+	if (!(tx_ring->desc = pci_alloc_consistent(pdev, tx_ring->size,
+	                                           &tx_ring->dma))) {
 		ret_val = 2;
 		goto err_nomem;
 	}
-	memset(txdr->desc, 0, txdr->size);
-	txdr->next_to_use = txdr->next_to_clean = 0;
+	memset(tx_ring->desc, 0, tx_ring->size);
+	tx_ring->next_to_use = tx_ring->next_to_clean = 0;
 
-	E1000_WRITE_REG(&adapter->hw, TDBAL,
-			((uint64_t) txdr->dma & 0x00000000FFFFFFFF));
-	E1000_WRITE_REG(&adapter->hw, TDBAH, ((uint64_t) txdr->dma >> 32));
-	E1000_WRITE_REG(&adapter->hw, TDLEN,
-			txdr->count * sizeof(struct e1000_tx_desc));
-	E1000_WRITE_REG(&adapter->hw, TDH, 0);
-	E1000_WRITE_REG(&adapter->hw, TDT, 0);
-	E1000_WRITE_REG(&adapter->hw, TCTL,
+	E1000_WRITE_REG(&adapter->hw, E1000_TDBAL,
+			((u64) tx_ring->dma & 0x00000000FFFFFFFF));
+	E1000_WRITE_REG(&adapter->hw, E1000_TDBAH, ((u64) tx_ring->dma >> 32));
+	E1000_WRITE_REG(&adapter->hw, E1000_TDLEN,
+			tx_ring->count * sizeof(struct e1000_tx_desc));
+	E1000_WRITE_REG(&adapter->hw, E1000_TDH, 0);
+	E1000_WRITE_REG(&adapter->hw, E1000_TDT, 0);
+	E1000_WRITE_REG(&adapter->hw, E1000_TCTL,
 			E1000_TCTL_PSP | E1000_TCTL_EN |
 			E1000_COLLISION_THRESHOLD << E1000_CT_SHIFT |
-			E1000_FDX_COLLISION_DISTANCE << E1000_COLD_SHIFT);
+			E1000_COLLISION_DISTANCE << E1000_COLD_SHIFT);
 
-	for(i = 0; i < txdr->count; i++) {
-		struct e1000_tx_desc *tx_desc = E1000_TX_DESC(*txdr, i);
+	for (i = 0; i < tx_ring->count; i++) {
+		struct e1000_tx_desc *tx_desc = E1000_TX_DESC(*tx_ring, i);
 		struct sk_buff *skb;
 		unsigned int size = 1024;
 
-		if(!(skb = alloc_skb(size, GFP_KERNEL))) {
+		if (!(skb = alloc_skb(size, GFP_KERNEL))) {
 			ret_val = 3;
 			goto err_nomem;
 		}
 		skb_put(skb, size);
-		txdr->buffer_info[i].skb = skb;
-		txdr->buffer_info[i].length = skb->len;
-		txdr->buffer_info[i].dma =
+		tx_ring->buffer_info[i].skb = skb;
+		tx_ring->buffer_info[i].length = skb->len;
+		tx_ring->buffer_info[i].dma =
 			pci_map_single(pdev, skb->data, skb->len,
 				       PCI_DMA_TODEVICE);
-		tx_desc->buffer_addr = cpu_to_le64(txdr->buffer_info[i].dma);
+		tx_desc->buffer_addr = cpu_to_le64(tx_ring->buffer_info[i].dma);
 		tx_desc->lower.data = cpu_to_le32(skb->len);
 		tx_desc->lower.data |= cpu_to_le32(E1000_TXD_CMD_EOP |
 						   E1000_TXD_CMD_IFCS |
@@ -983,52 +1128,53 @@
 
 	/* Setup Rx descriptor ring and Rx buffers */
 
-	rxdr->count = 80;
+	if (!rx_ring->count)
+		rx_ring->count = E1000_DEFAULT_RXD;
 
-	size = rxdr->count * sizeof(struct e1000_buffer);
-	if(!(rxdr->buffer_info = kmalloc(size, GFP_KERNEL))) {
+	size = rx_ring->count * sizeof(struct e1000_buffer);
+	if (!(rx_ring->buffer_info = kmalloc(size, GFP_KERNEL))) {
 		ret_val = 4;
 		goto err_nomem;
 	}
-	memset(rxdr->buffer_info, 0, size);
+	memset(rx_ring->buffer_info, 0, size);
 
-	rxdr->size = rxdr->count * sizeof(struct e1000_rx_desc);
-	if(!(rxdr->desc = pci_alloc_consistent(pdev, rxdr->size, &rxdr->dma))) {
+	rx_ring->size = rx_ring->count * sizeof(struct e1000_rx_desc);
+	if (!(rx_ring->desc = pci_alloc_consistent(pdev, rx_ring->size,
+	                                           &rx_ring->dma))) {
 		ret_val = 5;
 		goto err_nomem;
 	}
-	memset(rxdr->desc, 0, rxdr->size);
-	rxdr->next_to_use = rxdr->next_to_clean = 0;
+	memset(rx_ring->desc, 0, rx_ring->size);
+	rx_ring->next_to_use = rx_ring->next_to_clean = 0;
 
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl & ~E1000_RCTL_EN);
-	E1000_WRITE_REG(&adapter->hw, RDBAL,
-			((uint64_t) rxdr->dma & 0xFFFFFFFF));
-	E1000_WRITE_REG(&adapter->hw, RDBAH, ((uint64_t) rxdr->dma >> 32));
-	E1000_WRITE_REG(&adapter->hw, RDLEN, rxdr->size);
-	E1000_WRITE_REG(&adapter->hw, RDH, 0);
-	E1000_WRITE_REG(&adapter->hw, RDT, 0);
+	rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+	E1000_WRITE_REG(&adapter->hw, E1000_RDBAL,
+			((u64) rx_ring->dma & 0xFFFFFFFF));
+	E1000_WRITE_REG(&adapter->hw, E1000_RDBAH, ((u64) rx_ring->dma >> 32));
+	E1000_WRITE_REG(&adapter->hw, E1000_RDLEN, rx_ring->size);
+	E1000_WRITE_REG(&adapter->hw, E1000_RDH, 0);
+	E1000_WRITE_REG(&adapter->hw, E1000_RDT, 0);
 	rctl = E1000_RCTL_EN | E1000_RCTL_BAM | E1000_RCTL_SZ_2048 |
 		E1000_RCTL_LBM_NO | E1000_RCTL_RDMTS_HALF |
-		(adapter->hw.mc_filter_type << E1000_RCTL_MO_SHIFT);
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+		(adapter->hw.mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
 
-	for(i = 0; i < rxdr->count; i++) {
-		struct e1000_rx_desc *rx_desc = E1000_RX_DESC(*rxdr, i);
+	for (i = 0; i < rx_ring->count; i++) {
+		struct e1000_rx_desc *rx_desc = E1000_RX_DESC(*rx_ring, i);
 		struct sk_buff *skb;
 
-		if(!(skb = alloc_skb(E1000_RXBUFFER_2048 + NET_IP_ALIGN,
-				     GFP_KERNEL))) {
+		if (!(skb = alloc_skb(E1000_RXBUFFER_2048 + NET_IP_ALIGN,
+				GFP_KERNEL))) {
 			ret_val = 6;
 			goto err_nomem;
 		}
 		skb_reserve(skb, NET_IP_ALIGN);
-		rxdr->buffer_info[i].skb = skb;
-		rxdr->buffer_info[i].length = E1000_RXBUFFER_2048;
-		rxdr->buffer_info[i].dma =
+		rx_ring->buffer_info[i].skb = skb;
+		rx_ring->buffer_info[i].dma =
 			pci_map_single(pdev, skb->data, E1000_RXBUFFER_2048,
 				       PCI_DMA_FROMDEVICE);
-		rx_desc->buffer_addr = cpu_to_le64(rxdr->buffer_info[i].dma);
+		rx_desc->buffer_addr = cpu_to_le64(rx_ring->buffer_info[i].dma);
 		memset(skb->data, 0x00, skb->len);
 	}
 
@@ -1039,8 +1185,7 @@
 	return ret_val;
 }
 
-static void
-e1000_phy_disable_receiver(struct e1000_adapter *adapter)
+static void e1000_phy_disable_receiver(struct e1000_adapter *adapter)
 {
 	/* Write out to PHY registers 29 and 30 to disable the Receiver. */
 	e1000_write_phy_reg(&adapter->hw, 29, 0x001F);
@@ -1049,10 +1194,9 @@
 	e1000_write_phy_reg(&adapter->hw, 30, 0x8FF0);
 }
 
-static void
-e1000_phy_reset_clk_and_crs(struct e1000_adapter *adapter)
+static void e1000_phy_reset_clk_and_crs(struct e1000_adapter *adapter)
 {
-	uint16_t phy_reg;
+	u16 phy_reg;
 
 	/* Because we reset the PHY above, we need to re-force TX_CLK in the
 	 * Extended PHY Specific Control Register to 25MHz clock.  This
@@ -1073,22 +1217,21 @@
 		M88E1000_PHY_SPEC_CTRL, phy_reg);
 }
 
-static int
-e1000_nonintegrated_phy_loopback(struct e1000_adapter *adapter)
+static int e1000_nonintegrated_phy_loopback(struct e1000_adapter *adapter)
 {
-	uint32_t ctrl_reg;
-	uint16_t phy_reg;
+	u32 ctrl_reg;
+	u16 phy_reg;
 
 	/* Setup the Device Control Register for PHY loopback test. */
 
-	ctrl_reg = E1000_READ_REG(&adapter->hw, CTRL);
+	ctrl_reg = E1000_READ_REG(&adapter->hw, E1000_CTRL);
 	ctrl_reg |= (E1000_CTRL_ILOS |		/* Invert Loss-Of-Signal */
 		     E1000_CTRL_FRCSPD |	/* Set the Force Speed Bit */
 		     E1000_CTRL_FRCDPX |	/* Set the Force Duplex Bit */
 		     E1000_CTRL_SPD_1000 |	/* Force Speed to 1000 */
 		     E1000_CTRL_FD);		/* Force Duplex to FULL */
 
-	E1000_WRITE_REG(&adapter->hw, CTRL, ctrl_reg);
+	E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl_reg);
 
 	/* Read the PHY Specific Control Register (0x10) */
 	e1000_read_phy_reg(&adapter->hw, M88E1000_PHY_SPEC_CTRL, &phy_reg);
@@ -1100,12 +1243,12 @@
 	e1000_write_phy_reg(&adapter->hw, M88E1000_PHY_SPEC_CTRL, phy_reg);
 
 	/* Perform software reset on the PHY */
-	e1000_phy_reset(&adapter->hw);
+	e1000_phy_commit(&adapter->hw);
 
 	/* Have to setup TX_CLK and TX_CRS after software reset */
 	e1000_phy_reset_clk_and_crs(adapter);
 
-	e1000_write_phy_reg(&adapter->hw, PHY_CTRL, 0x8100);
+	e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, 0x8100);
 
 	/* Wait for reset to complete. */
 	udelay(500);
@@ -1117,74 +1260,91 @@
 	e1000_phy_disable_receiver(adapter);
 
 	/* Set the loopback bit in the PHY control register. */
-	e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &phy_reg);
+	e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &phy_reg);
 	phy_reg |= MII_CR_LOOPBACK;
-	e1000_write_phy_reg(&adapter->hw, PHY_CTRL, phy_reg);
+	e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, phy_reg);
 
 	/* Setup TX_CLK and TX_CRS one more time. */
 	e1000_phy_reset_clk_and_crs(adapter);
 
 	/* Check Phy Configuration */
-	e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &phy_reg);
-	if(phy_reg != 0x4100)
+	e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &phy_reg);
+	if (phy_reg != 0x4100)
 		 return 9;
 
 	e1000_read_phy_reg(&adapter->hw, M88E1000_EXT_PHY_SPEC_CTRL, &phy_reg);
-	if(phy_reg != 0x0070)
+	if (phy_reg != 0x0070)
 		return 10;
 
 	e1000_read_phy_reg(&adapter->hw, 29, &phy_reg);
-	if(phy_reg != 0x001A)
+	if (phy_reg != 0x001A)
 		return 11;
 
 	return 0;
 }
 
-static int
-e1000_integrated_phy_loopback(struct e1000_adapter *adapter)
+static int e1000_integrated_phy_loopback(struct e1000_adapter *adapter)
 {
-	uint32_t ctrl_reg = 0;
-	uint32_t stat_reg = 0;
+	u32 ctrl_reg = 0;
+	u32 stat_reg = 0;
 
-	adapter->hw.autoneg = FALSE;
+	adapter->hw.mac.autoneg = FALSE;
 
-	if(adapter->hw.phy_type == e1000_phy_m88) {
+	if (adapter->hw.phy.type == e1000_phy_m88) {
 		/* Auto-MDI/MDIX Off */
 		e1000_write_phy_reg(&adapter->hw,
 				    M88E1000_PHY_SPEC_CTRL, 0x0808);
 		/* reset to update Auto-MDI/MDIX */
-		e1000_write_phy_reg(&adapter->hw, PHY_CTRL, 0x9140);
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, 0x9140);
 		/* autoneg off */
-		e1000_write_phy_reg(&adapter->hw, PHY_CTRL, 0x8140);
-	}
-	/* force 1000, set loopback */
-	e1000_write_phy_reg(&adapter->hw, PHY_CTRL, 0x4140);
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, 0x8140);
+	} else if (adapter->hw.phy.type == e1000_phy_gg82563)
+		e1000_write_phy_reg(&adapter->hw,
+		                    GG82563_PHY_KMRN_MODE_CTRL,
+		                    0x1CC);
+
+	ctrl_reg = E1000_READ_REG(&adapter->hw, E1000_CTRL);
 
-	/* Now set up the MAC to the same speed/duplex as the PHY. */
-	ctrl_reg = E1000_READ_REG(&adapter->hw, CTRL);
-	ctrl_reg &= ~E1000_CTRL_SPD_SEL; /* Clear the speed sel bits */
-	ctrl_reg |= (E1000_CTRL_FRCSPD | /* Set the Force Speed Bit */
-		     E1000_CTRL_FRCDPX | /* Set the Force Duplex Bit */
-		     E1000_CTRL_SPD_1000 |/* Force Speed to 1000 */
-		     E1000_CTRL_FD);	 /* Force Duplex to FULL */
+	if (adapter->hw.phy.type == e1000_phy_ife) {
+		/* force 100, set loopback */
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, 0x6100);
+
+		/* Now set up the MAC to the same speed/duplex as the PHY. */
+		ctrl_reg &= ~E1000_CTRL_SPD_SEL; /* Clear the speed sel bits */
+		ctrl_reg |= (E1000_CTRL_FRCSPD | /* Set the Force Speed Bit */
+			     E1000_CTRL_FRCDPX | /* Set the Force Duplex Bit */
+			     E1000_CTRL_SPD_100 |/* Force Speed to 100 */
+			     E1000_CTRL_FD);	 /* Force Duplex to FULL */
+	} else {
+		/* force 1000, set loopback */
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, 0x4140);
 
-	if(adapter->hw.media_type == e1000_media_type_copper &&
-	   adapter->hw.phy_type == e1000_phy_m88) {
+		/* Now set up the MAC to the same speed/duplex as the PHY. */
+		ctrl_reg = E1000_READ_REG(&adapter->hw, E1000_CTRL);
+		ctrl_reg &= ~E1000_CTRL_SPD_SEL; /* Clear the speed sel bits */
+		ctrl_reg |= (E1000_CTRL_FRCSPD | /* Set the Force Speed Bit */
+			     E1000_CTRL_FRCDPX | /* Set the Force Duplex Bit */
+			     E1000_CTRL_SPD_1000 |/* Force Speed to 1000 */
+			     E1000_CTRL_FD);	 /* Force Duplex to FULL */
+	}
+
+	if (adapter->hw.media_type == e1000_media_type_copper &&
+	   adapter->hw.phy.type == e1000_phy_m88) {
 		ctrl_reg |= E1000_CTRL_ILOS; /* Invert Loss of Signal */
 	} else {
-		/* Set the ILOS bit on the fiber Nic is half
-		 * duplex link is detected. */
-		stat_reg = E1000_READ_REG(&adapter->hw, STATUS);
-		if((stat_reg & E1000_STATUS_FD) == 0)
+		/* Set the ILOS bit on the fiber Nic if half duplex link is
+		 * detected. */
+		stat_reg = E1000_READ_REG(&adapter->hw, E1000_STATUS);
+		if ((stat_reg & E1000_STATUS_FD) == 0)
 			ctrl_reg |= (E1000_CTRL_ILOS | E1000_CTRL_SLU);
 	}
 
-	E1000_WRITE_REG(&adapter->hw, CTRL, ctrl_reg);
+	E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl_reg);
 
 	/* Disable the receiver on the PHY so when a cable is plugged in, the
 	 * PHY does not begin to autoneg when a cable is reconnected to the NIC.
 	 */
-	if(adapter->hw.phy_type == e1000_phy_m88)
+	if (adapter->hw.phy.type == e1000_phy_m88)
 		e1000_phy_disable_receiver(adapter);
 
 	udelay(500);
@@ -1192,22 +1352,57 @@
 	return 0;
 }
 
-static int
-e1000_set_phy_loopback(struct e1000_adapter *adapter)
+static int e1000_set_82571_fiber_loopback(struct e1000_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	int link = 0;
+
+	/* special requirements for 82571/82572 fiber adapters */
+
+	/* jump through hoops to make sure link is up because serdes
+	 * link is hardwired up */
+	ctrl |= E1000_CTRL_SLU;
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	/* disable autoneg */
+	ctrl = E1000_READ_REG(hw, E1000_TXCW);
+	ctrl &= ~(1 << 31);
+	E1000_WRITE_REG(hw, E1000_TXCW, ctrl);
+
+	link = (E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_LU);
+
+	if (!link) {
+		/* set invert loss of signal */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= E1000_CTRL_ILOS;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	}
+
+	/* special write to serdes control register to enable SerDes analog
+	 * loopback */
+#define E1000_SERDES_LB_ON 0x410
+	E1000_WRITE_REG(hw, E1000_SCTL, E1000_SERDES_LB_ON);
+	msleep(10);
+
+	return 0;
+}
+
+static int e1000_set_phy_loopback(struct e1000_adapter *adapter)
 {
-	uint16_t phy_reg = 0;
-	uint16_t count = 0;
+	u16 phy_reg = 0;
+	u16 count = 0;
 
-	switch (adapter->hw.mac_type) {
+	switch (adapter->hw.mac.type) {
 	case e1000_82543:
-		if(adapter->hw.media_type == e1000_media_type_copper) {
+		if (adapter->hw.media_type == e1000_media_type_copper) {
 			/* Attempt to setup Loopback mode on Non-integrated PHY.
 			 * Some PHY registers get corrupted at random, so
 			 * attempt this 10 times.
 			 */
-			while(e1000_nonintegrated_phy_loopback(adapter) &&
+			while (e1000_nonintegrated_phy_loopback(adapter) &&
 			      count++ < 10);
-			if(count < 11)
+			if (count < 11)
 				return 0;
 		}
 		break;
@@ -1222,6 +1417,12 @@
 	case e1000_82541_rev_2:
 	case e1000_82547:
 	case e1000_82547_rev_2:
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_82573:
+	case e1000_80003es2lan:
+	case e1000_ich8lan:
+	case e1000_ich9lan:
 		return e1000_integrated_phy_loopback(adapter);
 		break;
 
@@ -1229,9 +1430,9 @@
 		/* Default PHY loopback work is to read the MII
 		 * control register and assert bit 14 (loopback mode).
 		 */
-		e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &phy_reg);
+		e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &phy_reg);
 		phy_reg |= MII_CR_LOOPBACK;
-		e1000_write_phy_reg(&adapter->hw, PHY_CTRL, phy_reg);
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, phy_reg);
 		return 0;
 		break;
 	}
@@ -1239,73 +1440,138 @@
 	return 8;
 }
 
-static int
-e1000_setup_loopback_test(struct e1000_adapter *adapter)
+/* only call this for fiber/serdes connections to es2lan */
+static int e1000_set_es2lan_mac_loopback(struct e1000_adapter *adapter)
 {
-	uint32_t rctl;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrlext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	u32 ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* save CTRL_EXT to restore later, reuse an empty variable (unused
+	   on mac_type 80003es2lan) */
+	adapter->tx_fifo_head = ctrlext;
+
+	/* clear the serdes mode bits, putting the device into mac loopback */
+	ctrlext &= ~E1000_CTRL_EXT_LINK_MODE_PCIE_SERDES;
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrlext);
+
+	/* force speed to 1000/FD, link up */
+	ctrl &= ~(E1000_CTRL_SPD_1000 | E1000_CTRL_SPD_100);
+	ctrl |= (E1000_CTRL_SLU | E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX |
+	         E1000_CTRL_SPD_1000 | E1000_CTRL_FD);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	/* set mac loopback */
+	ctrl = E1000_READ_REG(hw, E1000_RCTL);
+	ctrl |= E1000_RCTL_LBM_MAC;
+	E1000_WRITE_REG(hw, E1000_RCTL, ctrl);
+
+	/* set testing mode parameters (no need to reset later) */
+#define KMRNCTRLSTA_OPMODE (0x1F << 16)
+#define KMRNCTRLSTA_OPMODE_1GB_FD_GMII 0x0582
+	E1000_WRITE_REG(hw, E1000_KMRNCTRLSTA,
+		(KMRNCTRLSTA_OPMODE | KMRNCTRLSTA_OPMODE_1GB_FD_GMII));
 
-	if(adapter->hw.media_type == e1000_media_type_fiber ||
-	   adapter->hw.media_type == e1000_media_type_internal_serdes) {
-		if(adapter->hw.mac_type == e1000_82545 ||
-		   adapter->hw.mac_type == e1000_82546 ||
-		   adapter->hw.mac_type == e1000_82545_rev_3 ||
-		   adapter->hw.mac_type == e1000_82546_rev_3)
+	return 0;
+}
+
+static int e1000_setup_loopback_test(struct e1000_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rctl;
+
+	if (hw->media_type == e1000_media_type_fiber ||
+	    hw->media_type == e1000_media_type_internal_serdes) {
+		switch (hw->mac.type) {
+		case e1000_80003es2lan:
+			return e1000_set_es2lan_mac_loopback(adapter);
+			break;
+		case e1000_82545:
+		case e1000_82546:
+		case e1000_82545_rev_3:
+		case e1000_82546_rev_3:
 			return e1000_set_phy_loopback(adapter);
-		else {
-			rctl = E1000_READ_REG(&adapter->hw, RCTL);
+			break;
+		case e1000_82571:
+		case e1000_82572:
+			return e1000_set_82571_fiber_loopback(adapter);
+			break;
+		default:
+			rctl = E1000_READ_REG(hw, E1000_RCTL);
 			rctl |= E1000_RCTL_LBM_TCVR;
-			E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+			E1000_WRITE_REG(hw, E1000_RCTL, rctl);
 			return 0;
 		}
-	} else if(adapter->hw.media_type == e1000_media_type_copper)
+	} else if (hw->media_type == e1000_media_type_copper)
 		return e1000_set_phy_loopback(adapter);
 
 	return 7;
 }
 
-static void
-e1000_loopback_cleanup(struct e1000_adapter *adapter)
+static void e1000_loopback_cleanup(struct e1000_adapter *adapter)
 {
-	uint32_t rctl;
-	uint16_t phy_reg;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rctl;
+	u16 phy_reg;
 
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
 	rctl &= ~(E1000_RCTL_LBM_TCVR | E1000_RCTL_LBM_MAC);
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl);
 
-	if(adapter->hw.media_type == e1000_media_type_copper ||
-	   ((adapter->hw.media_type == e1000_media_type_fiber ||
-	     adapter->hw.media_type == e1000_media_type_internal_serdes) &&
-	    (adapter->hw.mac_type == e1000_82545 ||
-	     adapter->hw.mac_type == e1000_82546 ||
-	     adapter->hw.mac_type == e1000_82545_rev_3 ||
-	     adapter->hw.mac_type == e1000_82546_rev_3))) {
-		adapter->hw.autoneg = TRUE;
-		e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &phy_reg);
-		if(phy_reg & MII_CR_LOOPBACK) {
+	switch (hw->mac.type) {
+	case e1000_80003es2lan:
+		if (hw->media_type == e1000_media_type_fiber ||
+		    hw->media_type == e1000_media_type_internal_serdes) {
+			/* restore CTRL_EXT, stealing space from tx_fifo_head */
+			E1000_WRITE_REG(hw, E1000_CTRL_EXT, adapter->tx_fifo_head);
+			adapter->tx_fifo_head = 0;
+		}
+		/* fall through */
+	case e1000_82571:
+	case e1000_82572:
+		if (hw->media_type == e1000_media_type_fiber ||
+		    hw->media_type == e1000_media_type_internal_serdes) {
+#define E1000_SERDES_LB_OFF 0x400
+			E1000_WRITE_REG(hw, E1000_SCTL, E1000_SERDES_LB_OFF);
+			msleep(10);
+			break;
+		}
+		/* Fall Through */
+	case e1000_82545:
+	case e1000_82546:
+	case e1000_82545_rev_3:
+	case e1000_82546_rev_3:
+	default:
+		hw->mac.autoneg = TRUE;
+		if (hw->phy.type == e1000_phy_gg82563)
+			e1000_write_phy_reg(hw,
+					    GG82563_PHY_KMRN_MODE_CTRL,
+					    0x180);
+		e1000_read_phy_reg(hw, PHY_CONTROL, &phy_reg);
+		if (phy_reg & MII_CR_LOOPBACK) {
 			phy_reg &= ~MII_CR_LOOPBACK;
-			e1000_write_phy_reg(&adapter->hw, PHY_CTRL, phy_reg);
-			e1000_phy_reset(&adapter->hw);
+			e1000_write_phy_reg(hw, PHY_CONTROL, phy_reg);
+			e1000_phy_commit(hw);
 		}
+		break;
 	}
 }
 
-static void
-e1000_create_lbtest_frame(struct sk_buff *skb, unsigned int frame_size)
+static void e1000_create_lbtest_frame(struct sk_buff *skb,
+                                      unsigned int frame_size)
 {
 	memset(skb->data, 0xFF, frame_size);
-	frame_size = (frame_size % 2) ? (frame_size - 1) : frame_size;
+	frame_size &= ~1;
 	memset(&skb->data[frame_size / 2], 0xAA, frame_size / 2 - 1);
 	memset(&skb->data[frame_size / 2 + 10], 0xBE, 1);
 	memset(&skb->data[frame_size / 2 + 12], 0xAF, 1);
 }
 
-static int
-e1000_check_lbtest_frame(struct sk_buff *skb, unsigned int frame_size)
+static int e1000_check_lbtest_frame(struct sk_buff *skb, unsigned int frame_size)
 {
-	frame_size = (frame_size % 2) ? (frame_size - 1) : frame_size;
-	if(*(skb->data + 3) == 0xFF) {
-		if((*(skb->data + frame_size / 2 + 10) == 0xBE) &&
+	frame_size &= ~1;
+	if (*(skb->data + 3) == 0xFF) {
+		if ((*(skb->data + frame_size / 2 + 10) == 0xBE) &&
 		   (*(skb->data + frame_size / 2 + 12) == 0xAF)) {
 			return 0;
 		}
@@ -1313,202 +1579,342 @@
 	return 13;
 }
 
-static int
-e1000_run_loopback_test(struct e1000_adapter *adapter)
+static int e1000_run_loopback_test(struct e1000_adapter *adapter)
 {
-	struct e1000_desc_ring *txdr = &adapter->test_tx_ring;
-	struct e1000_desc_ring *rxdr = &adapter->test_rx_ring;
+	struct e1000_tx_ring *tx_ring = &adapter->test_tx_ring;
+	struct e1000_rx_ring *rx_ring = &adapter->test_rx_ring;
 	struct pci_dev *pdev = adapter->pdev;
-	int i;
+	int i, j, k, l, lc, good_cnt, ret_val=0;
+	unsigned long time;
 
-	E1000_WRITE_REG(&adapter->hw, RDT, rxdr->count - 1);
+	E1000_WRITE_REG(&adapter->hw, E1000_RDT, rx_ring->count - 1);
 
-	for(i = 0; i < 64; i++) {
-		e1000_create_lbtest_frame(txdr->buffer_info[i].skb, 1024);
-		pci_dma_sync_single_for_device(pdev, txdr->buffer_info[i].dma,
-					    txdr->buffer_info[i].length,
-					    PCI_DMA_TODEVICE);
-	}
-	E1000_WRITE_REG(&adapter->hw, TDT, i);
-
-	msec_delay(200);
+	/* Calculate the loop count based on the largest descriptor ring
+	 * The idea is to wrap the largest ring a number of times using 64
+	 * send/receive pairs during each loop
+	 */
 
-	pci_dma_sync_single_for_cpu(pdev, rxdr->buffer_info[0].dma,
-			    rxdr->buffer_info[0].length, PCI_DMA_FROMDEVICE);
+	if (rx_ring->count <= tx_ring->count)
+		lc = ((tx_ring->count / 64) * 2) + 1;
+	else
+		lc = ((rx_ring->count / 64) * 2) + 1;
 
-	return e1000_check_lbtest_frame(rxdr->buffer_info[0].skb, 1024);
+	k = l = 0;
+	for (j = 0; j <= lc; j++) { /* loop count loop */
+		for (i = 0; i < 64; i++) { /* send the packets */
+			e1000_create_lbtest_frame(tx_ring->buffer_info[i].skb,
+					1024);
+			pci_dma_sync_single_for_device(pdev,
+					tx_ring->buffer_info[k].dma,
+				    	tx_ring->buffer_info[k].length,
+				    	PCI_DMA_TODEVICE);
+			if (unlikely(++k == tx_ring->count)) k = 0;
+		}
+		E1000_WRITE_REG(&adapter->hw, E1000_TDT, k);
+		msleep(200);
+		time = jiffies; /* set the start time for the receive */
+		good_cnt = 0;
+		do { /* receive the sent packets */
+			pci_dma_sync_single_for_cpu(pdev,
+			                rx_ring->buffer_info[l].dma,
+			                E1000_RXBUFFER_2048,
+			                PCI_DMA_FROMDEVICE);
+
+			ret_val = e1000_check_lbtest_frame(
+					rx_ring->buffer_info[l].skb,
+				   	1024);
+			if (!ret_val)
+				good_cnt++;
+			if (unlikely(++l == rx_ring->count)) l = 0;
+			/* time + 20 msecs (200 msecs on 2.4) is more than
+			 * enough time to complete the receives, if it's
+			 * exceeded, break and error off
+			 */
+		} while (good_cnt < 64 && jiffies < (time + 20));
+		if (good_cnt != 64) {
+			ret_val = 13; /* ret_val is the same as mis-compare */
+			break;
+		}
+		if (jiffies >= (time + 2)) {
+			ret_val = 14; /* error code for time out error */
+			break;
+		}
+	} /* end loop count loop */
+	return ret_val;
 }
 
-static int
-e1000_loopback_test(struct e1000_adapter *adapter, uint64_t *data)
+static int e1000_loopback_test(struct e1000_adapter *adapter, u64 *data)
 {
-	if((*data = e1000_setup_desc_rings(adapter))) goto err_loopback;
-	if((*data = e1000_setup_loopback_test(adapter))) goto err_loopback;
+	/* PHY loopback cannot be performed if SoL/IDER
+	 * sessions are active */
+	if (e1000_check_reset_block(&adapter->hw)) {
+		DPRINTK(DRV, ERR, "Cannot do PHY loopback test "
+		        "when SoL/IDER is active.\n");
+		*data = 0;
+		goto out;
+	}
+
+	if ((*data = e1000_setup_desc_rings(adapter)))
+		goto out;
+	if ((*data = e1000_setup_loopback_test(adapter)))
+		goto err_loopback;
 	*data = e1000_run_loopback_test(adapter);
 	e1000_loopback_cleanup(adapter);
-	e1000_free_desc_rings(adapter);
+
 err_loopback:
+	e1000_free_desc_rings(adapter);
+out:
 	return *data;
 }
 
-static int
-e1000_link_test(struct e1000_adapter *adapter, uint64_t *data)
+static int e1000_link_test(struct e1000_adapter *adapter, u64 *data)
 {
 	*data = 0;
-	e1000_check_for_link(&adapter->hw);
+	if (adapter->hw.media_type == e1000_media_type_internal_serdes) {
+		int i = 0;
+		adapter->hw.mac.serdes_has_link = FALSE;
+
+		/* On some blade server designs, link establishment
+		 * could take as long as 2-3 minutes */
+		do {
+			e1000_check_for_link(&adapter->hw);
+			if (adapter->hw.mac.serdes_has_link == TRUE)
+				return *data;
+			msleep(20);
+		} while (i++ < 3750);
 
-	if(!(E1000_READ_REG(&adapter->hw, STATUS) & E1000_STATUS_LU)) {
 		*data = 1;
+	} else {
+		e1000_check_for_link(&adapter->hw);
+		if (adapter->hw.mac.autoneg)
+			msleep(4000);
+
+		if (!(E1000_READ_REG(&adapter->hw, E1000_STATUS) & E1000_STATUS_LU)) {
+			*data = 1;
+		}
 	}
 	return *data;
 }
 
-static int 
-e1000_diag_test_count(struct net_device *netdev)
+static int e1000_diag_test_count(struct net_device *netdev)
 {
 	return E1000_TEST_LEN;
 }
 
-static void
-e1000_diag_test(struct net_device *netdev,
-		   struct ethtool_test *eth_test, uint64_t *data)
+extern void e1000_power_up_phy(struct e1000_adapter *);
+
+static void e1000_diag_test(struct net_device *netdev,
+                            struct ethtool_test *eth_test, u64 *data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u16 autoneg_advertised;
+	u8 forced_speed_duplex, autoneg;
 	boolean_t if_running = netif_running(netdev);
 
-	if(eth_test->flags == ETH_TEST_FL_OFFLINE) {
+	set_bit(__E1000_TESTING, &adapter->state);
+	if (eth_test->flags == ETH_TEST_FL_OFFLINE) {
 		/* Offline tests */
 
 		/* save speed, duplex, autoneg settings */
-		uint16_t autoneg_advertised = adapter->hw.autoneg_advertised;
-		uint8_t forced_speed_duplex = adapter->hw.forced_speed_duplex;
-		uint8_t autoneg = adapter->hw.autoneg;
+		autoneg_advertised = adapter->hw.phy.autoneg_advertised;
+		forced_speed_duplex = adapter->hw.mac.forced_speed_duplex;
+		autoneg = adapter->hw.mac.autoneg;
+
+		DPRINTK(HW, INFO, "offline testing starting\n");
 
 		/* Link test performed before hardware reset so autoneg doesn't
 		 * interfere with test result */
-		if(e1000_link_test(adapter, &data[4]))
+		if (e1000_link_test(adapter, &data[4]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
-		if(if_running)
-			e1000_down(adapter);
+		if (if_running)
+			/* indicate we're in test mode */
+			dev_close(netdev);
 		else
 			e1000_reset(adapter);
 
-		if(e1000_reg_test(adapter, &data[0]))
+		if (e1000_reg_test(adapter, &data[0]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
 		e1000_reset(adapter);
-		if(e1000_eeprom_test(adapter, &data[1]))
+		if (e1000_eeprom_test(adapter, &data[1]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
 		e1000_reset(adapter);
-		if(e1000_intr_test(adapter, &data[2]))
+		if (e1000_intr_test(adapter, &data[2]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
 		e1000_reset(adapter);
-		if(e1000_loopback_test(adapter, &data[3]))
+		/* make sure the phy is powered up */
+		e1000_power_up_phy(adapter);
+		if (e1000_loopback_test(adapter, &data[3]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
 		/* restore speed, duplex, autoneg settings */
-		adapter->hw.autoneg_advertised = autoneg_advertised;
-		adapter->hw.forced_speed_duplex = forced_speed_duplex;
-		adapter->hw.autoneg = autoneg;
+		adapter->hw.phy.autoneg_advertised = autoneg_advertised;
+		adapter->hw.mac.forced_speed_duplex = forced_speed_duplex;
+		adapter->hw.mac.autoneg = autoneg;
 
+		/* force this routine to wait until autoneg complete/timeout */
+		adapter->hw.phy.wait_for_link = TRUE;
 		e1000_reset(adapter);
-		if(if_running)
-			e1000_up(adapter);
+		adapter->hw.phy.wait_for_link = FALSE;
+
+		clear_bit(__E1000_TESTING, &adapter->state);
+		if (if_running)
+			dev_open(netdev);
 	} else {
+		DPRINTK(HW, INFO, "online testing starting\n");
 		/* Online tests */
-		if(e1000_link_test(adapter, &data[4]))
+		if (e1000_link_test(adapter, &data[4]))
 			eth_test->flags |= ETH_TEST_FL_FAILED;
 
-		/* Offline tests aren't run; pass by default */
+		/* Online tests aren't run; pass by default */
 		data[0] = 0;
 		data[1] = 0;
 		data[2] = 0;
 		data[3] = 0;
+
+		clear_bit(__E1000_TESTING, &adapter->state);
 	}
+	msleep_interruptible(4 * 1000);
 }
 
-static void
-e1000_get_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)
+static int e1000_wol_exclusion(struct e1000_adapter *adapter,
+                               struct ethtool_wolinfo *wol)
 {
-	struct e1000_adapter *adapter = netdev->priv;
 	struct e1000_hw *hw = &adapter->hw;
+	int retval = 1; /* fail by default */
 
-	switch(adapter->hw.device_id) {
+	switch (hw->device_id) {
 	case E1000_DEV_ID_82542:
 	case E1000_DEV_ID_82543GC_FIBER:
 	case E1000_DEV_ID_82543GC_COPPER:
 	case E1000_DEV_ID_82544EI_FIBER:
 	case E1000_DEV_ID_82546EB_QUAD_COPPER:
+	case E1000_DEV_ID_82545EM_FIBER:
+	case E1000_DEV_ID_82545EM_COPPER:
+	case E1000_DEV_ID_82546GB_QUAD_COPPER:
+	case E1000_DEV_ID_82546GB_PCIE:
+		/* these don't support WoL at all */
 		wol->supported = 0;
-		wol->wolopts   = 0;
-		return;
-
+		break;
 	case E1000_DEV_ID_82546EB_FIBER:
 	case E1000_DEV_ID_82546GB_FIBER:
-		/* Wake events only supported on port A for dual fiber */
-		if(E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1) {
+	case E1000_DEV_ID_82571EB_FIBER:
+	case E1000_DEV_ID_82571EB_SERDES:
+	case E1000_DEV_ID_82571EB_COPPER:
+		/* Wake events not supported on port B */
+		if (E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_FUNC_1) {
 			wol->supported = 0;
-			wol->wolopts   = 0;
-			return;
+			break;
 		}
-		/* Fall Through */
-
+		/* return success for non excluded adapter ports */
+		retval = 0;
+		break;
+	case E1000_DEV_ID_82571EB_QUAD_COPPER:
+	case E1000_DEV_ID_82571EB_QUAD_FIBER:
+	case E1000_DEV_ID_82571EB_QUAD_COPPER_LP:
+	case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
+		/* quad port adapters only support WoL on port A */
+		if (!adapter->flags.quad_port_a) {
+			wol->supported = 0;
+			break;
+		}
+		/* return success for non excluded adapter ports */
+		retval = 0;
+		break;
 	default:
-		wol->supported = WAKE_UCAST | WAKE_MCAST |
-				 WAKE_BCAST | WAKE_MAGIC;
+		/* dual port cards only support WoL on port A from now on
+		 * unless it was enabled in the eeprom for port B
+		 * so exclude FUNC_1 ports from having WoL enabled */
+		if (E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_FUNC_1 &&
+		    !adapter->eeprom_wol) {
+			wol->supported = 0;
+			break;
+		}
+
+		retval = 0;
+	}
+
+	return retval;
+}
+
+static void e1000_get_wol(struct net_device *netdev,
+                          struct ethtool_wolinfo *wol)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-		wol->wolopts = 0;
-		if(adapter->wol & E1000_WUFC_EX)
-			wol->wolopts |= WAKE_UCAST;
-		if(adapter->wol & E1000_WUFC_MC)
-			wol->wolopts |= WAKE_MCAST;
-		if(adapter->wol & E1000_WUFC_BC)
-			wol->wolopts |= WAKE_BCAST;
-		if(adapter->wol & E1000_WUFC_MAG)
-			wol->wolopts |= WAKE_MAGIC;
+	wol->supported = WAKE_UCAST | WAKE_MCAST |
+	                 WAKE_BCAST | WAKE_MAGIC;
+	wol->wolopts = 0;
+
+	/* this function will set ->supported = 0 and return 1 if wol is not
+	 * supported by this hardware */
+	if (e1000_wol_exclusion(adapter, wol))
 		return;
+
+	/* apply any specific unsupported masks here */
+	switch (adapter->hw.device_id) {
+	case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
+		/* KSP3 does not support UCAST wake-ups */
+		wol->supported &= ~WAKE_UCAST;
+
+		if (adapter->wol & E1000_WUFC_EX)
+			DPRINTK(DRV, ERR, "Interface does not support "
+		        "directed (unicast) frame wake-up packets\n");
+		break;
+	default:
+		break;
 	}
+
+	if (adapter->wol & E1000_WUFC_EX)
+		wol->wolopts |= WAKE_UCAST;
+	if (adapter->wol & E1000_WUFC_MC)
+		wol->wolopts |= WAKE_MCAST;
+	if (adapter->wol & E1000_WUFC_BC)
+		wol->wolopts |= WAKE_BCAST;
+	if (adapter->wol & E1000_WUFC_MAG)
+		wol->wolopts |= WAKE_MAGIC;
+
+	return;
 }
 
-static int
-e1000_set_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)
+static int e1000_set_wol(struct net_device *netdev,
+                         struct ethtool_wolinfo *wol)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
 
-	switch(adapter->hw.device_id) {
-	case E1000_DEV_ID_82542:
-	case E1000_DEV_ID_82543GC_FIBER:
-	case E1000_DEV_ID_82543GC_COPPER:
-	case E1000_DEV_ID_82544EI_FIBER:
-	case E1000_DEV_ID_82546EB_QUAD_COPPER:
-		return wol->wolopts ? -EOPNOTSUPP : 0;
+	if (wol->wolopts & (WAKE_PHY | WAKE_ARP | WAKE_MAGICSECURE))
+		return -EOPNOTSUPP;
 
-	case E1000_DEV_ID_82546EB_FIBER:
-	case E1000_DEV_ID_82546GB_FIBER:
-		/* Wake events only supported on port A for dual fiber */
-		if(E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)
-			return wol->wolopts ? -EOPNOTSUPP : 0;
-		/* Fall Through */
+	if (e1000_wol_exclusion(adapter, wol))
+		return wol->wolopts ? -EOPNOTSUPP : 0;
 
-	default:
-		if(wol->wolopts & (WAKE_PHY | WAKE_ARP | WAKE_MAGICSECURE))
+	switch (hw->device_id) {
+	case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
+		if (wol->wolopts & WAKE_UCAST) {
+			DPRINTK(DRV, ERR, "Interface does not support "
+		        "directed (unicast) frame wake-up packets\n");
 			return -EOPNOTSUPP;
+		}
+		break;
+	default:
+		break;
+	}
 
-		adapter->wol = 0;
+	/* these settings will always override what we currently have */
+	adapter->wol = 0;
 
-		if(wol->wolopts & WAKE_UCAST)
-			adapter->wol |= E1000_WUFC_EX;
-		if(wol->wolopts & WAKE_MCAST)
-			adapter->wol |= E1000_WUFC_MC;
-		if(wol->wolopts & WAKE_BCAST)
-			adapter->wol |= E1000_WUFC_BC;
-		if(wol->wolopts & WAKE_MAGIC)
-			adapter->wol |= E1000_WUFC_MAG;
-	}
+	if (wol->wolopts & WAKE_UCAST)
+		adapter->wol |= E1000_WUFC_EX;
+	if (wol->wolopts & WAKE_MCAST)
+		adapter->wol |= E1000_WUFC_MC;
+	if (wol->wolopts & WAKE_BCAST)
+		adapter->wol |= E1000_WUFC_BC;
+	if (wol->wolopts & WAKE_MAGIC)
+		adapter->wol |= E1000_WUFC_MAG;
 
 	return 0;
 }
@@ -1519,12 +1925,11 @@
 /* bit defines for adapter->led_status */
 #define E1000_LED_ON		0
 
-static void
-e1000_led_blink_callback(unsigned long data)
+static void e1000_led_blink_callback(unsigned long data)
 {
 	struct e1000_adapter *adapter = (struct e1000_adapter *) data;
 
-	if(test_and_change_bit(E1000_LED_ON, &adapter->led_status))
+	if (test_and_change_bit(E1000_LED_ON, &adapter->led_status))
 		e1000_led_off(&adapter->hw);
 	else
 		e1000_led_on(&adapter->hw);
@@ -1532,27 +1937,38 @@
 	mod_timer(&adapter->blink_timer, jiffies + E1000_ID_INTERVAL);
 }
 
-static int
-e1000_phys_id(struct net_device *netdev, uint32_t data)
+static int e1000_phys_id(struct net_device *netdev, u32 data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	if(!data || data > (uint32_t)(MAX_SCHEDULE_TIMEOUT / HZ))
-		data = (uint32_t)(MAX_SCHEDULE_TIMEOUT / HZ);
+	if (!data || data > (u32)(MAX_SCHEDULE_TIMEOUT / HZ))
+		data = (u32)(MAX_SCHEDULE_TIMEOUT / HZ);
 
-	if(!adapter->blink_timer.function) {
-		init_timer(&adapter->blink_timer);
-		adapter->blink_timer.function = e1000_led_blink_callback;
-		adapter->blink_timer.data = (unsigned long) adapter;
+	if (adapter->hw.mac.type < e1000_82571) {
+		if (!adapter->blink_timer.function) {
+			init_timer(&adapter->blink_timer);
+			adapter->blink_timer.function = e1000_led_blink_callback;
+			adapter->blink_timer.data = (unsigned long) adapter;
+		}
+		e1000_setup_led(&adapter->hw);
+		mod_timer(&adapter->blink_timer, jiffies);
+		msleep_interruptible(data * 1000);
+		del_timer_sync(&adapter->blink_timer);
+	} else if (adapter->hw.phy.type == e1000_phy_ife) {
+		if (!adapter->blink_timer.function) {
+			init_timer(&adapter->blink_timer);
+			adapter->blink_timer.function = e1000_led_blink_callback;
+			adapter->blink_timer.data = (unsigned long) adapter;
+		}
+		mod_timer(&adapter->blink_timer, jiffies);
+		msleep_interruptible(data * 1000);
+		del_timer_sync(&adapter->blink_timer);
+		e1000_write_phy_reg(&(adapter->hw), IFE_PHY_SPECIAL_CONTROL_LED, 0);
+	} else {
+		e1000_blink_led(&adapter->hw);
+		msleep_interruptible(data * 1000);
 	}
 
-	e1000_setup_led(&adapter->hw);
-	mod_timer(&adapter->blink_timer, jiffies);
-
-	set_current_state(TASK_INTERRUPTIBLE);
-
-	schedule_timeout(data * HZ);
-	del_timer_sync(&adapter->blink_timer);
 	e1000_led_off(&adapter->hw);
 	clear_bit(E1000_LED_ON, &adapter->led_status);
 	e1000_cleanup_led(&adapter->hw);
@@ -1560,59 +1976,101 @@
 	return 0;
 }
 
-static int
-e1000_nway_reset(struct net_device *netdev)
+static int e1000_nway_reset(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	if(netif_running(netdev)) {
-		e1000_down(adapter);
-		e1000_up(adapter);
-	}
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	if (netif_running(netdev))
+		e1000_reinit_locked(adapter);
 	return 0;
 }
 
-static int 
-e1000_get_stats_count(struct net_device *netdev)
+static int e1000_get_stats_count(struct net_device *netdev)
 {
 	return E1000_STATS_LEN;
 }
 
-static void 
-e1000_get_ethtool_stats(struct net_device *netdev, 
-		struct ethtool_stats *stats, uint64_t *data)
+static void e1000_get_ethtool_stats(struct net_device *netdev,
+                                    struct ethtool_stats *stats, u64 *data)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+#ifdef CONFIG_E1000_MQ
+	u64 *queue_stat;
+	int stat_count = sizeof(struct e1000_queue_stats) / sizeof(u64);
+	int j, k;
+#endif
 	int i;
 
 	e1000_update_stats(adapter);
-	for(i = 0; i < E1000_STATS_LEN; i++) {
-		char *p = (char *)adapter+e1000_gstrings_stats[i].stat_offset;	
-		data[i] = (e1000_gstrings_stats[i].sizeof_stat == 
-			sizeof(uint64_t)) ? *(uint64_t *)p : *(uint32_t *)p;
+	for (i = 0; i < E1000_GLOBAL_STATS_LEN; i++) {
+		char *p = (char *)adapter+e1000_gstrings_stats[i].stat_offset;
+		data[i] = (e1000_gstrings_stats[i].sizeof_stat ==
+			sizeof(u64)) ? *(u64 *)p : *(u32 *)p;
+	}
+#ifdef CONFIG_E1000_MQ
+	if (adapter->num_tx_queues > 1) {
+		for (j = 0; j < adapter->num_tx_queues; j++) {
+			queue_stat = (u64 *)&adapter->tx_ring[j].tx_stats;
+			for (k = 0; k < stat_count; k++)
+				data[i + k] = queue_stat[k];
+			i += k;
+		}
 	}
+	if (adapter->num_rx_queues > 1) {
+		for (j = 0; j < adapter->num_rx_queues; j++) {
+			queue_stat = (u64 *)&adapter->rx_ring[j].rx_stats;
+			for (k = 0; k < stat_count; k++)
+				data[i + k] = queue_stat[k];
+			i += k;
+		}
+	}
+#endif
+/*	BUG_ON(i != E1000_STATS_LEN); */
 }
 
-static void 
-e1000_get_strings(struct net_device *netdev, uint32_t stringset, uint8_t *data)
+static void e1000_get_strings(struct net_device *netdev, u32 stringset,
+                              u8 *data)
 {
+#ifdef CONFIG_E1000_MQ
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+#endif
+	u8 *p = data;
 	int i;
 
-	switch(stringset) {
+	switch (stringset) {
 	case ETH_SS_TEST:
-		memcpy(data, *e1000_gstrings_test, 
+		memcpy(data, *e1000_gstrings_test,
 			E1000_TEST_LEN*ETH_GSTRING_LEN);
 		break;
 	case ETH_SS_STATS:
-		for (i=0; i < E1000_STATS_LEN; i++) {
-			memcpy(data + i * ETH_GSTRING_LEN, 
-			e1000_gstrings_stats[i].stat_string,
-			ETH_GSTRING_LEN);
+		for (i = 0; i < E1000_GLOBAL_STATS_LEN; i++) {
+			memcpy(p, e1000_gstrings_stats[i].stat_string,
+			       ETH_GSTRING_LEN);
+			p += ETH_GSTRING_LEN;
 		}
+#ifdef CONFIG_E1000_MQ
+		if (adapter->num_tx_queues > 1) {
+			for (i = 0; i < adapter->num_tx_queues; i++) {
+				sprintf(p, "tx_queue_%u_packets", i);
+				p += ETH_GSTRING_LEN;
+				sprintf(p, "tx_queue_%u_bytes", i);
+				p += ETH_GSTRING_LEN;
+			}
+		}
+		if (adapter->num_rx_queues > 1) {
+			for (i = 0; i < adapter->num_rx_queues; i++) {
+				sprintf(p, "rx_queue_%u_packets", i);
+				p += ETH_GSTRING_LEN;
+				sprintf(p, "rx_queue_%u_bytes", i);
+				p += ETH_GSTRING_LEN;
+			}
+		}
+#endif
+/*		BUG_ON(p - data != E1000_STATS_LEN * ETH_GSTRING_LEN); */
 		break;
 	}
 }
 
-struct ethtool_ops e1000_ethtool_ops = {
+static struct ethtool_ops e1000_ethtool_ops = {
 	.get_settings           = e1000_get_settings,
 	.set_settings           = e1000_set_settings,
 	.get_drvinfo            = e1000_get_drvinfo,
@@ -1620,8 +2078,8 @@
 	.get_regs               = e1000_get_regs,
 	.get_wol                = e1000_get_wol,
 	.set_wol                = e1000_set_wol,
-	.get_msglevel	        = e1000_get_msglevel,
-	.set_msglevel	        = e1000_set_msglevel,
+	.get_msglevel           = e1000_get_msglevel,
+	.set_msglevel           = e1000_set_msglevel,
 	.nway_reset             = e1000_nway_reset,
 	.get_link               = ethtool_op_get_link,
 	.get_eeprom_len         = e1000_get_eeprom_len,
@@ -1629,17 +2087,17 @@
 	.set_eeprom             = e1000_set_eeprom,
 	.get_ringparam          = e1000_get_ringparam,
 	.set_ringparam          = e1000_set_ringparam,
-	.get_pauseparam		= e1000_get_pauseparam,
-	.set_pauseparam		= e1000_set_pauseparam,
-	.get_rx_csum		= e1000_get_rx_csum,
-	.set_rx_csum		= e1000_set_rx_csum,
-	.get_tx_csum		= e1000_get_tx_csum,
-	.set_tx_csum		= e1000_set_tx_csum,
-	.get_sg			= ethtool_op_get_sg,
-	.set_sg			= ethtool_op_set_sg,
+	.get_pauseparam         = e1000_get_pauseparam,
+	.set_pauseparam         = e1000_set_pauseparam,
+	.get_rx_csum            = e1000_get_rx_csum,
+	.set_rx_csum            = e1000_set_rx_csum,
+	.get_tx_csum            = e1000_get_tx_csum,
+	.set_tx_csum            = e1000_set_tx_csum,
+	.get_sg                 = ethtool_op_get_sg,
+	.set_sg                 = ethtool_op_set_sg,
 #ifdef NETIF_F_TSO
-	.get_tso		= ethtool_op_get_tso,
-	.set_tso		= e1000_set_tso,
+	.get_tso                = ethtool_op_get_tso,
+	.set_tso                = e1000_set_tso,
 #endif
 	.self_test_count        = e1000_diag_test_count,
 	.self_test              = e1000_diag_test,
@@ -1647,9 +2105,13 @@
 	.phys_id                = e1000_phys_id,
 	.get_stats_count        = e1000_get_stats_count,
 	.get_ethtool_stats      = e1000_get_ethtool_stats,
+#ifdef ETHTOOL_GPERMADDR
+	.get_perm_addr          = ethtool_op_get_perm_addr,
+#endif
 };
 
-void set_ethtool_ops(struct net_device *netdev)
+void e1000_set_ethtool_ops(struct net_device *netdev)
 {
 	SET_ETHTOOL_OPS(netdev, &e1000_ethtool_ops);
 }
+#endif	/* SIOCETHTOOL */
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000.h linux-2.6.9/drivers/net/e1000/e1000.h
--- linux-2.6.9.src/drivers/net/e1000/e1000.h	2004-10-18 23:53:06.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000.h	2007-07-16 13:33:15.000000000 +0200
@@ -1,27 +1,27 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
@@ -32,48 +32,9 @@
 #ifndef _E1000_H_
 #define _E1000_H_
 
-#include <linux/stddef.h>
-#include <linux/config.h>
-#include <linux/module.h>
-#include <linux/types.h>
-#include <asm/byteorder.h>
-#include <linux/init.h>
-#include <linux/mm.h>
-#include <linux/errno.h>
-#include <linux/ioport.h>
-#include <linux/pci.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/etherdevice.h>
-#include <linux/skbuff.h>
-#include <linux/delay.h>
-#include <linux/timer.h>
-#include <linux/slab.h>
-#include <linux/vmalloc.h>
-#include <linux/interrupt.h>
-#include <linux/string.h>
-#include <linux/pagemap.h>
-#include <linux/dma-mapping.h>
-#include <asm/bitops.h>
-#include <asm/io.h>
-#include <asm/irq.h>
-#include <linux/capability.h>
-#include <linux/in.h>
-#include <linux/ip.h>
-#include <linux/tcp.h>
-#include <linux/udp.h>
-#include <net/pkt_sched.h>
-#include <linux/list.h>
-#include <linux/rtnetlink.h>
-#include <linux/reboot.h>
-#ifdef NETIF_F_TSO
-#include <net/checksum.h>
-#endif
-#include <linux/workqueue.h>
-#include <linux/mii.h>
-#include <linux/ethtool.h>
-#include <linux/if_vlan.h>
-#include <linux/moduleparam.h>
+#include "kcompat.h"
+
+#include "e1000_api.h"
 
 #define BAR_0		0
 #define BAR_1		1
@@ -84,13 +45,7 @@
 
 struct e1000_adapter;
 
-#include "e1000_hw.h"
-
-#ifdef DBG
-#define E1000_DBG(args...) printk(KERN_DEBUG "e1000: " args)
-#else
 #define E1000_DBG(args...)
-#endif
 
 #define E1000_ERR(args...) printk(KERN_ERR "e1000: " args)
 
@@ -113,7 +68,18 @@
 #define E1000_MIN_RXD                       80
 #define E1000_MAX_82544_RXD               4096
 
+#ifdef CONFIG_E1000_MQ
+#define E1000_MAX_TX_QUEUES                  4
+#endif
+
+/* this is the size past which hardware will drop packets when setting LPE=0 */
+#define MAXIMUM_ETHERNET_VLAN_SIZE 1522
+
 /* Supported Rx Buffer Sizes */
+#define E1000_RXBUFFER_128   128    /* Used for packet split */
+#define E1000_RXBUFFER_256   256    /* Used for packet split */
+#define E1000_RXBUFFER_512   512
+#define E1000_RXBUFFER_1024  1024
 #define E1000_RXBUFFER_2048  2048
 #define E1000_RXBUFFER_4096  4096
 #define E1000_RXBUFFER_8192  8192
@@ -128,9 +94,8 @@
 #define E1000_TX_HEAD_ADDR_SHIFT 7
 #define E1000_PBA_TX_MASK 0xFFFF0000
 
-/* Flow Control Watermarks */
-#define E1000_FC_HIGH_DIFF 0x1638  /* High: 5688 bytes below Rx FIFO size */
-#define E1000_FC_LOW_DIFF 0x1640   /* Low:  5696 bytes below Rx FIFO size */
+/* Early Receive defines */
+#define E1000_ERT_2048 0x100
 
 #define E1000_FC_PAUSE_TIME 0x0680 /* 858 usec */
 
@@ -139,28 +104,48 @@
 /* How many Rx Buffers do we bundle into one write to the hardware ? */
 #define E1000_RX_BUFFER_WRITE	16	/* Must be power of 2 */
 
-#define AUTO_ALL_MODES       0
-#define E1000_EEPROM_APME    0x0400
+#define AUTO_ALL_MODES            0
+#define E1000_EEPROM_82544_APM    0x0004
+#define E1000_EEPROM_APME         0x0400
 
 #ifndef E1000_MASTER_SLAVE
 /* Switch to override PHY master/slave setting */
 #define E1000_MASTER_SLAVE	e1000_ms_hw_default
 #endif
 
-/* only works for sizes that are powers of 2 */
-#define E1000_ROUNDUP(i, size) ((i) = (((i) + (size) - 1) & ~((size) - 1)))
+#ifdef NETIF_F_HW_VLAN_TX
+#define E1000_MNG_VLAN_NONE -1
+#endif
+/* Number of packet split data buffers (not including the header buffer) */
+#define PS_PAGE_BUFFERS MAX_PS_BUFFERS-1
 
 /* wrapper around a pointer to a socket buffer,
  * so a DMA handle can be stored along with the buffer */
 struct e1000_buffer {
 	struct sk_buff *skb;
-	uint64_t dma;
+	dma_addr_t dma;
 	unsigned long time_stamp;
-	uint16_t length;
-	uint16_t next_to_watch;
+	u16 length;
+	u16 next_to_watch;
 };
 
-struct e1000_desc_ring {
+struct e1000_rx_buffer {
+	struct sk_buff *skb;
+	dma_addr_t dma;
+	struct page *page;
+};
+	
+#ifdef CONFIG_E1000_MQ
+struct e1000_queue_stats {
+	u64 packets;
+	u64 bytes;
+};
+#endif
+
+struct e1000_ps_page { struct page *ps_page[PS_PAGE_BUFFERS]; };
+struct e1000_ps_page_dma { u64 ps_page_dma[PS_PAGE_BUFFERS]; };
+
+struct e1000_tx_ring {
 	/* pointer to the descriptor ring memory */
 	void *desc;
 	/* physical address of the descriptor ring */
@@ -175,12 +160,58 @@
 	unsigned int next_to_clean;
 	/* array of buffer information structs */
 	struct e1000_buffer *buffer_info;
+
+#ifdef CONFIG_E1000_MQ
+	/* for tx ring cleanup - needed for multiqueue */
+	spinlock_t tx_queue_lock;
+#endif
+	spinlock_t tx_lock;
+	u16 tdh;
+	u16 tdt;
+#ifdef CONFIG_E1000_MQ
+	struct e1000_queue_stats tx_stats;
+#endif
+	boolean_t last_tx_tso;
+};
+
+struct e1000_rx_ring {
+	/* pointer to the descriptor ring memory */
+	void *desc;
+	/* physical address of the descriptor ring */
+	dma_addr_t dma;
+	/* length of descriptor ring in bytes */
+	unsigned int size;
+	/* number of descriptors in the ring */
+	unsigned int count;
+	/* next descriptor to associate a buffer with */
+	unsigned int next_to_use;
+	/* next descriptor to check for DD status bit */
+	unsigned int next_to_clean;
+	/* array of buffer information structs */
+	struct e1000_rx_buffer *buffer_info;
+	/* arrays of page information for packet split */
+	struct e1000_ps_page *ps_page;
+	struct e1000_ps_page_dma *ps_page_dma;
+	struct sk_buff *rx_skb_top;
+
+	/* cpu for rx queue */
+	int cpu;
+
+	u16 rdh;
+	u16 rdt;
+#ifdef CONFIG_E1000_MQ
+	struct e1000_queue_stats rx_stats;
+#endif
 };
 
 #define E1000_DESC_UNUSED(R) \
 	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
 	(R)->next_to_clean - (R)->next_to_use - 1)
 
+#define E1000_RX_DESC_PS(R, i)	    \
+	(&(((union e1000_rx_desc_packet_split *)((R).desc))[i]))
+#define E1000_RX_DESC_EXT(R, i)	    \
+	(&(((union e1000_rx_desc_extended *)((R).desc))[i]))
 #define E1000_GET_DESC(R, i, type)	(&(((struct type *)((R).desc))[i]))
 #define E1000_RX_DESC(R, i)		E1000_GET_DESC(R, i, e1000_rx_desc)
 #define E1000_TX_DESC(R, i)		E1000_GET_DESC(R, i, e1000_tx_desc)
@@ -192,51 +223,95 @@
 	struct timer_list tx_fifo_stall_timer;
 	struct timer_list watchdog_timer;
 	struct timer_list phy_info_timer;
+#ifdef NETIF_F_HW_VLAN_TX
 	struct vlan_group *vlgrp;
-	uint32_t bd_number;
-	uint32_t rx_buffer_len;
-	uint32_t part_num;
-	uint32_t wol;
-	uint32_t smartspeed;
-	uint32_t en_mng_pt;
-	uint16_t link_speed;
-	uint16_t link_duplex;
+	u16 mng_vlan_id;
+#endif
+	u32 bd_number;
+	u32 rx_buffer_len;
+	u32 wol;
+	u32 smartspeed;
+	u32 en_mng_pt;
+	u16 link_speed;
+	u16 link_duplex;
 	spinlock_t stats_lock;
+#ifdef CONFIG_E1000_NAPI
+	spinlock_t tx_queue_lock;
+#endif
 	atomic_t irq_sem;
-	struct work_struct tx_timeout_task;
-	uint8_t fc_autoneg;
+	unsigned int total_tx_bytes;
+	unsigned int total_tx_packets;
+	unsigned int total_rx_bytes;
+	unsigned int total_rx_packets;
+	/* Interrupt Throttle Rate */
+	u32 itr;
+	u32 itr_setting;
+	u16 tx_itr;
+	u16 rx_itr;
+
+	struct work_struct reset_task;
+	struct work_struct watchdog_task;
+	u8 fc_autoneg;
 
+#ifdef ETHTOOL_PHYS_ID
 	struct timer_list blink_timer;
 	unsigned long led_status;
+#endif
 
 	/* TX */
-	struct e1000_desc_ring tx_ring;
-	spinlock_t tx_lock;
-	uint32_t txd_cmd;
-	uint32_t tx_int_delay;
-	uint32_t tx_abs_int_delay;
-	uint32_t gotcl;
-	uint64_t gotcl_old;
-	uint64_t tpt_old;
-	uint64_t colc_old;
-	uint32_t tx_fifo_head;
-	uint32_t tx_head_addr;
-	uint32_t tx_fifo_size;
+	struct e1000_tx_ring *tx_ring;      /* One per active queue */
+#ifdef CONFIG_E1000_MQ
+	struct e1000_tx_ring **cpu_tx_ring; /* per-cpu */
+#endif
+	unsigned int restart_queue;
+	unsigned long tx_queue_len;
+	u32 txd_cmd;
+	u32 tx_int_delay;
+	u32 tx_abs_int_delay;
+	u32 gotcl;
+	u64 gotcl_old;
+	u64 tpt_old;
+	u64 colc_old;
+	u32 tx_timeout_count;
+	u32 tx_fifo_head;
+	u32 tx_head_addr;
+	u32 tx_fifo_size;
+	u8 tx_timeout_factor;
 	atomic_t tx_fifo_stall;
 	boolean_t pcix_82544;
+	boolean_t detect_tx_hung;
 
 	/* RX */
-	struct e1000_desc_ring rx_ring;
-	uint64_t hw_csum_err;
-	uint64_t hw_csum_good;
-	uint32_t rx_int_delay;
-	uint32_t rx_abs_int_delay;
+#ifdef CONFIG_E1000_NAPI
+	boolean_t (*clean_rx) (struct e1000_adapter *adapter,
+			       struct e1000_rx_ring *rx_ring,
+			       int *work_done, int work_to_do);
+#else
+	boolean_t (*clean_rx) (struct e1000_adapter *adapter,
+			       struct e1000_rx_ring *rx_ring);
+#endif
+	void (*alloc_rx_buf) (struct e1000_adapter *adapter,
+			      struct e1000_rx_ring *rx_ring,
+				int cleaned_count);
+	struct e1000_rx_ring *rx_ring;      /* One per active queue */
+#ifdef CONFIG_E1000_NAPI
+	struct net_device *polling_netdev;  /* One per active queue */
+#endif
+	int num_tx_queues;
+	int num_rx_queues;
+
+	u64 hw_csum_err;
+	u64 hw_csum_good;
+	u64 rx_hdr_split;
+	u32 alloc_rx_buff_failed;
+	u32 rx_int_delay;
+	u32 rx_abs_int_delay;
 	boolean_t rx_csum;
-	uint32_t gorcl;
-	uint64_t gorcl_old;
+	unsigned int rx_ps_pages;
+	u32 gorcl;
+	u64 gorcl_old;
+	u16 rx_ps_bsize0;
 
-	/* Interrupt Throttle Rate */
-	uint32_t itr;
 
 	/* OS defined structs */
 	struct net_device *netdev;
@@ -249,12 +324,48 @@
 	struct e1000_phy_info phy_info;
 	struct e1000_phy_stats phy_stats;
 
-	uint32_t test_icr;
-	struct e1000_desc_ring test_tx_ring;
-	struct e1000_desc_ring test_rx_ring;
+#ifdef ETHTOOL_TEST
+	u32 test_icr;
+	struct e1000_tx_ring test_tx_ring;
+	struct e1000_rx_ring test_rx_ring;
+#endif
 
 
-	uint32_t pci_state[16];
 	int msg_enable;
+	/* to not mess up cache alignment, always add to the bottom */
+	unsigned long state;
+	u32 eeprom_wol;
+
+	u32 *config_space;
+
+	/* hardware capability, feature, and workaround flags */
+	struct {
+		unsigned int has_smbus:1;
+		unsigned int has_manc2h:1;
+#ifdef CONFIG_PCI_MSI
+		unsigned int has_msi:1;
+		unsigned int msi_enabled:1;
+#endif
+		unsigned int has_intr_moderation:1;
+		unsigned int rx_needs_restart:1;
+		unsigned int bad_tx_carrier_stats_fd:1;
+		unsigned int int_assert_auto_mask:1;
+		unsigned int quad_port_a:1;
+		unsigned int smart_power_down:1;
+#ifdef NETIF_F_TSO
+		unsigned int has_tso:1;
+#ifdef NETIF_F_TSO6
+		unsigned int has_tso6:1;
+#endif
+		unsigned int tso_force:1;
+#endif
+	} flags;
+
+};
+
+enum e1000_state_t {
+	__E1000_TESTING,
+	__E1000_RESETTING,
+	__E1000_DOWN
 };
 #endif /* _E1000_H_ */
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_hw.c linux-2.6.9/drivers/net/e1000/e1000_hw.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_hw.c	2004-10-18 23:55:28.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_hw.c	2007-07-03 11:07:40.000000000 +0200
@@ -1,7 +1,7 @@
 /*******************************************************************************
 
   
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
+  Copyright(c) 1999 - 2006 Intel Corporation. All rights reserved.
   
   This program is free software; you can redistribute it and/or modify it 
   under the terms of the GNU General Public License as published by the Free 
@@ -22,6 +22,7 @@
   
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
@@ -30,6 +31,7 @@
  * Shared functions for accessing and configuring the MAC
  */
 
+
 #include "e1000_hw.h"
 
 static int32_t e1000_set_phy_type(struct e1000_hw *hw);
@@ -63,9 +65,14 @@
 static int32_t e1000_acquire_eeprom(struct e1000_hw *hw);
 static void e1000_release_eeprom(struct e1000_hw *hw);
 static void e1000_standby_eeprom(struct e1000_hw *hw);
-static int32_t e1000_id_led_init(struct e1000_hw * hw);
 static int32_t e1000_set_vco_speed(struct e1000_hw *hw);
+static int32_t e1000_polarity_reversal_workaround(struct e1000_hw *hw);
 static int32_t e1000_set_phy_mode(struct e1000_hw *hw);
+static int32_t e1000_host_if_read_cookie(struct e1000_hw *hw, uint8_t *buffer);
+static uint8_t e1000_calculate_mng_checksum(char *buffer, uint32_t length);
+static int32_t e1000_configure_kmrn_for_10_100(struct e1000_hw *hw,
+                                               uint16_t duplex);
+static int32_t e1000_configure_kmrn_for_1000(struct e1000_hw *hw);
 
 /* IGP cable length table */
 static const
@@ -79,6 +86,17 @@
       100, 100, 100, 100, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,
       110, 110, 110, 110, 110, 110, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120};
 
+static const
+uint16_t e1000_igp_2_cable_length_table[IGP02E1000_AGC_LENGTH_TABLE_SIZE] =
+    { 0, 0, 0, 0, 0, 0, 0, 0, 3, 5, 8, 11, 13, 16, 18, 21,
+      0, 0, 0, 3, 6, 10, 13, 16, 19, 23, 26, 29, 32, 35, 38, 41,
+      6, 10, 14, 18, 22, 26, 30, 33, 37, 41, 44, 48, 51, 54, 58, 61,
+      21, 26, 31, 35, 40, 44, 49, 53, 57, 61, 65, 68, 72, 75, 79, 82,
+      40, 45, 51, 56, 61, 66, 70, 75, 79, 83, 87, 91, 94, 98, 101, 104,
+      60, 66, 72, 77, 82, 87, 92, 96, 100, 104, 108, 111, 114, 117, 119, 121,
+      83, 89, 95, 100, 105, 109, 113, 116, 119, 122, 124,
+      104, 109, 114, 118, 121, 124};
+
 
 /******************************************************************************
  * Set the phy type member in the hw struct.
@@ -90,20 +108,37 @@
 {
     DEBUGFUNC("e1000_set_phy_type");
 
-    switch(hw->phy_id) {
+    if (hw->mac_type == e1000_undefined)
+        return -E1000_ERR_PHY_TYPE;
+
+    switch (hw->phy_id) {
     case M88E1000_E_PHY_ID:
     case M88E1000_I_PHY_ID:
     case M88E1011_I_PHY_ID:
+    case M88E1111_I_PHY_ID:
         hw->phy_type = e1000_phy_m88;
         break;
     case IGP01E1000_I_PHY_ID:
-        if(hw->mac_type == e1000_82541 ||
-           hw->mac_type == e1000_82541_rev_2 ||
-           hw->mac_type == e1000_82547 ||
-           hw->mac_type == e1000_82547_rev_2) {
+        if (hw->mac_type == e1000_82541 ||
+            hw->mac_type == e1000_82541_rev_2 ||
+            hw->mac_type == e1000_82547 ||
+            hw->mac_type == e1000_82547_rev_2) {
             hw->phy_type = e1000_phy_igp;
             break;
         }
+    case IGP03E1000_E_PHY_ID:
+        hw->phy_type = e1000_phy_igp_3;
+        break;
+    case IFE_E_PHY_ID:
+    case IFE_PLUS_E_PHY_ID:
+    case IFE_C_E_PHY_ID:
+        hw->phy_type = e1000_phy_ife;
+        break;
+    case GG82563_E_PHY_ID:
+        if (hw->mac_type == e1000_80003es2lan) {
+            hw->phy_type = e1000_phy_gg82563;
+            break;
+        }
         /* Fall Through */
     default:
         /* Should never have loaded on this device */
@@ -114,6 +149,7 @@
     return E1000_SUCCESS;
 }
 
+
 /******************************************************************************
  * IGP phy init script - initializes the GbE PHY
  *
@@ -122,16 +158,30 @@
 static void
 e1000_phy_init_script(struct e1000_hw *hw)
 {
+    uint32_t ret_val;
+    uint16_t phy_saved_data;
+
     DEBUGFUNC("e1000_phy_init_script");
 
-    if(hw->phy_init_script) {
+    if (hw->phy_init_script) {
+        msec_delay(20);
+
+        /* Save off the current value of register 0x2F5B to be restored at
+         * the end of this routine. */
+        ret_val = e1000_read_phy_reg(hw, 0x2F5B, &phy_saved_data);
+
+        /* Disabled the PHY transmitter */
+        e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+
         msec_delay(20);
 
         e1000_write_phy_reg(hw,0x0000,0x0140);
 
         msec_delay(5);
 
-        if(hw->mac_type == e1000_82541 || hw->mac_type == e1000_82547) {
+        switch (hw->mac_type) {
+        case e1000_82541:
+        case e1000_82547:
             e1000_write_phy_reg(hw, 0x1F95, 0x0001);
 
             e1000_write_phy_reg(hw, 0x1F71, 0xBD21);
@@ -149,28 +199,39 @@
             e1000_write_phy_reg(hw, 0x1F96, 0x003F);
 
             e1000_write_phy_reg(hw, 0x2010, 0x0008);
-        } else {
+            break;
+
+        case e1000_82541_rev_2:
+        case e1000_82547_rev_2:
             e1000_write_phy_reg(hw, 0x1F73, 0x0099);
+            break;
+        default:
+            break;
         }
 
         e1000_write_phy_reg(hw, 0x0000, 0x3300);
 
-        if(hw->mac_type == e1000_82547) {
+        msec_delay(20);
+
+        /* Now enable the transmitter */
+        e1000_write_phy_reg(hw, 0x2F5B, phy_saved_data);
+
+        if (hw->mac_type == e1000_82547) {
             uint16_t fused, fine, coarse;
 
             /* Move to analog registers page */
             e1000_read_phy_reg(hw, IGP01E1000_ANALOG_SPARE_FUSE_STATUS, &fused);
 
-            if(!(fused & IGP01E1000_ANALOG_SPARE_FUSE_ENABLED)) {
+            if (!(fused & IGP01E1000_ANALOG_SPARE_FUSE_ENABLED)) {
                 e1000_read_phy_reg(hw, IGP01E1000_ANALOG_FUSE_STATUS, &fused);
 
                 fine = fused & IGP01E1000_ANALOG_FUSE_FINE_MASK;
                 coarse = fused & IGP01E1000_ANALOG_FUSE_COARSE_MASK;
 
-                if(coarse > IGP01E1000_ANALOG_FUSE_COARSE_THRESH) {
+                if (coarse > IGP01E1000_ANALOG_FUSE_COARSE_THRESH) {
                     coarse -= IGP01E1000_ANALOG_FUSE_COARSE_10;
                     fine -= IGP01E1000_ANALOG_FUSE_FINE_1;
-                } else if(coarse == IGP01E1000_ANALOG_FUSE_COARSE_THRESH)
+                } else if (coarse == IGP01E1000_ANALOG_FUSE_COARSE_THRESH)
                     fine -= IGP01E1000_ANALOG_FUSE_FINE_10;
 
                 fused = (fused & IGP01E1000_ANALOG_FUSE_POLY_MASK) |
@@ -243,10 +304,14 @@
     case E1000_DEV_ID_82546GB_COPPER:
     case E1000_DEV_ID_82546GB_FIBER:
     case E1000_DEV_ID_82546GB_SERDES:
+    case E1000_DEV_ID_82546GB_PCIE:
+    case E1000_DEV_ID_82546GB_QUAD_COPPER:
+    case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
         hw->mac_type = e1000_82546_rev_3;
         break;
     case E1000_DEV_ID_82541EI:
     case E1000_DEV_ID_82541EI_MOBILE:
+    case E1000_DEV_ID_82541ER_LOM:
         hw->mac_type = e1000_82541;
         break;
     case E1000_DEV_ID_82541ER:
@@ -256,17 +321,60 @@
         hw->mac_type = e1000_82541_rev_2;
         break;
     case E1000_DEV_ID_82547EI:
+    case E1000_DEV_ID_82547EI_MOBILE:
         hw->mac_type = e1000_82547;
         break;
     case E1000_DEV_ID_82547GI:
         hw->mac_type = e1000_82547_rev_2;
         break;
+    case E1000_DEV_ID_82571EB_COPPER:
+    case E1000_DEV_ID_82571EB_FIBER:
+    case E1000_DEV_ID_82571EB_SERDES:
+    case E1000_DEV_ID_82571EB_QUAD_COPPER:
+            hw->mac_type = e1000_82571;
+        break;
+    case E1000_DEV_ID_82572EI_COPPER:
+    case E1000_DEV_ID_82572EI_FIBER:
+    case E1000_DEV_ID_82572EI_SERDES:
+    case E1000_DEV_ID_82572EI:
+        hw->mac_type = e1000_82572;
+        break;
+    case E1000_DEV_ID_82573E:
+    case E1000_DEV_ID_82573E_IAMT:
+    case E1000_DEV_ID_82573L:
+        hw->mac_type = e1000_82573;
+        break;
+    case E1000_DEV_ID_80003ES2LAN_COPPER_SPT:
+    case E1000_DEV_ID_80003ES2LAN_SERDES_SPT:
+    case E1000_DEV_ID_80003ES2LAN_COPPER_DPT:
+    case E1000_DEV_ID_80003ES2LAN_SERDES_DPT:
+        hw->mac_type = e1000_80003es2lan;
+        break;
+    case E1000_DEV_ID_ICH8_IGP_M_AMT:
+    case E1000_DEV_ID_ICH8_IGP_AMT:
+    case E1000_DEV_ID_ICH8_IGP_C:
+    case E1000_DEV_ID_ICH8_IFE:
+    case E1000_DEV_ID_ICH8_IGP_M:
+        hw->mac_type = e1000_ich8lan;
+        break;
     default:
         /* Should never have loaded on this device */
         return -E1000_ERR_MAC_TYPE;
     }
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
+    case e1000_ich8lan:
+        hw->swfwhw_semaphore_present = TRUE;
+        hw->asf_firmware_present = TRUE;
+        break;
+    case e1000_80003es2lan:
+        hw->swfw_sync_present = TRUE;
+        /* fall through */
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_82573:
+        hw->eeprom_semaphore_present = TRUE;
+        /* fall through */
     case e1000_82541:
     case e1000_82547:
     case e1000_82541_rev_2:
@@ -292,7 +400,7 @@
 
     DEBUGFUNC("e1000_set_media_type");
 
-    if(hw->mac_type != e1000_82543) {
+    if (hw->mac_type != e1000_82543) {
         /* tbi_compatibility is only valid on 82543 */
         hw->tbi_compatibility_en = FALSE;
     }
@@ -300,21 +408,34 @@
     switch (hw->device_id) {
     case E1000_DEV_ID_82545GM_SERDES:
     case E1000_DEV_ID_82546GB_SERDES:
+    case E1000_DEV_ID_82571EB_SERDES:
+    case E1000_DEV_ID_82572EI_SERDES:
+    case E1000_DEV_ID_80003ES2LAN_SERDES_DPT:
         hw->media_type = e1000_media_type_internal_serdes;
         break;
     default:
-        if(hw->mac_type >= e1000_82543) {
+        switch (hw->mac_type) {
+        case e1000_82542_rev2_0:
+        case e1000_82542_rev2_1:
+            hw->media_type = e1000_media_type_fiber;
+            break;
+        case e1000_ich8lan:
+        case e1000_82573:
+            /* The STATUS_TBIMODE bit is reserved or reused for the this
+             * device.
+             */
+            hw->media_type = e1000_media_type_copper;
+            break;
+        default:
             status = E1000_READ_REG(hw, STATUS);
-            if(status & E1000_STATUS_TBIMODE) {
+            if (status & E1000_STATUS_TBIMODE) {
                 hw->media_type = e1000_media_type_fiber;
                 /* tbi_compatibility not valid on fiber */
                 hw->tbi_compatibility_en = FALSE;
             } else {
                 hw->media_type = e1000_media_type_copper;
             }
-        } else {
-            /* This is an 82542 (fiber only) */
-            hw->media_type = e1000_media_type_fiber;
+            break;
         }
     }
 }
@@ -332,15 +453,27 @@
     uint32_t icr;
     uint32_t manc;
     uint32_t led_ctrl;
+    uint32_t timeout;
+    uint32_t extcnf_ctrl;
+    int32_t ret_val;
 
     DEBUGFUNC("e1000_reset_hw");
 
     /* For 82542 (rev 2.0), disable MWI before issuing a device reset */
-    if(hw->mac_type == e1000_82542_rev2_0) {
+    if (hw->mac_type == e1000_82542_rev2_0) {
         DEBUGOUT("Disabling MWI on 82542 rev 2.0\n");
         e1000_pci_clear_mwi(hw);
     }
 
+    if (hw->bus_type == e1000_bus_type_pci_express) {
+        /* Prevent the PCI-E bus from sticking if there is no TLP connection
+         * on the last TLP read/write transaction when MAC is reset.
+         */
+        if (e1000_disable_pciex_master(hw) != E1000_SUCCESS) {
+            DEBUGOUT("PCI-E Master disable polling has failed.\n");
+        }
+    }
+
     /* Clear interrupt mask to stop board from generating interrupts */
     DEBUGOUT("Masking off all interrupts\n");
     E1000_WRITE_REG(hw, IMC, 0xffffffff);
@@ -364,11 +497,41 @@
     ctrl = E1000_READ_REG(hw, CTRL);
 
     /* Must reset the PHY before resetting the MAC */
-    if((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
-        E1000_WRITE_REG_IO(hw, CTRL, (ctrl | E1000_CTRL_PHY_RST));
+    if ((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
+        E1000_WRITE_REG(hw, CTRL, (ctrl | E1000_CTRL_PHY_RST));
         msec_delay(5);
     }
 
+    /* Must acquire the MDIO ownership before MAC reset.
+     * Ownership defaults to firmware after a reset. */
+    if (hw->mac_type == e1000_82573) {
+        timeout = 10;
+
+        extcnf_ctrl = E1000_READ_REG(hw, EXTCNF_CTRL);
+        extcnf_ctrl |= E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP;
+
+        do {
+            E1000_WRITE_REG(hw, EXTCNF_CTRL, extcnf_ctrl);
+            extcnf_ctrl = E1000_READ_REG(hw, EXTCNF_CTRL);
+
+            if (extcnf_ctrl & E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP)
+                break;
+            else
+                extcnf_ctrl |= E1000_EXTCNF_CTRL_MDIO_SW_OWNERSHIP;
+
+            msec_delay(2);
+            timeout--;
+        } while (timeout);
+    }
+
+    /* Workaround for ICH8 bit corruption issue in FIFO memory */
+    if (hw->mac_type == e1000_ich8lan) {
+        /* Set Tx and Rx buffer allocation to 8k apiece. */
+        E1000_WRITE_REG(hw, PBA, E1000_PBA_8K);
+        /* Set Packet Buffer Size to 16k. */
+        E1000_WRITE_REG(hw, PBS, E1000_PBS_16K);
+    }
+
     /* Issue a global reset to the MAC.  This will reset the chip's
      * transmit, receive, DMA, and link units.  It will not effect
      * the current PCI configuration.  The global reset bit is self-
@@ -376,7 +539,7 @@
      */
     DEBUGOUT("Issuing a global reset to MAC\n");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
         case e1000_82544:
         case e1000_82540:
         case e1000_82545:
@@ -392,6 +555,20 @@
             /* Reset is performed on a shadow of the control register */
             E1000_WRITE_REG(hw, CTRL_DUP, (ctrl | E1000_CTRL_RST));
             break;
+        case e1000_ich8lan:
+            if (!hw->phy_reset_disable &&
+                e1000_check_phy_reset_block(hw) == E1000_SUCCESS) {
+                /* e1000_ich8lan PHY HW reset requires MAC CORE reset
+                 * at the same time to make sure the interface between
+                 * MAC and the external PHY is reset.
+                 */
+                ctrl |= E1000_CTRL_PHY_RST;
+            }
+
+            e1000_get_software_flag(hw);
+            E1000_WRITE_REG(hw, CTRL, (ctrl | E1000_CTRL_RST));
+            msec_delay(5);
+            break;
         default:
             E1000_WRITE_REG(hw, CTRL, (ctrl | E1000_CTRL_RST));
             break;
@@ -401,13 +578,13 @@
      * device.  Later controllers reload the EEPROM automatically, so just wait
      * for reload to complete.
      */
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
         case e1000_82542_rev2_0:
         case e1000_82542_rev2_1:
         case e1000_82543:
         case e1000_82544:
             /* Wait for reset to complete */
-            udelay(10);
+            usec_delay(10);
             ctrl_ext = E1000_READ_REG(hw, CTRL_EXT);
             ctrl_ext |= E1000_CTRL_EXT_EE_RST;
             E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
@@ -422,6 +599,24 @@
             /* Wait for EEPROM reload */
             msec_delay(20);
             break;
+        case e1000_82573:
+            if (e1000_is_onboard_nvm_eeprom(hw) == FALSE) {
+                usec_delay(10);
+                ctrl_ext = E1000_READ_REG(hw, CTRL_EXT);
+                ctrl_ext |= E1000_CTRL_EXT_EE_RST;
+                E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
+                E1000_WRITE_FLUSH(hw);
+            }
+            /* fall through */
+        case e1000_82571:
+        case e1000_82572:
+        case e1000_ich8lan:
+        case e1000_80003es2lan:
+            ret_val = e1000_get_auto_rd_done(hw);
+            if (ret_val)
+                /* We don't want to continue accessing MAC registers. */
+                return ret_val;
+            break;
         default:
             /* Wait for EEPROM reload (it happens automatically) */
             msec_delay(5);
@@ -429,13 +624,13 @@
     }
 
     /* Disable HW ARPs on ASF enabled adapters */
-    if(hw->mac_type >= e1000_82540) {
+    if (hw->mac_type >= e1000_82540 && hw->mac_type <= e1000_82547_rev_2) {
         manc = E1000_READ_REG(hw, MANC);
         manc &= ~(E1000_MANC_ARP_EN);
         E1000_WRITE_REG(hw, MANC, manc);
     }
 
-    if((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
+    if ((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
         e1000_phy_init_script(hw);
 
         /* Configure activity LED after PHY reset */
@@ -453,11 +648,17 @@
     icr = E1000_READ_REG(hw, ICR);
 
     /* If MWI was previously enabled, reenable it. */
-    if(hw->mac_type == e1000_82542_rev2_0) {
-        if(hw->pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
+    if (hw->mac_type == e1000_82542_rev2_0) {
+        if (hw->pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
             e1000_pci_set_mwi(hw);
     }
 
+    if (hw->mac_type == e1000_ich8lan) {
+        uint32_t kab = E1000_READ_REG(hw, KABGTXD);
+        kab |= E1000_KABGTXD_BGSQLBIAS;
+        E1000_WRITE_REG(hw, KABGTXD, kab);
+    }
+
     return E1000_SUCCESS;
 }
 
@@ -482,11 +683,25 @@
     uint16_t pcix_stat_hi_word;
     uint16_t cmd_mmrbc;
     uint16_t stat_mmrbc;
+    uint32_t mta_size;
+    uint32_t reg_data;
+    uint32_t ctrl_ext;
+
     DEBUGFUNC("e1000_init_hw");
 
+    if (hw->mac_type == e1000_ich8lan) {
+        reg_data = E1000_READ_REG(hw, TARC0);
+        reg_data |= 0x30000000;
+        E1000_WRITE_REG(hw, TARC0, reg_data);
+
+        reg_data = E1000_READ_REG(hw, STATUS);
+        reg_data &= ~0x80000000;
+        E1000_WRITE_REG(hw, STATUS, reg_data);
+    }
+
     /* Initialize Identification LED */
     ret_val = e1000_id_led_init(hw);
-    if(ret_val) {
+    if (ret_val) {
         DEBUGOUT("Error Initializing Identification LED\n");
         return ret_val;
     }
@@ -496,12 +711,15 @@
 
     /* Disabling VLAN filtering. */
     DEBUGOUT("Initializing the IEEE VLAN\n");
-    E1000_WRITE_REG(hw, VET, 0);
-
-    e1000_clear_vfta(hw);
+    /* VET hardcoded to standard value and VFTA removed in ICH8 LAN */
+    if (hw->mac_type != e1000_ich8lan) {
+        if (hw->mac_type < e1000_82545_rev_3)
+            E1000_WRITE_REG(hw, VET, 0);
+        e1000_clear_vfta(hw);
+    }
 
     /* For 82542 (rev 2.0), disable MWI and put the receiver into reset */
-    if(hw->mac_type == e1000_82542_rev2_0) {
+    if (hw->mac_type == e1000_82542_rev2_0) {
         DEBUGOUT("Disabling MWI on 82542 rev 2.0\n");
         e1000_pci_clear_mwi(hw);
         E1000_WRITE_REG(hw, RCTL, E1000_RCTL_RST);
@@ -515,35 +733,43 @@
     e1000_init_rx_addrs(hw);
 
     /* For 82542 (rev 2.0), take the receiver out of reset and enable MWI */
-    if(hw->mac_type == e1000_82542_rev2_0) {
+    if (hw->mac_type == e1000_82542_rev2_0) {
         E1000_WRITE_REG(hw, RCTL, 0);
         E1000_WRITE_FLUSH(hw);
         msec_delay(1);
-        if(hw->pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
+        if (hw->pci_cmd_word & CMD_MEM_WRT_INVALIDATE)
             e1000_pci_set_mwi(hw);
     }
 
     /* Zero out the Multicast HASH table */
     DEBUGOUT("Zeroing the MTA\n");
-    for(i = 0; i < E1000_MC_TBL_SIZE; i++)
+    mta_size = E1000_MC_TBL_SIZE;
+    if (hw->mac_type == e1000_ich8lan)
+        mta_size = E1000_MC_TBL_SIZE_ICH8LAN;
+    for (i = 0; i < mta_size; i++) {
         E1000_WRITE_REG_ARRAY(hw, MTA, i, 0);
+        /* use write flush to prevent Memory Write Block (MWB) from
+         * occuring when accessing our register space */
+        E1000_WRITE_FLUSH(hw);
+    }
 
     /* Set the PCI priority bit correctly in the CTRL register.  This
      * determines if the adapter gives priority to receives, or if it
-     * gives equal priority to transmits and receives.
+     * gives equal priority to transmits and receives.  Valid only on
+     * 82542 and 82543 silicon.
      */
-    if(hw->dma_fairness) {
+    if (hw->dma_fairness && hw->mac_type <= e1000_82543) {
         ctrl = E1000_READ_REG(hw, CTRL);
         E1000_WRITE_REG(hw, CTRL, ctrl | E1000_CTRL_PRIOR);
     }
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82545_rev_3:
     case e1000_82546_rev_3:
         break;
     default:
         /* Workaround for PCI-X problem when BIOS sets MMRBC incorrectly. */
-        if(hw->bus_type == e1000_bus_type_pcix) {
+        if (hw->bus_type == e1000_bus_type_pcix) {
             e1000_read_pci_cfg(hw, PCIX_COMMAND_REGISTER, &pcix_cmd_word);
             e1000_read_pci_cfg(hw, PCIX_STATUS_REGISTER_HI,
                 &pcix_stat_hi_word);
@@ -551,9 +777,9 @@
                 PCIX_COMMAND_MMRBC_SHIFT;
             stat_mmrbc = (pcix_stat_hi_word & PCIX_STATUS_HI_MMRBC_MASK) >>
                 PCIX_STATUS_HI_MMRBC_SHIFT;
-            if(stat_mmrbc == PCIX_STATUS_HI_MMRBC_4K)
+            if (stat_mmrbc == PCIX_STATUS_HI_MMRBC_4K)
                 stat_mmrbc = PCIX_STATUS_HI_MMRBC_2K;
-            if(cmd_mmrbc > stat_mmrbc) {
+            if (cmd_mmrbc > stat_mmrbc) {
                 pcix_cmd_word &= ~PCIX_COMMAND_MMRBC_MASK;
                 pcix_cmd_word |= stat_mmrbc << PCIX_COMMAND_MMRBC_SHIFT;
                 e1000_write_pci_cfg(hw, PCIX_COMMAND_REGISTER,
@@ -563,16 +789,78 @@
         break;
     }
 
+    /* More time needed for PHY to initialize */
+    if (hw->mac_type == e1000_ich8lan)
+        msec_delay(15);
+
     /* Call a subroutine to configure the link and setup flow control. */
     ret_val = e1000_setup_link(hw);
 
     /* Set the transmit descriptor write-back policy */
-    if(hw->mac_type > e1000_82544) {
+    if (hw->mac_type > e1000_82544) {
         ctrl = E1000_READ_REG(hw, TXDCTL);
         ctrl = (ctrl & ~E1000_TXDCTL_WTHRESH) | E1000_TXDCTL_FULL_TX_DESC_WB;
+        switch (hw->mac_type) {
+        default:
+            break;
+        case e1000_82571:
+        case e1000_82572:
+        case e1000_82573:
+        case e1000_ich8lan:
+        case e1000_80003es2lan:
+            ctrl |= E1000_TXDCTL_COUNT_DESC;
+            break;
+        }
         E1000_WRITE_REG(hw, TXDCTL, ctrl);
     }
 
+    if (hw->mac_type == e1000_82573) {
+        e1000_enable_tx_pkt_filtering(hw);
+    }
+
+    switch (hw->mac_type) {
+    default:
+        break;
+    case e1000_80003es2lan:
+        /* Enable retransmit on late collisions */
+        reg_data = E1000_READ_REG(hw, TCTL);
+        reg_data |= E1000_TCTL_RTLC;
+        E1000_WRITE_REG(hw, TCTL, reg_data);
+
+        /* Configure Gigabit Carry Extend Padding */
+        reg_data = E1000_READ_REG(hw, TCTL_EXT);
+        reg_data &= ~E1000_TCTL_EXT_GCEX_MASK;
+        reg_data |= DEFAULT_80003ES2LAN_TCTL_EXT_GCEX;
+        E1000_WRITE_REG(hw, TCTL_EXT, reg_data);
+
+        /* Configure Transmit Inter-Packet Gap */
+        reg_data = E1000_READ_REG(hw, TIPG);
+        reg_data &= ~E1000_TIPG_IPGT_MASK;
+        reg_data |= DEFAULT_80003ES2LAN_TIPG_IPGT_1000;
+        E1000_WRITE_REG(hw, TIPG, reg_data);
+
+        reg_data = E1000_READ_REG_ARRAY(hw, FFLT, 0x0001);
+        reg_data &= ~0x00100000;
+        E1000_WRITE_REG_ARRAY(hw, FFLT, 0x0001, reg_data);
+        /* Fall through */
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_ich8lan:
+        ctrl = E1000_READ_REG(hw, TXDCTL1);
+        ctrl = (ctrl & ~E1000_TXDCTL_WTHRESH) | E1000_TXDCTL_FULL_TX_DESC_WB;
+        if (hw->mac_type >= e1000_82571)
+            ctrl |= E1000_TXDCTL_COUNT_DESC;
+        E1000_WRITE_REG(hw, TXDCTL1, ctrl);
+        break;
+    }
+
+
+    if (hw->mac_type == e1000_82573) {
+        uint32_t gcr = E1000_READ_REG(hw, GCR);
+        gcr |= E1000_GCR_L1_ACT_WITHOUT_L0S_RX;
+        E1000_WRITE_REG(hw, GCR, gcr);
+    }
+
     /* Clear all of the statistics registers (clear on read).  It is
      * important that we do this after we have tried to establish link
      * because the symbol error count will increment wildly if there
@@ -580,6 +868,20 @@
      */
     e1000_clear_hw_cntrs(hw);
 
+    /* ICH8 No-snoop bits are opposite polarity.
+     * Set to snoop by default after reset. */
+    if (hw->mac_type == e1000_ich8lan)
+        e1000_set_pci_ex_no_snoop(hw, PCI_EX_82566_SNOOP_ALL);
+
+    if (hw->device_id == E1000_DEV_ID_82546GB_QUAD_COPPER ||
+        hw->device_id == E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3) {
+        ctrl_ext = E1000_READ_REG(hw, CTRL_EXT);
+        /* Relaxed ordering must be disabled to avoid a parity
+         * error crash in a PCI slot. */
+        ctrl_ext |= E1000_CTRL_EXT_RO_DIS;
+        E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
+    }
+
     return ret_val;
 }
 
@@ -596,10 +898,10 @@
 
     DEBUGFUNC("e1000_adjust_serdes_amplitude");
 
-    if(hw->media_type != e1000_media_type_internal_serdes)
+    if (hw->media_type != e1000_media_type_internal_serdes)
         return E1000_SUCCESS;
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82545_rev_3:
     case e1000_82546_rev_3:
         break;
@@ -612,11 +914,11 @@
         return ret_val;
     }
 
-    if(eeprom_data != EEPROM_RESERVED_WORD) {
+    if (eeprom_data != EEPROM_RESERVED_WORD) {
         /* Adjust SERDES output amplitude only. */
-        eeprom_data &= EEPROM_SERDES_AMPLITUDE_MASK; 
+        eeprom_data &= EEPROM_SERDES_AMPLITUDE_MASK;
         ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_EXT_CTRL, eeprom_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
     }
 
@@ -643,6 +945,11 @@
 
     DEBUGFUNC("e1000_setup_link");
 
+    /* In the case of the phy reset being blocked, we already have a link.
+     * We do not have to set it up again. */
+    if (e1000_check_phy_reset_block(hw))
+        return E1000_SUCCESS;
+
     /* Read and store word 0x0F of the EEPROM. This word contains bits
      * that determine the hardware's default PAUSE (flow control) mode,
      * a bit that determines whether the HW defaults to enabling or
@@ -651,29 +958,38 @@
      * control setting, then the variable hw->fc will
      * be initialized based on a value in the EEPROM.
      */
-    if(e1000_read_eeprom(hw, EEPROM_INIT_CONTROL2_REG, 1, &eeprom_data) < 0) {
-        DEBUGOUT("EEPROM Read Error\n");
-        return -E1000_ERR_EEPROM;
-    }
-
-    if(hw->fc == e1000_fc_default) {
-        if((eeprom_data & EEPROM_WORD0F_PAUSE_MASK) == 0)
-            hw->fc = e1000_fc_none;
-        else if((eeprom_data & EEPROM_WORD0F_PAUSE_MASK) ==
-                EEPROM_WORD0F_ASM_DIR)
-            hw->fc = e1000_fc_tx_pause;
-        else
+    if (hw->fc == e1000_fc_default) {
+        switch (hw->mac_type) {
+        case e1000_ich8lan:
+        case e1000_82573:
             hw->fc = e1000_fc_full;
+            break;
+        default:
+            ret_val = e1000_read_eeprom(hw, EEPROM_INIT_CONTROL2_REG,
+                                        1, &eeprom_data);
+            if (ret_val) {
+                DEBUGOUT("EEPROM Read Error\n");
+                return -E1000_ERR_EEPROM;
+            }
+            if ((eeprom_data & EEPROM_WORD0F_PAUSE_MASK) == 0)
+                hw->fc = e1000_fc_none;
+            else if ((eeprom_data & EEPROM_WORD0F_PAUSE_MASK) ==
+                    EEPROM_WORD0F_ASM_DIR)
+                hw->fc = e1000_fc_tx_pause;
+            else
+                hw->fc = e1000_fc_full;
+            break;
+        }
     }
 
     /* We want to save off the original Flow Control configuration just
      * in case we get disconnected and then reconnected into a different
      * hub or switch with different Flow Control capabilities.
      */
-    if(hw->mac_type == e1000_82542_rev2_0)
+    if (hw->mac_type == e1000_82542_rev2_0)
         hw->fc &= (~e1000_fc_tx_pause);
 
-    if((hw->mac_type < e1000_82543) && (hw->report_tx_early == 1))
+    if ((hw->mac_type < e1000_82543) && (hw->report_tx_early == 1))
         hw->fc &= (~e1000_fc_rx_pause);
 
     hw->original_fc = hw->fc;
@@ -687,7 +1003,13 @@
      * signal detection.  So this should be done before e1000_setup_pcs_link()
      * or e1000_phy_setup() is called.
      */
-    if(hw->mac_type == e1000_82543) {
+    if (hw->mac_type == e1000_82543) {
+        ret_val = e1000_read_eeprom(hw, EEPROM_INIT_CONTROL2_REG,
+                                    1, &eeprom_data);
+        if (ret_val) {
+            DEBUGOUT("EEPROM Read Error\n");
+            return -E1000_ERR_EEPROM;
+        }
         ctrl_ext = ((eeprom_data & EEPROM_WORD0F_SWPDIO_EXT) <<
                     SWDPIO__EXT_SHIFT);
         E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
@@ -705,9 +1027,13 @@
      */
     DEBUGOUT("Initializing the Flow Control address, type and timer regs\n");
 
-    E1000_WRITE_REG(hw, FCAL, FLOW_CONTROL_ADDRESS_LOW);
-    E1000_WRITE_REG(hw, FCAH, FLOW_CONTROL_ADDRESS_HIGH);
-    E1000_WRITE_REG(hw, FCT, FLOW_CONTROL_TYPE);
+    /* FCAL/H and FCT are hardcoded to standard values in e1000_ich8lan. */
+    if (hw->mac_type != e1000_ich8lan) {
+        E1000_WRITE_REG(hw, FCT, FLOW_CONTROL_TYPE);
+        E1000_WRITE_REG(hw, FCAH, FLOW_CONTROL_ADDRESS_HIGH);
+        E1000_WRITE_REG(hw, FCAL, FLOW_CONTROL_ADDRESS_LOW);
+    }
+
     E1000_WRITE_REG(hw, FCTTV, hw->fc_pause_time);
 
     /* Set the flow control receive threshold registers.  Normally,
@@ -716,14 +1042,14 @@
      * ability to transmit pause frames in not enabled, then these
      * registers will be set to 0.
      */
-    if(!(hw->fc & e1000_fc_tx_pause)) {
+    if (!(hw->fc & e1000_fc_tx_pause)) {
         E1000_WRITE_REG(hw, FCRTL, 0);
         E1000_WRITE_REG(hw, FCRTH, 0);
     } else {
         /* We need to set up the Receive Threshold high and low water marks
          * as well as (optionally) enabling the transmission of XON frames.
          */
-        if(hw->fc_send_xon) {
+        if (hw->fc_send_xon) {
             E1000_WRITE_REG(hw, FCRTL, (hw->fc_low_water | E1000_FCRTL_XONE));
             E1000_WRITE_REG(hw, FCRTH, hw->fc_high_water);
         } else {
@@ -755,6 +1081,14 @@
 
     DEBUGFUNC("e1000_setup_fiber_serdes_link");
 
+    /* On 82571 and 82572 Fiber connections, SerDes loopback mode persists
+     * until explicitly turned off or a power cycle is performed.  A read to
+     * the register does not indicate its status.  Therefore, we ensure
+     * loopback mode is disabled during initialization.
+     */
+    if (hw->mac_type == e1000_82571 || hw->mac_type == e1000_82572)
+        E1000_WRITE_REG(hw, SCTL, E1000_DISABLE_SERDES_LOOPBACK);
+
     /* On adapters with a MAC newer than 82544, SW Defineable pin 1 will be
      * set when the optics detect a signal. On older adapters, it will be
      * cleared when there is a signal.  This applies to fiber media only.
@@ -762,11 +1096,11 @@
      * the EEPROM.
      */
     ctrl = E1000_READ_REG(hw, CTRL);
-    if(hw->media_type == e1000_media_type_fiber)
+    if (hw->media_type == e1000_media_type_fiber)
         signal = (hw->mac_type > e1000_82544) ? E1000_CTRL_SWDPIN1 : 0;
 
     ret_val = e1000_adjust_serdes_amplitude(hw);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     /* Take the link out of reset */
@@ -774,7 +1108,7 @@
 
     /* Adjust VCO speed to improve BER performance */
     ret_val = e1000_set_vco_speed(hw);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     e1000_config_collision_dist(hw);
@@ -845,15 +1179,15 @@
      * less than 500 milliseconds even if the other end is doing it in SW).
      * For internal serdes, we just assume a signal is present, then poll.
      */
-    if(hw->media_type == e1000_media_type_internal_serdes ||
+    if (hw->media_type == e1000_media_type_internal_serdes ||
        (E1000_READ_REG(hw, CTRL) & E1000_CTRL_SWDPIN1) == signal) {
         DEBUGOUT("Looking for Link\n");
-        for(i = 0; i < (LINK_UP_TIMEOUT / 10); i++) {
+        for (i = 0; i < (LINK_UP_TIMEOUT / 10); i++) {
             msec_delay(10);
             status = E1000_READ_REG(hw, STATUS);
-            if(status & E1000_STATUS_LU) break;
+            if (status & E1000_STATUS_LU) break;
         }
-        if(i == (LINK_UP_TIMEOUT / 10)) {
+        if (i == (LINK_UP_TIMEOUT / 10)) {
             DEBUGOUT("Never got a valid link from auto-neg!!!\n");
             hw->autoneg_failed = 1;
             /* AutoNeg failed to achieve a link, so we'll call
@@ -862,7 +1196,7 @@
              * non-autonegotiating link partners.
              */
             ret_val = e1000_check_for_link(hw);
-            if(ret_val) {
+            if (ret_val) {
                 DEBUGOUT("Error while checking for link\n");
                 return ret_val;
             }
@@ -878,39 +1212,39 @@
 }
 
 /******************************************************************************
-* Detects which PHY is present and the speed and duplex
+* Make sure we have a valid PHY and change PHY mode before link setup.
 *
 * hw - Struct containing variables accessed by shared code
 ******************************************************************************/
 static int32_t
-e1000_setup_copper_link(struct e1000_hw *hw)
+e1000_copper_link_preconfig(struct e1000_hw *hw)
 {
     uint32_t ctrl;
-    uint32_t led_ctrl;
     int32_t ret_val;
-    uint16_t i;
     uint16_t phy_data;
 
-    DEBUGFUNC("e1000_setup_copper_link");
+    DEBUGFUNC("e1000_copper_link_preconfig");
 
     ctrl = E1000_READ_REG(hw, CTRL);
     /* With 82543, we need to force speed and duplex on the MAC equal to what
      * the PHY speed and duplex configuration is. In addition, we need to
      * perform a hardware reset on the PHY to take it out of reset.
      */
-    if(hw->mac_type > e1000_82543) {
+    if (hw->mac_type > e1000_82543) {
         ctrl |= E1000_CTRL_SLU;
         ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
         E1000_WRITE_REG(hw, CTRL, ctrl);
     } else {
         ctrl |= (E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX | E1000_CTRL_SLU);
         E1000_WRITE_REG(hw, CTRL, ctrl);
-        e1000_phy_hw_reset(hw);
+        ret_val = e1000_phy_hw_reset(hw);
+        if (ret_val)
+            return ret_val;
     }
 
     /* Make sure we have a valid PHY */
     ret_val = e1000_detect_gig_phy(hw);
-    if(ret_val) {
+    if (ret_val) {
         DEBUGOUT("Error, did not detect valid phy.\n");
         return ret_val;
     }
@@ -918,337 +1252,641 @@
 
     /* Set PHY to class A mode (if necessary) */
     ret_val = e1000_set_phy_mode(hw);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
-    if((hw->mac_type == e1000_82545_rev_3) ||
+    if ((hw->mac_type == e1000_82545_rev_3) ||
        (hw->mac_type == e1000_82546_rev_3)) {
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
         phy_data |= 0x00000008;
         ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
     }
 
-    if(hw->mac_type <= e1000_82543 ||
-       hw->mac_type == e1000_82541 || hw->mac_type == e1000_82547 ||
-       hw->mac_type == e1000_82541_rev_2 || hw->mac_type == e1000_82547_rev_2)
+    if (hw->mac_type <= e1000_82543 ||
+        hw->mac_type == e1000_82541 || hw->mac_type == e1000_82547 ||
+        hw->mac_type == e1000_82541_rev_2 || hw->mac_type == e1000_82547_rev_2)
         hw->phy_reset_disable = FALSE;
 
-    if(!hw->phy_reset_disable) {
-        if (hw->phy_type == e1000_phy_igp) {
+   return E1000_SUCCESS;
+}
 
-            ret_val = e1000_phy_reset(hw);
-            if(ret_val) {
-                DEBUGOUT("Error Resetting the PHY\n");
-                return ret_val;
-            }
 
-            /* Wait 10ms for MAC to configure PHY from eeprom settings */
-            msec_delay(15);
+/********************************************************************
+* Copper link setup for e1000_phy_igp series.
+*
+* hw - Struct containing variables accessed by shared code
+*********************************************************************/
+static int32_t
+e1000_copper_link_igp_setup(struct e1000_hw *hw)
+{
+    uint32_t led_ctrl;
+    int32_t ret_val;
+    uint16_t phy_data;
 
-            /* Configure activity LED after PHY reset */
-            led_ctrl = E1000_READ_REG(hw, LEDCTL);
-            led_ctrl &= IGP_ACTIVITY_LED_MASK;
-            led_ctrl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
-            E1000_WRITE_REG(hw, LEDCTL, led_ctrl);
+    DEBUGFUNC("e1000_copper_link_igp_setup");
 
-            /* disable lplu d3 during driver init */
-            ret_val = e1000_set_d3_lplu_state(hw, FALSE);
-            if(ret_val) {
-                DEBUGOUT("Error Disabling LPLU D3\n");
-                return ret_val;
-            }
+    if (hw->phy_reset_disable)
+        return E1000_SUCCESS;
 
-            /* Configure mdi-mdix settings */
-            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL,
-                                         &phy_data);
-            if(ret_val)
-                return ret_val;
+    ret_val = e1000_phy_reset(hw);
+    if (ret_val) {
+        DEBUGOUT("Error Resetting the PHY\n");
+        return ret_val;
+    }
 
-            if((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
-                hw->dsp_config_state = e1000_dsp_config_disabled;
-                /* Force MDI for IGP B-0 PHY */
-                phy_data &= ~(IGP01E1000_PSCR_AUTO_MDIX |
-                              IGP01E1000_PSCR_FORCE_MDI_MDIX);
-                hw->mdix = 1;
+    /* Wait 15ms for MAC to configure PHY from eeprom settings */
+    msec_delay(15);
+    if (hw->mac_type != e1000_ich8lan) {
+    /* Configure activity LED after PHY reset */
+    led_ctrl = E1000_READ_REG(hw, LEDCTL);
+    led_ctrl &= IGP_ACTIVITY_LED_MASK;
+    led_ctrl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
+    E1000_WRITE_REG(hw, LEDCTL, led_ctrl);
+    }
 
-            } else {
-                hw->dsp_config_state = e1000_dsp_config_enabled;
-                phy_data &= ~IGP01E1000_PSCR_AUTO_MDIX;
+    /* The NVM settings will configure LPLU in D3 for IGP2 and IGP3 PHYs */
+    if (hw->phy_type == e1000_phy_igp) {
+        /* disable lplu d3 during driver init */
+        ret_val = e1000_set_d3_lplu_state(hw, FALSE);
+        if (ret_val) {
+            DEBUGOUT("Error Disabling LPLU D3\n");
+            return ret_val;
+        }
+    }
 
-                switch (hw->mdix) {
-                case 1:
-                    phy_data &= ~IGP01E1000_PSCR_FORCE_MDI_MDIX;
-                    break;
-                case 2:
-                    phy_data |= IGP01E1000_PSCR_FORCE_MDI_MDIX;
-                    break;
-                case 0:
-                default:
-                    phy_data |= IGP01E1000_PSCR_AUTO_MDIX;
-                    break;
-                }
-            }
-            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL,
-                                          phy_data);
-            if(ret_val)
-                return ret_val;
+    /* disable lplu d0 during driver init */
+    ret_val = e1000_set_d0_lplu_state(hw, FALSE);
+    if (ret_val) {
+        DEBUGOUT("Error Disabling LPLU D0\n");
+        return ret_val;
+    }
+    /* Configure mdi-mdix settings */
+    ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, &phy_data);
+    if (ret_val)
+        return ret_val;
 
-            /* set auto-master slave resolution settings */
-            if(hw->autoneg) {
-                e1000_ms_type phy_ms_setting = hw->master_slave;
+    if ((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
+        hw->dsp_config_state = e1000_dsp_config_disabled;
+        /* Force MDI for earlier revs of the IGP PHY */
+        phy_data &= ~(IGP01E1000_PSCR_AUTO_MDIX | IGP01E1000_PSCR_FORCE_MDI_MDIX);
+        hw->mdix = 1;
 
-                if(hw->ffe_config_state == e1000_ffe_config_active)
-                    hw->ffe_config_state = e1000_ffe_config_enabled;
+    } else {
+        hw->dsp_config_state = e1000_dsp_config_enabled;
+        phy_data &= ~IGP01E1000_PSCR_AUTO_MDIX;
 
-                if(hw->dsp_config_state == e1000_dsp_config_activated)
-                    hw->dsp_config_state = e1000_dsp_config_enabled;
+        switch (hw->mdix) {
+        case 1:
+            phy_data &= ~IGP01E1000_PSCR_FORCE_MDI_MDIX;
+            break;
+        case 2:
+            phy_data |= IGP01E1000_PSCR_FORCE_MDI_MDIX;
+            break;
+        case 0:
+        default:
+            phy_data |= IGP01E1000_PSCR_AUTO_MDIX;
+            break;
+        }
+    }
+    ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, phy_data);
+    if (ret_val)
+        return ret_val;
 
-                /* when autonegotiation advertisment is only 1000Mbps then we
-                 * should disable SmartSpeed and enable Auto MasterSlave
-                 * resolution as hardware default. */
-                if(hw->autoneg_advertised == ADVERTISE_1000_FULL) {
-                    /* Disable SmartSpeed */
-                    ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
-                                                 &phy_data);
-                    if(ret_val)
-                        return ret_val;
-                    phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
-                    ret_val = e1000_write_phy_reg(hw,
-                                                  IGP01E1000_PHY_PORT_CONFIG,
-                                                  phy_data);
-                    if(ret_val)
-                        return ret_val;
-                    /* Set auto Master/Slave resolution process */
-                    ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &phy_data);
-                    if(ret_val)
-                        return ret_val;
-                    phy_data &= ~CR_1000T_MS_ENABLE;
-                    ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, phy_data);
-                    if(ret_val)
-                        return ret_val;
-                }
+    /* set auto-master slave resolution settings */
+    if (hw->autoneg) {
+        e1000_ms_type phy_ms_setting = hw->master_slave;
 
-                ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &phy_data);
-                if(ret_val)
-                    return ret_val;
+        if (hw->ffe_config_state == e1000_ffe_config_active)
+            hw->ffe_config_state = e1000_ffe_config_enabled;
 
-                /* load defaults for future use */
-                hw->original_master_slave = (phy_data & CR_1000T_MS_ENABLE) ?
-                                            ((phy_data & CR_1000T_MS_VALUE) ?
-                                             e1000_ms_force_master :
-                                             e1000_ms_force_slave) :
-                                             e1000_ms_auto;
-
-                switch (phy_ms_setting) {
-                case e1000_ms_force_master:
-                    phy_data |= (CR_1000T_MS_ENABLE | CR_1000T_MS_VALUE);
-                    break;
-                case e1000_ms_force_slave:
-                    phy_data |= CR_1000T_MS_ENABLE;
-                    phy_data &= ~(CR_1000T_MS_VALUE);
-                    break;
-                case e1000_ms_auto:
-                    phy_data &= ~CR_1000T_MS_ENABLE;
-                default:
-                    break;
-                }
-                ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, phy_data);
-                if(ret_val)
-                    return ret_val;
-            }
-        } else {
-            /* Enable CRS on TX. This must be set for half-duplex operation. */
-            ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL,
+        if (hw->dsp_config_state == e1000_dsp_config_activated)
+            hw->dsp_config_state = e1000_dsp_config_enabled;
+
+        /* when autonegotiation advertisment is only 1000Mbps then we
+          * should disable SmartSpeed and enable Auto MasterSlave
+          * resolution as hardware default. */
+        if (hw->autoneg_advertised == ADVERTISE_1000_FULL) {
+            /* Disable SmartSpeed */
+            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
                                          &phy_data);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
-
-            phy_data |= M88E1000_PSCR_ASSERT_CRS_ON_TX;
-
-            /* Options:
-             *   MDI/MDI-X = 0 (default)
-             *   0 - Auto for all speeds
-             *   1 - MDI mode
-             *   2 - MDI-X mode
-             *   3 - Auto for 1000Base-T only (MDI-X for 10/100Base-T modes)
-             */
-            phy_data &= ~M88E1000_PSCR_AUTO_X_MODE;
-
-            switch (hw->mdix) {
-            case 1:
-                phy_data |= M88E1000_PSCR_MDI_MANUAL_MODE;
-                break;
-            case 2:
-                phy_data |= M88E1000_PSCR_MDIX_MANUAL_MODE;
-                break;
-            case 3:
-                phy_data |= M88E1000_PSCR_AUTO_X_1000T;
-                break;
-            case 0:
-            default:
-                phy_data |= M88E1000_PSCR_AUTO_X_MODE;
-                break;
-            }
-
-            /* Options:
-             *   disable_polarity_correction = 0 (default)
-             *       Automatic Correction for Reversed Cable Polarity
-             *   0 - Disabled
-             *   1 - Enabled
-             */
-            phy_data &= ~M88E1000_PSCR_POLARITY_REVERSAL;
-            if(hw->disable_polarity_correction == 1)
-                phy_data |= M88E1000_PSCR_POLARITY_REVERSAL;
-            ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL,
+            phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
                                           phy_data);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
-
-            /* Force TX_CLK in the Extended PHY Specific Control Register
-             * to 25MHz clock.
-             */
-            ret_val = e1000_read_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL,
-                                         &phy_data);
-            if(ret_val)
+            /* Set auto Master/Slave resolution process */
+            ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &phy_data);
+            if (ret_val)
                 return ret_val;
-
-            phy_data |= M88E1000_EPSCR_TX_CLK_25;
-
-            if (hw->phy_revision < M88E1011_I_REV_4) {
-                /* Configure Master and Slave downshift values */
-                phy_data &= ~(M88E1000_EPSCR_MASTER_DOWNSHIFT_MASK |
-                              M88E1000_EPSCR_SLAVE_DOWNSHIFT_MASK);
-                phy_data |= (M88E1000_EPSCR_MASTER_DOWNSHIFT_1X |
-                             M88E1000_EPSCR_SLAVE_DOWNSHIFT_1X);
-                ret_val = e1000_write_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL,
-                                              phy_data);
-                if(ret_val)
-                    return ret_val;
-            }
-
-            /* SW Reset the PHY so all changes take effect */
-            ret_val = e1000_phy_reset(hw);
-            if(ret_val) {
-                DEBUGOUT("Error Resetting the PHY\n");
+            phy_data &= ~CR_1000T_MS_ENABLE;
+            ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, phy_data);
+            if (ret_val)
                 return ret_val;
-            }
         }
 
-        /* Options:
-         *   autoneg = 1 (default)
-         *      PHY will advertise value(s) parsed from
-         *      autoneg_advertised and fc
-         *   autoneg = 0
-         *      PHY will be set to 10H, 10F, 100H, or 100F
-         *      depending on value parsed from forced_speed_duplex.
-         */
-
-        /* Is autoneg enabled?  This is enabled by default or by software
-         * override.  If so, call e1000_phy_setup_autoneg routine to parse the
-         * autoneg_advertised and fc options. If autoneg is NOT enabled, then
-         * the user should have provided a speed/duplex override.  If so, then
-         * call e1000_phy_force_speed_duplex to parse and set this up.
-         */
-        if(hw->autoneg) {
-            /* Perform some bounds checking on the hw->autoneg_advertised
-             * parameter.  If this variable is zero, then set it to the default.
-             */
-            hw->autoneg_advertised &= AUTONEG_ADVERTISE_SPEED_DEFAULT;
+        ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &phy_data);
+        if (ret_val)
+            return ret_val;
 
-            /* If autoneg_advertised is zero, we assume it was not defaulted
-             * by the calling code so we set to advertise full capability.
-             */
-            if(hw->autoneg_advertised == 0)
-                hw->autoneg_advertised = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+        /* load defaults for future use */
+        hw->original_master_slave = (phy_data & CR_1000T_MS_ENABLE) ?
+                                        ((phy_data & CR_1000T_MS_VALUE) ?
+                                         e1000_ms_force_master :
+                                         e1000_ms_force_slave) :
+                                         e1000_ms_auto;
+
+        switch (phy_ms_setting) {
+        case e1000_ms_force_master:
+            phy_data |= (CR_1000T_MS_ENABLE | CR_1000T_MS_VALUE);
+            break;
+        case e1000_ms_force_slave:
+            phy_data |= CR_1000T_MS_ENABLE;
+            phy_data &= ~(CR_1000T_MS_VALUE);
+            break;
+        case e1000_ms_auto:
+            phy_data &= ~CR_1000T_MS_ENABLE;
+            default:
+            break;
+        }
+        ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, phy_data);
+        if (ret_val)
+            return ret_val;
+    }
 
-            DEBUGOUT("Reconfiguring auto-neg advertisement params\n");
-            ret_val = e1000_phy_setup_autoneg(hw);
-            if(ret_val) {
-                DEBUGOUT("Error Setting up Auto-Negotiation\n");
-                return ret_val;
-            }
-            DEBUGOUT("Restarting Auto-Neg\n");
+    return E1000_SUCCESS;
+}
 
-            /* Restart auto-negotiation by setting the Auto Neg Enable bit and
-             * the Auto Neg Restart bit in the PHY control register.
-             */
-            ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
-            if(ret_val)
-                return ret_val;
+/********************************************************************
+* Copper link setup for e1000_phy_gg82563 series.
+*
+* hw - Struct containing variables accessed by shared code
+*********************************************************************/
+static int32_t
+e1000_copper_link_ggp_setup(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t phy_data;
+    uint32_t reg_data;
 
-            phy_data |= (MII_CR_AUTO_NEG_EN | MII_CR_RESTART_AUTO_NEG);
-            ret_val = e1000_write_phy_reg(hw, PHY_CTRL, phy_data);
-            if(ret_val)
-                return ret_val;
+    DEBUGFUNC("e1000_copper_link_ggp_setup");
 
-            /* Does the user want to wait for Auto-Neg to complete here, or
-             * check at a later time (for example, callback routine).
-             */
-            if(hw->wait_autoneg_complete) {
-                ret_val = e1000_wait_autoneg(hw);
-                if(ret_val) {
-                    DEBUGOUT("Error while waiting for autoneg to complete\n");
-                    return ret_val;
-                }
-            }
-            hw->get_link_status = TRUE;
-        } else {
-            DEBUGOUT("Forcing speed and duplex\n");
-            ret_val = e1000_phy_force_speed_duplex(hw);
-            if(ret_val) {
-                DEBUGOUT("Error Forcing Speed and Duplex\n");
-                return ret_val;
-            }
-        }
-    } /* !hw->phy_reset_disable */
+    if (!hw->phy_reset_disable) {
 
-    /* Check link status. Wait up to 100 microseconds for link to become
-     * valid.
-     */
-    for(i = 0; i < 10; i++) {
-        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+        /* Enable CRS on TX for half-duplex operation. */
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL,
+                                     &phy_data);
+        if (ret_val)
             return ret_val;
-        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+
+        phy_data |= GG82563_MSCR_ASSERT_CRS_ON_TX;
+        /* Use 25MHz for both link down and 1000BASE-T for Tx clock */
+        phy_data |= GG82563_MSCR_TX_CLK_1000MBPS_25MHZ;
+
+        ret_val = e1000_write_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL,
+                                      phy_data);
+        if (ret_val)
             return ret_val;
 
-        if(phy_data & MII_SR_LINK_STATUS) {
-            /* We have link, so we need to finish the config process:
-             *   1) Set up the MAC to the current PHY speed/duplex
-             *      if we are on 82543.  If we
-             *      are on newer silicon, we only need to configure
-             *      collision distance in the Transmit Control Register.
-             *   2) Set up flow control on the MAC to that established with
-             *      the link partner.
-             */
-            if(hw->mac_type >= e1000_82544) {
-                e1000_config_collision_dist(hw);
-            } else {
-                ret_val = e1000_config_mac_to_phy(hw);
-                if(ret_val) {
-                    DEBUGOUT("Error configuring MAC to PHY settings\n");
-                    return ret_val;
-                }
-            }
-            ret_val = e1000_config_fc_after_link_up(hw);
-            if(ret_val) {
-                DEBUGOUT("Error Configuring Flow Control\n");
+        /* Options:
+         *   MDI/MDI-X = 0 (default)
+         *   0 - Auto for all speeds
+         *   1 - MDI mode
+         *   2 - MDI-X mode
+         *   3 - Auto for 1000Base-T only (MDI-X for 10/100Base-T modes)
+         */
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_SPEC_CTRL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data &= ~GG82563_PSCR_CROSSOVER_MODE_MASK;
+
+        switch (hw->mdix) {
+        case 1:
+            phy_data |= GG82563_PSCR_CROSSOVER_MODE_MDI;
+            break;
+        case 2:
+            phy_data |= GG82563_PSCR_CROSSOVER_MODE_MDIX;
+            break;
+        case 0:
+        default:
+            phy_data |= GG82563_PSCR_CROSSOVER_MODE_AUTO;
+            break;
+        }
+
+        /* Options:
+         *   disable_polarity_correction = 0 (default)
+         *       Automatic Correction for Reversed Cable Polarity
+         *   0 - Disabled
+         *   1 - Enabled
+         */
+        phy_data &= ~GG82563_PSCR_POLARITY_REVERSAL_DISABLE;
+        if (hw->disable_polarity_correction == 1)
+            phy_data |= GG82563_PSCR_POLARITY_REVERSAL_DISABLE;
+        ret_val = e1000_write_phy_reg(hw, GG82563_PHY_SPEC_CTRL, phy_data);
+
+        if (ret_val)
+            return ret_val;
+
+        /* SW Reset the PHY so all changes take effect */
+        ret_val = e1000_phy_reset(hw);
+        if (ret_val) {
+            DEBUGOUT("Error Resetting the PHY\n");
+            return ret_val;
+        }
+    } /* phy_reset_disable */
+
+    if (hw->mac_type == e1000_80003es2lan) {
+        /* Bypass RX and TX FIFO's */
+        ret_val = e1000_write_kmrn_reg(hw, E1000_KUMCTRLSTA_OFFSET_FIFO_CTRL,
+                                       E1000_KUMCTRLSTA_FIFO_CTRL_RX_BYPASS |
+                                       E1000_KUMCTRLSTA_FIFO_CTRL_TX_BYPASS);
+        if (ret_val)
+            return ret_val;
+
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_SPEC_CTRL_2, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data &= ~GG82563_PSCR2_REVERSE_AUTO_NEG;
+        ret_val = e1000_write_phy_reg(hw, GG82563_PHY_SPEC_CTRL_2, phy_data);
+
+        if (ret_val)
+            return ret_val;
+
+        reg_data = E1000_READ_REG(hw, CTRL_EXT);
+        reg_data &= ~(E1000_CTRL_EXT_LINK_MODE_MASK);
+        E1000_WRITE_REG(hw, CTRL_EXT, reg_data);
+
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_PWR_MGMT_CTRL,
+                                          &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        /* Do not init these registers when the HW is in IAMT mode, since the
+         * firmware will have already initialized them.  We only initialize
+         * them if the HW is not in IAMT mode.
+         */
+        if (e1000_check_mng_mode(hw) == FALSE) {
+            /* Enable Electrical Idle on the PHY */
+            phy_data |= GG82563_PMCR_ENABLE_ELECTRICAL_IDLE;
+            ret_val = e1000_write_phy_reg(hw, GG82563_PHY_PWR_MGMT_CTRL,
+                                          phy_data);
+            if (ret_val)
+                return ret_val;
+
+            ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+                                         &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            phy_data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+            ret_val = e1000_write_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL,
+                                          phy_data);
+
+            if (ret_val)
+                return ret_val;
+        }
+
+        /* Workaround: Disable padding in Kumeran interface in the MAC
+         * and in the PHY to avoid CRC errors.
+         */
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_INBAND_CTRL,
+                                     &phy_data);
+        if (ret_val)
+            return ret_val;
+        phy_data |= GG82563_ICR_DIS_PADDING;
+        ret_val = e1000_write_phy_reg(hw, GG82563_PHY_INBAND_CTRL,
+                                      phy_data);
+        if (ret_val)
+            return ret_val;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/********************************************************************
+* Copper link setup for e1000_phy_m88 series.
+*
+* hw - Struct containing variables accessed by shared code
+*********************************************************************/
+static int32_t
+e1000_copper_link_mgp_setup(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t phy_data;
+
+    DEBUGFUNC("e1000_copper_link_mgp_setup");
+
+    if (hw->phy_reset_disable)
+        return E1000_SUCCESS;
+
+    /* Enable CRS on TX. This must be set for half-duplex operation. */
+    ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+    if (ret_val)
+        return ret_val;
+
+    phy_data |= M88E1000_PSCR_ASSERT_CRS_ON_TX;
+
+    /* Options:
+     *   MDI/MDI-X = 0 (default)
+     *   0 - Auto for all speeds
+     *   1 - MDI mode
+     *   2 - MDI-X mode
+     *   3 - Auto for 1000Base-T only (MDI-X for 10/100Base-T modes)
+     */
+    phy_data &= ~M88E1000_PSCR_AUTO_X_MODE;
+
+    switch (hw->mdix) {
+    case 1:
+        phy_data |= M88E1000_PSCR_MDI_MANUAL_MODE;
+        break;
+    case 2:
+        phy_data |= M88E1000_PSCR_MDIX_MANUAL_MODE;
+        break;
+    case 3:
+        phy_data |= M88E1000_PSCR_AUTO_X_1000T;
+        break;
+    case 0:
+    default:
+        phy_data |= M88E1000_PSCR_AUTO_X_MODE;
+        break;
+    }
+
+    /* Options:
+     *   disable_polarity_correction = 0 (default)
+     *       Automatic Correction for Reversed Cable Polarity
+     *   0 - Disabled
+     *   1 - Enabled
+     */
+    phy_data &= ~M88E1000_PSCR_POLARITY_REVERSAL;
+    if (hw->disable_polarity_correction == 1)
+        phy_data |= M88E1000_PSCR_POLARITY_REVERSAL;
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
+    if (ret_val)
+        return ret_val;
+
+    if (hw->phy_revision < M88E1011_I_REV_4) {
+        /* Force TX_CLK in the Extended PHY Specific Control Register
+         * to 25MHz clock.
+         */
+        ret_val = e1000_read_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data |= M88E1000_EPSCR_TX_CLK_25;
+
+        if ((hw->phy_revision == E1000_REVISION_2) &&
+            (hw->phy_id == M88E1111_I_PHY_ID)) {
+            /* Vidalia Phy, set the downshift counter to 5x */
+            phy_data &= ~(M88EC018_EPSCR_DOWNSHIFT_COUNTER_MASK);
+            phy_data |= M88EC018_EPSCR_DOWNSHIFT_COUNTER_5X;
+            ret_val = e1000_write_phy_reg(hw,
+                                        M88E1000_EXT_PHY_SPEC_CTRL, phy_data);
+            if (ret_val)
+                return ret_val;
+        } else {
+            /* Configure Master and Slave downshift values */
+            phy_data &= ~(M88E1000_EPSCR_MASTER_DOWNSHIFT_MASK |
+                              M88E1000_EPSCR_SLAVE_DOWNSHIFT_MASK);
+            phy_data |= (M88E1000_EPSCR_MASTER_DOWNSHIFT_1X |
+                             M88E1000_EPSCR_SLAVE_DOWNSHIFT_1X);
+            ret_val = e1000_write_phy_reg(hw,
+                                        M88E1000_EXT_PHY_SPEC_CTRL, phy_data);
+            if (ret_val)
+               return ret_val;
+        }
+    }
+
+    /* SW Reset the PHY so all changes take effect */
+    ret_val = e1000_phy_reset(hw);
+    if (ret_val) {
+        DEBUGOUT("Error Resetting the PHY\n");
+        return ret_val;
+    }
+
+   return E1000_SUCCESS;
+}
+
+/********************************************************************
+* Setup auto-negotiation and flow control advertisements,
+* and then perform auto-negotiation.
+*
+* hw - Struct containing variables accessed by shared code
+*********************************************************************/
+static int32_t
+e1000_copper_link_autoneg(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t phy_data;
+
+    DEBUGFUNC("e1000_copper_link_autoneg");
+
+    /* Perform some bounds checking on the hw->autoneg_advertised
+     * parameter.  If this variable is zero, then set it to the default.
+     */
+    hw->autoneg_advertised &= AUTONEG_ADVERTISE_SPEED_DEFAULT;
+
+    /* If autoneg_advertised is zero, we assume it was not defaulted
+     * by the calling code so we set to advertise full capability.
+     */
+    if (hw->autoneg_advertised == 0)
+        hw->autoneg_advertised = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+
+    /* IFE phy only supports 10/100 */
+    if (hw->phy_type == e1000_phy_ife)
+        hw->autoneg_advertised &= AUTONEG_ADVERTISE_10_100_ALL;
+
+    DEBUGOUT("Reconfiguring auto-neg advertisement params\n");
+    ret_val = e1000_phy_setup_autoneg(hw);
+    if (ret_val) {
+        DEBUGOUT("Error Setting up Auto-Negotiation\n");
+        return ret_val;
+    }
+    DEBUGOUT("Restarting Auto-Neg\n");
+
+    /* Restart auto-negotiation by setting the Auto Neg Enable bit and
+     * the Auto Neg Restart bit in the PHY control register.
+     */
+    ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
+    if (ret_val)
+        return ret_val;
+
+    phy_data |= (MII_CR_AUTO_NEG_EN | MII_CR_RESTART_AUTO_NEG);
+    ret_val = e1000_write_phy_reg(hw, PHY_CTRL, phy_data);
+    if (ret_val)
+        return ret_val;
+
+    /* Does the user want to wait for Auto-Neg to complete here, or
+     * check at a later time (for example, callback routine).
+     */
+    if (hw->wait_autoneg_complete) {
+        ret_val = e1000_wait_autoneg(hw);
+        if (ret_val) {
+            DEBUGOUT("Error while waiting for autoneg to complete\n");
+            return ret_val;
+        }
+    }
+
+    hw->get_link_status = TRUE;
+
+    return E1000_SUCCESS;
+}
+
+/******************************************************************************
+* Config the MAC and the PHY after link is up.
+*   1) Set up the MAC to the current PHY speed/duplex
+*      if we are on 82543.  If we
+*      are on newer silicon, we only need to configure
+*      collision distance in the Transmit Control Register.
+*   2) Set up flow control on the MAC to that established with
+*      the link partner.
+*   3) Config DSP to improve Gigabit link quality for some PHY revisions.
+*
+* hw - Struct containing variables accessed by shared code
+******************************************************************************/
+static int32_t
+e1000_copper_link_postconfig(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    DEBUGFUNC("e1000_copper_link_postconfig");
+
+    if (hw->mac_type >= e1000_82544) {
+        e1000_config_collision_dist(hw);
+    } else {
+        ret_val = e1000_config_mac_to_phy(hw);
+        if (ret_val) {
+            DEBUGOUT("Error configuring MAC to PHY settings\n");
+            return ret_val;
+        }
+    }
+    ret_val = e1000_config_fc_after_link_up(hw);
+    if (ret_val) {
+        DEBUGOUT("Error Configuring Flow Control\n");
+        return ret_val;
+    }
+
+    /* Config DSP to improve Giga link quality */
+    if (hw->phy_type == e1000_phy_igp) {
+        ret_val = e1000_config_dsp_after_link_change(hw, TRUE);
+        if (ret_val) {
+            DEBUGOUT("Error Configuring DSP after link up\n");
+            return ret_val;
+        }
+    }
+
+    return E1000_SUCCESS;
+}
+
+/******************************************************************************
+* Detects which PHY is present and setup the speed and duplex
+*
+* hw - Struct containing variables accessed by shared code
+******************************************************************************/
+static int32_t
+e1000_setup_copper_link(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t i;
+    uint16_t phy_data;
+    uint16_t reg_data;
+
+    DEBUGFUNC("e1000_setup_copper_link");
+
+    switch (hw->mac_type) {
+    case e1000_80003es2lan:
+    case e1000_ich8lan:
+        /* Set the mac to wait the maximum time between each
+         * iteration and increase the max iterations when
+         * polling the phy; this fixes erroneous timeouts at 10Mbps. */
+        ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 4), 0xFFFF);
+        if (ret_val)
+            return ret_val;
+        ret_val = e1000_read_kmrn_reg(hw, GG82563_REG(0x34, 9), &reg_data);
+        if (ret_val)
+            return ret_val;
+        reg_data |= 0x3F;
+        ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 9), reg_data);
+        if (ret_val)
+            return ret_val;
+    default:
+        break;
+    }
+
+    /* Check if it is a valid PHY and set PHY mode if necessary. */
+    ret_val = e1000_copper_link_preconfig(hw);
+    if (ret_val)
+        return ret_val;
+
+    switch (hw->mac_type) {
+    case e1000_80003es2lan:
+        /* Kumeran registers are written-only */
+        reg_data = E1000_KUMCTRLSTA_INB_CTRL_LINK_STATUS_TX_TIMEOUT_DEFAULT;
+        reg_data |= E1000_KUMCTRLSTA_INB_CTRL_DIS_PADDING;
+        ret_val = e1000_write_kmrn_reg(hw, E1000_KUMCTRLSTA_OFFSET_INB_CTRL,
+                                       reg_data);
+        if (ret_val)
+            return ret_val;
+        break;
+    default:
+        break;
+    }
+
+    if (hw->phy_type == e1000_phy_igp ||
+        hw->phy_type == e1000_phy_igp_3 ||
+        hw->phy_type == e1000_phy_igp_2) {
+        ret_val = e1000_copper_link_igp_setup(hw);
+        if (ret_val)
+            return ret_val;
+    } else if (hw->phy_type == e1000_phy_m88) {
+        ret_val = e1000_copper_link_mgp_setup(hw);
+        if (ret_val)
+            return ret_val;
+    } else if (hw->phy_type == e1000_phy_gg82563) {
+        ret_val = e1000_copper_link_ggp_setup(hw);
+        if (ret_val)
+            return ret_val;
+    }
+
+    if (hw->autoneg) {
+        /* Setup autoneg and flow control advertisement
+          * and perform autonegotiation */
+        ret_val = e1000_copper_link_autoneg(hw);
+        if (ret_val)
+            return ret_val;
+    } else {
+        /* PHY will be set to 10H, 10F, 100H,or 100F
+          * depending on value from forced_speed_duplex. */
+        DEBUGOUT("Forcing speed and duplex\n");
+        ret_val = e1000_phy_force_speed_duplex(hw);
+        if (ret_val) {
+            DEBUGOUT("Error Forcing Speed and Duplex\n");
+            return ret_val;
+        }
+    }
+
+    /* Check link status. Wait up to 100 microseconds for link to become
+     * valid.
+     */
+    for (i = 0; i < 10; i++) {
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
+        if (ret_val)
+            return ret_val;
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        if (phy_data & MII_SR_LINK_STATUS) {
+            /* Config the MAC and PHY after link is up */
+            ret_val = e1000_copper_link_postconfig(hw);
+            if (ret_val)
                 return ret_val;
-            }
-            DEBUGOUT("Valid link established!!!\n");
 
-            if(hw->phy_type == e1000_phy_igp) {
-                ret_val = e1000_config_dsp_after_link_change(hw, TRUE);
-                if(ret_val) {
-                    DEBUGOUT("Error Configuring DSP after link up\n");
-                    return ret_val;
-                }
-            }
             DEBUGOUT("Valid link established!!!\n");
             return E1000_SUCCESS;
         }
-        udelay(10);
+        usec_delay(10);
     }
 
     DEBUGOUT("Unable to establish link!!!\n");
@@ -1256,6 +1894,79 @@
 }
 
 /******************************************************************************
+* Configure the MAC-to-PHY interface for 10/100Mbps
+*
+* hw - Struct containing variables accessed by shared code
+******************************************************************************/
+static int32_t
+e1000_configure_kmrn_for_10_100(struct e1000_hw *hw, uint16_t duplex)
+{
+    int32_t ret_val = E1000_SUCCESS;
+    uint32_t tipg;
+    uint16_t reg_data;
+
+    DEBUGFUNC("e1000_configure_kmrn_for_10_100");
+
+    reg_data = E1000_KUMCTRLSTA_HD_CTRL_10_100_DEFAULT;
+    ret_val = e1000_write_kmrn_reg(hw, E1000_KUMCTRLSTA_OFFSET_HD_CTRL,
+                                   reg_data);
+    if (ret_val)
+        return ret_val;
+
+    /* Configure Transmit Inter-Packet Gap */
+    tipg = E1000_READ_REG(hw, TIPG);
+    tipg &= ~E1000_TIPG_IPGT_MASK;
+    tipg |= DEFAULT_80003ES2LAN_TIPG_IPGT_10_100;
+    E1000_WRITE_REG(hw, TIPG, tipg);
+
+    ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, &reg_data);
+
+    if (ret_val)
+        return ret_val;
+
+    if (duplex == HALF_DUPLEX)
+        reg_data |= GG82563_KMCR_PASS_FALSE_CARRIER;
+    else
+        reg_data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+
+    ret_val = e1000_write_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, reg_data);
+
+    return ret_val;
+}
+
+static int32_t
+e1000_configure_kmrn_for_1000(struct e1000_hw *hw)
+{
+    int32_t ret_val = E1000_SUCCESS;
+    uint16_t reg_data;
+    uint32_t tipg;
+
+    DEBUGFUNC("e1000_configure_kmrn_for_1000");
+
+    reg_data = E1000_KUMCTRLSTA_HD_CTRL_1000_DEFAULT;
+    ret_val = e1000_write_kmrn_reg(hw, E1000_KUMCTRLSTA_OFFSET_HD_CTRL,
+                                   reg_data);
+    if (ret_val)
+        return ret_val;
+
+    /* Configure Transmit Inter-Packet Gap */
+    tipg = E1000_READ_REG(hw, TIPG);
+    tipg &= ~E1000_TIPG_IPGT_MASK;
+    tipg |= DEFAULT_80003ES2LAN_TIPG_IPGT_1000;
+    E1000_WRITE_REG(hw, TIPG, tipg);
+
+    ret_val = e1000_read_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, &reg_data);
+
+    if (ret_val)
+        return ret_val;
+
+    reg_data &= ~GG82563_KMCR_PASS_FALSE_CARRIER;
+    ret_val = e1000_write_phy_reg(hw, GG82563_PHY_KMRN_MODE_CTRL, reg_data);
+
+    return ret_val;
+}
+
+/******************************************************************************
 * Configures PHY autoneg and flow control advertisement settings
 *
 * hw - Struct containing variables accessed by shared code
@@ -1271,13 +1982,16 @@
 
     /* Read the MII Auto-Neg Advertisement Register (Address 4). */
     ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_ADV, &mii_autoneg_adv_reg);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
-    /* Read the MII 1000Base-T Control Register (Address 9). */
-    ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &mii_1000t_ctrl_reg);
-    if(ret_val)
-        return ret_val;
+    if (hw->phy_type != e1000_phy_ife) {
+        /* Read the MII 1000Base-T Control Register (Address 9). */
+        ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &mii_1000t_ctrl_reg);
+        if (ret_val)
+            return ret_val;
+    } else
+        mii_1000t_ctrl_reg=0;
 
     /* Need to parse both autoneg_advertised and fc and set up
      * the appropriate PHY registers.  First we will parse for
@@ -1296,38 +2010,41 @@
     DEBUGOUT1("autoneg_advertised %x\n", hw->autoneg_advertised);
 
     /* Do we want to advertise 10 Mb Half Duplex? */
-    if(hw->autoneg_advertised & ADVERTISE_10_HALF) {
+    if (hw->autoneg_advertised & ADVERTISE_10_HALF) {
         DEBUGOUT("Advertise 10mb Half duplex\n");
         mii_autoneg_adv_reg |= NWAY_AR_10T_HD_CAPS;
     }
 
     /* Do we want to advertise 10 Mb Full Duplex? */
-    if(hw->autoneg_advertised & ADVERTISE_10_FULL) {
+    if (hw->autoneg_advertised & ADVERTISE_10_FULL) {
         DEBUGOUT("Advertise 10mb Full duplex\n");
         mii_autoneg_adv_reg |= NWAY_AR_10T_FD_CAPS;
     }
 
     /* Do we want to advertise 100 Mb Half Duplex? */
-    if(hw->autoneg_advertised & ADVERTISE_100_HALF) {
+    if (hw->autoneg_advertised & ADVERTISE_100_HALF) {
         DEBUGOUT("Advertise 100mb Half duplex\n");
         mii_autoneg_adv_reg |= NWAY_AR_100TX_HD_CAPS;
     }
 
     /* Do we want to advertise 100 Mb Full Duplex? */
-    if(hw->autoneg_advertised & ADVERTISE_100_FULL) {
+    if (hw->autoneg_advertised & ADVERTISE_100_FULL) {
         DEBUGOUT("Advertise 100mb Full duplex\n");
         mii_autoneg_adv_reg |= NWAY_AR_100TX_FD_CAPS;
     }
 
     /* We do not allow the Phy to advertise 1000 Mb Half Duplex */
-    if(hw->autoneg_advertised & ADVERTISE_1000_HALF) {
+    if (hw->autoneg_advertised & ADVERTISE_1000_HALF) {
         DEBUGOUT("Advertise 1000mb Half duplex requested, request denied!\n");
     }
 
     /* Do we want to advertise 1000 Mb Full Duplex? */
-    if(hw->autoneg_advertised & ADVERTISE_1000_FULL) {
+    if (hw->autoneg_advertised & ADVERTISE_1000_FULL) {
         DEBUGOUT("Advertise 1000mb Full duplex\n");
         mii_1000t_ctrl_reg |= CR_1000T_FD_CAPS;
+        if (hw->phy_type == e1000_phy_ife) {
+            DEBUGOUT("e1000_phy_ife is a 10/100 PHY. Gigabit speed is not supported.\n");
+        }
     }
 
     /* Check for a software override of the flow control settings, and
@@ -1384,14 +2101,16 @@
     }
 
     ret_val = e1000_write_phy_reg(hw, PHY_AUTONEG_ADV, mii_autoneg_adv_reg);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     DEBUGOUT1("Auto-Neg Advertising %x\n", mii_autoneg_adv_reg);
 
-    ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, mii_1000t_ctrl_reg);
-    if(ret_val)
-        return ret_val;
+    if (hw->phy_type != e1000_phy_ife) {
+        ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, mii_1000t_ctrl_reg);
+        if (ret_val)
+            return ret_val;
+    }
 
     return E1000_SUCCESS;
 }
@@ -1430,7 +2149,7 @@
 
     /* Read the MII Control Register. */
     ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &mii_ctrl_reg);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     /* We need to disable autoneg in order to force link and duplex. */
@@ -1438,8 +2157,8 @@
     mii_ctrl_reg &= ~MII_CR_AUTO_NEG_EN;
 
     /* Are we forcing Full or Half Duplex? */
-    if(hw->forced_speed_duplex == e1000_100_full ||
-       hw->forced_speed_duplex == e1000_10_full) {
+    if (hw->forced_speed_duplex == e1000_100_full ||
+        hw->forced_speed_duplex == e1000_10_full) {
         /* We want to force full duplex so we SET the full duplex bits in the
          * Device and MII Control Registers.
          */
@@ -1456,7 +2175,7 @@
     }
 
     /* Are we forcing 100Mbps??? */
-    if(hw->forced_speed_duplex == e1000_100_full ||
+    if (hw->forced_speed_duplex == e1000_100_full ||
        hw->forced_speed_duplex == e1000_100_half) {
         /* Set the 100Mb bit and turn off the 1000Mb and 10Mb bits. */
         ctrl |= E1000_CTRL_SPD_100;
@@ -1476,9 +2195,10 @@
     /* Write the configured values back to the Device Control Reg. */
     E1000_WRITE_REG(hw, CTRL, ctrl);
 
-    if (hw->phy_type == e1000_phy_m88) {
+    if ((hw->phy_type == e1000_phy_m88) ||
+        (hw->phy_type == e1000_phy_gg82563)) {
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         /* Clear Auto-Crossover to force MDI manually. M88E1000 requires MDI
@@ -1486,35 +2206,47 @@
          */
         phy_data &= ~M88E1000_PSCR_AUTO_X_MODE;
         ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         DEBUGOUT1("M88E1000 PSCR: %x \n", phy_data);
 
         /* Need to reset the PHY or these changes will be ignored */
         mii_ctrl_reg |= MII_CR_RESET;
+    /* Disable MDI-X support for 10/100 */
+    } else if (hw->phy_type == e1000_phy_ife) {
+        ret_val = e1000_read_phy_reg(hw, IFE_PHY_MDIX_CONTROL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data &= ~IFE_PMC_AUTO_MDIX;
+        phy_data &= ~IFE_PMC_FORCE_MDIX;
+
+        ret_val = e1000_write_phy_reg(hw, IFE_PHY_MDIX_CONTROL, phy_data);
+        if (ret_val)
+            return ret_val;
     } else {
         /* Clear Auto-Crossover to force MDI manually.  IGP requires MDI
          * forced whenever speed or duplex are forced.
          */
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_data &= ~IGP01E1000_PSCR_AUTO_MDIX;
         phy_data &= ~IGP01E1000_PSCR_FORCE_MDI_MDIX;
 
         ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
     }
 
     /* Write back the modified PHY MII control register. */
     ret_val = e1000_write_phy_reg(hw, PHY_CTRL, mii_ctrl_reg);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
-    udelay(1);
+    usec_delay(1);
 
     /* The wait_autoneg_complete flag may be a little misleading here.
      * Since we are forcing speed and duplex, Auto-Neg is not enabled.
@@ -1523,48 +2255,50 @@
      * only if the user has set wait_autoneg_complete to 1, which is
      * the default.
      */
-    if(hw->wait_autoneg_complete) {
+    if (hw->wait_autoneg_complete) {
         /* We will wait for autoneg to complete. */
         DEBUGOUT("Waiting for forced speed/duplex link.\n");
         mii_status_reg = 0;
 
         /* We will wait for autoneg to complete or 4.5 seconds to expire. */
-        for(i = PHY_FORCE_TIME; i > 0; i--) {
+        for (i = PHY_FORCE_TIME; i > 0; i--) {
             /* Read the MII Status Register and wait for Auto-Neg Complete bit
              * to be set.
              */
             ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
-            if(mii_status_reg & MII_SR_LINK_STATUS) break;
+            if (mii_status_reg & MII_SR_LINK_STATUS) break;
             msec_delay(100);
         }
-        if((i == 0) && (hw->phy_type == e1000_phy_m88)) {
+        if ((i == 0) &&
+           ((hw->phy_type == e1000_phy_m88) ||
+            (hw->phy_type == e1000_phy_gg82563))) {
             /* We didn't get link.  Reset the DSP and wait again for link. */
             ret_val = e1000_phy_reset_dsp(hw);
-            if(ret_val) {
+            if (ret_val) {
                 DEBUGOUT("Error Resetting PHY DSP\n");
                 return ret_val;
             }
         }
         /* This loop will early-out if the link condition has been met.  */
-        for(i = PHY_FORCE_TIME; i > 0; i--) {
-            if(mii_status_reg & MII_SR_LINK_STATUS) break;
+        for (i = PHY_FORCE_TIME; i > 0; i--) {
+            if (mii_status_reg & MII_SR_LINK_STATUS) break;
             msec_delay(100);
             /* Read the MII Status Register and wait for Auto-Neg Complete bit
              * to be set.
              */
             ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
         }
     }
@@ -1575,24 +2309,53 @@
          * defaults back to a 2.5MHz clock when the PHY is reset.
          */
         ret_val = e1000_read_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_data |= M88E1000_EPSCR_TX_CLK_25;
         ret_val = e1000_write_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         /* In addition, because of the s/w reset above, we need to enable CRS on
          * TX.  This must be set for both full and half duplex operation.
          */
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_data |= M88E1000_PSCR_ASSERT_CRS_ON_TX;
         ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
-        if(ret_val)
+        if (ret_val)
+            return ret_val;
+
+        if ((hw->mac_type == e1000_82544 || hw->mac_type == e1000_82543) &&
+            (!hw->autoneg) && (hw->forced_speed_duplex == e1000_10_full ||
+             hw->forced_speed_duplex == e1000_10_half)) {
+            ret_val = e1000_polarity_reversal_workaround(hw);
+            if (ret_val)
+                return ret_val;
+        }
+    } else if (hw->phy_type == e1000_phy_gg82563) {
+        /* The TX_CLK of the Extended PHY Specific Control Register defaults
+         * to 2.5MHz on a reset.  We need to re-force it back to 25MHz, if
+         * we're not in a forced 10/duplex configuration. */
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data &= ~GG82563_MSCR_TX_CLK_MASK;
+        if ((hw->forced_speed_duplex == e1000_10_full) ||
+            (hw->forced_speed_duplex == e1000_10_half))
+            phy_data |= GG82563_MSCR_TX_CLK_10MBPS_2_5MHZ;
+        else
+            phy_data |= GG82563_MSCR_TX_CLK_100MBPS_25MHZ;
+
+        /* Also due to the reset, we need to enable CRS on Tx. */
+        phy_data |= GG82563_MSCR_ASSERT_CRS_ON_TX;
+
+        ret_val = e1000_write_phy_reg(hw, GG82563_PHY_MAC_SPEC_CTRL, phy_data);
+        if (ret_val)
             return ret_val;
     }
     return E1000_SUCCESS;
@@ -1609,14 +2372,19 @@
 void
 e1000_config_collision_dist(struct e1000_hw *hw)
 {
-    uint32_t tctl;
+    uint32_t tctl, coll_dist;
 
     DEBUGFUNC("e1000_config_collision_dist");
 
+    if (hw->mac_type < e1000_82543)
+        coll_dist = E1000_COLLISION_DISTANCE_82542;
+    else
+        coll_dist = E1000_COLLISION_DISTANCE;
+
     tctl = E1000_READ_REG(hw, TCTL);
 
     tctl &= ~E1000_TCTL_COLD;
-    tctl |= E1000_COLLISION_DISTANCE << E1000_COLD_SHIFT;
+    tctl |= coll_dist << E1000_COLD_SHIFT;
 
     E1000_WRITE_REG(hw, TCTL, tctl);
     E1000_WRITE_FLUSH(hw);
@@ -1640,6 +2408,11 @@
 
     DEBUGFUNC("e1000_config_mac_to_phy");
 
+    /* 82544 or newer MAC, Auto Speed Detection takes care of
+    * MAC speed/duplex configuration.*/
+    if (hw->mac_type >= e1000_82544)
+        return E1000_SUCCESS;
+
     /* Read the Device Control Register and set the bits to Force Speed
      * and Duplex.
      */
@@ -1650,45 +2423,25 @@
     /* Set up duplex in the Device Control and Transmit Control
      * registers depending on negotiated values.
      */
-    if (hw->phy_type == e1000_phy_igp) {
-        ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS,
-                                     &phy_data);
-        if(ret_val)
-            return ret_val;
-
-        if(phy_data & IGP01E1000_PSSR_FULL_DUPLEX) ctrl |= E1000_CTRL_FD;
-        else ctrl &= ~E1000_CTRL_FD;
-
-        e1000_config_collision_dist(hw);
+    ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
+    if (ret_val)
+        return ret_val;
 
-        /* Set up speed in the Device Control register depending on
-         * negotiated values.
-         */
-        if((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
-           IGP01E1000_PSSR_SPEED_1000MBPS)
-            ctrl |= E1000_CTRL_SPD_1000;
-        else if((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
-                IGP01E1000_PSSR_SPEED_100MBPS)
-            ctrl |= E1000_CTRL_SPD_100;
-    } else {
-        ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS,
-                                     &phy_data);
-        if(ret_val)
-            return ret_val;
+    if (phy_data & M88E1000_PSSR_DPLX)
+        ctrl |= E1000_CTRL_FD;
+    else
+        ctrl &= ~E1000_CTRL_FD;
 
-        if(phy_data & M88E1000_PSSR_DPLX) ctrl |= E1000_CTRL_FD;
-        else ctrl &= ~E1000_CTRL_FD;
+    e1000_config_collision_dist(hw);
 
-        e1000_config_collision_dist(hw);
+    /* Set up speed in the Device Control register depending on
+     * negotiated values.
+     */
+    if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_1000MBS)
+        ctrl |= E1000_CTRL_SPD_1000;
+    else if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_100MBS)
+        ctrl |= E1000_CTRL_SPD_100;
 
-        /* Set up speed in the Device Control register depending on
-         * negotiated values.
-         */
-        if((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_1000MBS)
-            ctrl |= E1000_CTRL_SPD_1000;
-        else if((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_100MBS)
-            ctrl |= E1000_CTRL_SPD_100;
-    }
     /* Write the configured values back to the Device Control Reg. */
     E1000_WRITE_REG(hw, CTRL, ctrl);
     return E1000_SUCCESS;
@@ -1754,7 +2507,7 @@
     }
 
     /* Disable TX Flow Control for 82542 (rev 2.0) */
-    if(hw->mac_type == e1000_82542_rev2_0)
+    if (hw->mac_type == e1000_82542_rev2_0)
         ctrl &= (~E1000_CTRL_TFCE);
 
     E1000_WRITE_REG(hw, CTRL, ctrl);
@@ -1788,11 +2541,12 @@
      * so we had to force link.  In this case, we need to force the
      * configuration of the MAC to match the "fc" parameter.
      */
-    if(((hw->media_type == e1000_media_type_fiber) && (hw->autoneg_failed)) ||
-       ((hw->media_type == e1000_media_type_internal_serdes) && (hw->autoneg_failed)) ||
-       ((hw->media_type == e1000_media_type_copper) && (!hw->autoneg))) {
+    if (((hw->media_type == e1000_media_type_fiber) && (hw->autoneg_failed)) ||
+        ((hw->media_type == e1000_media_type_internal_serdes) &&
+         (hw->autoneg_failed)) ||
+        ((hw->media_type == e1000_media_type_copper) && (!hw->autoneg))) {
         ret_val = e1000_force_mac_fc(hw);
-        if(ret_val) {
+        if (ret_val) {
             DEBUGOUT("Error forcing flow control settings\n");
             return ret_val;
         }
@@ -1803,19 +2557,19 @@
      * has completed, and if so, how the PHY and link partner has
      * flow control configured.
      */
-    if((hw->media_type == e1000_media_type_copper) && hw->autoneg) {
+    if ((hw->media_type == e1000_media_type_copper) && hw->autoneg) {
         /* Read the MII Status Register and check to see if AutoNeg
          * has completed.  We read this twice because this reg has
          * some "sticky" (latched) bits.
          */
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        if(mii_status_reg & MII_SR_AUTONEG_COMPLETE) {
+        if (mii_status_reg & MII_SR_AUTONEG_COMPLETE) {
             /* The AutoNeg process has completed, so we now need to
              * read both the Auto Negotiation Advertisement Register
              * (Address 4) and the Auto_Negotiation Base Page Ability
@@ -1824,11 +2578,11 @@
              */
             ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_ADV,
                                          &mii_nway_adv_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
             ret_val = e1000_read_phy_reg(hw, PHY_LP_ABILITY,
                                          &mii_nway_lp_ability_reg);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             /* Two bits in the Auto Negotiation Advertisement Register
@@ -1865,20 +2619,20 @@
              *   1   |   DC    |   1   |   DC    | e1000_fc_full
              *
              */
-            if((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
-               (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE)) {
+            if ((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+                (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE)) {
                 /* Now we need to check if the user selected RX ONLY
                  * of pause frames.  In this case, we had to advertise
                  * FULL flow control because we could not advertise RX
                  * ONLY. Hence, we must now check to see if we need to
                  * turn OFF  the TRANSMISSION of PAUSE frames.
                  */
-                if(hw->original_fc == e1000_fc_full) {
+                if (hw->original_fc == e1000_fc_full) {
                     hw->fc = e1000_fc_full;
-                    DEBUGOUT("Flow Control = FULL.\r\n");
+                    DEBUGOUT("Flow Control = FULL.\n");
                 } else {
                     hw->fc = e1000_fc_rx_pause;
-                    DEBUGOUT("Flow Control = RX PAUSE frames only.\r\n");
+                    DEBUGOUT("Flow Control = RX PAUSE frames only.\n");
                 }
             }
             /* For receiving PAUSE frames ONLY.
@@ -1889,12 +2643,12 @@
              *   0   |    1    |   1   |    1    | e1000_fc_tx_pause
              *
              */
-            else if(!(mii_nway_adv_reg & NWAY_AR_PAUSE) &&
-                    (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
-                    (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
-                    (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
+            else if (!(mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+                     (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
+                     (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
+                     (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
                 hw->fc = e1000_fc_tx_pause;
-                DEBUGOUT("Flow Control = TX PAUSE frames only.\r\n");
+                DEBUGOUT("Flow Control = TX PAUSE frames only.\n");
             }
             /* For transmitting PAUSE frames ONLY.
              *
@@ -1904,12 +2658,12 @@
              *   1   |    1    |   0   |    1    | e1000_fc_rx_pause
              *
              */
-            else if((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
-                    (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
-                    !(mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
-                    (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
+            else if ((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+                     (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
+                     !(mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
+                     (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
                 hw->fc = e1000_fc_rx_pause;
-                DEBUGOUT("Flow Control = RX PAUSE frames only.\r\n");
+                DEBUGOUT("Flow Control = RX PAUSE frames only.\n");
             }
             /* Per the IEEE spec, at this point flow control should be
              * disabled.  However, we want to consider that we could
@@ -1931,14 +2685,14 @@
              * be asked to delay transmission of packets than asking
              * our link partner to pause transmission of frames.
              */
-            else if((hw->original_fc == e1000_fc_none ||
-                     hw->original_fc == e1000_fc_tx_pause) ||
-                    hw->fc_strict_ieee) {
+            else if ((hw->original_fc == e1000_fc_none ||
+                      hw->original_fc == e1000_fc_tx_pause) ||
+                      hw->fc_strict_ieee) {
                 hw->fc = e1000_fc_none;
-                DEBUGOUT("Flow Control = NONE.\r\n");
+                DEBUGOUT("Flow Control = NONE.\n");
             } else {
                 hw->fc = e1000_fc_rx_pause;
-                DEBUGOUT("Flow Control = RX PAUSE frames only.\r\n");
+                DEBUGOUT("Flow Control = RX PAUSE frames only.\n");
             }
 
             /* Now we need to do one last check...  If we auto-
@@ -1946,24 +2700,24 @@
              * enabled per IEEE 802.3 spec.
              */
             ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
-            if(ret_val) {
+            if (ret_val) {
                 DEBUGOUT("Error getting link speed and duplex\n");
                 return ret_val;
             }
 
-            if(duplex == HALF_DUPLEX)
+            if (duplex == HALF_DUPLEX)
                 hw->fc = e1000_fc_none;
 
             /* Now we call a subroutine to actually force the MAC
              * controller to use the correct flow control settings.
              */
             ret_val = e1000_force_mac_fc(hw);
-            if(ret_val) {
+            if (ret_val) {
                 DEBUGOUT("Error forcing flow control settings\n");
                 return ret_val;
             }
         } else {
-            DEBUGOUT("Copper PHY and Auto Neg has not completed.\r\n");
+            DEBUGOUT("Copper PHY and Auto Neg has not completed.\n");
         }
     }
     return E1000_SUCCESS;
@@ -1983,6 +2737,7 @@
     uint32_t ctrl;
     uint32_t status;
     uint32_t rctl;
+    uint32_t icr;
     uint32_t signal = 0;
     int32_t ret_val;
     uint16_t phy_data;
@@ -1996,13 +2751,13 @@
      * set when the optics detect a signal. On older adapters, it will be
      * cleared when there is a signal.  This applies to fiber media only.
      */
-    if((hw->media_type == e1000_media_type_fiber) ||
-       (hw->media_type == e1000_media_type_internal_serdes)) {
+    if ((hw->media_type == e1000_media_type_fiber) ||
+        (hw->media_type == e1000_media_type_internal_serdes)) {
         rxcw = E1000_READ_REG(hw, RXCW);
 
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             signal = (hw->mac_type > e1000_82544) ? E1000_CTRL_SWDPIN1 : 0;
-            if(status & E1000_STATUS_LU)
+            if (status & E1000_STATUS_LU)
                 hw->get_link_status = FALSE;
         }
     }
@@ -2013,25 +2768,44 @@
      * receive a Link Status Change interrupt or we have Rx Sequence
      * Errors.
      */
-    if((hw->media_type == e1000_media_type_copper) && hw->get_link_status) {
+    if ((hw->media_type == e1000_media_type_copper) && hw->get_link_status) {
         /* First we want to see if the MII Status Register reports
          * link.  If so, then we want to get the current speed/duplex
          * of the PHY.
          * Read the register twice since the link bit is sticky.
          */
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        if(phy_data & MII_SR_LINK_STATUS) {
+        if (phy_data & MII_SR_LINK_STATUS) {
             hw->get_link_status = FALSE;
             /* Check if there was DownShift, must be checked immediately after
              * link-up */
             e1000_check_downshift(hw);
 
+            /* If we are on 82544 or 82543 silicon and speed/duplex
+             * are forced to 10H or 10F, then we will implement the polarity
+             * reversal workaround.  We disable interrupts first, and upon
+             * returning, place the devices interrupt state to its previous
+             * value except for the link status change interrupt which will
+             * happen due to the execution of this workaround.
+             */
+
+            if ((hw->mac_type == e1000_82544 || hw->mac_type == e1000_82543) &&
+                (!hw->autoneg) &&
+                (hw->forced_speed_duplex == e1000_10_full ||
+                 hw->forced_speed_duplex == e1000_10_half)) {
+                E1000_WRITE_REG(hw, IMC, 0xffffffff);
+                ret_val = e1000_polarity_reversal_workaround(hw);
+                icr = E1000_READ_REG(hw, ICR);
+                E1000_WRITE_REG(hw, ICS, (icr & ~E1000_ICS_LSC));
+                E1000_WRITE_REG(hw, IMS, IMS_ENABLE_MASK);
+            }
+
         } else {
             /* No link detected */
             e1000_config_dsp_after_link_change(hw, FALSE);
@@ -2041,7 +2815,7 @@
         /* If we are forcing speed/duplex, then we simply return since
          * we have already determined whether we have link or not.
          */
-        if(!hw->autoneg) return -E1000_ERR_CONFIG;
+        if (!hw->autoneg) return -E1000_ERR_CONFIG;
 
         /* optimize the dsp settings for the igp phy */
         e1000_config_dsp_after_link_change(hw, TRUE);
@@ -2054,11 +2828,11 @@
          * speed/duplex on the MAC to the current PHY speed/duplex
          * settings.
          */
-        if(hw->mac_type >= e1000_82544)
+        if (hw->mac_type >= e1000_82544)
             e1000_config_collision_dist(hw);
         else {
             ret_val = e1000_config_mac_to_phy(hw);
-            if(ret_val) {
+            if (ret_val) {
                 DEBUGOUT("Error configuring MAC to PHY settings\n");
                 return ret_val;
             }
@@ -2069,7 +2843,7 @@
          * have had to re-autoneg with a different link partner.
          */
         ret_val = e1000_config_fc_after_link_up(hw);
-        if(ret_val) {
+        if (ret_val) {
             DEBUGOUT("Error configuring flow control\n");
             return ret_val;
         }
@@ -2081,14 +2855,18 @@
          * at gigabit speed, then TBI compatibility is not needed.  If we are
          * at gigabit speed, we turn on TBI compatibility.
          */
-	if(hw->tbi_compatibility_en) {
+        if (hw->tbi_compatibility_en) {
             uint16_t speed, duplex;
-            e1000_get_speed_and_duplex(hw, &speed, &duplex);
-            if(speed != SPEED_1000) {
+            ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
+            if (ret_val) {
+                DEBUGOUT("Error getting link speed and duplex\n");
+                return ret_val;
+            }
+            if (speed != SPEED_1000) {
                 /* If link speed is not set to gigabit speed, we do not need
                  * to enable TBI compatibility.
                  */
-                if(hw->tbi_compatibility_on) {
+                if (hw->tbi_compatibility_on) {
                     /* If we previously were in the mode, turn it off. */
                     rctl = E1000_READ_REG(hw, RCTL);
                     rctl &= ~E1000_RCTL_SBP;
@@ -2101,7 +2879,7 @@
                  * packets. Some frames have an additional byte on the end and
                  * will look like CRC errors to to the hardware.
                  */
-                if(!hw->tbi_compatibility_on) {
+                if (!hw->tbi_compatibility_on) {
                     hw->tbi_compatibility_on = TRUE;
                     rctl = E1000_READ_REG(hw, RCTL);
                     rctl |= E1000_RCTL_SBP;
@@ -2117,16 +2895,16 @@
      * auto-negotiation time to complete, in case the cable was just plugged
      * in. The autoneg_failed flag does this.
      */
-    else if((((hw->media_type == e1000_media_type_fiber) &&
+    else if ((((hw->media_type == e1000_media_type_fiber) &&
               ((ctrl & E1000_CTRL_SWDPIN1) == signal)) ||
-             (hw->media_type == e1000_media_type_internal_serdes)) &&
-            (!(status & E1000_STATUS_LU)) &&
-            (!(rxcw & E1000_RXCW_C))) {
-        if(hw->autoneg_failed == 0) {
+              (hw->media_type == e1000_media_type_internal_serdes)) &&
+              (!(status & E1000_STATUS_LU)) &&
+              (!(rxcw & E1000_RXCW_C))) {
+        if (hw->autoneg_failed == 0) {
             hw->autoneg_failed = 1;
             return 0;
         }
-        DEBUGOUT("NOT RXing /C/, disable AutoNeg and force link.\r\n");
+        DEBUGOUT("NOT RXing /C/, disable AutoNeg and force link.\n");
 
         /* Disable auto-negotiation in the TXCW register */
         E1000_WRITE_REG(hw, TXCW, (hw->txcw & ~E1000_TXCW_ANE));
@@ -2138,7 +2916,7 @@
 
         /* Configure Flow Control after forcing link up. */
         ret_val = e1000_config_fc_after_link_up(hw);
-        if(ret_val) {
+        if (ret_val) {
             DEBUGOUT("Error configuring flow control\n");
             return ret_val;
         }
@@ -2148,10 +2926,10 @@
      * Device Control register in an attempt to auto-negotiate with our link
      * partner.
      */
-    else if(((hw->media_type == e1000_media_type_fiber) ||
-             (hw->media_type == e1000_media_type_internal_serdes)) &&
-            (ctrl & E1000_CTRL_SLU) && (rxcw & E1000_RXCW_C)) {
-        DEBUGOUT("RXing /C/, enable AutoNeg and stop forcing link.\r\n");
+    else if (((hw->media_type == e1000_media_type_fiber) ||
+              (hw->media_type == e1000_media_type_internal_serdes)) &&
+              (ctrl & E1000_CTRL_SLU) && (rxcw & E1000_RXCW_C)) {
+        DEBUGOUT("RXing /C/, enable AutoNeg and stop forcing link.\n");
         E1000_WRITE_REG(hw, TXCW, hw->txcw);
         E1000_WRITE_REG(hw, CTRL, (ctrl & ~E1000_CTRL_SLU));
 
@@ -2160,12 +2938,12 @@
     /* If we force link for non-auto-negotiation switch, check link status
      * based on MAC synchronization for internal serdes media type.
      */
-    else if((hw->media_type == e1000_media_type_internal_serdes) &&
-            !(E1000_TXCW_ANE & E1000_READ_REG(hw, TXCW))) {
+    else if ((hw->media_type == e1000_media_type_internal_serdes) &&
+             !(E1000_TXCW_ANE & E1000_READ_REG(hw, TXCW))) {
         /* SYNCH bit and IV bit are sticky. */
-        udelay(10);
-        if(E1000_RXCW_SYNCH & E1000_READ_REG(hw, RXCW)) {
-            if(!(rxcw & E1000_RXCW_IV)) {
+        usec_delay(10);
+        if (E1000_RXCW_SYNCH & E1000_READ_REG(hw, RXCW)) {
+            if (!(rxcw & E1000_RXCW_IV)) {
                 hw->serdes_link_down = FALSE;
                 DEBUGOUT("SERDES: Link is up.\n");
             }
@@ -2174,8 +2952,8 @@
             DEBUGOUT("SERDES: Link is down.\n");
         }
     }
-    if((hw->media_type == e1000_media_type_internal_serdes) &&
-       (E1000_TXCW_ANE & E1000_READ_REG(hw, TXCW))) {
+    if ((hw->media_type == e1000_media_type_internal_serdes) &&
+        (E1000_TXCW_ANE & E1000_READ_REG(hw, TXCW))) {
         hw->serdes_link_down = !(E1000_STATUS_LU & E1000_READ_REG(hw, STATUS));
     }
     return E1000_SUCCESS;
@@ -2199,12 +2977,12 @@
 
     DEBUGFUNC("e1000_get_speed_and_duplex");
 
-    if(hw->mac_type >= e1000_82543) {
+    if (hw->mac_type >= e1000_82543) {
         status = E1000_READ_REG(hw, STATUS);
-        if(status & E1000_STATUS_SPEED_1000) {
+        if (status & E1000_STATUS_SPEED_1000) {
             *speed = SPEED_1000;
             DEBUGOUT("1000 Mbs, ");
-        } else if(status & E1000_STATUS_SPEED_100) {
+        } else if (status & E1000_STATUS_SPEED_100) {
             *speed = SPEED_100;
             DEBUGOUT("100 Mbs, ");
         } else {
@@ -2212,15 +2990,15 @@
             DEBUGOUT("10 Mbs, ");
         }
 
-        if(status & E1000_STATUS_FD) {
+        if (status & E1000_STATUS_FD) {
             *duplex = FULL_DUPLEX;
-            DEBUGOUT("Full Duplex\r\n");
+            DEBUGOUT("Full Duplex\n");
         } else {
             *duplex = HALF_DUPLEX;
-            DEBUGOUT(" Half Duplex\r\n");
+            DEBUGOUT(" Half Duplex\n");
         }
     } else {
-        DEBUGOUT("1000 Mbs, Full Duplex\r\n");
+        DEBUGOUT("1000 Mbs, Full Duplex\n");
         *speed = SPEED_1000;
         *duplex = FULL_DUPLEX;
     }
@@ -2229,23 +3007,39 @@
      * if it is operating at half duplex.  Here we set the duplex settings to
      * match the duplex in the link partner's capabilities.
      */
-    if(hw->phy_type == e1000_phy_igp && hw->speed_downgraded) {
+    if (hw->phy_type == e1000_phy_igp && hw->speed_downgraded) {
         ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_EXP, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        if(!(phy_data & NWAY_ER_LP_NWAY_CAPS))
+        if (!(phy_data & NWAY_ER_LP_NWAY_CAPS))
             *duplex = HALF_DUPLEX;
         else {
             ret_val = e1000_read_phy_reg(hw, PHY_LP_ABILITY, &phy_data);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
-            if((*speed == SPEED_100 && !(phy_data & NWAY_LPAR_100TX_FD_CAPS)) ||
+            if ((*speed == SPEED_100 && !(phy_data & NWAY_LPAR_100TX_FD_CAPS)) ||
                (*speed == SPEED_10 && !(phy_data & NWAY_LPAR_10T_FD_CAPS)))
                 *duplex = HALF_DUPLEX;
         }
     }
 
+    if ((hw->mac_type == e1000_80003es2lan) &&
+        (hw->media_type == e1000_media_type_copper)) {
+        if (*speed == SPEED_1000)
+            ret_val = e1000_configure_kmrn_for_1000(hw);
+        else
+            ret_val = e1000_configure_kmrn_for_10_100(hw, *duplex);
+        if (ret_val)
+            return ret_val;
+    }
+
+    if ((hw->phy_type == e1000_phy_igp_3) && (*speed == SPEED_1000)) {
+        ret_val = e1000_kumeran_lock_loss_workaround(hw);
+        if (ret_val)
+            return ret_val;
+    }
+
     return E1000_SUCCESS;
 }
 
@@ -2265,17 +3059,17 @@
     DEBUGOUT("Waiting for Auto-Neg to complete.\n");
 
     /* We will wait for autoneg to complete or 4.5 seconds to expire. */
-    for(i = PHY_AUTO_NEG_TIME; i > 0; i--) {
+    for (i = PHY_AUTO_NEG_TIME; i > 0; i--) {
         /* Read the MII Status Register and wait for Auto-Neg
          * Complete bit to be set.
          */
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
-        if(phy_data & MII_SR_AUTONEG_COMPLETE) {
+        if (phy_data & MII_SR_AUTONEG_COMPLETE) {
             return E1000_SUCCESS;
         }
         msec_delay(100);
@@ -2298,7 +3092,7 @@
      */
     E1000_WRITE_REG(hw, CTRL, (*ctrl | E1000_CTRL_MDC));
     E1000_WRITE_FLUSH(hw);
-    udelay(10);
+    usec_delay(10);
 }
 
 /******************************************************************************
@@ -2316,7 +3110,7 @@
      */
     E1000_WRITE_REG(hw, CTRL, (*ctrl & ~E1000_CTRL_MDC));
     E1000_WRITE_FLUSH(hw);
-    udelay(10);
+    usec_delay(10);
 }
 
 /******************************************************************************
@@ -2348,19 +3142,21 @@
     /* Set MDIO_DIR and MDC_DIR direction bits to be used as output pins. */
     ctrl |= (E1000_CTRL_MDIO_DIR | E1000_CTRL_MDC_DIR);
 
-    while(mask) {
+    while (mask) {
         /* A "1" is shifted out to the PHY by setting the MDIO bit to "1" and
          * then raising and lowering the Management Data Clock. A "0" is
          * shifted out to the PHY by setting the MDIO bit to "0" and then
          * raising and lowering the clock.
          */
-        if(data & mask) ctrl |= E1000_CTRL_MDIO;
-        else ctrl &= ~E1000_CTRL_MDIO;
+        if (data & mask)
+            ctrl |= E1000_CTRL_MDIO;
+        else
+            ctrl &= ~E1000_CTRL_MDIO;
 
         E1000_WRITE_REG(hw, CTRL, ctrl);
         E1000_WRITE_FLUSH(hw);
 
-        udelay(10);
+        usec_delay(10);
 
         e1000_raise_mdi_clk(hw, &ctrl);
         e1000_lower_mdi_clk(hw, &ctrl);
@@ -2406,12 +3202,13 @@
     e1000_raise_mdi_clk(hw, &ctrl);
     e1000_lower_mdi_clk(hw, &ctrl);
 
-    for(data = 0, i = 0; i < 16; i++) {
+    for (data = 0, i = 0; i < 16; i++) {
         data = data << 1;
         e1000_raise_mdi_clk(hw, &ctrl);
         ctrl = E1000_READ_REG(hw, CTRL);
         /* Check to see if we shifted in a "1". */
-        if(ctrl & E1000_CTRL_MDIO) data |= 1;
+        if (ctrl & E1000_CTRL_MDIO)
+            data |= 1;
         e1000_lower_mdi_clk(hw, &ctrl);
     }
 
@@ -2421,6 +3218,80 @@
     return data;
 }
 
+int32_t
+e1000_swfw_sync_acquire(struct e1000_hw *hw, uint16_t mask)
+{
+    uint32_t swfw_sync = 0;
+    uint32_t swmask = mask;
+    uint32_t fwmask = mask << 16;
+    int32_t timeout = 200;
+
+    DEBUGFUNC("e1000_swfw_sync_acquire");
+
+    if (hw->swfwhw_semaphore_present)
+        return e1000_get_software_flag(hw);
+
+    if (!hw->swfw_sync_present)
+        return e1000_get_hw_eeprom_semaphore(hw);
+
+    while (timeout) {
+            if (e1000_get_hw_eeprom_semaphore(hw))
+                return -E1000_ERR_SWFW_SYNC;
+
+            swfw_sync = E1000_READ_REG(hw, SW_FW_SYNC);
+            if (!(swfw_sync & (fwmask | swmask))) {
+                break;
+            }
+
+            /* firmware currently using resource (fwmask) */
+            /* or other software thread currently using resource (swmask) */
+            e1000_put_hw_eeprom_semaphore(hw);
+            msec_delay_irq(5);
+            timeout--;
+    }
+
+    if (!timeout) {
+        DEBUGOUT("Driver can't access resource, SW_FW_SYNC timeout.\n");
+        return -E1000_ERR_SWFW_SYNC;
+    }
+
+    swfw_sync |= swmask;
+    E1000_WRITE_REG(hw, SW_FW_SYNC, swfw_sync);
+
+    e1000_put_hw_eeprom_semaphore(hw);
+    return E1000_SUCCESS;
+}
+
+void
+e1000_swfw_sync_release(struct e1000_hw *hw, uint16_t mask)
+{
+    uint32_t swfw_sync;
+    uint32_t swmask = mask;
+
+    DEBUGFUNC("e1000_swfw_sync_release");
+
+    if (hw->swfwhw_semaphore_present) {
+        e1000_release_software_flag(hw);
+        return;
+    }
+
+    if (!hw->swfw_sync_present) {
+        e1000_put_hw_eeprom_semaphore(hw);
+        return;
+    }
+
+    /* if (e1000_get_hw_eeprom_semaphore(hw))
+     *    return -E1000_ERR_SWFW_SYNC; */
+    while (e1000_get_hw_eeprom_semaphore(hw) != E1000_SUCCESS);
+        /* empty */
+
+    swfw_sync = E1000_READ_REG(hw, SW_FW_SYNC);
+    swfw_sync &= ~swmask;
+    E1000_WRITE_REG(hw, SW_FW_SYNC, swfw_sync);
+
+    e1000_put_hw_eeprom_semaphore(hw);
+}
+
 /*****************************************************************************
 * Reads the value from a PHY register, if the value is on a specific non zero
 * page, sets the page first.
@@ -2433,20 +3304,56 @@
                    uint16_t *phy_data)
 {
     uint32_t ret_val;
+    uint16_t swfw;
 
     DEBUGFUNC("e1000_read_phy_reg");
 
-    if(hw->phy_type == e1000_phy_igp &&
+    if ((hw->mac_type == e1000_80003es2lan) &&
+        (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)) {
+        swfw = E1000_SWFW_PHY1_SM;
+    } else {
+        swfw = E1000_SWFW_PHY0_SM;
+    }
+    if (e1000_swfw_sync_acquire(hw, swfw))
+        return -E1000_ERR_SWFW_SYNC;
+
+    if ((hw->phy_type == e1000_phy_igp ||
+        hw->phy_type == e1000_phy_igp_3 ||
+        hw->phy_type == e1000_phy_igp_2) &&
        (reg_addr > MAX_PHY_MULTI_PAGE_REG)) {
         ret_val = e1000_write_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT,
                                          (uint16_t)reg_addr);
-        if(ret_val)
+        if (ret_val) {
+            e1000_swfw_sync_release(hw, swfw);
             return ret_val;
+        }
+    } else if (hw->phy_type == e1000_phy_gg82563) {
+        if (((reg_addr & MAX_PHY_REG_ADDRESS) > MAX_PHY_MULTI_PAGE_REG) ||
+            (hw->mac_type == e1000_80003es2lan)) {
+            /* Select Configuration Page */
+            if ((reg_addr & MAX_PHY_REG_ADDRESS) < GG82563_MIN_ALT_REG) {
+                ret_val = e1000_write_phy_reg_ex(hw, GG82563_PHY_PAGE_SELECT,
+                          (uint16_t)((uint16_t)reg_addr >> GG82563_PAGE_SHIFT));
+            } else {
+                /* Use Alternative Page Select register to access
+                 * registers 30 and 31
+                 */
+                ret_val = e1000_write_phy_reg_ex(hw,
+                                                 GG82563_PHY_PAGE_SELECT_ALT,
+                          (uint16_t)((uint16_t)reg_addr >> GG82563_PAGE_SHIFT));
+            }
+
+            if (ret_val) {
+                e1000_swfw_sync_release(hw, swfw);
+                return ret_val;
+            }
+        }
     }
 
-    ret_val = e1000_read_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT & reg_addr,
+    ret_val = e1000_read_phy_reg_ex(hw, MAX_PHY_REG_ADDRESS & reg_addr,
                                     phy_data);
 
+    e1000_swfw_sync_release(hw, swfw);
     return ret_val;
 }
 
@@ -2461,12 +3368,12 @@
 
     DEBUGFUNC("e1000_read_phy_reg_ex");
 
-    if(reg_addr > MAX_PHY_REG_ADDRESS) {
+    if (reg_addr > MAX_PHY_REG_ADDRESS) {
         DEBUGOUT1("PHY Address %d is out of range\n", reg_addr);
         return -E1000_ERR_PARAM;
     }
 
-    if(hw->mac_type > e1000_82543) {
+    if (hw->mac_type > e1000_82543) {
         /* Set up Op-code, Phy Address, and register address in the MDI
          * Control register.  The MAC will take care of interfacing with the
          * PHY to retrieve the desired data.
@@ -2478,16 +3385,16 @@
         E1000_WRITE_REG(hw, MDIC, mdic);
 
         /* Poll the ready bit to see if the MDI read completed */
-        for(i = 0; i < 64; i++) {
-            udelay(50);
+        for (i = 0; i < 64; i++) {
+            usec_delay(50);
             mdic = E1000_READ_REG(hw, MDIC);
-            if(mdic & E1000_MDIC_READY) break;
+            if (mdic & E1000_MDIC_READY) break;
         }
-        if(!(mdic & E1000_MDIC_READY)) {
+        if (!(mdic & E1000_MDIC_READY)) {
             DEBUGOUT("MDI Read did not complete\n");
             return -E1000_ERR_PHY;
         }
-        if(mdic & E1000_MDIC_ERROR) {
+        if (mdic & E1000_MDIC_ERROR) {
             DEBUGOUT("MDI Error\n");
             return -E1000_ERR_PHY;
         }
@@ -2537,20 +3444,56 @@
                     uint16_t phy_data)
 {
     uint32_t ret_val;
+    uint16_t swfw;
 
     DEBUGFUNC("e1000_write_phy_reg");
 
-    if(hw->phy_type == e1000_phy_igp &&
+    if ((hw->mac_type == e1000_80003es2lan) &&
+        (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)) {
+        swfw = E1000_SWFW_PHY1_SM;
+    } else {
+        swfw = E1000_SWFW_PHY0_SM;
+    }
+    if (e1000_swfw_sync_acquire(hw, swfw))
+        return -E1000_ERR_SWFW_SYNC;
+
+    if ((hw->phy_type == e1000_phy_igp ||
+        hw->phy_type == e1000_phy_igp_3 ||
+        hw->phy_type == e1000_phy_igp_2) &&
        (reg_addr > MAX_PHY_MULTI_PAGE_REG)) {
         ret_val = e1000_write_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT,
                                          (uint16_t)reg_addr);
-        if(ret_val)
+        if (ret_val) {
+            e1000_swfw_sync_release(hw, swfw);
             return ret_val;
+        }
+    } else if (hw->phy_type == e1000_phy_gg82563) {
+        if (((reg_addr & MAX_PHY_REG_ADDRESS) > MAX_PHY_MULTI_PAGE_REG) ||
+            (hw->mac_type == e1000_80003es2lan)) {
+            /* Select Configuration Page */
+            if ((reg_addr & MAX_PHY_REG_ADDRESS) < GG82563_MIN_ALT_REG) {
+                ret_val = e1000_write_phy_reg_ex(hw, GG82563_PHY_PAGE_SELECT,
+                          (uint16_t)((uint16_t)reg_addr >> GG82563_PAGE_SHIFT));
+            } else {
+                /* Use Alternative Page Select register to access
+                 * registers 30 and 31
+                 */
+                ret_val = e1000_write_phy_reg_ex(hw,
+                                                 GG82563_PHY_PAGE_SELECT_ALT,
+                          (uint16_t)((uint16_t)reg_addr >> GG82563_PAGE_SHIFT));
+            }
+
+            if (ret_val) {
+                e1000_swfw_sync_release(hw, swfw);
+                return ret_val;
+            }
+        }
     }
 
-    ret_val = e1000_write_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT & reg_addr,
+    ret_val = e1000_write_phy_reg_ex(hw, MAX_PHY_REG_ADDRESS & reg_addr,
                                      phy_data);
 
+    e1000_swfw_sync_release(hw, swfw);
     return ret_val;
 }
 
@@ -2565,12 +3508,12 @@
 
     DEBUGFUNC("e1000_write_phy_reg_ex");
 
-    if(reg_addr > MAX_PHY_REG_ADDRESS) {
+    if (reg_addr > MAX_PHY_REG_ADDRESS) {
         DEBUGOUT1("PHY Address %d is out of range\n", reg_addr);
         return -E1000_ERR_PARAM;
     }
 
-    if(hw->mac_type > e1000_82543) {
+    if (hw->mac_type > e1000_82543) {
         /* Set up Op-code, Phy Address, register address, and data intended
          * for the PHY register in the MDI Control register.  The MAC will take
          * care of interfacing with the PHY to send the desired data.
@@ -2583,12 +3526,12 @@
         E1000_WRITE_REG(hw, MDIC, mdic);
 
         /* Poll the ready bit to see if the MDI read completed */
-        for(i = 0; i < 640; i++) {
-            udelay(5);
+        for (i = 0; i < 641; i++) {
+            usec_delay(5);
             mdic = E1000_READ_REG(hw, MDIC);
-            if(mdic & E1000_MDIC_READY) break;
+            if (mdic & E1000_MDIC_READY) break;
         }
-        if(!(mdic & E1000_MDIC_READY)) {
+        if (!(mdic & E1000_MDIC_READY)) {
             DEBUGOUT("MDI Write did not complete\n");
             return -E1000_ERR_PHY;
         }
@@ -2617,31 +3560,121 @@
     return E1000_SUCCESS;
 }
 
+int32_t
+e1000_read_kmrn_reg(struct e1000_hw *hw,
+                    uint32_t reg_addr,
+                    uint16_t *data)
+{
+    uint32_t reg_val;
+    uint16_t swfw;
+    DEBUGFUNC("e1000_read_kmrn_reg");
+
+    if ((hw->mac_type == e1000_80003es2lan) &&
+        (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)) {
+        swfw = E1000_SWFW_PHY1_SM;
+    } else {
+        swfw = E1000_SWFW_PHY0_SM;
+    }
+    if (e1000_swfw_sync_acquire(hw, swfw))
+        return -E1000_ERR_SWFW_SYNC;
+
+    /* Write register address */
+    reg_val = ((reg_addr << E1000_KUMCTRLSTA_OFFSET_SHIFT) &
+              E1000_KUMCTRLSTA_OFFSET) |
+              E1000_KUMCTRLSTA_REN;
+    E1000_WRITE_REG(hw, KUMCTRLSTA, reg_val);
+    usec_delay(2);
+
+    /* Read the data returned */
+    reg_val = E1000_READ_REG(hw, KUMCTRLSTA);
+    *data = (uint16_t)reg_val;
+
+    e1000_swfw_sync_release(hw, swfw);
+    return E1000_SUCCESS;
+}
+
+int32_t
+e1000_write_kmrn_reg(struct e1000_hw *hw,
+                     uint32_t reg_addr,
+                     uint16_t data)
+{
+    uint32_t reg_val;
+    uint16_t swfw;
+    DEBUGFUNC("e1000_write_kmrn_reg");
+
+    if ((hw->mac_type == e1000_80003es2lan) &&
+        (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)) {
+        swfw = E1000_SWFW_PHY1_SM;
+    } else {
+        swfw = E1000_SWFW_PHY0_SM;
+    }
+    if (e1000_swfw_sync_acquire(hw, swfw))
+        return -E1000_ERR_SWFW_SYNC;
+
+    reg_val = ((reg_addr << E1000_KUMCTRLSTA_OFFSET_SHIFT) &
+              E1000_KUMCTRLSTA_OFFSET) | data;
+    E1000_WRITE_REG(hw, KUMCTRLSTA, reg_val);
+    usec_delay(2);
+
+    e1000_swfw_sync_release(hw, swfw);
+    return E1000_SUCCESS;
+}
+
 /******************************************************************************
 * Returns the PHY to the power-on reset state
 *
 * hw - Struct containing variables accessed by shared code
 ******************************************************************************/
-void
+int32_t
 e1000_phy_hw_reset(struct e1000_hw *hw)
 {
     uint32_t ctrl, ctrl_ext;
     uint32_t led_ctrl;
+    int32_t ret_val;
+    uint16_t swfw;
 
     DEBUGFUNC("e1000_phy_hw_reset");
 
+    /* In the case of the phy reset being blocked, it's not an error, we
+     * simply return success without performing the reset. */
+    ret_val = e1000_check_phy_reset_block(hw);
+    if (ret_val)
+        return E1000_SUCCESS;
+
     DEBUGOUT("Resetting Phy...\n");
 
-    if(hw->mac_type > e1000_82543) {
+    if (hw->mac_type > e1000_82543) {
+        if ((hw->mac_type == e1000_80003es2lan) &&
+            (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)) {
+            swfw = E1000_SWFW_PHY1_SM;
+        } else {
+            swfw = E1000_SWFW_PHY0_SM;
+        }
+        if (e1000_swfw_sync_acquire(hw, swfw)) {
+            e1000_release_software_semaphore(hw);
+            return -E1000_ERR_SWFW_SYNC;
+        }
         /* Read the device control register and assert the E1000_CTRL_PHY_RST
          * bit. Then, take it out of reset.
+         * For pre-e1000_82571 hardware, we delay for 10ms between the assert
+         * and deassert.  For e1000_82571 hardware and later, we instead delay
+         * for 50us between and 10ms after the deassertion.
          */
         ctrl = E1000_READ_REG(hw, CTRL);
         E1000_WRITE_REG(hw, CTRL, ctrl | E1000_CTRL_PHY_RST);
         E1000_WRITE_FLUSH(hw);
-        msec_delay(10);
+
+        if (hw->mac_type < e1000_82571)
+            msec_delay(10);
+        else
+            usec_delay(100);
+
         E1000_WRITE_REG(hw, CTRL, ctrl);
         E1000_WRITE_FLUSH(hw);
+
+        if (hw->mac_type >= e1000_82571)
+            msec_delay_irq(10);
+        e1000_swfw_sync_release(hw, swfw);
     } else {
         /* Read the Extended Device Control Register, assert the PHY_RESET_DIR
          * bit to put the PHY into reset. Then, take it out of reset.
@@ -2656,15 +3689,26 @@
         E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
         E1000_WRITE_FLUSH(hw);
     }
-    udelay(150);
+    usec_delay(150);
 
-    if((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
+    if ((hw->mac_type == e1000_82541) || (hw->mac_type == e1000_82547)) {
         /* Configure activity LED after PHY reset */
         led_ctrl = E1000_READ_REG(hw, LEDCTL);
         led_ctrl &= IGP_ACTIVITY_LED_MASK;
         led_ctrl |= (IGP_ACTIVITY_LED_ENABLE | IGP_LED3_MODE);
         E1000_WRITE_REG(hw, LEDCTL, led_ctrl);
     }
+
+    /* Wait for FW to finish PHY configuration. */
+    ret_val = e1000_get_phy_cfg_done(hw);
+    if (ret_val != E1000_SUCCESS)
+        return ret_val;
+    e1000_release_software_semaphore(hw);
+
+    if ((hw->mac_type == e1000_ich8lan) && (hw->phy_type == e1000_phy_igp_3))
+        ret_val = e1000_init_lcd_from_nvm(hw);
+
+    return ret_val;
 }
 
 /******************************************************************************
@@ -2682,26 +3726,152 @@
 
     DEBUGFUNC("e1000_phy_reset");
 
-    if(hw->mac_type != e1000_82541_rev_2) {
+    /* In the case of the phy reset being blocked, it's not an error, we
+     * simply return success without performing the reset. */
+    ret_val = e1000_check_phy_reset_block(hw);
+    if (ret_val)
+        return E1000_SUCCESS;
+
+    switch (hw->mac_type) {
+    case e1000_82541_rev_2:
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_ich8lan:
+        ret_val = e1000_phy_hw_reset(hw);
+        if (ret_val)
+            return ret_val;
+
+        break;
+    default:
         ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_data |= MII_CR_RESET;
         ret_val = e1000_write_phy_reg(hw, PHY_CTRL, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        udelay(1);
-    } else e1000_phy_hw_reset(hw);
+        usec_delay(1);
+        break;
+    }
 
-    if(hw->phy_type == e1000_phy_igp)
+    if (hw->phy_type == e1000_phy_igp || hw->phy_type == e1000_phy_igp_2)
         e1000_phy_init_script(hw);
 
     return E1000_SUCCESS;
 }
 
 /******************************************************************************
+* Work-around for 82566 power-down: on D3 entry-
+* 1) disable gigabit link
+* 2) write VR power-down enable
+* 3) read it back
+* if successful continue, else issue LCD reset and repeat
+*
+* hw - struct containing variables accessed by shared code
+******************************************************************************/
+void
+e1000_phy_powerdown_workaround(struct e1000_hw *hw)
+{
+    int32_t reg;
+    uint16_t phy_data;
+    int32_t retry = 0;
+
+    DEBUGFUNC("e1000_phy_powerdown_workaround");
+
+    if (hw->phy_type != e1000_phy_igp_3)
+        return;
+
+    do {
+        /* Disable link */
+        reg = E1000_READ_REG(hw, PHY_CTRL);
+        E1000_WRITE_REG(hw, PHY_CTRL, reg | E1000_PHY_CTRL_GBE_DISABLE |
+                        E1000_PHY_CTRL_NOND0A_GBE_DISABLE);
+
+        /* Write VR power-down enable */
+        e1000_read_phy_reg(hw, IGP3_VR_CTRL, &phy_data);
+        e1000_write_phy_reg(hw, IGP3_VR_CTRL, phy_data |
+                            IGP3_VR_CTRL_MODE_SHUT);
+
+        /* Read it back and test */
+        e1000_read_phy_reg(hw, IGP3_VR_CTRL, &phy_data);
+        if ((phy_data & IGP3_VR_CTRL_MODE_SHUT) || retry)
+            break;
+
+        /* Issue PHY reset and repeat at most one more time */
+        reg = E1000_READ_REG(hw, CTRL);
+        E1000_WRITE_REG(hw, CTRL, reg | E1000_CTRL_PHY_RST);
+        retry++;
+    } while (retry);
+
+    return;
+
+}
+
+/******************************************************************************
+* Work-around for 82566 Kumeran PCS lock loss:
+* On link status change (i.e. PCI reset, speed change) and link is up and
+* speed is gigabit-
+* 0) if workaround is optionally disabled do nothing
+* 1) wait 1ms for Kumeran link to come up
+* 2) check Kumeran Diagnostic register PCS lock loss bit
+* 3) if not set the link is locked (all is good), otherwise...
+* 4) reset the PHY
+* 5) repeat up to 10 times
+* Note: this is only called for IGP3 copper when speed is 1gb.
+*
+* hw - struct containing variables accessed by shared code
+******************************************************************************/
+int32_t
+e1000_kumeran_lock_loss_workaround(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    int32_t reg;
+    int32_t cnt;
+    uint16_t phy_data;
+
+    if (hw->kmrn_lock_loss_workaround_disabled)
+        return E1000_SUCCESS;
+
+    /* Make sure link is up before proceeding.  If not just return.
+     * Attempting this while link is negotiating fouled up link
+     * stability */
+    ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
+    ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
+
+    if (phy_data & MII_SR_LINK_STATUS) {
+        for (cnt = 0; cnt < 10; cnt++) {
+            /* read once to clear */
+            ret_val = e1000_read_phy_reg(hw, IGP3_KMRN_DIAG, &phy_data);
+            if (ret_val)
+                return ret_val;
+            /* and again to get new status */
+            ret_val = e1000_read_phy_reg(hw, IGP3_KMRN_DIAG, &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            /* check for PCS lock */
+            if (!(phy_data & IGP3_KMRN_DIAG_PCS_LOCK_LOSS))
+                return E1000_SUCCESS;
+
+            /* Issue PHY reset */
+            e1000_phy_hw_reset(hw);
+            msec_delay_irq(5);
+        }
+        /* Disable GigE link negotiation */
+        reg = E1000_READ_REG(hw, PHY_CTRL);
+        E1000_WRITE_REG(hw, PHY_CTRL, reg | E1000_PHY_CTRL_GBE_DISABLE |
+                        E1000_PHY_CTRL_NOND0A_GBE_DISABLE);
+
+        /* unable to acquire PCS lock */
+        return E1000_ERR_PHY;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/******************************************************************************
 * Probes the expected PHY address for known PHY IDs
 *
 * hw - Struct containing variables accessed by shared code
@@ -2715,39 +3885,70 @@
 
     DEBUGFUNC("e1000_detect_gig_phy");
 
+    /* The 82571 firmware may still be configuring the PHY.  In this
+     * case, we cannot access the PHY until the configuration is done.  So
+     * we explicitly set the PHY values. */
+    if (hw->mac_type == e1000_82571 ||
+        hw->mac_type == e1000_82572) {
+        hw->phy_id = IGP01E1000_I_PHY_ID;
+        hw->phy_type = e1000_phy_igp_2;
+        return E1000_SUCCESS;
+    }
+
+    /* ESB-2 PHY reads require e1000_phy_gg82563 to be set because of a work-
+     * around that forces PHY page 0 to be set or the reads fail.  The rest of
+     * the code in this routine uses e1000_read_phy_reg to read the PHY ID.
+     * So for ESB-2 we need to have this set so our reads won't fail.  If the
+     * attached PHY is not a e1000_phy_gg82563, the routines below will figure
+     * this out as well. */
+    if (hw->mac_type == e1000_80003es2lan)
+        hw->phy_type = e1000_phy_gg82563;
+
     /* Read the PHY ID Registers to identify which PHY is onboard. */
     ret_val = e1000_read_phy_reg(hw, PHY_ID1, &phy_id_high);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     hw->phy_id = (uint32_t) (phy_id_high << 16);
-    udelay(20);
+    usec_delay(20);
     ret_val = e1000_read_phy_reg(hw, PHY_ID2, &phy_id_low);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     hw->phy_id |= (uint32_t) (phy_id_low & PHY_REVISION_MASK);
     hw->phy_revision = (uint32_t) phy_id_low & ~PHY_REVISION_MASK;
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82543:
-        if(hw->phy_id == M88E1000_E_PHY_ID) match = TRUE;
+        if (hw->phy_id == M88E1000_E_PHY_ID) match = TRUE;
         break;
     case e1000_82544:
-        if(hw->phy_id == M88E1000_I_PHY_ID) match = TRUE;
+        if (hw->phy_id == M88E1000_I_PHY_ID) match = TRUE;
         break;
     case e1000_82540:
     case e1000_82545:
     case e1000_82545_rev_3:
     case e1000_82546:
     case e1000_82546_rev_3:
-        if(hw->phy_id == M88E1011_I_PHY_ID) match = TRUE;
+        if (hw->phy_id == M88E1011_I_PHY_ID) match = TRUE;
         break;
     case e1000_82541:
     case e1000_82541_rev_2:
     case e1000_82547:
     case e1000_82547_rev_2:
-        if(hw->phy_id == IGP01E1000_I_PHY_ID) match = TRUE;
+        if (hw->phy_id == IGP01E1000_I_PHY_ID) match = TRUE;
+        break;
+    case e1000_82573:
+        if (hw->phy_id == M88E1111_I_PHY_ID) match = TRUE;
+        break;
+    case e1000_80003es2lan:
+        if (hw->phy_id == GG82563_E_PHY_ID) match = TRUE;
+        break;
+    case e1000_ich8lan:
+        if (hw->phy_id == IGP03E1000_E_PHY_ID) match = TRUE;
+        if (hw->phy_id == IFE_E_PHY_ID) match = TRUE;
+        if (hw->phy_id == IFE_PLUS_E_PHY_ID) match = TRUE;
+        if (hw->phy_id == IFE_C_E_PHY_ID) match = TRUE;
         break;
     default:
         DEBUGOUT1("Invalid MAC type %d\n", hw->mac_type);
@@ -2775,14 +3976,16 @@
     DEBUGFUNC("e1000_phy_reset_dsp");
 
     do {
-        ret_val = e1000_write_phy_reg(hw, 29, 0x001d);
-        if(ret_val) break;
+        if (hw->phy_type != e1000_phy_gg82563) {
+            ret_val = e1000_write_phy_reg(hw, 29, 0x001d);
+            if (ret_val) break;
+        }
         ret_val = e1000_write_phy_reg(hw, 30, 0x00c1);
-        if(ret_val) break;
+        if (ret_val) break;
         ret_val = e1000_write_phy_reg(hw, 30, 0x0000);
-        if(ret_val) break;
+        if (ret_val) break;
         ret_val = E1000_SUCCESS;
-    } while(0);
+    } while (0);
 
     return ret_val;
 }
@@ -2804,7 +4007,7 @@
 
     /* The downshift status is checked only once, after link is established,
      * and it stored in the hw->speed_downgraded parameter. */
-    phy_info->downshift = hw->speed_downgraded;
+    phy_info->downshift = (e1000_downshift)hw->speed_downgraded;
 
     /* IGP01E1000 does not need to support it. */
     phy_info->extended_10bt_distance = e1000_10bt_ext_dist_enable_normal;
@@ -2814,23 +4017,23 @@
 
     /* Check polarity status */
     ret_val = e1000_check_polarity(hw, &polarity);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_info->cable_polarity = polarity;
 
     ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_info->mdix_mode = (phy_data & IGP01E1000_PSSR_MDIX) >>
                           IGP01E1000_PSSR_MDIX_SHIFT;
 
-    if((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
+    if ((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
        IGP01E1000_PSSR_SPEED_1000MBPS) {
         /* Local/Remote Receiver Information are only valid at 1000 Mbps */
         ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_info->local_rx = (phy_data & SR_1000T_LOCAL_RX_STATUS) >>
@@ -2840,19 +4043,19 @@
 
         /* Get cable length */
         ret_val = e1000_get_cable_length(hw, &min_length, &max_length);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        /* transalte to old method */
+        /* Translate to old method */
         average = (max_length + min_length) / 2;
 
-        if(average <= e1000_igp_cable_length_50)
+        if (average <= e1000_igp_cable_length_50)
             phy_info->cable_length = e1000_cable_length_50;
-        else if(average <= e1000_igp_cable_length_80)
+        else if (average <= e1000_igp_cable_length_80)
             phy_info->cable_length = e1000_cable_length_50_80;
-        else if(average <= e1000_igp_cable_length_110)
+        else if (average <= e1000_igp_cable_length_110)
             phy_info->cable_length = e1000_cable_length_80_110;
-        else if(average <= e1000_igp_cable_length_140)
+        else if (average <= e1000_igp_cable_length_140)
             phy_info->cable_length = e1000_cable_length_110_140;
         else
             phy_info->cable_length = e1000_cable_length_140;
@@ -2862,6 +4065,53 @@
 }
 
 /******************************************************************************
+* Get PHY information from various PHY registers for ife PHY only.
+*
+* hw - Struct containing variables accessed by shared code
+* phy_info - PHY information structure
+******************************************************************************/
+int32_t
+e1000_phy_ife_get_info(struct e1000_hw *hw,
+                       struct e1000_phy_info *phy_info)
+{
+    int32_t ret_val;
+    uint16_t phy_data, polarity;
+
+    DEBUGFUNC("e1000_phy_ife_get_info");
+
+    phy_info->downshift = (e1000_downshift)hw->speed_downgraded;
+    phy_info->extended_10bt_distance = e1000_10bt_ext_dist_enable_normal;
+
+    ret_val = e1000_read_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, &phy_data);
+    if (ret_val)
+        return ret_val;
+    phy_info->polarity_correction =
+                        (phy_data & IFE_PSC_AUTO_POLARITY_DISABLE) >>
+                        IFE_PSC_AUTO_POLARITY_DISABLE_SHIFT;
+
+    if (phy_info->polarity_correction == e1000_polarity_reversal_enabled) {
+        ret_val = e1000_check_polarity(hw, &polarity);
+        if (ret_val)
+            return ret_val;
+    } else {
+        /* Polarity is forced. */
+        polarity = (phy_data & IFE_PSC_FORCE_POLARITY) >>
+                       IFE_PSC_FORCE_POLARITY_SHIFT;
+    }
+    phy_info->cable_polarity = polarity;
+
+    ret_val = e1000_read_phy_reg(hw, IFE_PHY_MDIX_CONTROL, &phy_data);
+    if (ret_val)
+        return ret_val;
+
+    phy_info->mdix_mode =
+                     (phy_data & (IFE_PMC_AUTO_MDIX | IFE_PMC_FORCE_MDIX)) >>
+                     IFE_PMC_MDIX_MODE_SHIFT;
+
+    return E1000_SUCCESS;
+}
+
+/******************************************************************************
 * Get PHY information from various PHY registers fot m88 PHY only.
 *
 * hw - Struct containing variables accessed by shared code
@@ -2878,10 +4128,10 @@
 
     /* The downshift status is checked only once, after link is established,
      * and it stored in the hw->speed_downgraded parameter. */
-    phy_info->downshift = hw->speed_downgraded;
+    phy_info->downshift = (e1000_downshift)hw->speed_downgraded;
 
     ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_info->extended_10bt_distance =
@@ -2893,27 +4143,35 @@
 
     /* Check polarity status */
     ret_val = e1000_check_polarity(hw, &polarity);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
-
     phy_info->cable_polarity = polarity;
 
     ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_info->mdix_mode = (phy_data & M88E1000_PSSR_MDIX) >>
                           M88E1000_PSSR_MDIX_SHIFT;
 
-    if(phy_data & M88E1000_PSSR_1000MBS) {
-        /* Cable Length Estimation and Local/Remote Receiver Informatoion
-         * are only valid at 1000 Mbps
-         */
-        phy_info->cable_length = ((phy_data & M88E1000_PSSR_CABLE_LENGTH) >>
-                                  M88E1000_PSSR_CABLE_LENGTH_SHIFT);
+    if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_1000MBS) {
+        /* Cable Length Estimation and Local/Remote Receiver Information
+         * are only valid at 1000 Mbps.
+         */
+        if (hw->phy_type != e1000_phy_gg82563) {
+            phy_info->cable_length = ((phy_data & M88E1000_PSSR_CABLE_LENGTH) >>
+                                      M88E1000_PSSR_CABLE_LENGTH_SHIFT);
+        } else {
+            ret_val = e1000_read_phy_reg(hw, GG82563_PHY_DSP_DISTANCE,
+                                         &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            phy_info->cable_length = phy_data & GG82563_DSPD_CABLE_LENGTH;
+        }
 
         ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_info->local_rx = (phy_data & SR_1000T_LOCAL_RX_STATUS) >>
@@ -2950,26 +4208,30 @@
     phy_info->local_rx = e1000_1000t_rx_status_undefined;
     phy_info->remote_rx = e1000_1000t_rx_status_undefined;
 
-    if(hw->media_type != e1000_media_type_copper) {
+    if (hw->media_type != e1000_media_type_copper) {
         DEBUGOUT("PHY info is only valid for copper media\n");
         return -E1000_ERR_CONFIG;
     }
 
     ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
-    if((phy_data & MII_SR_LINK_STATUS) != MII_SR_LINK_STATUS) {
+    if ((phy_data & MII_SR_LINK_STATUS) != MII_SR_LINK_STATUS) {
         DEBUGOUT("PHY info is only valid if link is up\n");
         return -E1000_ERR_CONFIG;
     }
 
-    if(hw->phy_type == e1000_phy_igp)
+    if (hw->phy_type == e1000_phy_igp ||
+        hw->phy_type == e1000_phy_igp_3 ||
+        hw->phy_type == e1000_phy_igp_2)
         return e1000_phy_igp_get_info(hw, phy_info);
+    else if (hw->phy_type == e1000_phy_ife)
+        return e1000_phy_ife_get_info(hw, phy_info);
     else
         return e1000_phy_m88_get_info(hw, phy_info);
 }
@@ -2979,7 +4241,7 @@
 {
     DEBUGFUNC("e1000_validate_mdi_settings");
 
-    if(!hw->autoneg && (hw->mdix == 0 || hw->mdix == 3)) {
+    if (!hw->autoneg && (hw->mdix == 0 || hw->mdix == 3)) {
         DEBUGOUT("Invalid MDI setting detected\n");
         hw->mdix = 1;
         return -E1000_ERR_CONFIG;
@@ -2990,15 +4252,17 @@
 
 /******************************************************************************
  * Sets up eeprom variables in the hw struct.  Must be called after mac_type
- * is configured.
+ * is configured.  Additionally, if this is ICH8, the flash controller GbE
+ * registers must be mapped, or this will crash.
  *
  * hw - Struct containing variables accessed by shared code
  *****************************************************************************/
-void
+int32_t
 e1000_init_eeprom_params(struct e1000_hw *hw)
 {
     struct e1000_eeprom_info *eeprom = &hw->eeprom;
     uint32_t eecd = E1000_READ_REG(hw, EECD);
+    int32_t ret_val = E1000_SUCCESS;
     uint16_t eeprom_size;
 
     DEBUGFUNC("e1000_init_eeprom_params");
@@ -3013,6 +4277,8 @@
         eeprom->opcode_bits = 3;
         eeprom->address_bits = 6;
         eeprom->delay_usec = 50;
+        eeprom->use_eerd = FALSE;
+        eeprom->use_eewr = FALSE;
         break;
     case e1000_82540:
     case e1000_82545:
@@ -3022,13 +4288,15 @@
         eeprom->type = e1000_eeprom_microwire;
         eeprom->opcode_bits = 3;
         eeprom->delay_usec = 50;
-        if(eecd & E1000_EECD_SIZE) {
+        if (eecd & E1000_EECD_SIZE) {
             eeprom->word_size = 256;
             eeprom->address_bits = 8;
         } else {
             eeprom->word_size = 64;
             eeprom->address_bits = 6;
         }
+        eeprom->use_eerd = FALSE;
+        eeprom->use_eewr = FALSE;
         break;
     case e1000_82541:
     case e1000_82541_rev_2:
@@ -3057,42 +4325,118 @@
                 eeprom->address_bits = 6;
             }
         }
+        eeprom->use_eerd = FALSE;
+        eeprom->use_eewr = FALSE;
         break;
-    default:
+    case e1000_82571:
+    case e1000_82572:
+        eeprom->type = e1000_eeprom_spi;
+        eeprom->opcode_bits = 8;
+        eeprom->delay_usec = 1;
+        if (eecd & E1000_EECD_ADDR_BITS) {
+            eeprom->page_size = 32;
+            eeprom->address_bits = 16;
+        } else {
+            eeprom->page_size = 8;
+            eeprom->address_bits = 8;
+        }
+        eeprom->use_eerd = FALSE;
+        eeprom->use_eewr = FALSE;
         break;
-    }
+    case e1000_82573:
+        eeprom->type = e1000_eeprom_spi;
+        eeprom->opcode_bits = 8;
+        eeprom->delay_usec = 1;
+        if (eecd & E1000_EECD_ADDR_BITS) {
+            eeprom->page_size = 32;
+            eeprom->address_bits = 16;
+        } else {
+            eeprom->page_size = 8;
+            eeprom->address_bits = 8;
+        }
+        eeprom->use_eerd = TRUE;
+        eeprom->use_eewr = TRUE;
+        if (e1000_is_onboard_nvm_eeprom(hw) == FALSE) {
+            eeprom->type = e1000_eeprom_flash;
+            eeprom->word_size = 2048;
+
+            /* Ensure that the Autonomous FLASH update bit is cleared due to
+             * Flash update issue on parts which use a FLASH for NVM. */
+            eecd &= ~E1000_EECD_AUPDEN;
+            E1000_WRITE_REG(hw, EECD, eecd);
+        }
+        break;
+    case e1000_80003es2lan:
+        eeprom->type = e1000_eeprom_spi;
+        eeprom->opcode_bits = 8;
+        eeprom->delay_usec = 1;
+        if (eecd & E1000_EECD_ADDR_BITS) {
+            eeprom->page_size = 32;
+            eeprom->address_bits = 16;
+        } else {
+            eeprom->page_size = 8;
+            eeprom->address_bits = 8;
+        }
+        eeprom->use_eerd = TRUE;
+        eeprom->use_eewr = FALSE;
+        break;
+    case e1000_ich8lan:
+    {
+        int32_t  i = 0;
+        uint32_t flash_size = E1000_READ_ICH8_REG(hw, ICH8_FLASH_GFPREG);
+
+        eeprom->type = e1000_eeprom_ich8;
+        eeprom->use_eerd = FALSE;
+        eeprom->use_eewr = FALSE;
+        eeprom->word_size = E1000_SHADOW_RAM_WORDS;
+
+        /* Zero the shadow RAM structure. But don't load it from NVM
+         * so as to save time for driver init */
+        if (hw->eeprom_shadow_ram != NULL) {
+            for (i = 0; i < E1000_SHADOW_RAM_WORDS; i++) {
+                hw->eeprom_shadow_ram[i].modified = FALSE;
+                hw->eeprom_shadow_ram[i].eeprom_word = 0xFFFF;
+            }
+        }
 
-    if (eeprom->type == e1000_eeprom_spi) {
-        eeprom->word_size = 64;
-        if (e1000_read_eeprom(hw, EEPROM_CFG, 1, &eeprom_size) == 0) {
-            eeprom_size &= EEPROM_SIZE_MASK;
+        hw->flash_base_addr = (flash_size & ICH8_GFPREG_BASE_MASK) *
+                              ICH8_FLASH_SECTOR_SIZE;
 
-            switch (eeprom_size) {
-            case EEPROM_SIZE_16KB:
-                eeprom->word_size = 8192;
-                break;
-            case EEPROM_SIZE_8KB:
-                eeprom->word_size = 4096;
-                break;
-            case EEPROM_SIZE_4KB:
-                eeprom->word_size = 2048;
-                break;
-            case EEPROM_SIZE_2KB:
-                eeprom->word_size = 1024;
-                break;
-            case EEPROM_SIZE_1KB:
-                eeprom->word_size = 512;
-                break;
-            case EEPROM_SIZE_512B:
-                eeprom->word_size = 256;
-                break;
-            case EEPROM_SIZE_128B:
-            default:
-                eeprom->word_size = 64;
-                break;
-            }
+        hw->flash_bank_size = ((flash_size >> 16) & ICH8_GFPREG_BASE_MASK) + 1;
+        hw->flash_bank_size -= (flash_size & ICH8_GFPREG_BASE_MASK);
+        hw->flash_bank_size *= ICH8_FLASH_SECTOR_SIZE;
+        hw->flash_bank_size /= 2 * sizeof(uint16_t);
+
+        break;
+    }
+    default:
+        break;
+    }
+
+    if (eeprom->type == e1000_eeprom_spi) {
+        /* eeprom_size will be an enum [0..8] that maps to eeprom sizes 128B to
+         * 32KB (incremented by powers of 2).
+         */
+        if (hw->mac_type <= e1000_82547_rev_2) {
+            /* Set to default value for initial eeprom read. */
+            eeprom->word_size = 64;
+            ret_val = e1000_read_eeprom(hw, EEPROM_CFG, 1, &eeprom_size);
+            if (ret_val)
+                return ret_val;
+            eeprom_size = (eeprom_size & EEPROM_SIZE_MASK) >> EEPROM_SIZE_SHIFT;
+            /* 256B eeprom size was not supported in earlier hardware, so we
+             * bump eeprom_size up one to ensure that "1" (which maps to 256B)
+             * is never the result used in the shifting logic below. */
+            if (eeprom_size)
+                eeprom_size++;
+        } else {
+            eeprom_size = (uint16_t)((eecd & E1000_EECD_SIZE_EX_MASK) >>
+                          E1000_EECD_SIZE_EX_SHIFT);
         }
+
+        eeprom->word_size = 1 << (eeprom_size + EEPROM_WORD_SIZE_SHIFT);
     }
+    return ret_val;
 }
 
 /******************************************************************************
@@ -3111,7 +4455,7 @@
     *eecd = *eecd | E1000_EECD_SK;
     E1000_WRITE_REG(hw, EECD, *eecd);
     E1000_WRITE_FLUSH(hw);
-    udelay(hw->eeprom.delay_usec);
+    usec_delay(hw->eeprom.delay_usec);
 }
 
 /******************************************************************************
@@ -3130,7 +4474,7 @@
     *eecd = *eecd & ~E1000_EECD_SK;
     E1000_WRITE_REG(hw, EECD, *eecd);
     E1000_WRITE_FLUSH(hw);
-    udelay(hw->eeprom.delay_usec);
+    usec_delay(hw->eeprom.delay_usec);
 }
 
 /******************************************************************************
@@ -3168,20 +4512,20 @@
          */
         eecd &= ~E1000_EECD_DI;
 
-        if(data & mask)
+        if (data & mask)
             eecd |= E1000_EECD_DI;
 
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
 
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
 
         e1000_raise_ee_clk(hw, &eecd);
         e1000_lower_ee_clk(hw, &eecd);
 
         mask = mask >> 1;
 
-    } while(mask);
+    } while (mask);
 
     /* We leave the "DI" bit set to "0" when we leave this routine. */
     eecd &= ~E1000_EECD_DI;
@@ -3213,14 +4557,14 @@
     eecd &= ~(E1000_EECD_DO | E1000_EECD_DI);
     data = 0;
 
-    for(i = 0; i < count; i++) {
+    for (i = 0; i < count; i++) {
         data = data << 1;
         e1000_raise_ee_clk(hw, &eecd);
 
         eecd = E1000_READ_REG(hw, EECD);
 
         eecd &= ~(E1000_EECD_DI);
-        if(eecd & E1000_EECD_DO)
+        if (eecd & E1000_EECD_DO)
             data |= 1;
 
         e1000_lower_ee_clk(hw, &eecd);
@@ -3245,24 +4589,29 @@
 
     DEBUGFUNC("e1000_acquire_eeprom");
 
+    if (e1000_swfw_sync_acquire(hw, E1000_SWFW_EEP_SM))
+        return -E1000_ERR_SWFW_SYNC;
     eecd = E1000_READ_REG(hw, EECD);
 
-    /* Request EEPROM Access */
-    if(hw->mac_type > e1000_82544) {
-        eecd |= E1000_EECD_REQ;
-        E1000_WRITE_REG(hw, EECD, eecd);
-        eecd = E1000_READ_REG(hw, EECD);
-        while((!(eecd & E1000_EECD_GNT)) &&
-              (i < E1000_EEPROM_GRANT_ATTEMPTS)) {
-            i++;
-            udelay(5);
-            eecd = E1000_READ_REG(hw, EECD);
-        }
-        if(!(eecd & E1000_EECD_GNT)) {
-            eecd &= ~E1000_EECD_REQ;
+    if (hw->mac_type != e1000_82573) {
+        /* Request EEPROM Access */
+        if (hw->mac_type > e1000_82544) {
+            eecd |= E1000_EECD_REQ;
             E1000_WRITE_REG(hw, EECD, eecd);
-            DEBUGOUT("Could not acquire EEPROM grant\n");
-            return -E1000_ERR_EEPROM;
+            eecd = E1000_READ_REG(hw, EECD);
+            while ((!(eecd & E1000_EECD_GNT)) &&
+                  (i < E1000_EEPROM_GRANT_ATTEMPTS)) {
+                i++;
+                usec_delay(5);
+                eecd = E1000_READ_REG(hw, EECD);
+            }
+            if (!(eecd & E1000_EECD_GNT)) {
+                eecd &= ~E1000_EECD_REQ;
+                E1000_WRITE_REG(hw, EECD, eecd);
+                DEBUGOUT("Could not acquire EEPROM grant\n");
+                e1000_swfw_sync_release(hw, E1000_SWFW_EEP_SM);
+                return -E1000_ERR_EEPROM;
+            }
         }
     }
 
@@ -3280,7 +4629,7 @@
         /* Clear SK and CS */
         eecd &= ~(E1000_EECD_CS | E1000_EECD_SK);
         E1000_WRITE_REG(hw, EECD, eecd);
-        udelay(1);
+        usec_delay(1);
     }
 
     return E1000_SUCCESS;
@@ -3299,39 +4648,39 @@
 
     eecd = E1000_READ_REG(hw, EECD);
 
-    if(eeprom->type == e1000_eeprom_microwire) {
+    if (eeprom->type == e1000_eeprom_microwire) {
         eecd &= ~(E1000_EECD_CS | E1000_EECD_SK);
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
 
         /* Clock high */
         eecd |= E1000_EECD_SK;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
 
         /* Select EEPROM */
         eecd |= E1000_EECD_CS;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
 
         /* Clock low */
         eecd &= ~E1000_EECD_SK;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
-    } else if(eeprom->type == e1000_eeprom_spi) {
+        usec_delay(eeprom->delay_usec);
+    } else if (eeprom->type == e1000_eeprom_spi) {
         /* Toggle CS to flush commands */
         eecd |= E1000_EECD_CS;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
         eecd &= ~E1000_EECD_CS;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(eeprom->delay_usec);
+        usec_delay(eeprom->delay_usec);
     }
 }
 
@@ -3355,8 +4704,8 @@
 
         E1000_WRITE_REG(hw, EECD, eecd);
 
-        udelay(hw->eeprom.delay_usec);
-    } else if(hw->eeprom.type == e1000_eeprom_microwire) {
+        usec_delay(hw->eeprom.delay_usec);
+    } else if (hw->eeprom.type == e1000_eeprom_microwire) {
         /* cleanup eeprom */
 
         /* CS on Microwire is active-high */
@@ -3368,20 +4717,22 @@
         eecd |= E1000_EECD_SK;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(hw->eeprom.delay_usec);
+        usec_delay(hw->eeprom.delay_usec);
 
         /* Falling edge of clock */
         eecd &= ~E1000_EECD_SK;
         E1000_WRITE_REG(hw, EECD, eecd);
         E1000_WRITE_FLUSH(hw);
-        udelay(hw->eeprom.delay_usec);
+        usec_delay(hw->eeprom.delay_usec);
     }
 
     /* Stop requesting EEPROM access */
-    if(hw->mac_type > e1000_82544) {
+    if (hw->mac_type > e1000_82544) {
         eecd &= ~E1000_EECD_REQ;
         E1000_WRITE_REG(hw, EECD, eecd);
     }
+
+    e1000_swfw_sync_release(hw, E1000_SWFW_EEP_SM);
 }
 
 /******************************************************************************
@@ -3410,16 +4761,16 @@
         if (!(spi_stat_reg & EEPROM_STATUS_RDY_SPI))
             break;
 
-        udelay(5);
+        usec_delay(5);
         retry_count += 5;
 
         e1000_standby_eeprom(hw);
-    } while(retry_count < EEPROM_MAX_RETRY_SPI);
+    } while (retry_count < EEPROM_MAX_RETRY_SPI);
 
     /* ATMEL SPI write time could vary from 0-20mSec on 3.3V devices (and
      * only 0-5mSec on 5V devices)
      */
-    if(retry_count >= EEPROM_MAX_RETRY_SPI) {
+    if (retry_count >= EEPROM_MAX_RETRY_SPI) {
         DEBUGOUT("SPI EEPROM Status error\n");
         return -E1000_ERR_EEPROM;
     }
@@ -3443,26 +4794,49 @@
 {
     struct e1000_eeprom_info *eeprom = &hw->eeprom;
     uint32_t i = 0;
+    int32_t ret_val;
 
     DEBUGFUNC("e1000_read_eeprom");
+
     /* A check for invalid values:  offset too large, too many words, and not
      * enough words.
      */
-    if((offset > eeprom->word_size) || (words > eeprom->word_size - offset) ||
+    if ((offset >= eeprom->word_size) || (words > eeprom->word_size - offset) ||
        (words == 0)) {
         DEBUGOUT("\"words\" parameter out of bounds\n");
         return -E1000_ERR_EEPROM;
     }
 
-    /* Prepare the EEPROM for reading  */
-    if(e1000_acquire_eeprom(hw) != E1000_SUCCESS)
-        return -E1000_ERR_EEPROM;
+    /* FLASH reads without acquiring the semaphore are safe */
+    if (e1000_is_onboard_nvm_eeprom(hw) == TRUE &&
+        hw->eeprom.use_eerd == FALSE) {
+        switch (hw->mac_type) {
+        case e1000_80003es2lan:
+            break;
+        default:
+            /* Prepare the EEPROM for reading  */
+            if (e1000_acquire_eeprom(hw) != E1000_SUCCESS)
+                return -E1000_ERR_EEPROM;
+            break;
+        }
+    }
+
+    if (eeprom->use_eerd == TRUE) {
+        ret_val = e1000_read_eeprom_eerd(hw, offset, words, data);
+        if ((e1000_is_onboard_nvm_eeprom(hw) == TRUE) ||
+            (hw->mac_type != e1000_82573))
+            e1000_release_eeprom(hw);
+        return ret_val;
+    }
 
-    if(eeprom->type == e1000_eeprom_spi) {
+    if (eeprom->type == e1000_eeprom_ich8)
+        return e1000_read_eeprom_ich8(hw, offset, words, data);
+
+    if (eeprom->type == e1000_eeprom_spi) {
         uint16_t word_in;
         uint8_t read_opcode = EEPROM_READ_OPCODE_SPI;
 
-        if(e1000_spi_eeprom_ready(hw)) {
+        if (e1000_spi_eeprom_ready(hw)) {
             e1000_release_eeprom(hw);
             return -E1000_ERR_EEPROM;
         }
@@ -3470,7 +4844,7 @@
         e1000_standby_eeprom(hw);
 
         /* Some SPI eeproms use the 8th address bit embedded in the opcode */
-        if((eeprom->address_bits == 8) && (offset >= 128))
+        if ((eeprom->address_bits == 8) && (offset >= 128))
             read_opcode |= EEPROM_A8_OPCODE_SPI;
 
         /* Send the READ command (opcode + addr)  */
@@ -3486,7 +4860,7 @@
             word_in = e1000_shift_in_ee_bits(hw, 16);
             data[i] = (word_in >> 8) | (word_in << 8);
         }
-    } else if(eeprom->type == e1000_eeprom_microwire) {
+    } else if (eeprom->type == e1000_eeprom_microwire) {
         for (i = 0; i < words; i++) {
             /* Send the READ command (opcode + addr)  */
             e1000_shift_out_ee_bits(hw, EEPROM_READ_OPCODE_MICROWIRE,
@@ -3508,6 +4882,141 @@
 }
 
 /******************************************************************************
+ * Reads a 16 bit word from the EEPROM using the EERD register.
+ *
+ * hw - Struct containing variables accessed by shared code
+ * offset - offset of  word in the EEPROM to read
+ * data - word read from the EEPROM
+ * words - number of words to read
+ *****************************************************************************/
+int32_t
+e1000_read_eeprom_eerd(struct e1000_hw *hw,
+                  uint16_t offset,
+                  uint16_t words,
+                  uint16_t *data)
+{
+    uint32_t i, eerd = 0;
+    int32_t error = 0;
+
+    for (i = 0; i < words; i++) {
+        eerd = ((offset+i) << E1000_EEPROM_RW_ADDR_SHIFT) +
+                         E1000_EEPROM_RW_REG_START;
+
+        E1000_WRITE_REG(hw, EERD, eerd);
+        error = e1000_poll_eerd_eewr_done(hw, E1000_EEPROM_POLL_READ);
+
+        if (error) {
+            break;
+        }
+        data[i] = (E1000_READ_REG(hw, EERD) >> E1000_EEPROM_RW_REG_DATA);
+
+    }
+
+    return error;
+}
+
+/******************************************************************************
+ * Writes a 16 bit word from the EEPROM using the EEWR register.
+ *
+ * hw - Struct containing variables accessed by shared code
+ * offset - offset of  word in the EEPROM to read
+ * data - word read from the EEPROM
+ * words - number of words to read
+ *****************************************************************************/
+int32_t
+e1000_write_eeprom_eewr(struct e1000_hw *hw,
+                   uint16_t offset,
+                   uint16_t words,
+                   uint16_t *data)
+{
+    uint32_t    register_value = 0;
+    uint32_t    i              = 0;
+    int32_t     error          = 0;
+
+    if (e1000_swfw_sync_acquire(hw, E1000_SWFW_EEP_SM))
+        return -E1000_ERR_SWFW_SYNC;
+
+    for (i = 0; i < words; i++) {
+        register_value = (data[i] << E1000_EEPROM_RW_REG_DATA) |
+                         ((offset+i) << E1000_EEPROM_RW_ADDR_SHIFT) |
+                         E1000_EEPROM_RW_REG_START;
+
+        error = e1000_poll_eerd_eewr_done(hw, E1000_EEPROM_POLL_WRITE);
+        if (error) {
+            break;
+        }
+
+        E1000_WRITE_REG(hw, EEWR, register_value);
+
+        error = e1000_poll_eerd_eewr_done(hw, E1000_EEPROM_POLL_WRITE);
+
+        if (error) {
+            break;
+        }
+    }
+
+    e1000_swfw_sync_release(hw, E1000_SWFW_EEP_SM);
+    return error;
+}
+
+/******************************************************************************
+ * Polls the status bit (bit 1) of the EERD to determine when the read is done.
+ *
+ * hw - Struct containing variables accessed by shared code
+ *****************************************************************************/
+int32_t
+e1000_poll_eerd_eewr_done(struct e1000_hw *hw, int eerd)
+{
+    uint32_t attempts = 100000;
+    uint32_t i, reg = 0;
+    int32_t done = E1000_ERR_EEPROM;
+
+    for (i = 0; i < attempts; i++) {
+        if (eerd == E1000_EEPROM_POLL_READ)
+            reg = E1000_READ_REG(hw, EERD);
+        else
+            reg = E1000_READ_REG(hw, EEWR);
+
+        if (reg & E1000_EEPROM_RW_REG_DONE) {
+            done = E1000_SUCCESS;
+            break;
+        }
+        usec_delay(5);
+    }
+
+    return done;
+}
+
+/***************************************************************************
+* Description:     Determines if the onboard NVM is FLASH or EEPROM.
+*
+* hw - Struct containing variables accessed by shared code
+****************************************************************************/
+boolean_t
+e1000_is_onboard_nvm_eeprom(struct e1000_hw *hw)
+{
+    uint32_t eecd = 0;
+
+    DEBUGFUNC("e1000_is_onboard_nvm_eeprom");
+
+    if (hw->mac_type == e1000_ich8lan)
+        return FALSE;
+
+    if (hw->mac_type == e1000_82573) {
+        eecd = E1000_READ_REG(hw, EECD);
+
+        /* Isolate bits 15 & 16 */
+        eecd = ((eecd >> 15) & 0x03);
+
+        /* If both bits are set, device is Flash type */
+        if (eecd == 0x03) {
+            return FALSE;
+        }
+    }
+    return TRUE;
+}
+
+/******************************************************************************
  * Verifies that the EEPROM has a valid checksum
  *
  * hw - Struct containing variables accessed by shared code
@@ -3524,15 +5033,48 @@
 
     DEBUGFUNC("e1000_validate_eeprom_checksum");
 
-    for(i = 0; i < (EEPROM_CHECKSUM_REG + 1); i++) {
-        if(e1000_read_eeprom(hw, i, 1, &eeprom_data) < 0) {
+    if ((hw->mac_type == e1000_82573) &&
+        (e1000_is_onboard_nvm_eeprom(hw) == FALSE)) {
+        /* Check bit 4 of word 10h.  If it is 0, firmware is done updating
+         * 10h-12h.  Checksum may need to be fixed. */
+        e1000_read_eeprom(hw, 0x10, 1, &eeprom_data);
+        if ((eeprom_data & 0x10) == 0) {
+            /* Read 0x23 and check bit 15.  This bit is a 1 when the checksum
+             * has already been fixed.  If the checksum is still wrong and this
+             * bit is a 1, we need to return bad checksum.  Otherwise, we need
+             * to set this bit to a 1 and update the checksum. */
+            e1000_read_eeprom(hw, 0x23, 1, &eeprom_data);
+            if ((eeprom_data & 0x8000) == 0) {
+                eeprom_data |= 0x8000;
+                e1000_write_eeprom(hw, 0x23, 1, &eeprom_data);
+                e1000_update_eeprom_checksum(hw);
+            }
+        }
+    }
+
+    if (hw->mac_type == e1000_ich8lan) {
+        /* Drivers must allocate the shadow ram structure for the
+         * EEPROM checksum to be updated.  Otherwise, this bit as well
+         * as the checksum must both be set correctly for this
+         * validation to pass.
+         */
+        e1000_read_eeprom(hw, 0x19, 1, &eeprom_data);
+        if ((eeprom_data & 0x40) == 0) {
+            eeprom_data |= 0x40;
+            e1000_write_eeprom(hw, 0x19, 1, &eeprom_data);
+            e1000_update_eeprom_checksum(hw);
+        }
+    }
+
+    for (i = 0; i < (EEPROM_CHECKSUM_REG + 1); i++) {
+        if (e1000_read_eeprom(hw, i, 1, &eeprom_data) < 0) {
             DEBUGOUT("EEPROM Read Error\n");
             return -E1000_ERR_EEPROM;
         }
         checksum += eeprom_data;
     }
 
-    if(checksum == (uint16_t) EEPROM_SUM)
+    if (checksum == (uint16_t) EEPROM_SUM)
         return E1000_SUCCESS;
     else {
         DEBUGOUT("EEPROM Checksum Invalid\n");
@@ -3551,22 +5093,33 @@
 int32_t
 e1000_update_eeprom_checksum(struct e1000_hw *hw)
 {
+    uint32_t ctrl_ext;
     uint16_t checksum = 0;
     uint16_t i, eeprom_data;
 
     DEBUGFUNC("e1000_update_eeprom_checksum");
 
-    for(i = 0; i < EEPROM_CHECKSUM_REG; i++) {
-        if(e1000_read_eeprom(hw, i, 1, &eeprom_data) < 0) {
+    for (i = 0; i < EEPROM_CHECKSUM_REG; i++) {
+        if (e1000_read_eeprom(hw, i, 1, &eeprom_data) < 0) {
             DEBUGOUT("EEPROM Read Error\n");
             return -E1000_ERR_EEPROM;
         }
         checksum += eeprom_data;
     }
     checksum = (uint16_t) EEPROM_SUM - checksum;
-    if(e1000_write_eeprom(hw, EEPROM_CHECKSUM_REG, 1, &checksum) < 0) {
+    if (e1000_write_eeprom(hw, EEPROM_CHECKSUM_REG, 1, &checksum) < 0) {
         DEBUGOUT("EEPROM Write Error\n");
         return -E1000_ERR_EEPROM;
+    } else if (hw->eeprom.type == e1000_eeprom_flash) {
+        e1000_commit_shadow_ram(hw);
+    } else if (hw->eeprom.type == e1000_eeprom_ich8) {
+        e1000_commit_shadow_ram(hw);
+        /* Reload the EEPROM, or else modifications will not appear
+         * until after next adapter reset. */
+        ctrl_ext = E1000_READ_REG(hw, CTRL_EXT);
+        ctrl_ext |= E1000_CTRL_EXT_EE_RST;
+        E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
+        msec_delay(10);
     }
     return E1000_SUCCESS;
 }
@@ -3596,17 +5149,24 @@
     /* A check for invalid values:  offset too large, too many words, and not
      * enough words.
      */
-    if((offset > eeprom->word_size) || (words > eeprom->word_size - offset) ||
+    if ((offset >= eeprom->word_size) || (words > eeprom->word_size - offset) ||
        (words == 0)) {
         DEBUGOUT("\"words\" parameter out of bounds\n");
         return -E1000_ERR_EEPROM;
     }
 
+    /* 82573 writes only through eewr */
+    if (eeprom->use_eewr == TRUE)
+        return e1000_write_eeprom_eewr(hw, offset, words, data);
+
+    if (eeprom->type == e1000_eeprom_ich8)
+        return e1000_write_eeprom_ich8(hw, offset, words, data);
+
     /* Prepare the EEPROM for writing  */
     if (e1000_acquire_eeprom(hw) != E1000_SUCCESS)
         return -E1000_ERR_EEPROM;
 
-    if(eeprom->type == e1000_eeprom_microwire) {
+    if (eeprom->type == e1000_eeprom_microwire) {
         status = e1000_write_eeprom_microwire(hw, offset, words, data);
     } else {
         status = e1000_write_eeprom_spi(hw, offset, words, data);
@@ -3642,7 +5202,7 @@
     while (widx < words) {
         uint8_t write_opcode = EEPROM_WRITE_OPCODE_SPI;
 
-        if(e1000_spi_eeprom_ready(hw)) return -E1000_ERR_EEPROM;
+        if (e1000_spi_eeprom_ready(hw)) return -E1000_ERR_EEPROM;
 
         e1000_standby_eeprom(hw);
 
@@ -3653,7 +5213,7 @@
         e1000_standby_eeprom(hw);
 
         /* Some SPI eeproms use the 8th address bit embedded in the opcode */
-        if((eeprom->address_bits == 8) && (offset >= 128))
+        if ((eeprom->address_bits == 8) && (offset >= 128))
             write_opcode |= EEPROM_A8_OPCODE_SPI;
 
         /* Send the Write command (8-bit opcode + addr) */
@@ -3675,7 +5235,7 @@
              * operation, while the smaller eeproms are capable of an 8-byte
              * PAGE WRITE operation.  Break the inner loop to pass new address
              */
-            if((((offset + widx)*2) % eeprom->page_size) == 0) {
+            if ((((offset + widx)*2) % eeprom->page_size) == 0) {
                 e1000_standby_eeprom(hw);
                 break;
             }
@@ -3741,12 +5301,12 @@
          * signal that the command has been completed by raising the DO signal.
          * If DO does not go high in 10 milliseconds, then error out.
          */
-        for(i = 0; i < 200; i++) {
+        for (i = 0; i < 200; i++) {
             eecd = E1000_READ_REG(hw, EECD);
-            if(eecd & E1000_EECD_DO) break;
-            udelay(50);
+            if (eecd & E1000_EECD_DO) break;
+            usec_delay(50);
         }
-        if(i == 200) {
+        if (i == 200) {
             DEBUGOUT("EEPROM Write did not complete\n");
             return -E1000_ERR_EEPROM;
         }
@@ -3772,6 +5332,171 @@
 }
 
 /******************************************************************************
+ * Flushes the cached eeprom to NVM. This is done by saving the modified values
+ * in the eeprom cache and the non modified values in the currently active bank
+ * to the new bank.
+ *
+ * hw - Struct containing variables accessed by shared code
+ * offset - offset of  word in the EEPROM to read
+ * data - word read from the EEPROM
+ * words - number of words to read
+ *****************************************************************************/
+int32_t
+e1000_commit_shadow_ram(struct e1000_hw *hw)
+{
+    uint32_t attempts = 100000;
+    uint32_t eecd = 0;
+    uint32_t flop = 0;
+    uint32_t i = 0;
+    int32_t error = E1000_SUCCESS;
+    uint32_t old_bank_offset = 0;
+    uint32_t new_bank_offset = 0;
+    uint32_t sector_retries = 0;
+    uint8_t low_byte = 0;
+    uint8_t high_byte = 0;
+    uint8_t temp_byte = 0;
+    boolean_t sector_write_failed = FALSE;
+
+    if (hw->mac_type == e1000_82573) {
+        /* The flop register will be used to determine if flash type is STM */
+        flop = E1000_READ_REG(hw, FLOP);
+        for (i=0; i < attempts; i++) {
+            eecd = E1000_READ_REG(hw, EECD);
+            if ((eecd & E1000_EECD_FLUPD) == 0) {
+                break;
+            }
+            usec_delay(5);
+        }
+
+        if (i == attempts) {
+            return -E1000_ERR_EEPROM;
+        }
+
+        /* If STM opcode located in bits 15:8 of flop, reset firmware */
+        if ((flop & 0xFF00) == E1000_STM_OPCODE) {
+            E1000_WRITE_REG(hw, HICR, E1000_HICR_FW_RESET);
+        }
+
+        /* Perform the flash update */
+        E1000_WRITE_REG(hw, EECD, eecd | E1000_EECD_FLUPD);
+
+        for (i=0; i < attempts; i++) {
+            eecd = E1000_READ_REG(hw, EECD);
+            if ((eecd & E1000_EECD_FLUPD) == 0) {
+                break;
+            }
+            usec_delay(5);
+        }
+
+        if (i == attempts) {
+            return -E1000_ERR_EEPROM;
+        }
+    }
+
+    if (hw->mac_type == e1000_ich8lan && hw->eeprom_shadow_ram != NULL) {
+        /* We're writing to the opposite bank so if we're on bank 1,
+         * write to bank 0 etc.  We also need to erase the segment that
+         * is going to be written */
+        if (!(E1000_READ_REG(hw, EECD) & E1000_EECD_SEC1VAL)) {
+            new_bank_offset = hw->flash_bank_size * 2;
+            old_bank_offset = 0;
+            e1000_erase_ich8_4k_segment(hw, 1);
+        } else {
+            old_bank_offset = hw->flash_bank_size * 2;
+            new_bank_offset = 0;
+            e1000_erase_ich8_4k_segment(hw, 0);
+        }
+
+        do {
+            sector_write_failed = FALSE;
+            /* Loop for every byte in the shadow RAM,
+             * which is in units of words. */
+            for (i = 0; i < E1000_SHADOW_RAM_WORDS; i++) {
+                /* Determine whether to write the value stored
+                 * in the other NVM bank or a modified value stored
+                 * in the shadow RAM */
+                if (hw->eeprom_shadow_ram[i].modified == TRUE) {
+                    low_byte = (uint8_t)hw->eeprom_shadow_ram[i].eeprom_word;
+                    e1000_read_ich8_byte(hw, (i << 1) + old_bank_offset,
+                                         &temp_byte);
+                    usec_delay(100);
+                    error = e1000_verify_write_ich8_byte(hw,
+                                                 (i << 1) + new_bank_offset,
+                                                 low_byte);
+                    if (error != E1000_SUCCESS)
+                        sector_write_failed = TRUE;
+                    high_byte =
+                        (uint8_t)(hw->eeprom_shadow_ram[i].eeprom_word >> 8);
+                    e1000_read_ich8_byte(hw, (i << 1) + old_bank_offset + 1,
+                                         &temp_byte);
+                    usec_delay(100);
+                } else {
+                    e1000_read_ich8_byte(hw, (i << 1) + old_bank_offset,
+                                         &low_byte);
+                    usec_delay(100);
+                    error = e1000_verify_write_ich8_byte(hw,
+                                 (i << 1) + new_bank_offset, low_byte);
+                    if (error != E1000_SUCCESS)
+                        sector_write_failed = TRUE;
+                    e1000_read_ich8_byte(hw, (i << 1) + old_bank_offset + 1,
+                                         &high_byte);
+                }
+
+                /* If the word is 0x13, then make sure the signature bits
+                 * (15:14) are 11b until the commit has completed.
+                 * This will allow us to write 10b which indicates the
+                 * signature is valid.  We want to do this after the write
+                 * has completed so that we don't mark the segment valid
+                 * while the write is still in progress */
+                if (i == E1000_ICH8_NVM_SIG_WORD)
+                    high_byte = E1000_ICH8_NVM_SIG_MASK | high_byte;
+
+                error = e1000_verify_write_ich8_byte(hw,
+                             (i << 1) + new_bank_offset + 1, high_byte);
+                if (error != E1000_SUCCESS)
+                    sector_write_failed = TRUE;
+
+                if (sector_write_failed == FALSE) {
+                    /* Clear the now not used entry in the cache */
+                    hw->eeprom_shadow_ram[i].modified = FALSE;
+                    hw->eeprom_shadow_ram[i].eeprom_word = 0xFFFF;
+                }
+            }
+
+            /* Don't bother writing the segment valid bits if sector
+             * programming failed. */
+            if (sector_write_failed == FALSE) {
+                /* Finally validate the new segment by setting bit 15:14
+                 * to 10b in word 0x13 , this can be done without an
+                 * erase as well since these bits are 11 to start with
+                 * and we need to change bit 14 to 0b */
+                e1000_read_ich8_byte(hw,
+                    E1000_ICH8_NVM_SIG_WORD * 2 + 1 + new_bank_offset,
+                    &high_byte);
+                high_byte &= 0xBF;
+                error = e1000_verify_write_ich8_byte(hw,
+                            E1000_ICH8_NVM_SIG_WORD * 2 + 1 + new_bank_offset,
+                            high_byte);
+                if (error != E1000_SUCCESS)
+                    sector_write_failed = TRUE;
+
+                /* And invalidate the previously valid segment by setting
+                 * its signature word (0x13) high_byte to 0b. This can be
+                 * done without an erase because flash erase sets all bits
+                 * to 1's. We can write 1's to 0's without an erase */
+                error = e1000_verify_write_ich8_byte(hw,
+                            E1000_ICH8_NVM_SIG_WORD * 2 + 1 + old_bank_offset,
+                            0);
+                if (error != E1000_SUCCESS)
+                    sector_write_failed = TRUE;
+            }
+        } while (++sector_retries < 10 && sector_write_failed == TRUE);
+    }
+
+    return error;
+}
+
+/******************************************************************************
  * Reads the adapter's part number from the EEPROM
  *
  * hw - Struct containing variables accessed by shared code
@@ -3787,7 +5512,7 @@
     DEBUGFUNC("e1000_read_part_num");
 
     /* Get word 0 from EEPROM */
-    if(e1000_read_eeprom(hw, offset, 1, &eeprom_data) < 0) {
+    if (e1000_read_eeprom(hw, offset, 1, &eeprom_data) < 0) {
         DEBUGOUT("EEPROM Read Error\n");
         return -E1000_ERR_EEPROM;
     }
@@ -3795,7 +5520,7 @@
     *part_num = (uint32_t) (eeprom_data << 16);
 
     /* Get word 1 from EEPROM */
-    if(e1000_read_eeprom(hw, ++offset, 1, &eeprom_data) < 0) {
+    if (e1000_read_eeprom(hw, ++offset, 1, &eeprom_data) < 0) {
         DEBUGOUT("EEPROM Read Error\n");
         return -E1000_ERR_EEPROM;
     }
@@ -3819,20 +5544,29 @@
 
     DEBUGFUNC("e1000_read_mac_addr");
 
-    for(i = 0; i < NODE_ADDRESS_SIZE; i += 2) {
+    for (i = 0; i < NODE_ADDRESS_SIZE; i += 2) {
         offset = i >> 1;
-        if(e1000_read_eeprom(hw, offset, 1, &eeprom_data) < 0) {
+        if (e1000_read_eeprom(hw, offset, 1, &eeprom_data) < 0) {
             DEBUGOUT("EEPROM Read Error\n");
             return -E1000_ERR_EEPROM;
         }
         hw->perm_mac_addr[i] = (uint8_t) (eeprom_data & 0x00FF);
         hw->perm_mac_addr[i+1] = (uint8_t) (eeprom_data >> 8);
     }
-    if(((hw->mac_type == e1000_82546) || (hw->mac_type == e1000_82546_rev_3)) &&
-       (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1))
+
+    switch (hw->mac_type) {
+    default:
+        break;
+    case e1000_82546:
+    case e1000_82546_rev_3:
+    case e1000_82571:
+    case e1000_80003es2lan:
+        if (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)
             hw->perm_mac_addr[5] ^= 0x01;
+        break;
+    }
 
-    for(i = 0; i < NODE_ADDRESS_SIZE; i++)
+    for (i = 0; i < NODE_ADDRESS_SIZE; i++)
         hw->mac_addr[i] = hw->perm_mac_addr[i];
     return E1000_SUCCESS;
 }
@@ -3850,6 +5584,7 @@
 e1000_init_rx_addrs(struct e1000_hw *hw)
 {
     uint32_t i;
+    uint32_t rar_num;
 
     DEBUGFUNC("e1000_init_rx_addrs");
 
@@ -3858,11 +5593,23 @@
 
     e1000_rar_set(hw, hw->mac_addr, 0);
 
+    rar_num = E1000_RAR_ENTRIES;
+
+    /* Reserve a spot for the Locally Administered Address to work around
+     * an 82571 issue in which a reset on one port will reload the MAC on
+     * the other port. */
+    if ((hw->mac_type == e1000_82571) && (hw->laa_is_present == TRUE))
+        rar_num -= 1;
+    if (hw->mac_type == e1000_ich8lan)
+        rar_num = E1000_RAR_ENTRIES_ICH8LAN;
+
     /* Zero out the other 15 receive addresses. */
     DEBUGOUT("Clearing RAR[1-15]\n");
-    for(i = 1; i < E1000_RAR_ENTRIES; i++) {
+    for (i = 1; i < rar_num; i++) {
         E1000_WRITE_REG_ARRAY(hw, RA, (i << 1), 0);
+        E1000_WRITE_FLUSH(hw);
         E1000_WRITE_REG_ARRAY(hw, RA, ((i << 1) + 1), 0);
+        E1000_WRITE_FLUSH(hw);
     }
 }
 
@@ -3889,6 +5636,8 @@
 {
     uint32_t hash_value;
     uint32_t i;
+    uint32_t num_rar_entry;
+    uint32_t num_mta_entry;
 
     DEBUGFUNC("e1000_mc_addr_list_update");
 
@@ -3897,19 +5646,34 @@
 
     /* Clear RAR[1-15] */
     DEBUGOUT(" Clearing RAR[1-15]\n");
-    for(i = rar_used_count; i < E1000_RAR_ENTRIES; i++) {
+    num_rar_entry = E1000_RAR_ENTRIES;
+    if (hw->mac_type == e1000_ich8lan)
+        num_rar_entry = E1000_RAR_ENTRIES_ICH8LAN;
+    /* Reserve a spot for the Locally Administered Address to work around
+     * an 82571 issue in which a reset on one port will reload the MAC on
+     * the other port. */
+    if ((hw->mac_type == e1000_82571) && (hw->laa_is_present == TRUE))
+        num_rar_entry -= 1;
+
+    for (i = rar_used_count; i < num_rar_entry; i++) {
         E1000_WRITE_REG_ARRAY(hw, RA, (i << 1), 0);
+        E1000_WRITE_FLUSH(hw);
         E1000_WRITE_REG_ARRAY(hw, RA, ((i << 1) + 1), 0);
+        E1000_WRITE_FLUSH(hw);
     }
 
     /* Clear the MTA */
     DEBUGOUT(" Clearing MTA\n");
-    for(i = 0; i < E1000_NUM_MTA_REGISTERS; i++) {
+    num_mta_entry = E1000_NUM_MTA_REGISTERS;
+    if (hw->mac_type == e1000_ich8lan)
+        num_mta_entry = E1000_NUM_MTA_REGISTERS_ICH8LAN;
+    for (i = 0; i < num_mta_entry; i++) {
         E1000_WRITE_REG_ARRAY(hw, MTA, i, 0);
+        E1000_WRITE_FLUSH(hw);
     }
 
     /* Add the new addresses */
-    for(i = 0; i < mc_addr_count; i++) {
+    for (i = 0; i < mc_addr_count; i++) {
         DEBUGOUT(" Adding the multicast addresses:\n");
         DEBUGOUT7(" MC Addr #%d =%.2X %.2X %.2X %.2X %.2X %.2X\n", i,
                   mc_addr_list[i * (ETH_LENGTH_OF_ADDRESS + pad)],
@@ -3928,7 +5692,7 @@
         /* Place this multicast address in the RAR if there is room, *
          * else put it in the MTA
          */
-        if(rar_used_count < E1000_RAR_ENTRIES) {
+        if (rar_used_count < num_rar_entry) {
             e1000_rar_set(hw,
                           mc_addr_list + (i * (ETH_LENGTH_OF_ADDRESS + pad)),
                           rar_used_count);
@@ -3961,24 +5725,47 @@
      * LSB                 MSB
      */
     case 0:
-        /* [47:36] i.e. 0x563 for above example address */
-        hash_value = ((mc_addr[4] >> 4) | (((uint16_t) mc_addr[5]) << 4));
+        if (hw->mac_type == e1000_ich8lan) {
+            /* [47:38] i.e. 0x158 for above example address */
+            hash_value = ((mc_addr[4] >> 6) | (((uint16_t) mc_addr[5]) << 2));
+        } else {
+            /* [47:36] i.e. 0x563 for above example address */
+            hash_value = ((mc_addr[4] >> 4) | (((uint16_t) mc_addr[5]) << 4));
+        }
         break;
     case 1:
-        /* [46:35] i.e. 0xAC6 for above example address */
-        hash_value = ((mc_addr[4] >> 3) | (((uint16_t) mc_addr[5]) << 5));
+        if (hw->mac_type == e1000_ich8lan) {
+            /* [46:37] i.e. 0x2B1 for above example address */
+            hash_value = ((mc_addr[4] >> 5) | (((uint16_t) mc_addr[5]) << 3));
+        } else {
+            /* [46:35] i.e. 0xAC6 for above example address */
+            hash_value = ((mc_addr[4] >> 3) | (((uint16_t) mc_addr[5]) << 5));
+        }
         break;
     case 2:
-        /* [45:34] i.e. 0x5D8 for above example address */
-        hash_value = ((mc_addr[4] >> 2) | (((uint16_t) mc_addr[5]) << 6));
+        if (hw->mac_type == e1000_ich8lan) {
+            /*[45:36] i.e. 0x163 for above example address */
+            hash_value = ((mc_addr[4] >> 4) | (((uint16_t) mc_addr[5]) << 4));
+        } else {
+            /* [45:34] i.e. 0x5D8 for above example address */
+            hash_value = ((mc_addr[4] >> 2) | (((uint16_t) mc_addr[5]) << 6));
+        }
         break;
     case 3:
-        /* [43:32] i.e. 0x634 for above example address */
-        hash_value = ((mc_addr[4]) | (((uint16_t) mc_addr[5]) << 8));
+        if (hw->mac_type == e1000_ich8lan) {
+            /* [43:34] i.e. 0x18D for above example address */
+            hash_value = ((mc_addr[4] >> 2) | (((uint16_t) mc_addr[5]) << 6));
+        } else {
+            /* [43:32] i.e. 0x634 for above example address */
+            hash_value = ((mc_addr[4]) | (((uint16_t) mc_addr[5]) << 8));
+        }
         break;
     }
 
     hash_value &= 0xFFF;
+    if (hw->mac_type == e1000_ich8lan)
+        hash_value &= 0x3FF;
+
     return hash_value;
 }
 
@@ -4005,6 +5792,8 @@
      * register are determined by the lower 5 bits of the value.
      */
     hash_reg = (hash_value >> 5) & 0x7F;
+    if (hw->mac_type == e1000_ich8lan)
+        hash_reg &= 0x1F;
     hash_bit = hash_value & 0x1F;
 
     mta = E1000_READ_REG_ARRAY(hw, MTA, hash_reg);
@@ -4015,12 +5804,15 @@
      * in the MTA, save off the previous entry before writing and
      * restore the old value after writing.
      */
-    if((hw->mac_type == e1000_82544) && ((hash_reg & 0x1) == 1)) {
+    if ((hw->mac_type == e1000_82544) && ((hash_reg & 0x1) == 1)) {
         temp = E1000_READ_REG_ARRAY(hw, MTA, (hash_reg - 1));
         E1000_WRITE_REG_ARRAY(hw, MTA, hash_reg, mta);
+        E1000_WRITE_FLUSH(hw);
         E1000_WRITE_REG_ARRAY(hw, MTA, (hash_reg - 1), temp);
+        E1000_WRITE_FLUSH(hw);
     } else {
         E1000_WRITE_REG_ARRAY(hw, MTA, hash_reg, mta);
+        E1000_WRITE_FLUSH(hw);
     }
 }
 
@@ -4044,11 +5836,42 @@
     rar_low = ((uint32_t) addr[0] |
                ((uint32_t) addr[1] << 8) |
                ((uint32_t) addr[2] << 16) | ((uint32_t) addr[3] << 24));
+    rar_high = ((uint32_t) addr[4] | ((uint32_t) addr[5] << 8));
 
-    rar_high = ((uint32_t) addr[4] | ((uint32_t) addr[5] << 8) | E1000_RAH_AV);
+    /* Disable Rx and flush all Rx frames before enabling RSS to avoid Rx
+     * unit hang.
+     *
+     * Description:
+     * If there are any Rx frames queued up or otherwise present in the HW
+     * before RSS is enabled, and then we enable RSS, the HW Rx unit will
+     * hang.  To work around this issue, we have to disable receives and
+     * flush out all Rx frames before we enable RSS. To do so, we modify we
+     * redirect all Rx traffic to manageability and then reset the HW.
+     * This flushes away Rx frames, and (since the redirections to
+     * manageability persists across resets) keeps new ones from coming in
+     * while we work.  Then, we clear the Address Valid AV bit for all MAC
+     * addresses and undo the re-direction to manageability.
+     * Now, frames are coming in again, but the MAC won't accept them, so
+     * far so good.  We now proceed to initialize RSS (if necessary) and
+     * configure the Rx unit.  Last, we re-enable the AV bits and continue
+     * on our merry way.
+     */
+    switch (hw->mac_type) {
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_80003es2lan:
+        if (hw->leave_av_bit_off == TRUE)
+            break;
+    default:
+        /* Indicate to hardware the Address is Valid. */
+        rar_high |= E1000_RAH_AV;
+        break;
+    }
 
     E1000_WRITE_REG_ARRAY(hw, RA, (index << 1), rar_low);
+    E1000_WRITE_FLUSH(hw);
     E1000_WRITE_REG_ARRAY(hw, RA, ((index << 1) + 1), rar_high);
+    E1000_WRITE_FLUSH(hw);
 }
 
 /******************************************************************************
@@ -4065,12 +5888,18 @@
 {
     uint32_t temp;
 
-    if((hw->mac_type == e1000_82544) && ((offset & 0x1) == 1)) {
+    if (hw->mac_type == e1000_ich8lan)
+        return;
+
+    if ((hw->mac_type == e1000_82544) && ((offset & 0x1) == 1)) {
         temp = E1000_READ_REG_ARRAY(hw, VFTA, (offset - 1));
         E1000_WRITE_REG_ARRAY(hw, VFTA, offset, value);
+        E1000_WRITE_FLUSH(hw);
         E1000_WRITE_REG_ARRAY(hw, VFTA, (offset - 1), temp);
+        E1000_WRITE_FLUSH(hw);
     } else {
         E1000_WRITE_REG_ARRAY(hw, VFTA, offset, value);
+        E1000_WRITE_FLUSH(hw);
     }
 }
 
@@ -4083,12 +5912,37 @@
 e1000_clear_vfta(struct e1000_hw *hw)
 {
     uint32_t offset;
+    uint32_t vfta_value = 0;
+    uint32_t vfta_offset = 0;
+    uint32_t vfta_bit_in_reg = 0;
 
-    for(offset = 0; offset < E1000_VLAN_FILTER_TBL_SIZE; offset++)
-        E1000_WRITE_REG_ARRAY(hw, VFTA, offset, 0);
+    if (hw->mac_type == e1000_ich8lan)
+        return;
+
+    if (hw->mac_type == e1000_82573) {
+        if (hw->mng_cookie.vlan_id != 0) {
+            /* The VFTA is a 4096b bit-field, each identifying a single VLAN
+             * ID.  The following operations determine which 32b entry
+             * (i.e. offset) into the array we want to set the VLAN ID
+             * (i.e. bit) of the manageability unit. */
+            vfta_offset = (hw->mng_cookie.vlan_id >>
+                           E1000_VFTA_ENTRY_SHIFT) &
+                          E1000_VFTA_ENTRY_MASK;
+            vfta_bit_in_reg = 1 << (hw->mng_cookie.vlan_id &
+                                    E1000_VFTA_ENTRY_BIT_SHIFT_MASK);
+        }
+    }
+    for (offset = 0; offset < E1000_VLAN_FILTER_TBL_SIZE; offset++) {
+        /* If the offset we want to clear is the same offset of the
+         * manageability VLAN ID, then clear all bits except that of the
+         * manageability unit */
+        vfta_value = (offset == vfta_offset) ? vfta_bit_in_reg : 0;
+        E1000_WRITE_REG_ARRAY(hw, VFTA, offset, vfta_value);
+        E1000_WRITE_FLUSH(hw);
+    }
 }
 
-static int32_t
+int32_t
 e1000_id_led_init(struct e1000_hw * hw)
 {
     uint32_t ledctl;
@@ -4100,7 +5954,7 @@
 
     DEBUGFUNC("e1000_id_led_init");
 
-    if(hw->mac_type < e1000_82540) {
+    if (hw->mac_type < e1000_82540) {
         /* Nothing to do */
         return E1000_SUCCESS;
     }
@@ -4110,15 +5964,24 @@
     hw->ledctl_mode1 = hw->ledctl_default;
     hw->ledctl_mode2 = hw->ledctl_default;
 
-    if(e1000_read_eeprom(hw, EEPROM_ID_LED_SETTINGS, 1, &eeprom_data) < 0) {
+    if (e1000_read_eeprom(hw, EEPROM_ID_LED_SETTINGS, 1, &eeprom_data) < 0) {
         DEBUGOUT("EEPROM Read Error\n");
         return -E1000_ERR_EEPROM;
     }
-    if((eeprom_data== ID_LED_RESERVED_0000) ||
-       (eeprom_data == ID_LED_RESERVED_FFFF)) eeprom_data = ID_LED_DEFAULT;
-    for(i = 0; i < 4; i++) {
+
+    if ((hw->mac_type == e1000_82573) &&
+        (eeprom_data == ID_LED_RESERVED_82573))
+        eeprom_data = ID_LED_DEFAULT_82573;
+    else if ((eeprom_data == ID_LED_RESERVED_0000) ||
+            (eeprom_data == ID_LED_RESERVED_FFFF)) {
+        if (hw->mac_type == e1000_ich8lan)
+            eeprom_data = ID_LED_DEFAULT_ICH8LAN;
+        else
+            eeprom_data = ID_LED_DEFAULT;
+    }
+    for (i = 0; i < 4; i++) {
         temp = (eeprom_data >> (i << 2)) & led_mask;
-        switch(temp) {
+        switch (temp) {
         case ID_LED_ON1_DEF2:
         case ID_LED_ON1_ON2:
         case ID_LED_ON1_OFF2:
@@ -4135,7 +5998,7 @@
             /* Do nothing */
             break;
         }
-        switch(temp) {
+        switch (temp) {
         case ID_LED_DEF1_ON2:
         case ID_LED_ON1_ON2:
         case ID_LED_OFF1_ON2:
@@ -4169,7 +6032,7 @@
 
     DEBUGFUNC("e1000_setup_led");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82542_rev2_0:
     case e1000_82542_rev2_1:
     case e1000_82543:
@@ -4183,16 +6046,16 @@
         /* Turn off PHY Smart Power Down (if enabled) */
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_GMII_FIFO,
                                      &hw->phy_spd_default);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO,
                                       (uint16_t)(hw->phy_spd_default &
                                       ~IGP01E1000_GMII_SPD));
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         /* Fall Through */
     default:
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             ledctl = E1000_READ_REG(hw, LEDCTL);
             /* Save current LEDCTL settings */
             hw->ledctl_default = ledctl;
@@ -4203,7 +6066,7 @@
             ledctl |= (E1000_LEDCTL_MODE_LED_OFF <<
                        E1000_LEDCTL_LED0_MODE_SHIFT);
             E1000_WRITE_REG(hw, LEDCTL, ledctl);
-        } else if(hw->media_type == e1000_media_type_copper)
+        } else if (hw->media_type == e1000_media_type_copper)
             E1000_WRITE_REG(hw, LEDCTL, hw->ledctl_mode1);
         break;
     }
@@ -4211,6 +6074,45 @@
     return E1000_SUCCESS;
 }
 
+
+/******************************************************************************
+ * Used on 82571 and later Si that has LED blink bits.
+ * Callers must use their own timer and should have already called
+ * e1000_id_led_init()
+ * Call e1000_cleanup led() to stop blinking
+ *
+ * hw - Struct containing variables accessed by shared code
+ *****************************************************************************/
+int32_t
+e1000_blink_led_start(struct e1000_hw *hw)
+{
+    int16_t  i;
+    uint32_t ledctl_blink = 0;
+
+    DEBUGFUNC("e1000_id_led_blink_on");
+
+    if (hw->mac_type < e1000_82571) {
+        /* Nothing to do */
+        return E1000_SUCCESS;
+    }
+    if (hw->media_type == e1000_media_type_fiber) {
+        /* always blink LED0 for PCI-E fiber */
+        ledctl_blink = E1000_LEDCTL_LED0_BLINK |
+                     (E1000_LEDCTL_MODE_LED_ON << E1000_LEDCTL_LED0_MODE_SHIFT);
+    } else {
+        /* set the blink bit for each LED that's "on" (0x0E) in ledctl_mode2 */
+        ledctl_blink = hw->ledctl_mode2;
+        for (i=0; i < 4; i++)
+            if (((hw->ledctl_mode2 >> (i * 8)) & 0xFF) ==
+                E1000_LEDCTL_MODE_LED_ON)
+                ledctl_blink |= (E1000_LEDCTL_LED0_BLINK << (i * 8));
+    }
+
+    E1000_WRITE_REG(hw, LEDCTL, ledctl_blink);
+
+    return E1000_SUCCESS;
+}
+
 /******************************************************************************
  * Restores the saved state of the SW controlable LED.
  *
@@ -4223,7 +6125,7 @@
 
     DEBUGFUNC("e1000_cleanup_led");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82542_rev2_0:
     case e1000_82542_rev2_1:
     case e1000_82543:
@@ -4237,10 +6139,14 @@
         /* Turn on PHY Smart Power Down (if previously enabled) */
         ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO,
                                       hw->phy_spd_default);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         /* Fall Through */
     default:
+        if (hw->phy_type == e1000_phy_ife) {
+            e1000_write_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL_LED, 0);
+            break;
+        }
         /* Restore LEDCTL settings */
         E1000_WRITE_REG(hw, LEDCTL, hw->ledctl_default);
         break;
@@ -4261,7 +6167,7 @@
 
     DEBUGFUNC("e1000_led_on");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82542_rev2_0:
     case e1000_82542_rev2_1:
     case e1000_82543:
@@ -4270,7 +6176,7 @@
         ctrl |= E1000_CTRL_SWDPIO0;
         break;
     case e1000_82544:
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             /* Set SW Defineable Pin 0 to turn on the LED */
             ctrl |= E1000_CTRL_SWDPIN0;
             ctrl |= E1000_CTRL_SWDPIO0;
@@ -4281,11 +6187,14 @@
         }
         break;
     default:
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             /* Clear SW Defineable Pin 0 to turn on the LED */
             ctrl &= ~E1000_CTRL_SWDPIN0;
             ctrl |= E1000_CTRL_SWDPIO0;
-        } else if(hw->media_type == e1000_media_type_copper) {
+        } else if (hw->phy_type == e1000_phy_ife) {
+            e1000_write_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL_LED,
+                 (IFE_PSCL_PROBE_MODE | IFE_PSCL_PROBE_LEDS_ON));
+        } else if (hw->media_type == e1000_media_type_copper) {
             E1000_WRITE_REG(hw, LEDCTL, hw->ledctl_mode2);
             return E1000_SUCCESS;
         }
@@ -4309,7 +6218,7 @@
 
     DEBUGFUNC("e1000_led_off");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82542_rev2_0:
     case e1000_82542_rev2_1:
     case e1000_82543:
@@ -4318,7 +6227,7 @@
         ctrl |= E1000_CTRL_SWDPIO0;
         break;
     case e1000_82544:
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             /* Clear SW Defineable Pin 0 to turn off the LED */
             ctrl &= ~E1000_CTRL_SWDPIN0;
             ctrl |= E1000_CTRL_SWDPIO0;
@@ -4329,11 +6238,14 @@
         }
         break;
     default:
-        if(hw->media_type == e1000_media_type_fiber) {
+        if (hw->media_type == e1000_media_type_fiber) {
             /* Set SW Defineable Pin 0 to turn off the LED */
             ctrl |= E1000_CTRL_SWDPIN0;
             ctrl |= E1000_CTRL_SWDPIO0;
-        } else if(hw->media_type == e1000_media_type_copper) {
+        } else if (hw->phy_type == e1000_phy_ife) {
+            e1000_write_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL_LED,
+                 (IFE_PSCL_PROBE_MODE | IFE_PSCL_PROBE_LEDS_OFF));
+        } else if (hw->media_type == e1000_media_type_copper) {
             E1000_WRITE_REG(hw, LEDCTL, hw->ledctl_mode1);
             return E1000_SUCCESS;
         }
@@ -4371,12 +6283,16 @@
     temp = E1000_READ_REG(hw, XOFFRXC);
     temp = E1000_READ_REG(hw, XOFFTXC);
     temp = E1000_READ_REG(hw, FCRUC);
+
+    if (hw->mac_type != e1000_ich8lan) {
     temp = E1000_READ_REG(hw, PRC64);
     temp = E1000_READ_REG(hw, PRC127);
     temp = E1000_READ_REG(hw, PRC255);
     temp = E1000_READ_REG(hw, PRC511);
     temp = E1000_READ_REG(hw, PRC1023);
     temp = E1000_READ_REG(hw, PRC1522);
+    }
+
     temp = E1000_READ_REG(hw, GPRC);
     temp = E1000_READ_REG(hw, BPRC);
     temp = E1000_READ_REG(hw, MPRC);
@@ -4396,16 +6312,20 @@
     temp = E1000_READ_REG(hw, TOTH);
     temp = E1000_READ_REG(hw, TPR);
     temp = E1000_READ_REG(hw, TPT);
+
+    if (hw->mac_type != e1000_ich8lan) {
     temp = E1000_READ_REG(hw, PTC64);
     temp = E1000_READ_REG(hw, PTC127);
     temp = E1000_READ_REG(hw, PTC255);
     temp = E1000_READ_REG(hw, PTC511);
     temp = E1000_READ_REG(hw, PTC1023);
     temp = E1000_READ_REG(hw, PTC1522);
+    }
+
     temp = E1000_READ_REG(hw, MPTC);
     temp = E1000_READ_REG(hw, BPTC);
 
-    if(hw->mac_type < e1000_82543) return;
+    if (hw->mac_type < e1000_82543) return;
 
     temp = E1000_READ_REG(hw, ALGNERRC);
     temp = E1000_READ_REG(hw, RXERRC);
@@ -4414,11 +6334,26 @@
     temp = E1000_READ_REG(hw, TSCTC);
     temp = E1000_READ_REG(hw, TSCTFC);
 
-    if(hw->mac_type <= e1000_82544) return;
+    if (hw->mac_type <= e1000_82544) return;
 
     temp = E1000_READ_REG(hw, MGTPRC);
     temp = E1000_READ_REG(hw, MGTPDC);
     temp = E1000_READ_REG(hw, MGTPTC);
+
+    if (hw->mac_type <= e1000_82547_rev_2) return;
+
+    temp = E1000_READ_REG(hw, IAC);
+    temp = E1000_READ_REG(hw, ICRXOC);
+
+    if (hw->mac_type == e1000_ich8lan) return;
+
+    temp = E1000_READ_REG(hw, ICRXPTC);
+    temp = E1000_READ_REG(hw, ICRXATC);
+    temp = E1000_READ_REG(hw, ICTXPTC);
+    temp = E1000_READ_REG(hw, ICTXATC);
+    temp = E1000_READ_REG(hw, ICTXQEC);
+    temp = E1000_READ_REG(hw, ICTXQMTC);
+    temp = E1000_READ_REG(hw, ICRXDMTC);
 }
 
 /******************************************************************************
@@ -4436,8 +6371,8 @@
 {
     DEBUGFUNC("e1000_reset_adaptive");
 
-    if(hw->adaptive_ifs) {
-        if(!hw->ifs_params_forced) {
+    if (hw->adaptive_ifs) {
+        if (!hw->ifs_params_forced) {
             hw->current_ifs_val = 0;
             hw->ifs_min_val = IFS_MIN;
             hw->ifs_max_val = IFS_MAX;
@@ -4464,12 +6399,12 @@
 {
     DEBUGFUNC("e1000_update_adaptive");
 
-    if(hw->adaptive_ifs) {
-        if((hw->collision_delta * hw->ifs_ratio) > hw->tx_packet_delta) {
-            if(hw->tx_packet_delta > MIN_NUM_XMITS) {
+    if (hw->adaptive_ifs) {
+        if ((hw->collision_delta * hw->ifs_ratio) > hw->tx_packet_delta) {
+            if (hw->tx_packet_delta > MIN_NUM_XMITS) {
                 hw->in_ifs_mode = TRUE;
-                if(hw->current_ifs_val < hw->ifs_max_val) {
-                    if(hw->current_ifs_val == 0)
+                if (hw->current_ifs_val < hw->ifs_max_val) {
+                    if (hw->current_ifs_val == 0)
                         hw->current_ifs_val = hw->ifs_min_val;
                     else
                         hw->current_ifs_val += hw->ifs_step_size;
@@ -4477,7 +6412,7 @@
                 }
             }
         } else {
-            if(hw->in_ifs_mode && (hw->tx_packet_delta <= MIN_NUM_XMITS)) {
+            if (hw->in_ifs_mode && (hw->tx_packet_delta <= MIN_NUM_XMITS)) {
                 hw->current_ifs_val = 0;
                 hw->in_ifs_mode = FALSE;
                 E1000_WRITE_REG(hw, AIT, 0);
@@ -4524,46 +6459,46 @@
      * This could be simplified if all environments supported
      * 64-bit integers.
      */
-    if(carry_bit && ((stats->gorcl & 0x80000000) == 0))
+    if (carry_bit && ((stats->gorcl & 0x80000000) == 0))
         stats->gorch++;
     /* Is this a broadcast or multicast?  Check broadcast first,
      * since the test for a multicast frame will test positive on
      * a broadcast frame.
      */
-    if((mac_addr[0] == (uint8_t) 0xff) && (mac_addr[1] == (uint8_t) 0xff))
+    if ((mac_addr[0] == (uint8_t) 0xff) && (mac_addr[1] == (uint8_t) 0xff))
         /* Broadcast packet */
         stats->bprc++;
-    else if(*mac_addr & 0x01)
+    else if (*mac_addr & 0x01)
         /* Multicast packet */
         stats->mprc++;
 
-    if(frame_len == hw->max_frame_size) {
+    if (frame_len == hw->max_frame_size) {
         /* In this case, the hardware has overcounted the number of
          * oversize frames.
          */
-        if(stats->roc > 0)
+        if (stats->roc > 0)
             stats->roc--;
     }
 
     /* Adjust the bin counters when the extra byte put the frame in the
      * wrong bin. Remember that the frame_len was adjusted above.
      */
-    if(frame_len == 64) {
+    if (frame_len == 64) {
         stats->prc64++;
         stats->prc127--;
-    } else if(frame_len == 127) {
+    } else if (frame_len == 127) {
         stats->prc127++;
         stats->prc255--;
-    } else if(frame_len == 255) {
+    } else if (frame_len == 255) {
         stats->prc255++;
         stats->prc511--;
-    } else if(frame_len == 511) {
+    } else if (frame_len == 511) {
         stats->prc511++;
         stats->prc1023--;
-    } else if(frame_len == 1023) {
+    } else if (frame_len == 1023) {
         stats->prc1023++;
         stats->prc1522--;
-    } else if(frame_len == 1522) {
+    } else if (frame_len == 1522) {
         stats->prc1522++;
     }
 }
@@ -4578,41 +6513,57 @@
 {
     uint32_t status;
 
-    if(hw->mac_type < e1000_82543) {
+    switch (hw->mac_type) {
+    case e1000_82542_rev2_0:
+    case e1000_82542_rev2_1:
         hw->bus_type = e1000_bus_type_unknown;
         hw->bus_speed = e1000_bus_speed_unknown;
         hw->bus_width = e1000_bus_width_unknown;
-        return;
-    }
-
-    status = E1000_READ_REG(hw, STATUS);
-    hw->bus_type = (status & E1000_STATUS_PCIX_MODE) ?
-                   e1000_bus_type_pcix : e1000_bus_type_pci;
+        break;
+    case e1000_82572:
+    case e1000_82573:
+        hw->bus_type = e1000_bus_type_pci_express;
+        hw->bus_speed = e1000_bus_speed_2500;
+        hw->bus_width = e1000_bus_width_pciex_1;
+        break;
+    case e1000_82571:
+    case e1000_ich8lan:
+    case e1000_80003es2lan:
+        hw->bus_type = e1000_bus_type_pci_express;
+        hw->bus_speed = e1000_bus_speed_2500;
+        hw->bus_width = e1000_bus_width_pciex_4;
+        break;
+    default:
+        status = E1000_READ_REG(hw, STATUS);
+        hw->bus_type = (status & E1000_STATUS_PCIX_MODE) ?
+                       e1000_bus_type_pcix : e1000_bus_type_pci;
 
-    if(hw->device_id == E1000_DEV_ID_82546EB_QUAD_COPPER) {
-        hw->bus_speed = (hw->bus_type == e1000_bus_type_pci) ?
-                        e1000_bus_speed_66 : e1000_bus_speed_120;
-    } else if(hw->bus_type == e1000_bus_type_pci) {
-        hw->bus_speed = (status & E1000_STATUS_PCI66) ?
-                        e1000_bus_speed_66 : e1000_bus_speed_33;
-    } else {
-        switch (status & E1000_STATUS_PCIX_SPEED) {
-        case E1000_STATUS_PCIX_SPEED_66:
-            hw->bus_speed = e1000_bus_speed_66;
-            break;
-        case E1000_STATUS_PCIX_SPEED_100:
-            hw->bus_speed = e1000_bus_speed_100;
-            break;
-        case E1000_STATUS_PCIX_SPEED_133:
-            hw->bus_speed = e1000_bus_speed_133;
-            break;
-        default:
-            hw->bus_speed = e1000_bus_speed_reserved;
-            break;
+        if (hw->device_id == E1000_DEV_ID_82546EB_QUAD_COPPER) {
+            hw->bus_speed = (hw->bus_type == e1000_bus_type_pci) ?
+                            e1000_bus_speed_66 : e1000_bus_speed_120;
+        } else if (hw->bus_type == e1000_bus_type_pci) {
+            hw->bus_speed = (status & E1000_STATUS_PCI66) ?
+                            e1000_bus_speed_66 : e1000_bus_speed_33;
+        } else {
+            switch (status & E1000_STATUS_PCIX_SPEED) {
+            case E1000_STATUS_PCIX_SPEED_66:
+                hw->bus_speed = e1000_bus_speed_66;
+                break;
+            case E1000_STATUS_PCIX_SPEED_100:
+                hw->bus_speed = e1000_bus_speed_100;
+                break;
+            case E1000_STATUS_PCIX_SPEED_133:
+                hw->bus_speed = e1000_bus_speed_133;
+                break;
+            default:
+                hw->bus_speed = e1000_bus_speed_reserved;
+                break;
+            }
         }
+        hw->bus_width = (status & E1000_STATUS_BUS64) ?
+                        e1000_bus_width_64 : e1000_bus_width_32;
+        break;
     }
-    hw->bus_width = (status & E1000_STATUS_BUS64) ?
-                    e1000_bus_width_64 : e1000_bus_width_32;
 }
 /******************************************************************************
  * Reads a value from one of the devices registers using port I/O (as opposed
@@ -4675,23 +6626,25 @@
 {
     int32_t ret_val;
     uint16_t agc_value = 0;
-    uint16_t cur_agc, min_agc = IGP01E1000_AGC_LENGTH_TABLE_SIZE;
     uint16_t i, phy_data;
+    uint16_t cable_length;
 
     DEBUGFUNC("e1000_get_cable_length");
 
     *min_length = *max_length = 0;
 
     /* Use old method for Phy older than IGP */
-    if(hw->phy_type == e1000_phy_m88) {
+    if (hw->phy_type == e1000_phy_m88) {
+
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS,
                                      &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
+        cable_length = (phy_data & M88E1000_PSSR_CABLE_LENGTH) >>
+                       M88E1000_PSSR_CABLE_LENGTH_SHIFT;
 
         /* Convert the enum value to ranged values */
-        switch((phy_data & M88E1000_PSSR_CABLE_LENGTH) >>
-               M88E1000_PSSR_CABLE_LENGTH_SHIFT) {
+        switch (cable_length) {
         case e1000_cable_length_50:
             *min_length = 0;
             *max_length = e1000_igp_cable_length_50;
@@ -4716,36 +6669,66 @@
             return -E1000_ERR_PHY;
             break;
         }
-    } else if(hw->phy_type == e1000_phy_igp) { /* For IGP PHY */
+    } else if (hw->phy_type == e1000_phy_gg82563) {
+        ret_val = e1000_read_phy_reg(hw, GG82563_PHY_DSP_DISTANCE,
+                                     &phy_data);
+        if (ret_val)
+            return ret_val;
+        cable_length = phy_data & GG82563_DSPD_CABLE_LENGTH;
+
+        switch (cable_length) {
+        case e1000_gg_cable_length_60:
+            *min_length = 0;
+            *max_length = e1000_igp_cable_length_60;
+            break;
+        case e1000_gg_cable_length_60_115:
+            *min_length = e1000_igp_cable_length_60;
+            *max_length = e1000_igp_cable_length_115;
+            break;
+        case e1000_gg_cable_length_115_150:
+            *min_length = e1000_igp_cable_length_115;
+            *max_length = e1000_igp_cable_length_150;
+            break;
+        case e1000_gg_cable_length_150:
+            *min_length = e1000_igp_cable_length_150;
+            *max_length = e1000_igp_cable_length_180;
+            break;
+        default:
+            return -E1000_ERR_PHY;
+            break;
+        }
+    } else if (hw->phy_type == e1000_phy_igp) { /* For IGP PHY */
+        uint16_t cur_agc_value;
+        uint16_t min_agc_value = IGP01E1000_AGC_LENGTH_TABLE_SIZE;
         uint16_t agc_reg_array[IGP01E1000_PHY_CHANNEL_NUM] =
                                                          {IGP01E1000_PHY_AGC_A,
                                                           IGP01E1000_PHY_AGC_B,
                                                           IGP01E1000_PHY_AGC_C,
                                                           IGP01E1000_PHY_AGC_D};
         /* Read the AGC registers for all channels */
-        for(i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+        for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
 
             ret_val = e1000_read_phy_reg(hw, agc_reg_array[i], &phy_data);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
-            cur_agc = phy_data >> IGP01E1000_AGC_LENGTH_SHIFT;
+            cur_agc_value = phy_data >> IGP01E1000_AGC_LENGTH_SHIFT;
 
-            /* Array bound check. */
-            if((cur_agc >= IGP01E1000_AGC_LENGTH_TABLE_SIZE - 1) ||
-               (cur_agc == 0))
+            /* Value bound check. */
+            if ((cur_agc_value >= IGP01E1000_AGC_LENGTH_TABLE_SIZE - 1) ||
+                (cur_agc_value == 0))
                 return -E1000_ERR_PHY;
 
-            agc_value += cur_agc;
+            agc_value += cur_agc_value;
 
             /* Update minimal AGC value. */
-            if(min_agc > cur_agc)
-                min_agc = cur_agc;
+            if (min_agc_value > cur_agc_value)
+                min_agc_value = cur_agc_value;
         }
 
         /* Remove the minimal AGC result for length < 50m */
-        if(agc_value < IGP01E1000_PHY_CHANNEL_NUM * e1000_igp_cable_length_50) {
-            agc_value -= min_agc;
+        if (agc_value < IGP01E1000_PHY_CHANNEL_NUM * e1000_igp_cable_length_50) {
+            agc_value -= min_agc_value;
 
             /* Get the average length of the remaining 3 channels */
             agc_value /= (IGP01E1000_PHY_CHANNEL_NUM - 1);
@@ -4761,6 +6744,51 @@
                        IGP01E1000_AGC_RANGE) : 0;
         *max_length = e1000_igp_cable_length_table[agc_value] +
                       IGP01E1000_AGC_RANGE;
+    } else if (hw->phy_type == e1000_phy_igp_2 ||
+               hw->phy_type == e1000_phy_igp_3) {
+        uint16_t cur_agc_index, max_agc_index = 0;
+        uint16_t min_agc_index = IGP02E1000_AGC_LENGTH_TABLE_SIZE - 1;
+        uint16_t agc_reg_array[IGP02E1000_PHY_CHANNEL_NUM] =
+                                                         {IGP02E1000_PHY_AGC_A,
+                                                          IGP02E1000_PHY_AGC_B,
+                                                          IGP02E1000_PHY_AGC_C,
+                                                          IGP02E1000_PHY_AGC_D};
+        /* Read the AGC registers for all channels */
+        for (i = 0; i < IGP02E1000_PHY_CHANNEL_NUM; i++) {
+            ret_val = e1000_read_phy_reg(hw, agc_reg_array[i], &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            /* Getting bits 15:9, which represent the combination of course and
+             * fine gain values.  The result is a number that can be put into
+             * the lookup table to obtain the approximate cable length. */
+            cur_agc_index = (phy_data >> IGP02E1000_AGC_LENGTH_SHIFT) &
+                            IGP02E1000_AGC_LENGTH_MASK;
+
+            /* Array index bound check. */
+            if ((cur_agc_index >= IGP02E1000_AGC_LENGTH_TABLE_SIZE) ||
+                (cur_agc_index == 0))
+                return -E1000_ERR_PHY;
+
+            /* Remove min & max AGC values from calculation. */
+            if (e1000_igp_2_cable_length_table[min_agc_index] >
+                e1000_igp_2_cable_length_table[cur_agc_index])
+                min_agc_index = cur_agc_index;
+            if (e1000_igp_2_cable_length_table[max_agc_index] <
+                e1000_igp_2_cable_length_table[cur_agc_index])
+                max_agc_index = cur_agc_index;
+
+            agc_value += e1000_igp_2_cable_length_table[cur_agc_index];
+        }
+
+        agc_value -= (e1000_igp_2_cable_length_table[min_agc_index] +
+                      e1000_igp_2_cable_length_table[max_agc_index]);
+        agc_value /= (IGP02E1000_PHY_CHANNEL_NUM - 2);
+
+        /* Calculate cable length with the error range of +/- 10 meters. */
+        *min_length = ((agc_value - IGP02E1000_AGC_RANGE) > 0) ?
+                       (agc_value - IGP02E1000_AGC_RANGE) : 0;
+        *max_length = agc_value + IGP02E1000_AGC_RANGE;
     }
 
     return E1000_SUCCESS;
@@ -4791,30 +6819,33 @@
 
     DEBUGFUNC("e1000_check_polarity");
 
-    if(hw->phy_type == e1000_phy_m88) {
+    if ((hw->phy_type == e1000_phy_m88) ||
+        (hw->phy_type == e1000_phy_gg82563)) {
         /* return the Polarity bit in the Status register. */
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS,
                                      &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
         *polarity = (phy_data & M88E1000_PSSR_REV_POLARITY) >>
                     M88E1000_PSSR_REV_POLARITY_SHIFT;
-    } else if(hw->phy_type == e1000_phy_igp) {
+    } else if (hw->phy_type == e1000_phy_igp ||
+              hw->phy_type == e1000_phy_igp_3 ||
+              hw->phy_type == e1000_phy_igp_2) {
         /* Read the Status register to check the speed */
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS,
                                      &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         /* If speed is 1000 Mbps, must read the IGP01E1000_PHY_PCS_INIT_REG to
          * find the polarity status */
-        if((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
+        if ((phy_data & IGP01E1000_PSSR_SPEED_MASK) ==
            IGP01E1000_PSSR_SPEED_1000MBPS) {
 
             /* Read the GIG initialization PCS register (0x00B4) */
             ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PCS_INIT_REG,
                                          &phy_data);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             /* Check the polarity bits */
@@ -4824,6 +6855,13 @@
              * 100 Mbps this bit is always 0) */
             *polarity = phy_data & IGP01E1000_PSSR_POLARITY_REVERSED;
         }
+    } else if (hw->phy_type == e1000_phy_ife) {
+        ret_val = e1000_read_phy_reg(hw, IFE_PHY_EXTENDED_STATUS_CONTROL,
+                                     &phy_data);
+        if (ret_val)
+            return ret_val;
+        *polarity = (phy_data & IFE_PESC_POLARITY_REVERSED) >>
+                           IFE_PESC_POLARITY_REVERSED_SHIFT;
     }
     return E1000_SUCCESS;
 }
@@ -4836,7 +6874,7 @@
  *                                1 - Downshift ocured.
  *
  * returns: - E1000_ERR_XXX
- *            E1000_SUCCESS 
+ *            E1000_SUCCESS
  *
  * For phy's older then IGP, this function reads the Downshift bit in the Phy
  * Specific Status register.  For IGP phy's, it reads the Downgrade bit in the
@@ -4851,23 +6889,29 @@
 
     DEBUGFUNC("e1000_check_downshift");
 
-    if(hw->phy_type == e1000_phy_igp) {
+    if (hw->phy_type == e1000_phy_igp ||
+        hw->phy_type == e1000_phy_igp_3 ||
+        hw->phy_type == e1000_phy_igp_2) {
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_LINK_HEALTH,
                                      &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         hw->speed_downgraded = (phy_data & IGP01E1000_PLHR_SS_DOWNGRADE) ? 1 : 0;
-    }
-    else if(hw->phy_type == e1000_phy_m88) {
+    } else if ((hw->phy_type == e1000_phy_m88) ||
+               (hw->phy_type == e1000_phy_gg82563)) {
         ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS,
                                      &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         hw->speed_downgraded = (phy_data & M88E1000_PSSR_DOWNSHIFT) >>
                                M88E1000_PSSR_DOWNSHIFT_SHIFT;
+    } else if (hw->phy_type == e1000_phy_ife) {
+        /* e1000_phy_ife supports 10/100 speed only */
+        hw->speed_downgraded = FALSE;
     }
+
     return E1000_SUCCESS;
 }
 
@@ -4888,7 +6932,7 @@
                                    boolean_t link_up)
 {
     int32_t ret_val;
-    uint16_t phy_data, speed, duplex, i;
+    uint16_t phy_data, phy_saved_data, speed, duplex, i;
     uint16_t dsp_reg_array[IGP01E1000_PHY_CHANNEL_NUM] =
                                         {IGP01E1000_PHY_AGC_PARAM_A,
                                         IGP01E1000_PHY_AGC_PARAM_B,
@@ -4898,40 +6942,42 @@
 
     DEBUGFUNC("e1000_config_dsp_after_link_change");
 
-    if(hw->phy_type != e1000_phy_igp)
+    if (hw->phy_type != e1000_phy_igp)
         return E1000_SUCCESS;
 
-    if(link_up) {
+    if (link_up) {
         ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
-        if(ret_val) {
+        if (ret_val) {
             DEBUGOUT("Error getting link speed and duplex\n");
             return ret_val;
         }
 
-        if(speed == SPEED_1000) {
+        if (speed == SPEED_1000) {
 
-            e1000_get_cable_length(hw, &min_length, &max_length);
+            ret_val = e1000_get_cable_length(hw, &min_length, &max_length);
+            if (ret_val)
+                return ret_val;
 
-            if((hw->dsp_config_state == e1000_dsp_config_enabled) &&
+            if ((hw->dsp_config_state == e1000_dsp_config_enabled) &&
                 min_length >= e1000_igp_cable_length_50) {
 
-                for(i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+                for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
                     ret_val = e1000_read_phy_reg(hw, dsp_reg_array[i],
                                                  &phy_data);
-                    if(ret_val)
+                    if (ret_val)
                         return ret_val;
 
                     phy_data &= ~IGP01E1000_PHY_EDAC_MU_INDEX;
 
                     ret_val = e1000_write_phy_reg(hw, dsp_reg_array[i],
                                                   phy_data);
-                    if(ret_val)
+                    if (ret_val)
                         return ret_val;
                 }
                 hw->dsp_config_state = e1000_dsp_config_activated;
             }
 
-            if((hw->ffe_config_state == e1000_ffe_config_enabled) &&
+            if ((hw->ffe_config_state == e1000_ffe_config_enabled) &&
                (min_length < e1000_igp_cable_length_50)) {
 
                 uint16_t ffe_idle_err_timeout = FFE_IDLE_ERR_COUNT_TIMEOUT_20;
@@ -4940,74 +6986,121 @@
                 /* clear previous idle error counts */
                 ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS,
                                              &phy_data);
-                if(ret_val)
+                if (ret_val)
                     return ret_val;
 
-                for(i = 0; i < ffe_idle_err_timeout; i++) {
-                    udelay(1000);
+                for (i = 0; i < ffe_idle_err_timeout; i++) {
+                    usec_delay(1000);
                     ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS,
                                                  &phy_data);
-                    if(ret_val)
+                    if (ret_val)
                         return ret_val;
 
                     idle_errs += (phy_data & SR_1000T_IDLE_ERROR_CNT);
-                    if(idle_errs > SR_1000T_PHY_EXCESSIVE_IDLE_ERR_COUNT) {
+                    if (idle_errs > SR_1000T_PHY_EXCESSIVE_IDLE_ERR_COUNT) {
                         hw->ffe_config_state = e1000_ffe_config_active;
 
                         ret_val = e1000_write_phy_reg(hw,
                                     IGP01E1000_PHY_DSP_FFE,
                                     IGP01E1000_PHY_DSP_FFE_CM_CP);
-                        if(ret_val)
+                        if (ret_val)
                             return ret_val;
                         break;
                     }
 
-                    if(idle_errs)
+                    if (idle_errs)
                         ffe_idle_err_timeout = FFE_IDLE_ERR_COUNT_TIMEOUT_100;
                 }
             }
         }
     } else {
-        if(hw->dsp_config_state == e1000_dsp_config_activated) {
+        if (hw->dsp_config_state == e1000_dsp_config_activated) {
+            /* Save off the current value of register 0x2F5B to be restored at
+             * the end of the routines. */
+            ret_val = e1000_read_phy_reg(hw, 0x2F5B, &phy_saved_data);
+
+            if (ret_val)
+                return ret_val;
+
+            /* Disable the PHY transmitter */
+            ret_val = e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+
+            if (ret_val)
+                return ret_val;
+
+            msec_delay_irq(20);
+
             ret_val = e1000_write_phy_reg(hw, 0x0000,
                                           IGP01E1000_IEEE_FORCE_GIGA);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
-            for(i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
+            for (i = 0; i < IGP01E1000_PHY_CHANNEL_NUM; i++) {
                 ret_val = e1000_read_phy_reg(hw, dsp_reg_array[i], &phy_data);
-                if(ret_val)
+                if (ret_val)
                     return ret_val;
 
                 phy_data &= ~IGP01E1000_PHY_EDAC_MU_INDEX;
                 phy_data |=  IGP01E1000_PHY_EDAC_SIGN_EXT_9_BITS;
 
                 ret_val = e1000_write_phy_reg(hw,dsp_reg_array[i], phy_data);
-                if(ret_val)
+                if (ret_val)
                     return ret_val;
             }
 
             ret_val = e1000_write_phy_reg(hw, 0x0000,
                                           IGP01E1000_IEEE_RESTART_AUTONEG);
-            if(ret_val)
+            if (ret_val)
+                return ret_val;
+
+            msec_delay_irq(20);
+
+            /* Now enable the transmitter */
+            ret_val = e1000_write_phy_reg(hw, 0x2F5B, phy_saved_data);
+
+            if (ret_val)
                 return ret_val;
 
             hw->dsp_config_state = e1000_dsp_config_enabled;
         }
 
-        if(hw->ffe_config_state == e1000_ffe_config_active) {
+        if (hw->ffe_config_state == e1000_ffe_config_active) {
+            /* Save off the current value of register 0x2F5B to be restored at
+             * the end of the routines. */
+            ret_val = e1000_read_phy_reg(hw, 0x2F5B, &phy_saved_data);
+
+            if (ret_val)
+                return ret_val;
+
+            /* Disable the PHY transmitter */
+            ret_val = e1000_write_phy_reg(hw, 0x2F5B, 0x0003);
+
+            if (ret_val)
+                return ret_val;
+
+            msec_delay_irq(20);
+
             ret_val = e1000_write_phy_reg(hw, 0x0000,
                                           IGP01E1000_IEEE_FORCE_GIGA);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
             ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_DSP_FFE,
                                           IGP01E1000_PHY_DSP_FFE_DEFAULT);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             ret_val = e1000_write_phy_reg(hw, 0x0000,
                                           IGP01E1000_IEEE_RESTART_AUTONEG);
-            if(ret_val)
+            if (ret_val)
+                return ret_val;
+
+            msec_delay_irq(20);
+
+            /* Now enable the transmitter */
+            ret_val = e1000_write_phy_reg(hw, 0x2F5B, phy_saved_data);
+
+            if (ret_val)
                 return ret_val;
+
             hw->ffe_config_state = e1000_ffe_config_enabled;
         }
     }
@@ -5030,20 +7123,20 @@
 
     DEBUGFUNC("e1000_set_phy_mode");
 
-    if((hw->mac_type == e1000_82545_rev_3) &&
-       (hw->media_type == e1000_media_type_copper)) {
+    if ((hw->mac_type == e1000_82545_rev_3) &&
+        (hw->media_type == e1000_media_type_copper)) {
         ret_val = e1000_read_eeprom(hw, EEPROM_PHY_CLASS_WORD, 1, &eeprom_data);
-        if(ret_val) {
+        if (ret_val) {
             return ret_val;
         }
 
-        if((eeprom_data != EEPROM_RESERVED_WORD) &&
-           (eeprom_data & EEPROM_PHY_CLASS_A)) {
+        if ((eeprom_data != EEPROM_RESERVED_WORD) &&
+            (eeprom_data & EEPROM_PHY_CLASS_A)) {
             ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x000B);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
             ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0x8104);
-            if(ret_val)
+            if (ret_val)
                 return ret_val;
 
             hw->phy_reset_disable = FALSE;
@@ -5071,57 +7164,212 @@
 e1000_set_d3_lplu_state(struct e1000_hw *hw,
                         boolean_t active)
 {
+    uint32_t phy_ctrl = 0;
     int32_t ret_val;
     uint16_t phy_data;
     DEBUGFUNC("e1000_set_d3_lplu_state");
 
-    if(!((hw->mac_type == e1000_82541_rev_2) ||
-         (hw->mac_type == e1000_82547_rev_2)))
+    if (hw->phy_type != e1000_phy_igp && hw->phy_type != e1000_phy_igp_2
+        && hw->phy_type != e1000_phy_igp_3)
         return E1000_SUCCESS;
 
     /* During driver activity LPLU should not be used or it will attain link
      * from the lowest speeds starting from 10Mbps. The capability is used for
      * Dx transitions and states */
-    ret_val = e1000_read_phy_reg(hw, IGP01E1000_GMII_FIFO, &phy_data);
-    if(ret_val)
-        return ret_val;
-
-    if(!active) {
-        phy_data &= ~IGP01E1000_GMII_FLEX_SPD;
-        ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, phy_data);
-        if(ret_val)
+    if (hw->mac_type == e1000_82541_rev_2 || hw->mac_type == e1000_82547_rev_2) {
+        ret_val = e1000_read_phy_reg(hw, IGP01E1000_GMII_FIFO, &phy_data);
+        if (ret_val)
+            return ret_val;
+    } else if (hw->mac_type == e1000_ich8lan) {
+        /* MAC writes into PHY register based on the state transition
+         * and start auto-negotiation. SW driver can overwrite the settings
+         * in CSR PHY power control E1000_PHY_CTRL register. */
+        phy_ctrl = E1000_READ_REG(hw, PHY_CTRL);
+    } else {
+        ret_val = e1000_read_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, &phy_data);
+        if (ret_val)
             return ret_val;
+    }
+
+    if (!active) {
+        if (hw->mac_type == e1000_82541_rev_2 ||
+            hw->mac_type == e1000_82547_rev_2) {
+            phy_data &= ~IGP01E1000_GMII_FLEX_SPD;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, phy_data);
+            if (ret_val)
+                return ret_val;
+        } else {
+            if (hw->mac_type == e1000_ich8lan) {
+                phy_ctrl &= ~E1000_PHY_CTRL_NOND0A_LPLU;
+                E1000_WRITE_REG(hw, PHY_CTRL, phy_ctrl);
+            } else {
+                phy_data &= ~IGP02E1000_PM_D3_LPLU;
+                ret_val = e1000_write_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT,
+                                              phy_data);
+                if (ret_val)
+                    return ret_val;
+            }
+        }
 
         /* LPLU and SmartSpeed are mutually exclusive.  LPLU is used during
          * Dx states where the power conservation is most important.  During
          * driver activity we should enable SmartSpeed, so performance is
          * maintained. */
+        if (hw->smart_speed == e1000_smart_speed_on) {
+            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                         &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            phy_data |= IGP01E1000_PSCFR_SMART_SPEED;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                          phy_data);
+            if (ret_val)
+                return ret_val;
+        } else if (hw->smart_speed == e1000_smart_speed_off) {
+            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                         &phy_data);
+	    if (ret_val)
+                return ret_val;
+
+            phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                          phy_data);
+            if (ret_val)
+                return ret_val;
+        }
+
+    } else if ((hw->autoneg_advertised == AUTONEG_ADVERTISE_SPEED_DEFAULT) ||
+               (hw->autoneg_advertised == AUTONEG_ADVERTISE_10_ALL ) ||
+               (hw->autoneg_advertised == AUTONEG_ADVERTISE_10_100_ALL)) {
+
+        if (hw->mac_type == e1000_82541_rev_2 ||
+            hw->mac_type == e1000_82547_rev_2) {
+            phy_data |= IGP01E1000_GMII_FLEX_SPD;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, phy_data);
+            if (ret_val)
+                return ret_val;
+        } else {
+            if (hw->mac_type == e1000_ich8lan) {
+                phy_ctrl |= E1000_PHY_CTRL_NOND0A_LPLU;
+                E1000_WRITE_REG(hw, PHY_CTRL, phy_ctrl);
+            } else {
+                phy_data |= IGP02E1000_PM_D3_LPLU;
+                ret_val = e1000_write_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT,
+                                              phy_data);
+                if (ret_val)
+                    return ret_val;
+            }
+        }
+
+        /* When LPLU is enabled we should disable SmartSpeed */
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-        phy_data |= IGP01E1000_PSCFR_SMART_SPEED;
+        phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
         ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
-    } else if((hw->autoneg_advertised == AUTONEG_ADVERTISE_SPEED_DEFAULT) ||
-              (hw->autoneg_advertised == AUTONEG_ADVERTISE_10_ALL ) ||
-              (hw->autoneg_advertised == AUTONEG_ADVERTISE_10_100_ALL)) {
+    }
+    return E1000_SUCCESS;
+}
+
+/*****************************************************************************
+ *
+ * This function sets the lplu d0 state according to the active flag.  When
+ * activating lplu this function also disables smart speed and vise versa.
+ * lplu will not be activated unless the device autonegotiation advertisment
+ * meets standards of either 10 or 10/100 or 10/100/1000 at all duplexes.
+ * hw: Struct containing variables accessed by shared code
+ * active - true to enable lplu false to disable lplu.
+ *
+ * returns: - E1000_ERR_PHY if fail to read/write the PHY
+ *            E1000_SUCCESS at any other case.
+ *
+ ****************************************************************************/
+
+int32_t
+e1000_set_d0_lplu_state(struct e1000_hw *hw,
+                        boolean_t active)
+{
+    uint32_t phy_ctrl = 0;
+    int32_t ret_val;
+    uint16_t phy_data;
+    DEBUGFUNC("e1000_set_d0_lplu_state");
+
+    if (hw->mac_type <= e1000_82547_rev_2)
+        return E1000_SUCCESS;
 
-        phy_data |= IGP01E1000_GMII_FLEX_SPD;
-        ret_val = e1000_write_phy_reg(hw, IGP01E1000_GMII_FIFO, phy_data);
-        if(ret_val)
+    if (hw->mac_type == e1000_ich8lan) {
+        phy_ctrl = E1000_READ_REG(hw, PHY_CTRL);
+    } else {
+        ret_val = e1000_read_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, &phy_data);
+        if (ret_val)
             return ret_val;
+    }
+
+    if (!active) {
+        if (hw->mac_type == e1000_ich8lan) {
+            phy_ctrl &= ~E1000_PHY_CTRL_D0A_LPLU;
+            E1000_WRITE_REG(hw, PHY_CTRL, phy_ctrl);
+        } else {
+            phy_data &= ~IGP02E1000_PM_D0_LPLU;
+            ret_val = e1000_write_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, phy_data);
+            if (ret_val)
+                return ret_val;
+        }
+
+        /* LPLU and SmartSpeed are mutually exclusive.  LPLU is used during
+         * Dx states where the power conservation is most important.  During
+         * driver activity we should enable SmartSpeed, so performance is
+         * maintained. */
+        if (hw->smart_speed == e1000_smart_speed_on) {
+            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                         &phy_data);
+            if (ret_val)
+                return ret_val;
+
+            phy_data |= IGP01E1000_PSCFR_SMART_SPEED;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                          phy_data);
+            if (ret_val)
+                return ret_val;
+        } else if (hw->smart_speed == e1000_smart_speed_off) {
+            ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                         &phy_data);
+	    if (ret_val)
+                return ret_val;
+
+            phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+            ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG,
+                                          phy_data);
+            if (ret_val)
+                return ret_val;
+        }
+
+
+    } else {
+
+        if (hw->mac_type == e1000_ich8lan) {
+            phy_ctrl |= E1000_PHY_CTRL_D0A_LPLU;
+            E1000_WRITE_REG(hw, PHY_CTRL, phy_ctrl);
+        } else {
+            phy_data |= IGP02E1000_PM_D0_LPLU;
+            ret_val = e1000_write_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, phy_data);
+            if (ret_val)
+                return ret_val;
+        }
 
         /* When LPLU is enabled we should disable SmartSpeed */
         ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG, &phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
         phy_data &= ~IGP01E1000_PSCFR_SMART_SPEED;
         ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CONFIG, phy_data);
-        if(ret_val)
+        if (ret_val)
             return ret_val;
 
     }
@@ -5142,7 +7390,7 @@
 
     DEBUGFUNC("e1000_set_vco_speed");
 
-    switch(hw->mac_type) {
+    switch (hw->mac_type) {
     case e1000_82545_rev_3:
     case e1000_82546_rev_3:
        break;
@@ -5153,66 +7401,1693 @@
     /* Set PHY register 30, page 5, bit 8 to 0 */
 
     ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, &default_page);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0005);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_data &= ~M88E1000_PHY_VCO_REG_BIT8;
     ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     /* Set PHY register 30, page 4, bit 11 to 1 */
 
     ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0004);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, &phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     phy_data |= M88E1000_PHY_VCO_REG_BIT11;
     ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, phy_data);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, default_page);
-    if(ret_val)
+    if (ret_val)
         return ret_val;
 
     return E1000_SUCCESS;
 }
 
-/******************************************************************************
- * Verifies the hardware needs to allow ARPs to be processed by the host
+
+/*****************************************************************************
+ * This function reads the cookie from ARC ram.
  *
- * hw - Struct containing variables accessed by shared code
+ * returns: - E1000_SUCCESS .
+ ****************************************************************************/
+int32_t
+e1000_host_if_read_cookie(struct e1000_hw * hw, uint8_t *buffer)
+{
+    uint8_t i;
+    uint32_t offset = E1000_MNG_DHCP_COOKIE_OFFSET;
+    uint8_t length = E1000_MNG_DHCP_COOKIE_LENGTH;
+
+    length = (length >> 2);
+    offset = (offset >> 2);
+
+    for (i = 0; i < length; i++) {
+        *((uint32_t *) buffer + i) =
+            E1000_READ_REG_ARRAY_DWORD(hw, HOST_IF, offset + i);
+    }
+    return E1000_SUCCESS;
+}
+
+
+/*****************************************************************************
+ * This function checks whether the HOST IF is enabled for command operaton
+ * and also checks whether the previous command is completed.
+ * It busy waits in case of previous command is not completed.
+ *
+ * returns: - E1000_ERR_HOST_INTERFACE_COMMAND in case if is not ready or
+ *            timeout
+ *          - E1000_SUCCESS for success.
+ ****************************************************************************/
+int32_t
+e1000_mng_enable_host_if(struct e1000_hw * hw)
+{
+    uint32_t hicr;
+    uint8_t i;
+
+    /* Check that the host interface is enabled. */
+    hicr = E1000_READ_REG(hw, HICR);
+    if ((hicr & E1000_HICR_EN) == 0) {
+        DEBUGOUT("E1000_HOST_EN bit disabled.\n");
+        return -E1000_ERR_HOST_INTERFACE_COMMAND;
+    }
+    /* check the previous command is completed */
+    for (i = 0; i < E1000_MNG_DHCP_COMMAND_TIMEOUT; i++) {
+        hicr = E1000_READ_REG(hw, HICR);
+        if (!(hicr & E1000_HICR_C))
+            break;
+        msec_delay_irq(1);
+    }
+
+    if (i == E1000_MNG_DHCP_COMMAND_TIMEOUT) {
+        DEBUGOUT("Previous command timeout failed .\n");
+        return -E1000_ERR_HOST_INTERFACE_COMMAND;
+    }
+    return E1000_SUCCESS;
+}
+
+/*****************************************************************************
+ * This function writes the buffer content at the offset given on the host if.
+ * It also does alignment considerations to do the writes in most efficient way.
+ * Also fills up the sum of the buffer in *buffer parameter.
  *
- * returns: - TRUE/FALSE
+ * returns  - E1000_SUCCESS for success.
+ ****************************************************************************/
+int32_t
+e1000_mng_host_if_write(struct e1000_hw * hw, uint8_t *buffer,
+                        uint16_t length, uint16_t offset, uint8_t *sum)
+{
+    uint8_t *tmp;
+    uint8_t *bufptr = buffer;
+    uint32_t data = 0;
+    uint16_t remaining, i, j, prev_bytes;
+
+    /* sum = only sum of the data and it is not checksum */
+
+    if (length == 0 || offset + length > E1000_HI_MAX_MNG_DATA_LENGTH) {
+        return -E1000_ERR_PARAM;
+    }
+
+    tmp = (uint8_t *)&data;
+    prev_bytes = offset & 0x3;
+    offset &= 0xFFFC;
+    offset >>= 2;
+
+    if (prev_bytes) {
+        data = E1000_READ_REG_ARRAY_DWORD(hw, HOST_IF, offset);
+        for (j = prev_bytes; j < sizeof(uint32_t); j++) {
+            *(tmp + j) = *bufptr++;
+            *sum += *(tmp + j);
+        }
+        E1000_WRITE_REG_ARRAY_DWORD(hw, HOST_IF, offset, data);
+        length -= j - prev_bytes;
+        offset++;
+    }
+
+    remaining = length & 0x3;
+    length -= remaining;
+
+    /* Calculate length in DWORDs */
+    length >>= 2;
+
+    /* The device driver writes the relevant command block into the
+     * ram area. */
+    for (i = 0; i < length; i++) {
+        for (j = 0; j < sizeof(uint32_t); j++) {
+            *(tmp + j) = *bufptr++;
+            *sum += *(tmp + j);
+        }
+
+        E1000_WRITE_REG_ARRAY_DWORD(hw, HOST_IF, offset + i, data);
+    }
+    if (remaining) {
+        for (j = 0; j < sizeof(uint32_t); j++) {
+            if (j < remaining)
+                *(tmp + j) = *bufptr++;
+            else
+                *(tmp + j) = 0;
+
+            *sum += *(tmp + j);
+        }
+        E1000_WRITE_REG_ARRAY_DWORD(hw, HOST_IF, offset + i, data);
+    }
+
+    return E1000_SUCCESS;
+}
+
+
+/*****************************************************************************
+ * This function writes the command header after does the checksum calculation.
  *
- *****************************************************************************/
-uint32_t
-e1000_enable_mng_pass_thru(struct e1000_hw *hw)
+ * returns  - E1000_SUCCESS for success.
+ ****************************************************************************/
+int32_t
+e1000_mng_write_cmd_header(struct e1000_hw * hw,
+                           struct e1000_host_mng_command_header * hdr)
 {
-    uint32_t manc;
+    uint16_t i;
+    uint8_t sum;
+    uint8_t *buffer;
 
-    if (hw->asf_firmware_present) {
-        manc = E1000_READ_REG(hw, MANC);
+    /* Write the whole command header structure which includes sum of
+     * the buffer */
 
-        if (!(manc & E1000_MANC_RCV_TCO_EN) ||
-            !(manc & E1000_MANC_EN_MAC_ADDR_FILTER))
-            return FALSE;
-        if ((manc & E1000_MANC_SMBUS_EN) && !(manc & E1000_MANC_ASF_EN))
-            return TRUE;
+    uint16_t length = sizeof(struct e1000_host_mng_command_header);
+
+    sum = hdr->checksum;
+    hdr->checksum = 0;
+
+    buffer = (uint8_t *) hdr;
+    i = length;
+    while (i--)
+        sum += buffer[i];
+
+    hdr->checksum = 0 - sum;
+
+    length >>= 2;
+    /* The device driver writes the relevant command block into the ram area. */
+    for (i = 0; i < length; i++) {
+        E1000_WRITE_REG_ARRAY_DWORD(hw, HOST_IF, i, *((uint32_t *) hdr + i));
+        E1000_WRITE_FLUSH(hw);
     }
-    return FALSE;
+
+    return E1000_SUCCESS;
 }
 
+
+/*****************************************************************************
+ * This function indicates to ARC that a new command is pending which completes
+ * one write operation by the driver.
+ *
+ * returns  - E1000_SUCCESS for success.
+ ****************************************************************************/
+int32_t
+e1000_mng_write_commit(struct e1000_hw * hw)
+{
+    uint32_t hicr;
+
+    hicr = E1000_READ_REG(hw, HICR);
+    /* Setting this bit tells the ARC that a new command is pending. */
+    E1000_WRITE_REG(hw, HICR, hicr | E1000_HICR_C);
+
+    return E1000_SUCCESS;
+}
+
+
+/*****************************************************************************
+ * This function checks the mode of the firmware.
+ *
+ * returns  - TRUE when the mode is IAMT or FALSE.
+ ****************************************************************************/
+boolean_t
+e1000_check_mng_mode(struct e1000_hw *hw)
+{
+    uint32_t fwsm;
+
+    fwsm = E1000_READ_REG(hw, FWSM);
+
+    if (hw->mac_type == e1000_ich8lan) {
+        if ((fwsm & E1000_FWSM_MODE_MASK) ==
+            (E1000_MNG_ICH_IAMT_MODE << E1000_FWSM_MODE_SHIFT))
+            return TRUE;
+    } else if ((fwsm & E1000_FWSM_MODE_MASK) ==
+               (E1000_MNG_IAMT_MODE << E1000_FWSM_MODE_SHIFT))
+        return TRUE;
+
+    return FALSE;
+}
+
+
+/*****************************************************************************
+ * This function writes the dhcp info .
+ ****************************************************************************/
+int32_t
+e1000_mng_write_dhcp_info(struct e1000_hw * hw, uint8_t *buffer,
+			  uint16_t length)
+{
+    int32_t ret_val;
+    struct e1000_host_mng_command_header hdr;
+
+    hdr.command_id = E1000_MNG_DHCP_TX_PAYLOAD_CMD;
+    hdr.command_length = length;
+    hdr.reserved1 = 0;
+    hdr.reserved2 = 0;
+    hdr.checksum = 0;
+
+    ret_val = e1000_mng_enable_host_if(hw);
+    if (ret_val == E1000_SUCCESS) {
+        ret_val = e1000_mng_host_if_write(hw, buffer, length, sizeof(hdr),
+                                          &(hdr.checksum));
+        if (ret_val == E1000_SUCCESS) {
+            ret_val = e1000_mng_write_cmd_header(hw, &hdr);
+            if (ret_val == E1000_SUCCESS)
+                ret_val = e1000_mng_write_commit(hw);
+        }
+    }
+    return ret_val;
+}
+
+
+/*****************************************************************************
+ * This function calculates the checksum.
+ *
+ * returns  - checksum of buffer contents.
+ ****************************************************************************/
+uint8_t
+e1000_calculate_mng_checksum(char *buffer, uint32_t length)
+{
+    uint8_t sum = 0;
+    uint32_t i;
+
+    if (!buffer)
+        return 0;
+
+    for (i=0; i < length; i++)
+        sum += buffer[i];
+
+    return (uint8_t) (0 - sum);
+}
+
+/*****************************************************************************
+ * This function checks whether tx pkt filtering needs to be enabled or not.
+ *
+ * returns  - TRUE for packet filtering or FALSE.
+ ****************************************************************************/
+boolean_t
+e1000_enable_tx_pkt_filtering(struct e1000_hw *hw)
+{
+    /* called in init as well as watchdog timer functions */
+
+    int32_t ret_val, checksum;
+    boolean_t tx_filter = FALSE;
+    struct e1000_host_mng_dhcp_cookie *hdr = &(hw->mng_cookie);
+    uint8_t *buffer = (uint8_t *) &(hw->mng_cookie);
+
+    if (e1000_check_mng_mode(hw)) {
+        ret_val = e1000_mng_enable_host_if(hw);
+        if (ret_val == E1000_SUCCESS) {
+            ret_val = e1000_host_if_read_cookie(hw, buffer);
+            if (ret_val == E1000_SUCCESS) {
+                checksum = hdr->checksum;
+                hdr->checksum = 0;
+                if ((hdr->signature == E1000_IAMT_SIGNATURE) &&
+                    checksum == e1000_calculate_mng_checksum((char *)buffer,
+                                               E1000_MNG_DHCP_COOKIE_LENGTH)) {
+                    if (hdr->status &
+                        E1000_MNG_DHCP_COOKIE_STATUS_PARSING_SUPPORT)
+                        tx_filter = TRUE;
+                } else
+                    tx_filter = TRUE;
+            } else
+                tx_filter = TRUE;
+        }
+    }
+
+    hw->tx_pkt_filtering = tx_filter;
+    return tx_filter;
+}
+
+/******************************************************************************
+ * Verifies the hardware needs to allow ARPs to be processed by the host
+ *
+ * hw - Struct containing variables accessed by shared code
+ *
+ * returns: - TRUE/FALSE
+ *
+ *****************************************************************************/
+uint32_t
+e1000_enable_mng_pass_thru(struct e1000_hw *hw)
+{
+    uint32_t manc;
+    uint32_t fwsm, factps;
+
+    if (hw->asf_firmware_present) {
+        manc = E1000_READ_REG(hw, MANC);
+
+        if (!(manc & E1000_MANC_RCV_TCO_EN) ||
+            !(manc & E1000_MANC_EN_MAC_ADDR_FILTER))
+            return FALSE;
+        if (e1000_arc_subsystem_valid(hw) == TRUE) {
+            fwsm = E1000_READ_REG(hw, FWSM);
+            factps = E1000_READ_REG(hw, FACTPS);
+
+            if (((fwsm & E1000_FWSM_MODE_MASK) ==
+                (e1000_mng_mode_pt << E1000_FWSM_MODE_SHIFT)) &&
+                (factps & E1000_FACTPS_MNGCG))
+                return TRUE;
+        } else
+            if ((manc & E1000_MANC_SMBUS_EN) && !(manc & E1000_MANC_ASF_EN))
+                return TRUE;
+    }
+    return FALSE;
+}
+
+static int32_t
+e1000_polarity_reversal_workaround(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t mii_status_reg;
+    uint16_t i;
+
+    /* Polarity reversal workaround for forced 10F/10H links. */
+
+    /* Disable the transmitter on the PHY */
+
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0019);
+    if (ret_val)
+        return ret_val;
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFFFF);
+    if (ret_val)
+        return ret_val;
+
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0000);
+    if (ret_val)
+        return ret_val;
+
+    /* This loop will early-out if the NO link condition has been met. */
+    for (i = PHY_FORCE_TIME; i > 0; i--) {
+        /* Read the MII Status Register and wait for Link Status bit
+         * to be clear.
+         */
+
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+        if (ret_val)
+            return ret_val;
+
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+        if (ret_val)
+            return ret_val;
+
+        if ((mii_status_reg & ~MII_SR_LINK_STATUS) == 0) break;
+        msec_delay_irq(100);
+    }
+
+    /* Recommended delay time after link has been lost */
+    msec_delay_irq(1000);
+
+    /* Now we will re-enable th transmitter on the PHY */
+
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0019);
+    if (ret_val)
+        return ret_val;
+    msec_delay_irq(50);
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFFF0);
+    if (ret_val)
+        return ret_val;
+    msec_delay_irq(50);
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xFF00);
+    if (ret_val)
+        return ret_val;
+    msec_delay_irq(50);
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0x0000);
+    if (ret_val)
+        return ret_val;
+
+    ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_PAGE_SELECT, 0x0000);
+    if (ret_val)
+        return ret_val;
+
+    /* This loop will early-out if the link condition has been met. */
+    for (i = PHY_FORCE_TIME; i > 0; i--) {
+        /* Read the MII Status Register and wait for Link Status bit
+         * to be set.
+         */
+
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+        if (ret_val)
+            return ret_val;
+
+        ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+        if (ret_val)
+            return ret_val;
+
+        if (mii_status_reg & MII_SR_LINK_STATUS) break;
+        msec_delay_irq(100);
+    }
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ *
+ * Disables PCI-Express master access.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - none.
+ *
+ ***************************************************************************/
+void
+e1000_set_pci_express_master_disable(struct e1000_hw *hw)
+{
+    uint32_t ctrl;
+
+    DEBUGFUNC("e1000_set_pci_express_master_disable");
+
+    if (hw->bus_type != e1000_bus_type_pci_express)
+        return;
+
+    ctrl = E1000_READ_REG(hw, CTRL);
+    ctrl |= E1000_CTRL_GIO_MASTER_DISABLE;
+    E1000_WRITE_REG(hw, CTRL, ctrl);
+}
+
+/***************************************************************************
+ *
+ * Enables PCI-Express master access.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - none.
+ *
+ ***************************************************************************/
+void
+e1000_enable_pciex_master(struct e1000_hw *hw)
+{
+    uint32_t ctrl;
+
+    DEBUGFUNC("e1000_enable_pciex_master");
+
+    if (hw->bus_type != e1000_bus_type_pci_express)
+        return;
+
+    ctrl = E1000_READ_REG(hw, CTRL);
+    ctrl &= ~E1000_CTRL_GIO_MASTER_DISABLE;
+    E1000_WRITE_REG(hw, CTRL, ctrl);
+}
+
+/*******************************************************************************
+ *
+ * Disables PCI-Express master access and verifies there are no pending requests
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_ERR_MASTER_REQUESTS_PENDING if master disable bit hasn't
+ *            caused the master requests to be disabled.
+ *            E1000_SUCCESS master requests disabled.
+ *
+ ******************************************************************************/
+int32_t
+e1000_disable_pciex_master(struct e1000_hw *hw)
+{
+    int32_t timeout = MASTER_DISABLE_TIMEOUT;   /* 80ms */
+
+    DEBUGFUNC("e1000_disable_pciex_master");
+
+    if (hw->bus_type != e1000_bus_type_pci_express)
+        return E1000_SUCCESS;
+
+    e1000_set_pci_express_master_disable(hw);
+
+    while (timeout) {
+        if (!(E1000_READ_REG(hw, STATUS) & E1000_STATUS_GIO_MASTER_ENABLE))
+            break;
+        else
+            usec_delay(100);
+        timeout--;
+    }
+
+    if (!timeout) {
+        DEBUGOUT("Master requests are pending.\n");
+        return -E1000_ERR_MASTER_REQUESTS_PENDING;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/*******************************************************************************
+ *
+ * Check for EEPROM Auto Read bit done.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_ERR_RESET if fail to reset MAC
+ *            E1000_SUCCESS at any other case.
+ *
+ ******************************************************************************/
+int32_t
+e1000_get_auto_rd_done(struct e1000_hw *hw)
+{
+    int32_t timeout = AUTO_READ_DONE_TIMEOUT;
+
+    DEBUGFUNC("e1000_get_auto_rd_done");
+
+    switch (hw->mac_type) {
+    default:
+        msec_delay(5);
+        break;
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_82573:
+    case e1000_80003es2lan:
+    case e1000_ich8lan:
+        while (timeout) {
+            if (E1000_READ_REG(hw, EECD) & E1000_EECD_AUTO_RD)
+                break;
+            else msec_delay(1);
+            timeout--;
+        }
+
+        if (!timeout) {
+            DEBUGOUT("Auto read by HW from EEPROM has not completed.\n");
+            return -E1000_ERR_RESET;
+        }
+        break;
+    }
+
+    /* PHY configuration from NVM just starts after EECD_AUTO_RD sets to high.
+     * Need to wait for PHY configuration completion before accessing NVM
+     * and PHY. */
+    if (hw->mac_type == e1000_82573)
+        msec_delay(25);
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ * Checks if the PHY configuration is done
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_ERR_RESET if fail to reset MAC
+ *            E1000_SUCCESS at any other case.
+ *
+ ***************************************************************************/
+int32_t
+e1000_get_phy_cfg_done(struct e1000_hw *hw)
+{
+    int32_t timeout = PHY_CFG_TIMEOUT;
+    uint32_t cfg_mask = E1000_EEPROM_CFG_DONE;
+
+    DEBUGFUNC("e1000_get_phy_cfg_done");
+
+    switch (hw->mac_type) {
+    default:
+        msec_delay_irq(10);
+        break;
+    case e1000_80003es2lan:
+        /* Separate *_CFG_DONE_* bit for each port */
+        if (E1000_READ_REG(hw, STATUS) & E1000_STATUS_FUNC_1)
+            cfg_mask = E1000_EEPROM_CFG_DONE_PORT_1;
+        /* Fall Through */
+    case e1000_82571:
+    case e1000_82572:
+        while (timeout) {
+            if (E1000_READ_REG(hw, EEMNGCTL) & cfg_mask)
+                break;
+            else
+                msec_delay(1);
+            timeout--;
+        }
+
+        if (!timeout) {
+            DEBUGOUT("MNG configuration cycle has not completed.\n");
+            return -E1000_ERR_RESET;
+        }
+        break;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ *
+ * Using the combination of SMBI and SWESMBI semaphore bits when resetting
+ * adapter or Eeprom access.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_ERR_EEPROM if fail to access EEPROM.
+ *            E1000_SUCCESS at any other case.
+ *
+ ***************************************************************************/
+int32_t
+e1000_get_hw_eeprom_semaphore(struct e1000_hw *hw)
+{
+    int32_t timeout;
+    uint32_t swsm;
+
+    DEBUGFUNC("e1000_get_hw_eeprom_semaphore");
+
+    if (!hw->eeprom_semaphore_present)
+        return E1000_SUCCESS;
+
+    if (hw->mac_type == e1000_80003es2lan) {
+        /* Get the SW semaphore. */
+        if (e1000_get_software_semaphore(hw) != E1000_SUCCESS)
+            return -E1000_ERR_EEPROM;
+    }
+
+    /* Get the FW semaphore. */
+    timeout = hw->eeprom.word_size + 1;
+    while (timeout) {
+        swsm = E1000_READ_REG(hw, SWSM);
+        swsm |= E1000_SWSM_SWESMBI;
+        E1000_WRITE_REG(hw, SWSM, swsm);
+        /* if we managed to set the bit we got the semaphore. */
+        swsm = E1000_READ_REG(hw, SWSM);
+        if (swsm & E1000_SWSM_SWESMBI)
+            break;
+
+        usec_delay(50);
+        timeout--;
+    }
+
+    if (!timeout) {
+        /* Release semaphores */
+        e1000_put_hw_eeprom_semaphore(hw);
+        DEBUGOUT("Driver can't access the Eeprom - SWESMBI bit is set.\n");
+        return -E1000_ERR_EEPROM;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ * This function clears HW semaphore bits.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - None.
+ *
+ ***************************************************************************/
+void
+e1000_put_hw_eeprom_semaphore(struct e1000_hw *hw)
+{
+    uint32_t swsm;
+
+    DEBUGFUNC("e1000_put_hw_eeprom_semaphore");
+
+    if (!hw->eeprom_semaphore_present)
+        return;
+
+    swsm = E1000_READ_REG(hw, SWSM);
+    if (hw->mac_type == e1000_80003es2lan) {
+        /* Release both semaphores. */
+        swsm &= ~(E1000_SWSM_SMBI | E1000_SWSM_SWESMBI);
+    } else
+        swsm &= ~(E1000_SWSM_SWESMBI);
+    E1000_WRITE_REG(hw, SWSM, swsm);
+}
+
+/***************************************************************************
+ *
+ * Obtaining software semaphore bit (SMBI) before resetting PHY.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_ERR_RESET if fail to obtain semaphore.
+ *            E1000_SUCCESS at any other case.
+ *
+ ***************************************************************************/
+int32_t
+e1000_get_software_semaphore(struct e1000_hw *hw)
+{
+    int32_t timeout = hw->eeprom.word_size + 1;
+    uint32_t swsm;
+
+    DEBUGFUNC("e1000_get_software_semaphore");
+
+    if (hw->mac_type != e1000_80003es2lan)
+        return E1000_SUCCESS;
+
+    while (timeout) {
+        swsm = E1000_READ_REG(hw, SWSM);
+        /* If SMBI bit cleared, it is now set and we hold the semaphore */
+        if (!(swsm & E1000_SWSM_SMBI))
+            break;
+        msec_delay_irq(1);
+        timeout--;
+    }
+
+    if (!timeout) {
+        DEBUGOUT("Driver can't access device - SMBI bit is set.\n");
+        return -E1000_ERR_RESET;
+    }
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ *
+ * Release semaphore bit (SMBI).
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ ***************************************************************************/
+void
+e1000_release_software_semaphore(struct e1000_hw *hw)
+{
+    uint32_t swsm;
+
+    DEBUGFUNC("e1000_release_software_semaphore");
+
+    if (hw->mac_type != e1000_80003es2lan)
+        return;
+
+    swsm = E1000_READ_REG(hw, SWSM);
+    /* Release the SW semaphores.*/
+    swsm &= ~E1000_SWSM_SMBI;
+    E1000_WRITE_REG(hw, SWSM, swsm);
+}
+
+/******************************************************************************
+ * Checks if PHY reset is blocked due to SOL/IDER session, for example.
+ * Returning E1000_BLK_PHY_RESET isn't necessarily an error.  But it's up to
+ * the caller to figure out how to deal with it.
+ *
+ * hw - Struct containing variables accessed by shared code
+ *
+ * returns: - E1000_BLK_PHY_RESET
+ *            E1000_SUCCESS
+ *
+ *****************************************************************************/
+int32_t
+e1000_check_phy_reset_block(struct e1000_hw *hw)
+{
+    uint32_t manc = 0;
+    uint32_t fwsm = 0;
+
+    if (hw->mac_type == e1000_ich8lan) {
+        fwsm = E1000_READ_REG(hw, FWSM);
+        return (fwsm & E1000_FWSM_RSPCIPHY) ? E1000_SUCCESS
+                                            : E1000_BLK_PHY_RESET;
+    }
+
+    if (hw->mac_type > e1000_82547_rev_2)
+        manc = E1000_READ_REG(hw, MANC);
+    return (manc & E1000_MANC_BLK_PHY_RST_ON_IDE) ?
+	    E1000_BLK_PHY_RESET : E1000_SUCCESS;
+}
+
+uint8_t
+e1000_arc_subsystem_valid(struct e1000_hw *hw)
+{
+    uint32_t fwsm;
+
+    /* On 8257x silicon, registers in the range of 0x8800 - 0x8FFC
+     * may not be provided a DMA clock when no manageability features are
+     * enabled.  We do not want to perform any reads/writes to these registers
+     * if this is the case.  We read FWSM to determine the manageability mode.
+     */
+    switch (hw->mac_type) {
+    case e1000_82571:
+    case e1000_82572:
+    case e1000_82573:
+    case e1000_80003es2lan:
+        fwsm = E1000_READ_REG(hw, FWSM);
+        if ((fwsm & E1000_FWSM_MODE_MASK) != 0)
+            return TRUE;
+        break;
+    case e1000_ich8lan:
+        return TRUE;
+    default:
+        break;
+    }
+    return FALSE;
+}
+
+
+/******************************************************************************
+ * Configure PCI-Ex no-snoop
+ *
+ * hw - Struct containing variables accessed by shared code.
+ * no_snoop - Bitmap of no-snoop events.
+ *
+ * returns: E1000_SUCCESS
+ *
+ *****************************************************************************/
+int32_t
+e1000_set_pci_ex_no_snoop(struct e1000_hw *hw, uint32_t no_snoop)
+{
+    uint32_t gcr_reg = 0;
+
+    DEBUGFUNC("e1000_set_pci_ex_no_snoop");
+
+    if (hw->bus_type == e1000_bus_type_unknown)
+        e1000_get_bus_info(hw);
+
+    if (hw->bus_type != e1000_bus_type_pci_express)
+        return E1000_SUCCESS;
+
+    if (no_snoop) {
+        gcr_reg = E1000_READ_REG(hw, GCR);
+        gcr_reg &= ~(PCI_EX_NO_SNOOP_ALL);
+        gcr_reg |= no_snoop;
+        E1000_WRITE_REG(hw, GCR, gcr_reg);
+    }
+    if (hw->mac_type == e1000_ich8lan) {
+        uint32_t ctrl_ext;
+
+        E1000_WRITE_REG(hw, GCR, PCI_EX_82566_SNOOP_ALL);
+
+        ctrl_ext = E1000_READ_REG(hw, CTRL_EXT);
+        ctrl_ext |= E1000_CTRL_EXT_RO_DIS;
+        E1000_WRITE_REG(hw, CTRL_EXT, ctrl_ext);
+    }
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ *
+ * Get software semaphore FLAG bit (SWFLAG).
+ * SWFLAG is used to synchronize the access to all shared resource between
+ * SW, FW and HW.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ ***************************************************************************/
+int32_t
+e1000_get_software_flag(struct e1000_hw *hw)
+{
+    int32_t timeout = PHY_CFG_TIMEOUT;
+    uint32_t extcnf_ctrl;
+
+    DEBUGFUNC("e1000_get_software_flag");
+
+    if (hw->mac_type == e1000_ich8lan) {
+        while (timeout) {
+            extcnf_ctrl = E1000_READ_REG(hw, EXTCNF_CTRL);
+            extcnf_ctrl |= E1000_EXTCNF_CTRL_SWFLAG;
+            E1000_WRITE_REG(hw, EXTCNF_CTRL, extcnf_ctrl);
+
+            extcnf_ctrl = E1000_READ_REG(hw, EXTCNF_CTRL);
+            if (extcnf_ctrl & E1000_EXTCNF_CTRL_SWFLAG)
+                break;
+            msec_delay_irq(1);
+            timeout--;
+        }
+
+        if (!timeout) {
+            DEBUGOUT("FW or HW locks the resource too long.\n");
+            return -E1000_ERR_CONFIG;
+        }
+    }
+
+    return E1000_SUCCESS;
+}
+
+/***************************************************************************
+ *
+ * Release software semaphore FLAG bit (SWFLAG).
+ * SWFLAG is used to synchronize the access to all shared resource between
+ * SW, FW and HW.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ ***************************************************************************/
+void
+e1000_release_software_flag(struct e1000_hw *hw)
+{
+    uint32_t extcnf_ctrl;
+
+    DEBUGFUNC("e1000_release_software_flag");
+
+    if (hw->mac_type == e1000_ich8lan) {
+        extcnf_ctrl= E1000_READ_REG(hw, EXTCNF_CTRL);
+        extcnf_ctrl &= ~E1000_EXTCNF_CTRL_SWFLAG;
+        E1000_WRITE_REG(hw, EXTCNF_CTRL, extcnf_ctrl);
+    }
+
+    return;
+}
+
+/***************************************************************************
+ *
+ * Disable dynamic power down mode in ife PHY.
+ * It can be used to workaround band-gap problem.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ ***************************************************************************/
+int32_t
+e1000_ife_disable_dynamic_power_down(struct e1000_hw *hw)
+{
+    uint16_t phy_data;
+    int32_t ret_val = E1000_SUCCESS;
+
+    DEBUGFUNC("e1000_ife_disable_dynamic_power_down");
+
+    if (hw->phy_type == e1000_phy_ife) {
+        ret_val = e1000_read_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data |=  IFE_PSC_DISABLE_DYNAMIC_POWER_DOWN;
+        ret_val = e1000_write_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, phy_data);
+    }
+
+    return ret_val;
+}
+
+/***************************************************************************
+ *
+ * Enable dynamic power down mode in ife PHY.
+ * It can be used to workaround band-gap problem.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ ***************************************************************************/
+int32_t
+e1000_ife_enable_dynamic_power_down(struct e1000_hw *hw)
+{
+    uint16_t phy_data;
+    int32_t ret_val = E1000_SUCCESS;
+
+    DEBUGFUNC("e1000_ife_enable_dynamic_power_down");
+
+    if (hw->phy_type == e1000_phy_ife) {
+        ret_val = e1000_read_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, &phy_data);
+        if (ret_val)
+            return ret_val;
+
+        phy_data &=  ~IFE_PSC_DISABLE_DYNAMIC_POWER_DOWN;
+        ret_val = e1000_write_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, phy_data);
+    }
+
+    return ret_val;
+}
+
+/******************************************************************************
+ * Reads a 16 bit word or words from the EEPROM using the ICH8's flash access
+ * register.
+ *
+ * hw - Struct containing variables accessed by shared code
+ * offset - offset of word in the EEPROM to read
+ * data - word read from the EEPROM
+ * words - number of words to read
+ *****************************************************************************/
+int32_t
+e1000_read_eeprom_ich8(struct e1000_hw *hw, uint16_t offset, uint16_t words,
+                       uint16_t *data)
+{
+    int32_t  error = E1000_SUCCESS;
+    uint32_t flash_bank = 0;
+    uint32_t act_offset = 0;
+    uint32_t bank_offset = 0;
+    uint16_t word = 0;
+    uint16_t i = 0;
+
+    /* We need to know which is the valid flash bank.  In the event
+     * that we didn't allocate eeprom_shadow_ram, we may not be
+     * managing flash_bank.  So it cannot be trusted and needs
+     * to be updated with each read.
+     */
+    /* Value of bit 22 corresponds to the flash bank we're on. */
+    flash_bank = (E1000_READ_REG(hw, EECD) & E1000_EECD_SEC1VAL) ? 1 : 0;
+
+    /* Adjust offset appropriately if we're on bank 1 - adjust for word size */
+    bank_offset = flash_bank * (hw->flash_bank_size * 2);
+
+    error = e1000_get_software_flag(hw);
+    if (error != E1000_SUCCESS)
+        return error;
+
+    for (i = 0; i < words; i++) {
+        if (hw->eeprom_shadow_ram != NULL &&
+            hw->eeprom_shadow_ram[offset+i].modified == TRUE) {
+            data[i] = hw->eeprom_shadow_ram[offset+i].eeprom_word;
+        } else {
+            /* The NVM part needs a byte offset, hence * 2 */
+            act_offset = bank_offset + ((offset + i) * 2);
+            error = e1000_read_ich8_word(hw, act_offset, &word);
+            if (error != E1000_SUCCESS)
+                break;
+            data[i] = word;
+        }
+    }
+
+    e1000_release_software_flag(hw);
+
+    return error;
+}
+
+/******************************************************************************
+ * Writes a 16 bit word or words to the EEPROM using the ICH8's flash access
+ * register.  Actually, writes are written to the shadow ram cache in the hw
+ * structure hw->e1000_shadow_ram.  e1000_commit_shadow_ram flushes this to
+ * the NVM, which occurs when the NVM checksum is updated.
+ *
+ * hw - Struct containing variables accessed by shared code
+ * offset - offset of word in the EEPROM to write
+ * words - number of words to write
+ * data - words to write to the EEPROM
+ *****************************************************************************/
+int32_t
+e1000_write_eeprom_ich8(struct e1000_hw *hw, uint16_t offset, uint16_t words,
+                        uint16_t *data)
+{
+    uint32_t i = 0;
+    int32_t error = E1000_SUCCESS;
+
+    error = e1000_get_software_flag(hw);
+    if (error != E1000_SUCCESS)
+        return error;
+
+    /* A driver can write to the NVM only if it has eeprom_shadow_ram
+     * allocated.  Subsequent reads to the modified words are read from
+     * this cached structure as well.  Writes will only go into this
+     * cached structure unless it's followed by a call to
+     * e1000_update_eeprom_checksum() where it will commit the changes
+     * and clear the "modified" field.
+     */
+    if (hw->eeprom_shadow_ram != NULL) {
+        for (i = 0; i < words; i++) {
+            if ((offset + i) < E1000_SHADOW_RAM_WORDS) {
+                hw->eeprom_shadow_ram[offset+i].modified = TRUE;
+                hw->eeprom_shadow_ram[offset+i].eeprom_word = data[i];
+            } else {
+                error = -E1000_ERR_EEPROM;
+                break;
+            }
+        }
+    } else {
+        /* Drivers have the option to not allocate eeprom_shadow_ram as long
+         * as they don't perform any NVM writes.  An attempt in doing so
+         * will result in this error.
+         */
+        error = -E1000_ERR_EEPROM;
+    }
+
+    e1000_release_software_flag(hw);
+
+    return error;
+}
+
+/******************************************************************************
+ * This function does initial flash setup so that a new read/write/erase cycle
+ * can be started.
+ *
+ * hw - The pointer to the hw structure
+ ****************************************************************************/
+int32_t
+e1000_ich8_cycle_init(struct e1000_hw *hw)
+{
+    union ich8_hws_flash_status hsfsts;
+    int32_t error = E1000_ERR_EEPROM;
+    int32_t i     = 0;
+
+    DEBUGFUNC("e1000_ich8_cycle_init");
+
+    hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+
+    /* May be check the Flash Des Valid bit in Hw status */
+    if (hsfsts.hsf_status.fldesvalid == 0) {
+        DEBUGOUT("Flash descriptor invalid.  SW Sequencing must be used.");
+        return error;
+    }
+
+    /* Clear FCERR in Hw status by writing 1 */
+    /* Clear DAEL in Hw status by writing a 1 */
+    hsfsts.hsf_status.flcerr = 1;
+    hsfsts.hsf_status.dael = 1;
+
+    E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFSTS, hsfsts.regval);
+
+    /* Either we should have a hardware SPI cycle in progress bit to check
+     * against, in order to start a new cycle or FDONE bit should be changed
+     * in the hardware so that it is 1 after harware reset, which can then be
+     * used as an indication whether a cycle is in progress or has been
+     * completed .. we should also have some software semaphore mechanism to
+     * guard FDONE or the cycle in progress bit so that two threads access to
+     * those bits can be sequentiallized or a way so that 2 threads dont
+     * start the cycle at the same time */
+
+    if (hsfsts.hsf_status.flcinprog == 0) {
+        /* There is no cycle running at present, so we can start a cycle */
+        /* Begin by setting Flash Cycle Done. */
+        hsfsts.hsf_status.flcdone = 1;
+        E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFSTS, hsfsts.regval);
+        error = E1000_SUCCESS;
+    } else {
+        /* otherwise poll for sometime so the current cycle has a chance
+         * to end before giving up. */
+        for (i = 0; i < ICH8_FLASH_COMMAND_TIMEOUT; i++) {
+            hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+            if (hsfsts.hsf_status.flcinprog == 0) {
+                error = E1000_SUCCESS;
+                break;
+            }
+            usec_delay(1);
+        }
+        if (error == E1000_SUCCESS) {
+            /* Successful in waiting for previous cycle to timeout,
+             * now set the Flash Cycle Done. */
+            hsfsts.hsf_status.flcdone = 1;
+            E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFSTS, hsfsts.regval);
+        } else {
+            DEBUGOUT("Flash controller busy, cannot get access");
+        }
+    }
+    return error;
+}
+
+/******************************************************************************
+ * This function starts a flash cycle and waits for its completion
+ *
+ * hw - The pointer to the hw structure
+ ****************************************************************************/
+int32_t
+e1000_ich8_flash_cycle(struct e1000_hw *hw, uint32_t timeout)
+{
+    union ich8_hws_flash_ctrl hsflctl;
+    union ich8_hws_flash_status hsfsts;
+    int32_t error = E1000_ERR_EEPROM;
+    uint32_t i = 0;
+
+    /* Start a cycle by writing 1 in Flash Cycle Go in Hw Flash Control */
+    hsflctl.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFCTL);
+    hsflctl.hsf_ctrl.flcgo = 1;
+    E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFCTL, hsflctl.regval);
+
+    /* wait till FDONE bit is set to 1 */
+    do {
+        hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+        if (hsfsts.hsf_status.flcdone == 1)
+            break;
+        usec_delay(1);
+        i++;
+    } while (i < timeout);
+    if (hsfsts.hsf_status.flcdone == 1 && hsfsts.hsf_status.flcerr == 0) {
+        error = E1000_SUCCESS;
+    }
+    return error;
+}
+
+/******************************************************************************
+ * Reads a byte or word from the NVM using the ICH8 flash access registers.
+ *
+ * hw - The pointer to the hw structure
+ * index - The index of the byte or word to read.
+ * size - Size of data to read, 1=byte 2=word
+ * data - Pointer to the word to store the value read.
+ *****************************************************************************/
+int32_t
+e1000_read_ich8_data(struct e1000_hw *hw, uint32_t index,
+                     uint32_t size, uint16_t* data)
+{
+    union ich8_hws_flash_status hsfsts;
+    union ich8_hws_flash_ctrl hsflctl;
+    uint32_t flash_linear_address;
+    uint32_t flash_data = 0;
+    int32_t error = -E1000_ERR_EEPROM;
+    int32_t count = 0;
+
+    DEBUGFUNC("e1000_read_ich8_data");
+
+    if (size < 1  || size > 2 || data == 0x0 ||
+        index > ICH8_FLASH_LINEAR_ADDR_MASK)
+        return error;
+
+    flash_linear_address = (ICH8_FLASH_LINEAR_ADDR_MASK & index) +
+                           hw->flash_base_addr;
+
+    do {
+        usec_delay(1);
+        /* Steps */
+        error = e1000_ich8_cycle_init(hw);
+        if (error != E1000_SUCCESS)
+            break;
+
+        hsflctl.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFCTL);
+        /* 0b/1b corresponds to 1 or 2 byte size, respectively. */
+        hsflctl.hsf_ctrl.fldbcount = size - 1;
+        hsflctl.hsf_ctrl.flcycle = ICH8_CYCLE_READ;
+        E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFCTL, hsflctl.regval);
+
+        /* Write the last 24 bits of index into Flash Linear address field in
+         * Flash Address */
+        /* TODO: TBD maybe check the index against the size of flash */
+
+        E1000_WRITE_ICH8_REG(hw, ICH8_FLASH_FADDR, flash_linear_address);
+
+        error = e1000_ich8_flash_cycle(hw, ICH8_FLASH_COMMAND_TIMEOUT);
+
+        /* Check if FCERR is set to 1, if set to 1, clear it and try the whole
+         * sequence a few more times, else read in (shift in) the Flash Data0,
+         * the order is least significant byte first msb to lsb */
+        if (error == E1000_SUCCESS) {
+            flash_data = E1000_READ_ICH8_REG(hw, ICH8_FLASH_FDATA0);
+            if (size == 1) {
+                *data = (uint8_t)(flash_data & 0x000000FF);
+            } else if (size == 2) {
+                *data = (uint16_t)(flash_data & 0x0000FFFF);
+            }
+            break;
+        } else {
+            /* If we've gotten here, then things are probably completely hosed,
+             * but if the error condition is detected, it won't hurt to give
+             * it another try...ICH8_FLASH_CYCLE_REPEAT_COUNT times.
+             */
+            hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+            if (hsfsts.hsf_status.flcerr == 1) {
+                /* Repeat for some time before giving up. */
+                continue;
+            } else if (hsfsts.hsf_status.flcdone == 0) {
+                DEBUGOUT("Timeout error - flash cycle did not complete.");
+                break;
+            }
+        }
+    } while (count++ < ICH8_FLASH_CYCLE_REPEAT_COUNT);
+
+    return error;
+}
+
+/******************************************************************************
+ * Writes One /two bytes to the NVM using the ICH8 flash access registers.
+ *
+ * hw - The pointer to the hw structure
+ * index - The index of the byte/word to read.
+ * size - Size of data to read, 1=byte 2=word
+ * data - The byte(s) to write to the NVM.
+ *****************************************************************************/
+int32_t
+e1000_write_ich8_data(struct e1000_hw *hw, uint32_t index, uint32_t size,
+                      uint16_t data)
+{
+    union ich8_hws_flash_status hsfsts;
+    union ich8_hws_flash_ctrl hsflctl;
+    uint32_t flash_linear_address;
+    uint32_t flash_data = 0;
+    int32_t error = -E1000_ERR_EEPROM;
+    int32_t count = 0;
+
+    DEBUGFUNC("e1000_write_ich8_data");
+
+    if (size < 1  || size > 2 || data > size * 0xff ||
+        index > ICH8_FLASH_LINEAR_ADDR_MASK)
+        return error;
+
+    flash_linear_address = (ICH8_FLASH_LINEAR_ADDR_MASK & index) +
+                           hw->flash_base_addr;
+
+    do {
+        usec_delay(1);
+        /* Steps */
+        error = e1000_ich8_cycle_init(hw);
+        if (error != E1000_SUCCESS)
+            break;
+
+        hsflctl.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFCTL);
+        /* 0b/1b corresponds to 1 or 2 byte size, respectively. */
+        hsflctl.hsf_ctrl.fldbcount = size -1;
+        hsflctl.hsf_ctrl.flcycle = ICH8_CYCLE_WRITE;
+        E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFCTL, hsflctl.regval);
+
+        /* Write the last 24 bits of index into Flash Linear address field in
+         * Flash Address */
+        E1000_WRITE_ICH8_REG(hw, ICH8_FLASH_FADDR, flash_linear_address);
+
+        if (size == 1)
+            flash_data = (uint32_t)data & 0x00FF;
+        else
+            flash_data = (uint32_t)data;
+
+        E1000_WRITE_ICH8_REG(hw, ICH8_FLASH_FDATA0, flash_data);
+
+        /* check if FCERR is set to 1 , if set to 1, clear it and try the whole
+         * sequence a few more times else done */
+        error = e1000_ich8_flash_cycle(hw, ICH8_FLASH_COMMAND_TIMEOUT);
+        if (error == E1000_SUCCESS) {
+            break;
+        } else {
+            /* If we're here, then things are most likely completely hosed,
+             * but if the error condition is detected, it won't hurt to give
+             * it another try...ICH8_FLASH_CYCLE_REPEAT_COUNT times.
+             */
+            hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+            if (hsfsts.hsf_status.flcerr == 1) {
+                /* Repeat for some time before giving up. */
+                continue;
+            } else if (hsfsts.hsf_status.flcdone == 0) {
+                DEBUGOUT("Timeout error - flash cycle did not complete.");
+                break;
+            }
+        }
+    } while (count++ < ICH8_FLASH_CYCLE_REPEAT_COUNT);
+
+    return error;
+}
+
+/******************************************************************************
+ * Reads a single byte from the NVM using the ICH8 flash access registers.
+ *
+ * hw - pointer to e1000_hw structure
+ * index - The index of the byte to read.
+ * data - Pointer to a byte to store the value read.
+ *****************************************************************************/
+int32_t
+e1000_read_ich8_byte(struct e1000_hw *hw, uint32_t index, uint8_t* data)
+{
+    int32_t status = E1000_SUCCESS;
+    uint16_t word = 0;
+
+    status = e1000_read_ich8_data(hw, index, 1, &word);
+    if (status == E1000_SUCCESS) {
+        *data = (uint8_t)word;
+    }
+
+    return status;
+}
+
+/******************************************************************************
+ * Writes a single byte to the NVM using the ICH8 flash access registers.
+ * Performs verification by reading back the value and then going through
+ * a retry algorithm before giving up.
+ *
+ * hw - pointer to e1000_hw structure
+ * index - The index of the byte to write.
+ * byte - The byte to write to the NVM.
+ *****************************************************************************/
+int32_t
+e1000_verify_write_ich8_byte(struct e1000_hw *hw, uint32_t index, uint8_t byte)
+{
+    int32_t error = E1000_SUCCESS;
+    int32_t program_retries;
+    uint8_t temp_byte;
+
+    e1000_write_ich8_byte(hw, index, byte);
+    usec_delay(100);
+
+    for (program_retries = 0; program_retries < 100; program_retries++) {
+        e1000_read_ich8_byte(hw, index, &temp_byte);
+        if (temp_byte == byte)
+            break;
+        usec_delay(10);
+        e1000_write_ich8_byte(hw, index, byte);
+        usec_delay(100);
+    }
+    if (program_retries == 100)
+        error = E1000_ERR_EEPROM;
+
+    return error;
+}
+
+/******************************************************************************
+ * Writes a single byte to the NVM using the ICH8 flash access registers.
+ *
+ * hw - pointer to e1000_hw structure
+ * index - The index of the byte to read.
+ * data - The byte to write to the NVM.
+ *****************************************************************************/
+int32_t
+e1000_write_ich8_byte(struct e1000_hw *hw, uint32_t index, uint8_t data)
+{
+    int32_t status = E1000_SUCCESS;
+    uint16_t word = (uint16_t)data;
+
+    status = e1000_write_ich8_data(hw, index, 1, word);
+
+    return status;
+}
+
+/******************************************************************************
+ * Reads a word from the NVM using the ICH8 flash access registers.
+ *
+ * hw - pointer to e1000_hw structure
+ * index - The starting byte index of the word to read.
+ * data - Pointer to a word to store the value read.
+ *****************************************************************************/
+int32_t
+e1000_read_ich8_word(struct e1000_hw *hw, uint32_t index, uint16_t *data)
+{
+    int32_t status = E1000_SUCCESS;
+    status = e1000_read_ich8_data(hw, index, 2, data);
+    return status;
+}
+
+/******************************************************************************
+ * Writes a word to the NVM using the ICH8 flash access registers.
+ *
+ * hw - pointer to e1000_hw structure
+ * index - The starting byte index of the word to read.
+ * data - The word to write to the NVM.
+ *****************************************************************************/
+int32_t
+e1000_write_ich8_word(struct e1000_hw *hw, uint32_t index, uint16_t data)
+{
+    int32_t status = E1000_SUCCESS;
+    status = e1000_write_ich8_data(hw, index, 2, data);
+    return status;
+}
+
+/******************************************************************************
+ * Erases the bank specified. Each bank is a 4k block. Segments are 0 based.
+ * segment N is 4096 * N + flash_reg_addr.
+ *
+ * hw - pointer to e1000_hw structure
+ * segment - 0 for first segment, 1 for second segment, etc.
+ *****************************************************************************/
+int32_t
+e1000_erase_ich8_4k_segment(struct e1000_hw *hw, uint32_t segment)
+{
+    union ich8_hws_flash_status hsfsts;
+    union ich8_hws_flash_ctrl hsflctl;
+    uint32_t flash_linear_address;
+    int32_t  count = 0;
+    int32_t  error = E1000_ERR_EEPROM;
+    int32_t  iteration, seg_size;
+    int32_t  sector_size;
+    int32_t  j = 0;
+    int32_t  error_flag = 0;
+
+    hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+
+    /* Determine HW Sector size: Read BERASE bits of Hw flash Status register */
+    /* 00: The Hw sector is 256 bytes, hence we need to erase 16
+     *     consecutive sectors.  The start index for the nth Hw sector can be
+     *     calculated as = segment * 4096 + n * 256
+     * 01: The Hw sector is 4K bytes, hence we need to erase 1 sector.
+     *     The start index for the nth Hw sector can be calculated
+     *     as = segment * 4096
+     * 10: Error condition
+     * 11: The Hw sector size is much bigger than the size asked to
+     *     erase...error condition */
+    if (hsfsts.hsf_status.berasesz == 0x0) {
+        /* Hw sector size 256 */
+        sector_size = seg_size = ICH8_FLASH_SEG_SIZE_256;
+        iteration = ICH8_FLASH_SECTOR_SIZE / ICH8_FLASH_SEG_SIZE_256;
+    } else if (hsfsts.hsf_status.berasesz == 0x1) {
+        sector_size = seg_size = ICH8_FLASH_SEG_SIZE_4K;
+        iteration = 1;
+    } else if (hsfsts.hsf_status.berasesz == 0x3) {
+        sector_size = seg_size = ICH8_FLASH_SEG_SIZE_64K;
+        iteration = 1;
+    } else {
+        return error;
+    }
+
+    for (j = 0; j < iteration ; j++) {
+        do {
+            count++;
+            /* Steps */
+            error = e1000_ich8_cycle_init(hw);
+            if (error != E1000_SUCCESS) {
+                error_flag = 1;
+                break;
+            }
+
+            /* Write a value 11 (block Erase) in Flash Cycle field in Hw flash
+             * Control */
+            hsflctl.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFCTL);
+            hsflctl.hsf_ctrl.flcycle = ICH8_CYCLE_ERASE;
+            E1000_WRITE_ICH8_REG16(hw, ICH8_FLASH_HSFCTL, hsflctl.regval);
+
+            /* Write the last 24 bits of an index within the block into Flash
+             * Linear address field in Flash Address.  This probably needs to
+             * be calculated here based off the on-chip segment size and the
+             * software segment size assumed (4K) */
+            /* TBD */
+            flash_linear_address = segment * sector_size + j * seg_size;
+            flash_linear_address &= ICH8_FLASH_LINEAR_ADDR_MASK;
+            flash_linear_address += hw->flash_base_addr;
+
+            E1000_WRITE_ICH8_REG(hw, ICH8_FLASH_FADDR, flash_linear_address);
+
+            error = e1000_ich8_flash_cycle(hw, 1000000);
+            /* Check if FCERR is set to 1.  If 1, clear it and try the whole
+             * sequence a few more times else Done */
+            if (error == E1000_SUCCESS) {
+                break;
+            } else {
+                hsfsts.regval = E1000_READ_ICH8_REG16(hw, ICH8_FLASH_HSFSTS);
+                if (hsfsts.hsf_status.flcerr == 1) {
+                    /* repeat for some time before giving up */
+                    continue;
+                } else if (hsfsts.hsf_status.flcdone == 0) {
+                    error_flag = 1;
+                    break;
+                }
+            }
+        } while ((count < ICH8_FLASH_CYCLE_REPEAT_COUNT) && !error_flag);
+        if (error_flag == 1)
+            break;
+    }
+    if (error_flag != 1)
+        error = E1000_SUCCESS;
+    return error;
+}
+
+/******************************************************************************
+ *
+ * Reverse duplex setting without breaking the link.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *
+ *****************************************************************************/
+int32_t
+e1000_duplex_reversal(struct e1000_hw *hw)
+{
+    int32_t ret_val;
+    uint16_t phy_data;
+
+    if (hw->phy_type != e1000_phy_igp_3)
+        return E1000_SUCCESS;
+
+    ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
+    if (ret_val)
+        return ret_val;
+
+    phy_data ^= MII_CR_FULL_DUPLEX;
+
+    ret_val = e1000_write_phy_reg(hw, PHY_CTRL, phy_data);
+    if (ret_val)
+        return ret_val;
+
+    ret_val = e1000_read_phy_reg(hw, IGP3E1000_PHY_MISC_CTRL, &phy_data);
+    if (ret_val)
+        return ret_val;
+
+    phy_data |= IGP3_PHY_MISC_DUPLEX_MANUAL_SET;
+    ret_val = e1000_write_phy_reg(hw, IGP3E1000_PHY_MISC_CTRL, phy_data);
+
+    return ret_val;
+}
+
+int32_t
+e1000_init_lcd_from_nvm_config_region(struct e1000_hw *hw,
+                                      uint32_t cnf_base_addr, uint32_t cnf_size)
+{
+    uint32_t ret_val = E1000_SUCCESS;
+    uint16_t word_addr, reg_data, reg_addr;
+    uint16_t i;
+
+    /* cnf_base_addr is in DWORD */
+    word_addr = (uint16_t)(cnf_base_addr << 1);
+
+    /* cnf_size is returned in size of dwords */
+    for (i = 0; i < cnf_size; i++) {
+        ret_val = e1000_read_eeprom(hw, (word_addr + i*2), 1, &reg_data);
+        if (ret_val)
+            return ret_val;
+
+        ret_val = e1000_read_eeprom(hw, (word_addr + i*2 + 1), 1, &reg_addr);
+        if (ret_val)
+            return ret_val;
+
+        ret_val = e1000_get_software_flag(hw);
+        if (ret_val != E1000_SUCCESS)
+            return ret_val;
+
+        ret_val = e1000_write_phy_reg_ex(hw, (uint32_t)reg_addr, reg_data);
+
+        e1000_release_software_flag(hw);
+    }
+
+    return ret_val;
+}
+
+
+/******************************************************************************
+ * This function initializes the PHY from the NVM on ICH8 platforms. This
+ * is needed due to an issue where the NVM configuration is not properly
+ * autoloaded after power transitions. Therefore, after each PHY reset, we
+ * will load the configuration data out of the NVM manually.
+ *
+ * hw: Struct containing variables accessed by shared code
+ *****************************************************************************/
+int32_t
+e1000_init_lcd_from_nvm(struct e1000_hw *hw)
+{
+    uint32_t reg_data, cnf_base_addr, cnf_size, ret_val, loop;
+
+    if (hw->phy_type != e1000_phy_igp_3)
+          return E1000_SUCCESS;
+
+    /* Check if SW needs configure the PHY */
+    reg_data = E1000_READ_REG(hw, FEXTNVM);
+    if (!(reg_data & FEXTNVM_SW_CONFIG))
+        return E1000_SUCCESS;
+
+    /* Wait for basic configuration completes before proceeding*/
+    loop = 0;
+    do {
+        reg_data = E1000_READ_REG(hw, STATUS) & E1000_STATUS_LAN_INIT_DONE;
+        usec_delay(100);
+        loop++;
+    } while ((!reg_data) && (loop < 50));
+
+    /* Clear the Init Done bit for the next init event */
+    reg_data = E1000_READ_REG(hw, STATUS);
+    reg_data &= ~E1000_STATUS_LAN_INIT_DONE;
+    E1000_WRITE_REG(hw, STATUS, reg_data);
+
+    /* Make sure HW does not configure LCD from PHY extended configuration
+       before SW configuration */
+    reg_data = E1000_READ_REG(hw, EXTCNF_CTRL);
+    if ((reg_data & E1000_EXTCNF_CTRL_LCD_WRITE_ENABLE) == 0x0000) {
+        reg_data = E1000_READ_REG(hw, EXTCNF_SIZE);
+        cnf_size = reg_data & E1000_EXTCNF_SIZE_EXT_PCIE_LENGTH;
+        cnf_size >>= 16;
+        if (cnf_size) {
+            reg_data = E1000_READ_REG(hw, EXTCNF_CTRL);
+            cnf_base_addr = reg_data & E1000_EXTCNF_CTRL_EXT_CNF_POINTER;
+            /* cnf_base_addr is in DWORD */
+            cnf_base_addr >>= 16;
+
+            /* Configure LCD from extended configuration region. */
+            ret_val = e1000_init_lcd_from_nvm_config_region(hw, cnf_base_addr,
+                                                            cnf_size);
+            if (ret_val)
+                return ret_val;
+        }
+    }
+
+    return E1000_SUCCESS;
+}
+
+
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_hw.h linux-2.6.9/drivers/net/e1000/e1000_hw.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_hw.h	2004-10-18 23:55:06.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_hw.h	2007-07-16 13:33:15.000000000 +0200
@@ -1,2129 +1,693 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
 
-/* e1000_hw.h
- * Structures, enums, and macros for the MAC
- */
-
 #ifndef _E1000_HW_H_
 #define _E1000_HW_H_
 
 #include "e1000_osdep.h"
+#include "e1000_regs.h"
+#include "e1000_defines.h"
 
-
-/* Forward declarations of structures used by the shared code */
 struct e1000_hw;
-struct e1000_hw_stats;
 
-/* Enumerated types specific to the e1000 hardware */
-/* Media Access Controlers */
-typedef enum {
-    e1000_undefined = 0,
-    e1000_82542_rev2_0,
-    e1000_82542_rev2_1,
-    e1000_82543,
-    e1000_82544,
-    e1000_82540,
-    e1000_82545,
-    e1000_82545_rev_3,
-    e1000_82546,
-    e1000_82546_rev_3,
-    e1000_82541,
-    e1000_82541_rev_2,
-    e1000_82547,
-    e1000_82547_rev_2,
-    e1000_num_macs
+#define E1000_DEV_ID_82542                    0x1000
+#define E1000_DEV_ID_82543GC_FIBER            0x1001
+#define E1000_DEV_ID_82543GC_COPPER           0x1004
+#define E1000_DEV_ID_82544EI_COPPER           0x1008
+#define E1000_DEV_ID_82544EI_FIBER            0x1009
+#define E1000_DEV_ID_82544GC_COPPER           0x100C
+#define E1000_DEV_ID_82544GC_LOM              0x100D
+#define E1000_DEV_ID_82540EM                  0x100E
+#define E1000_DEV_ID_82540EM_LOM              0x1015
+#define E1000_DEV_ID_82540EP_LOM              0x1016
+#define E1000_DEV_ID_82540EP                  0x1017
+#define E1000_DEV_ID_82540EP_LP               0x101E
+#define E1000_DEV_ID_82545EM_COPPER           0x100F
+#define E1000_DEV_ID_82545EM_FIBER            0x1011
+#define E1000_DEV_ID_82545GM_COPPER           0x1026
+#define E1000_DEV_ID_82545GM_FIBER            0x1027
+#define E1000_DEV_ID_82545GM_SERDES           0x1028
+#define E1000_DEV_ID_82546EB_COPPER           0x1010
+#define E1000_DEV_ID_82546EB_FIBER            0x1012
+#define E1000_DEV_ID_82546EB_QUAD_COPPER      0x101D
+#define E1000_DEV_ID_82546GB_COPPER           0x1079
+#define E1000_DEV_ID_82546GB_FIBER            0x107A
+#define E1000_DEV_ID_82546GB_SERDES           0x107B
+#define E1000_DEV_ID_82546GB_PCIE             0x108A
+#define E1000_DEV_ID_82546GB_QUAD_COPPER      0x1099
+#define E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3 0x10B5
+#define E1000_DEV_ID_82541EI                  0x1013
+#define E1000_DEV_ID_82541EI_MOBILE           0x1018
+#define E1000_DEV_ID_82541ER_LOM              0x1014
+#define E1000_DEV_ID_82541ER                  0x1078
+#define E1000_DEV_ID_82541GI                  0x1076
+#define E1000_DEV_ID_82541GI_LF               0x107C
+#define E1000_DEV_ID_82541GI_MOBILE           0x1077
+#define E1000_DEV_ID_82547EI                  0x1019
+#define E1000_DEV_ID_82547EI_MOBILE           0x101A
+#define E1000_DEV_ID_82547GI                  0x1075
+#define E1000_DEV_ID_82571EB_COPPER           0x105E
+#define E1000_DEV_ID_82571EB_FIBER            0x105F
+#define E1000_DEV_ID_82571EB_SERDES           0x1060
+#define E1000_DEV_ID_82571EB_QUAD_COPPER      0x10A4
+#define E1000_DEV_ID_82571EB_QUAD_FIBER       0x10A5
+#define E1000_DEV_ID_82571EB_QUAD_COPPER_LP   0x10BC
+#define E1000_DEV_ID_82572EI_COPPER           0x107D
+#define E1000_DEV_ID_82572EI_FIBER            0x107E
+#define E1000_DEV_ID_82572EI_SERDES           0x107F
+#define E1000_DEV_ID_82572EI                  0x10B9
+#define E1000_DEV_ID_82573E                   0x108B
+#define E1000_DEV_ID_82573E_IAMT              0x108C
+#define E1000_DEV_ID_82573L                   0x109A
+#define E1000_DEV_ID_80003ES2LAN_COPPER_DPT   0x1096
+#define E1000_DEV_ID_80003ES2LAN_SERDES_DPT   0x1098
+#define E1000_DEV_ID_80003ES2LAN_COPPER_SPT   0x10BA
+#define E1000_DEV_ID_80003ES2LAN_SERDES_SPT   0x10BB
+#define E1000_DEV_ID_ICH8_IGP_M_AMT           0x1049
+#define E1000_DEV_ID_ICH8_IGP_AMT             0x104A
+#define E1000_DEV_ID_ICH8_IGP_C               0x104B
+#define E1000_DEV_ID_ICH8_IFE                 0x104C
+#define E1000_DEV_ID_ICH8_IFE_GT              0x10C4
+#define E1000_DEV_ID_ICH8_IFE_G               0x10C5
+#define E1000_DEV_ID_ICH8_IGP_M               0x104D
+#define E1000_DEV_ID_ICH9_IGP_AMT             0x10BD
+#define E1000_DEV_ID_ICH9_IGP_C               0x294C
+#define E1000_DEV_ID_ICH9_IFE                 0x10C0
+#define E1000_DEV_ID_ICH9_IFE_GT              0x10C3
+#define E1000_DEV_ID_ICH9_IFE_G               0x10C2
+
+#define E1000_REVISION_0 0
+#define E1000_REVISION_1 1
+#define E1000_REVISION_2 2
+#define E1000_REVISION_3 3
+#define E1000_REVISION_4 4
+
+#define E1000_FUNC_0     0
+#define E1000_FUNC_1     1
+
+typedef enum {
+	e1000_undefined = 0,
+	e1000_82542,
+	e1000_82543,
+	e1000_82544,
+	e1000_82540,
+	e1000_82545,
+	e1000_82545_rev_3,
+	e1000_82546,
+	e1000_82546_rev_3,
+	e1000_82541,
+	e1000_82541_rev_2,
+	e1000_82547,
+	e1000_82547_rev_2,
+	e1000_82571,
+	e1000_82572,
+	e1000_82573,
+	e1000_80003es2lan,
+	e1000_ich8lan,
+	e1000_ich9lan,
+	e1000_num_macs  /* List is 1-based, so subtract 1 for true count. */
 } e1000_mac_type;
 
 typedef enum {
-    e1000_eeprom_uninitialized = 0,
-    e1000_eeprom_spi,
-    e1000_eeprom_microwire,
-    e1000_num_eeprom_types
-} e1000_eeprom_type;
-
-/* Media Types */
-typedef enum {
-    e1000_media_type_copper = 0,
-    e1000_media_type_fiber = 1,
-    e1000_media_type_internal_serdes = 2,
-    e1000_num_media_types
+	e1000_media_type_unknown = 0,
+	e1000_media_type_copper = 1,
+	e1000_media_type_fiber = 2,
+	e1000_media_type_internal_serdes = 3,
+	e1000_num_media_types
 } e1000_media_type;
 
 typedef enum {
-    e1000_10_half = 0,
-    e1000_10_full = 1,
-    e1000_100_half = 2,
-    e1000_100_full = 3
-} e1000_speed_duplex_type;
-
-/* Flow Control Settings */
-typedef enum {
-    e1000_fc_none = 0,
-    e1000_fc_rx_pause = 1,
-    e1000_fc_tx_pause = 2,
-    e1000_fc_full = 3,
-    e1000_fc_default = 0xFF
-} e1000_fc_type;
+	e1000_nvm_unknown = 0,
+	e1000_nvm_none,
+	e1000_nvm_eeprom_spi,
+	e1000_nvm_eeprom_microwire,
+	e1000_nvm_flash_hw,
+	e1000_nvm_flash_sw
+} e1000_nvm_type;
+
+typedef enum {
+	e1000_nvm_override_none = 0,
+	e1000_nvm_override_spi_small,
+	e1000_nvm_override_spi_large,
+	e1000_nvm_override_microwire_small,
+	e1000_nvm_override_microwire_large
+} e1000_nvm_override;
+
+typedef enum {
+	e1000_phy_unknown = 0,
+	e1000_phy_none,
+	e1000_phy_m88,
+	e1000_phy_igp,
+	e1000_phy_igp_2,
+	e1000_phy_gg82563,
+	e1000_phy_igp_3,
+	e1000_phy_ife,
+} e1000_phy_type;
 
-/* PCI bus types */
 typedef enum {
-    e1000_bus_type_unknown = 0,
-    e1000_bus_type_pci,
-    e1000_bus_type_pcix,
-    e1000_bus_type_reserved
+	e1000_bus_type_unknown = 0,
+	e1000_bus_type_pci,
+	e1000_bus_type_pcix,
+	e1000_bus_type_pci_express,
+	e1000_bus_type_reserved
 } e1000_bus_type;
 
-/* PCI bus speeds */
 typedef enum {
-    e1000_bus_speed_unknown = 0,
-    e1000_bus_speed_33,
-    e1000_bus_speed_66,
-    e1000_bus_speed_100,
-    e1000_bus_speed_120,
-    e1000_bus_speed_133,
-    e1000_bus_speed_reserved
+	e1000_bus_speed_unknown = 0,
+	e1000_bus_speed_33,
+	e1000_bus_speed_66,
+	e1000_bus_speed_100,
+	e1000_bus_speed_120,
+	e1000_bus_speed_133,
+	e1000_bus_speed_2500,
+	e1000_bus_speed_reserved
 } e1000_bus_speed;
 
-/* PCI bus widths */
 typedef enum {
-    e1000_bus_width_unknown = 0,
-    e1000_bus_width_32,
-    e1000_bus_width_64,
-    e1000_bus_width_reserved
+	e1000_bus_width_unknown = 0,
+	e1000_bus_width_pcie_x1,
+	e1000_bus_width_pcie_x2,
+	e1000_bus_width_pcie_x4 = 4,
+	e1000_bus_width_32,
+	e1000_bus_width_64,
+	e1000_bus_width_reserved
 } e1000_bus_width;
 
-/* PHY status info structure and supporting enums */
-typedef enum {
-    e1000_cable_length_50 = 0,
-    e1000_cable_length_50_80,
-    e1000_cable_length_80_110,
-    e1000_cable_length_110_140,
-    e1000_cable_length_140,
-    e1000_cable_length_undefined = 0xFF
-} e1000_cable_length;
-
-typedef enum {
-    e1000_igp_cable_length_10  = 10,
-    e1000_igp_cable_length_20  = 20,
-    e1000_igp_cable_length_30  = 30,
-    e1000_igp_cable_length_40  = 40,
-    e1000_igp_cable_length_50  = 50,
-    e1000_igp_cable_length_60  = 60,
-    e1000_igp_cable_length_70  = 70,
-    e1000_igp_cable_length_80  = 80,
-    e1000_igp_cable_length_90  = 90,
-    e1000_igp_cable_length_100 = 100,
-    e1000_igp_cable_length_110 = 110,
-    e1000_igp_cable_length_120 = 120,
-    e1000_igp_cable_length_130 = 130,
-    e1000_igp_cable_length_140 = 140,
-    e1000_igp_cable_length_150 = 150,
-    e1000_igp_cable_length_160 = 160,
-    e1000_igp_cable_length_170 = 170,
-    e1000_igp_cable_length_180 = 180
-} e1000_igp_cable_length;
-
 typedef enum {
-    e1000_10bt_ext_dist_enable_normal = 0,
-    e1000_10bt_ext_dist_enable_lower,
-    e1000_10bt_ext_dist_enable_undefined = 0xFF
-} e1000_10bt_ext_dist_enable;
-
-typedef enum {
-    e1000_rev_polarity_normal = 0,
-    e1000_rev_polarity_reversed,
-    e1000_rev_polarity_undefined = 0xFF
-} e1000_rev_polarity;
-
-typedef enum {
-    e1000_downshift_normal = 0,
-    e1000_downshift_activated,
-    e1000_downshift_undefined = 0xFF
-} e1000_downshift;
-
-typedef enum {
-    e1000_polarity_reversal_enabled = 0,
-    e1000_polarity_reversal_disabled,
-    e1000_polarity_reversal_undefined = 0xFF
-} e1000_polarity_reversal;
-
-typedef enum {
-    e1000_auto_x_mode_manual_mdi = 0,
-    e1000_auto_x_mode_manual_mdix,
-    e1000_auto_x_mode_auto1,
-    e1000_auto_x_mode_auto2,
-    e1000_auto_x_mode_undefined = 0xFF
-} e1000_auto_x_mode;
-
-typedef enum {
-    e1000_1000t_rx_status_not_ok = 0,
-    e1000_1000t_rx_status_ok,
-    e1000_1000t_rx_status_undefined = 0xFF
+	e1000_1000t_rx_status_not_ok = 0,
+	e1000_1000t_rx_status_ok,
+	e1000_1000t_rx_status_undefined = 0xFF
 } e1000_1000t_rx_status;
 
 typedef enum {
-    e1000_phy_m88 = 0,
-    e1000_phy_igp,
-    e1000_phy_undefined = 0xFF
-} e1000_phy_type;
+	e1000_rev_polarity_normal = 0,
+	e1000_rev_polarity_reversed,
+	e1000_rev_polarity_undefined = 0xFF
+} e1000_rev_polarity;
 
 typedef enum {
-    e1000_ms_hw_default = 0,
-    e1000_ms_force_master,
-    e1000_ms_force_slave,
-    e1000_ms_auto
-} e1000_ms_type;
+	e1000_fc_none = 0,
+	e1000_fc_rx_pause,
+	e1000_fc_tx_pause,
+	e1000_fc_full,
+	e1000_fc_default = 0xFF
+} e1000_fc_mode;
 
 typedef enum {
-    e1000_ffe_config_enabled = 0,
-    e1000_ffe_config_active,
-    e1000_ffe_config_blocked
+	e1000_ffe_config_enabled = 0,
+	e1000_ffe_config_active,
+	e1000_ffe_config_blocked
 } e1000_ffe_config;
 
 typedef enum {
-    e1000_dsp_config_disabled = 0,
-    e1000_dsp_config_enabled,
-    e1000_dsp_config_activated,
-    e1000_dsp_config_undefined = 0xFF
+	e1000_dsp_config_disabled = 0,
+	e1000_dsp_config_enabled,
+	e1000_dsp_config_activated,
+	e1000_dsp_config_undefined = 0xFF
 } e1000_dsp_config;
 
-struct e1000_phy_info {
-    e1000_cable_length cable_length;
-    e1000_10bt_ext_dist_enable extended_10bt_distance;
-    e1000_rev_polarity cable_polarity;
-    e1000_downshift downshift;
-    e1000_polarity_reversal polarity_correction;
-    e1000_auto_x_mode mdix_mode;
-    e1000_1000t_rx_status local_rx;
-    e1000_1000t_rx_status remote_rx;
-};
-
-struct e1000_phy_stats {
-    uint32_t idle_errors;
-    uint32_t receive_errors;
-};
-
-struct e1000_eeprom_info {
-    e1000_eeprom_type type;
-    uint16_t word_size;
-    uint16_t opcode_bits;
-    uint16_t address_bits;
-    uint16_t delay_usec;
-    uint16_t page_size;
-};
-
-
-
-/* Error Codes */
-#define E1000_SUCCESS      0
-#define E1000_ERR_EEPROM   1
-#define E1000_ERR_PHY      2
-#define E1000_ERR_CONFIG   3
-#define E1000_ERR_PARAM    4
-#define E1000_ERR_MAC_TYPE 5
-#define E1000_ERR_PHY_TYPE 6
-
-/* Function prototypes */
-/* Initialization */
-int32_t e1000_reset_hw(struct e1000_hw *hw);
-int32_t e1000_init_hw(struct e1000_hw *hw);
-int32_t e1000_set_mac_type(struct e1000_hw *hw);
-void e1000_set_media_type(struct e1000_hw *hw);
-
-/* Link Configuration */
-int32_t e1000_setup_link(struct e1000_hw *hw);
-int32_t e1000_phy_setup_autoneg(struct e1000_hw *hw);
-void e1000_config_collision_dist(struct e1000_hw *hw);
-int32_t e1000_config_fc_after_link_up(struct e1000_hw *hw);
-int32_t e1000_check_for_link(struct e1000_hw *hw);
-int32_t e1000_get_speed_and_duplex(struct e1000_hw *hw, uint16_t * speed, uint16_t * duplex);
-int32_t e1000_wait_autoneg(struct e1000_hw *hw);
-int32_t e1000_force_mac_fc(struct e1000_hw *hw);
-
-/* PHY */
-int32_t e1000_read_phy_reg(struct e1000_hw *hw, uint32_t reg_addr, uint16_t *phy_data);
-int32_t e1000_write_phy_reg(struct e1000_hw *hw, uint32_t reg_addr, uint16_t data);
-void e1000_phy_hw_reset(struct e1000_hw *hw);
-int32_t e1000_phy_reset(struct e1000_hw *hw);
-int32_t e1000_detect_gig_phy(struct e1000_hw *hw);
-int32_t e1000_phy_get_info(struct e1000_hw *hw, struct e1000_phy_info *phy_info);
-int32_t e1000_phy_m88_get_info(struct e1000_hw *hw, struct e1000_phy_info *phy_info);
-int32_t e1000_phy_igp_get_info(struct e1000_hw *hw, struct e1000_phy_info *phy_info);
-int32_t e1000_get_cable_length(struct e1000_hw *hw, uint16_t *min_length, uint16_t *max_length);
-int32_t e1000_check_polarity(struct e1000_hw *hw, uint16_t *polarity);
-int32_t e1000_check_downshift(struct e1000_hw *hw);
-int32_t e1000_validate_mdi_setting(struct e1000_hw *hw);
-
-/* EEPROM Functions */
-void e1000_init_eeprom_params(struct e1000_hw *hw);
-int32_t e1000_read_eeprom(struct e1000_hw *hw, uint16_t reg, uint16_t words, uint16_t *data);
-int32_t e1000_validate_eeprom_checksum(struct e1000_hw *hw);
-int32_t e1000_update_eeprom_checksum(struct e1000_hw *hw);
-int32_t e1000_write_eeprom(struct e1000_hw *hw, uint16_t reg, uint16_t words, uint16_t *data);
-int32_t e1000_read_part_num(struct e1000_hw *hw, uint32_t * part_num);
-int32_t e1000_read_mac_addr(struct e1000_hw * hw);
-
-/* Filters (multicast, vlan, receive) */
-void e1000_init_rx_addrs(struct e1000_hw *hw);
-void e1000_mc_addr_list_update(struct e1000_hw *hw, uint8_t * mc_addr_list, uint32_t mc_addr_count, uint32_t pad, uint32_t rar_used_count);
-uint32_t e1000_hash_mc_addr(struct e1000_hw *hw, uint8_t * mc_addr);
-void e1000_mta_set(struct e1000_hw *hw, uint32_t hash_value);
-void e1000_rar_set(struct e1000_hw *hw, uint8_t * mc_addr, uint32_t rar_index);
-void e1000_write_vfta(struct e1000_hw *hw, uint32_t offset, uint32_t value);
-void e1000_clear_vfta(struct e1000_hw *hw);
-
-/* LED functions */
-int32_t e1000_setup_led(struct e1000_hw *hw);
-int32_t e1000_cleanup_led(struct e1000_hw *hw);
-int32_t e1000_led_on(struct e1000_hw *hw);
-int32_t e1000_led_off(struct e1000_hw *hw);
-
-/* Adaptive IFS Functions */
-
-/* Everything else */
-uint32_t e1000_enable_mng_pass_thru(struct e1000_hw *hw);
-void e1000_clear_hw_cntrs(struct e1000_hw *hw);
-void e1000_reset_adaptive(struct e1000_hw *hw);
-void e1000_update_adaptive(struct e1000_hw *hw);
-void e1000_tbi_adjust_stats(struct e1000_hw *hw, struct e1000_hw_stats *stats, uint32_t frame_len, uint8_t * mac_addr);
-void e1000_get_bus_info(struct e1000_hw *hw);
-void e1000_pci_set_mwi(struct e1000_hw *hw);
-void e1000_pci_clear_mwi(struct e1000_hw *hw);
-void e1000_read_pci_cfg(struct e1000_hw *hw, uint32_t reg, uint16_t * value);
-void e1000_write_pci_cfg(struct e1000_hw *hw, uint32_t reg, uint16_t * value);
-/* Port I/O is only supported on 82544 and newer */
-uint32_t e1000_io_read(struct e1000_hw *hw, unsigned long port);
-uint32_t e1000_read_reg_io(struct e1000_hw *hw, uint32_t offset);
-void e1000_io_write(struct e1000_hw *hw, unsigned long port, uint32_t value);
-void e1000_write_reg_io(struct e1000_hw *hw, uint32_t offset, uint32_t value);
-int32_t e1000_config_dsp_after_link_change(struct e1000_hw *hw, boolean_t link_up);
-int32_t e1000_set_d3_lplu_state(struct e1000_hw *hw, boolean_t active);
-
-#define E1000_READ_REG_IO(a, reg) \
-    e1000_read_reg_io((a), E1000_##reg)
-#define E1000_WRITE_REG_IO(a, reg, val) \
-    e1000_write_reg_io((a), E1000_##reg, val)
-
-/* PCI Device IDs */
-#define E1000_DEV_ID_82542               0x1000
-#define E1000_DEV_ID_82543GC_FIBER       0x1001
-#define E1000_DEV_ID_82543GC_COPPER      0x1004
-#define E1000_DEV_ID_82544EI_COPPER      0x1008
-#define E1000_DEV_ID_82544EI_FIBER       0x1009
-#define E1000_DEV_ID_82544GC_COPPER      0x100C
-#define E1000_DEV_ID_82544GC_LOM         0x100D
-#define E1000_DEV_ID_82540EM             0x100E
-#define E1000_DEV_ID_82540EM_LOM         0x1015
-#define E1000_DEV_ID_82540EP_LOM         0x1016
-#define E1000_DEV_ID_82540EP             0x1017
-#define E1000_DEV_ID_82540EP_LP          0x101E
-#define E1000_DEV_ID_82545EM_COPPER      0x100F
-#define E1000_DEV_ID_82545EM_FIBER       0x1011
-#define E1000_DEV_ID_82545GM_COPPER      0x1026
-#define E1000_DEV_ID_82545GM_FIBER       0x1027
-#define E1000_DEV_ID_82545GM_SERDES      0x1028
-#define E1000_DEV_ID_82546EB_COPPER      0x1010
-#define E1000_DEV_ID_82546EB_FIBER       0x1012
-#define E1000_DEV_ID_82546EB_QUAD_COPPER 0x101D
-#define E1000_DEV_ID_82541EI             0x1013
-#define E1000_DEV_ID_82541EI_MOBILE      0x1018
-#define E1000_DEV_ID_82541ER             0x1078
-#define E1000_DEV_ID_82547GI             0x1075
-#define E1000_DEV_ID_82541GI             0x1076
-#define E1000_DEV_ID_82541GI_MOBILE      0x1077
-#define E1000_DEV_ID_82541GI_LF          0x107C
-#define E1000_DEV_ID_82546GB_COPPER      0x1079
-#define E1000_DEV_ID_82546GB_FIBER       0x107A
-#define E1000_DEV_ID_82546GB_SERDES      0x107B
-#define E1000_DEV_ID_82547EI             0x1019
-#define NODE_ADDRESS_SIZE 6
-#define ETH_LENGTH_OF_ADDRESS 6
-
-/* MAC decode size is 128K - This is the size of BAR0 */
-#define MAC_DECODE_SIZE (128 * 1024)
-
-#define E1000_82542_2_0_REV_ID 2
-#define E1000_82542_2_1_REV_ID 3
-#define E1000_REVISION_0       0
-#define E1000_REVISION_1       1
-#define E1000_REVISION_2       2
-
-#define SPEED_10    10
-#define SPEED_100   100
-#define SPEED_1000  1000
-#define HALF_DUPLEX 1
-#define FULL_DUPLEX 2
-
-/* The sizes (in bytes) of a ethernet packet */
-#define ENET_HEADER_SIZE             14
-#define MAXIMUM_ETHERNET_FRAME_SIZE  1518 /* With FCS */
-#define MINIMUM_ETHERNET_FRAME_SIZE  64   /* With FCS */
-#define ETHERNET_FCS_SIZE            4
-#define MAXIMUM_ETHERNET_PACKET_SIZE \
-    (MAXIMUM_ETHERNET_FRAME_SIZE - ETHERNET_FCS_SIZE)
-#define MINIMUM_ETHERNET_PACKET_SIZE \
-    (MINIMUM_ETHERNET_FRAME_SIZE - ETHERNET_FCS_SIZE)
-#define CRC_LENGTH                   ETHERNET_FCS_SIZE
-#define MAX_JUMBO_FRAME_SIZE         0x3F00
-
-
-/* 802.1q VLAN Packet Sizes */
-#define VLAN_TAG_SIZE                     4     /* 802.3ac tag (not DMAed) */
-
-/* Ethertype field values */
-#define ETHERNET_IEEE_VLAN_TYPE 0x8100  /* 802.3ac packet */
-#define ETHERNET_IP_TYPE        0x0800  /* IP packets */
-#define ETHERNET_ARP_TYPE       0x0806  /* Address Resolution Protocol (ARP) */
-
-/* Packet Header defines */
-#define IP_PROTOCOL_TCP    6
-#define IP_PROTOCOL_UDP    0x11
-
-/* This defines the bits that are set in the Interrupt Mask
- * Set/Read Register.  Each bit is documented below:
- *   o RXDMT0 = Receive Descriptor Minimum Threshold hit (ring 0)
- *   o RXSEQ  = Receive Sequence Error
- */
-#define POLL_IMS_ENABLE_MASK ( \
-    E1000_IMS_RXDMT0 |         \
-    E1000_IMS_RXSEQ)
-
-/* This defines the bits that are set in the Interrupt Mask
- * Set/Read Register.  Each bit is documented below:
- *   o RXT0   = Receiver Timer Interrupt (ring 0)
- *   o TXDW   = Transmit Descriptor Written Back
- *   o RXDMT0 = Receive Descriptor Minimum Threshold hit (ring 0)
- *   o RXSEQ  = Receive Sequence Error
- *   o LSC    = Link Status Change
- */
-#define IMS_ENABLE_MASK ( \
-    E1000_IMS_RXT0   |    \
-    E1000_IMS_TXDW   |    \
-    E1000_IMS_RXDMT0 |    \
-    E1000_IMS_RXSEQ  |    \
-    E1000_IMS_LSC)
-
-/* Number of high/low register pairs in the RAR. The RAR (Receive Address
- * Registers) holds the directed and multicast addresses that we monitor. We
- * reserve one of these spots for our directed address, allowing us room for
- * E1000_RAR_ENTRIES - 1 multicast addresses.
- */
-#define E1000_RAR_ENTRIES 15
-
-#define MIN_NUMBER_OF_DESCRIPTORS 8
-#define MAX_NUMBER_OF_DESCRIPTORS 0xFFF8
-
 /* Receive Descriptor */
 struct e1000_rx_desc {
-    uint64_t buffer_addr; /* Address of the descriptor's data buffer */
-    uint16_t length;     /* Length of data DMAed into data buffer */
-    uint16_t csum;       /* Packet checksum */
-    uint8_t status;      /* Descriptor status */
-    uint8_t errors;      /* Descriptor Errors */
-    uint16_t special;
+	u64 buffer_addr; /* Address of the descriptor's data buffer */
+	u16 length;      /* Length of data DMAed into data buffer */
+	u16 csum;        /* Packet checksum */
+	u8  status;      /* Descriptor status */
+	u8  errors;      /* Descriptor Errors */
+	u16 special;
+};
+
+/* Receive Descriptor - Extended */
+union e1000_rx_desc_extended {
+	struct {
+		u64 buffer_addr;
+		u64 reserved;
+	} read;
+	struct {
+		struct {
+			u32 mrq;              /* Multiple Rx Queues */
+			union {
+				u32 rss;            /* RSS Hash */
+				struct {
+					u16 ip_id;  /* IP id */
+					u16 csum;   /* Packet Checksum */
+				} csum_ip;
+			} hi_dword;
+		} lower;
+		struct {
+			u32 status_error;     /* ext status/error */
+			u16 length;
+			u16 vlan;             /* VLAN tag */
+		} upper;
+	} wb;  /* writeback */
+};
+
+#define MAX_PS_BUFFERS 4
+/* Receive Descriptor - Packet Split */
+union e1000_rx_desc_packet_split {
+	struct {
+		/* one buffer for protocol header(s), three data buffers */
+		u64 buffer_addr[MAX_PS_BUFFERS];
+	} read;
+	struct {
+		struct {
+			u32 mrq;              /* Multiple Rx Queues */
+			union {
+				u32 rss;              /* RSS Hash */
+				struct {
+					u16 ip_id;    /* IP id */
+					u16 csum;     /* Packet Checksum */
+				} csum_ip;
+			} hi_dword;
+		} lower;
+		struct {
+			u32 status_error;     /* ext status/error */
+			u16 length0;          /* length of buffer 0 */
+			u16 vlan;             /* VLAN tag */
+		} middle;
+		struct {
+			u16 header_status;
+			u16 length[3];        /* length of buffers 1-3 */
+		} upper;
+		u64 reserved;
+	} wb; /* writeback */
 };
 
-/* Receive Decriptor bit definitions */
-#define E1000_RXD_STAT_DD       0x01    /* Descriptor Done */
-#define E1000_RXD_STAT_EOP      0x02    /* End of Packet */
-#define E1000_RXD_STAT_IXSM     0x04    /* Ignore checksum */
-#define E1000_RXD_STAT_VP       0x08    /* IEEE VLAN Packet */
-#define E1000_RXD_STAT_TCPCS    0x20    /* TCP xsum calculated */
-#define E1000_RXD_STAT_IPCS     0x40    /* IP xsum calculated */
-#define E1000_RXD_STAT_PIF      0x80    /* passed in-exact filter */
-#define E1000_RXD_ERR_CE        0x01    /* CRC Error */
-#define E1000_RXD_ERR_SE        0x02    /* Symbol Error */
-#define E1000_RXD_ERR_SEQ       0x04    /* Sequence Error */
-#define E1000_RXD_ERR_CXE       0x10    /* Carrier Extension Error */
-#define E1000_RXD_ERR_TCPE      0x20    /* TCP/UDP Checksum Error */
-#define E1000_RXD_ERR_IPE       0x40    /* IP Checksum Error */
-#define E1000_RXD_ERR_RXE       0x80    /* Rx Data Error */
-#define E1000_RXD_SPC_VLAN_MASK 0x0FFF  /* VLAN ID is in lower 12 bits */
-#define E1000_RXD_SPC_PRI_MASK  0xE000  /* Priority is in upper 3 bits */
-#define E1000_RXD_SPC_PRI_SHIFT 0x000D  /* Priority is in upper 3 of 16 */
-#define E1000_RXD_SPC_CFI_MASK  0x1000  /* CFI is bit 12 */
-#define E1000_RXD_SPC_CFI_SHIFT 0x000C  /* CFI is bit 12 */
-
-/* mask to determine if packets should be dropped due to frame errors */
-#define E1000_RXD_ERR_FRAME_ERR_MASK ( \
-    E1000_RXD_ERR_CE  |                \
-    E1000_RXD_ERR_SE  |                \
-    E1000_RXD_ERR_SEQ |                \
-    E1000_RXD_ERR_CXE |                \
-    E1000_RXD_ERR_RXE)
-
 /* Transmit Descriptor */
 struct e1000_tx_desc {
-    uint64_t buffer_addr;       /* Address of the descriptor's data buffer */
-    union {
-        uint32_t data;
-        struct {
-            uint16_t length;    /* Data buffer length */
-            uint8_t cso;        /* Checksum offset */
-            uint8_t cmd;        /* Descriptor control */
-        } flags;
-    } lower;
-    union {
-        uint32_t data;
-        struct {
-            uint8_t status;     /* Descriptor status */
-            uint8_t css;        /* Checksum start */
-            uint16_t special;
-        } fields;
-    } upper;
+	u64 buffer_addr;      /* Address of the descriptor's data buffer */
+	union {
+		u32 data;
+		struct {
+			u16 length;    /* Data buffer length */
+			u8 cso;        /* Checksum offset */
+			u8 cmd;        /* Descriptor control */
+		} flags;
+	} lower;
+	union {
+		u32 data;
+		struct {
+			u8 status;     /* Descriptor status */
+			u8 css;        /* Checksum start */
+			u16 special;
+		} fields;
+	} upper;
 };
 
-/* Transmit Descriptor bit definitions */
-#define E1000_TXD_DTYP_D     0x00100000 /* Data Descriptor */
-#define E1000_TXD_DTYP_C     0x00000000 /* Context Descriptor */
-#define E1000_TXD_POPTS_IXSM 0x01       /* Insert IP checksum */
-#define E1000_TXD_POPTS_TXSM 0x02       /* Insert TCP/UDP checksum */
-#define E1000_TXD_CMD_EOP    0x01000000 /* End of Packet */
-#define E1000_TXD_CMD_IFCS   0x02000000 /* Insert FCS (Ethernet CRC) */
-#define E1000_TXD_CMD_IC     0x04000000 /* Insert Checksum */
-#define E1000_TXD_CMD_RS     0x08000000 /* Report Status */
-#define E1000_TXD_CMD_RPS    0x10000000 /* Report Packet Sent */
-#define E1000_TXD_CMD_DEXT   0x20000000 /* Descriptor extension (0 = legacy) */
-#define E1000_TXD_CMD_VLE    0x40000000 /* Add VLAN tag */
-#define E1000_TXD_CMD_IDE    0x80000000 /* Enable Tidv register */
-#define E1000_TXD_STAT_DD    0x00000001 /* Descriptor Done */
-#define E1000_TXD_STAT_EC    0x00000002 /* Excess Collisions */
-#define E1000_TXD_STAT_LC    0x00000004 /* Late Collisions */
-#define E1000_TXD_STAT_TU    0x00000008 /* Transmit underrun */
-#define E1000_TXD_CMD_TCP    0x01000000 /* TCP packet */
-#define E1000_TXD_CMD_IP     0x02000000 /* IP packet */
-#define E1000_TXD_CMD_TSE    0x04000000 /* TCP Seg enable */
-#define E1000_TXD_STAT_TC    0x00000004 /* Tx Underrun */
-
 /* Offload Context Descriptor */
 struct e1000_context_desc {
-    union {
-        uint32_t ip_config;
-        struct {
-            uint8_t ipcss;      /* IP checksum start */
-            uint8_t ipcso;      /* IP checksum offset */
-            uint16_t ipcse;     /* IP checksum end */
-        } ip_fields;
-    } lower_setup;
-    union {
-        uint32_t tcp_config;
-        struct {
-            uint8_t tucss;      /* TCP checksum start */
-            uint8_t tucso;      /* TCP checksum offset */
-            uint16_t tucse;     /* TCP checksum end */
-        } tcp_fields;
-    } upper_setup;
-    uint32_t cmd_and_length;    /* */
-    union {
-        uint32_t data;
-        struct {
-            uint8_t status;     /* Descriptor status */
-            uint8_t hdr_len;    /* Header length */
-            uint16_t mss;       /* Maximum segment size */
-        } fields;
-    } tcp_seg_setup;
+	union {
+		u32 ip_config;
+		struct {
+			u8 ipcss;      /* IP checksum start */
+			u8 ipcso;      /* IP checksum offset */
+			u16 ipcse;     /* IP checksum end */
+		} ip_fields;
+	} lower_setup;
+	union {
+		u32 tcp_config;
+		struct {
+			u8 tucss;      /* TCP checksum start */
+			u8 tucso;      /* TCP checksum offset */
+			u16 tucse;     /* TCP checksum end */
+		} tcp_fields;
+	} upper_setup;
+	u32 cmd_and_length;
+	union {
+		u32 data;
+		struct {
+			u8 status;     /* Descriptor status */
+			u8 hdr_len;    /* Header length */
+			u16 mss;       /* Maximum segment size */
+		} fields;
+	} tcp_seg_setup;
 };
 
 /* Offload data descriptor */
 struct e1000_data_desc {
-    uint64_t buffer_addr;       /* Address of the descriptor's buffer address */
-    union {
-        uint32_t data;
-        struct {
-            uint16_t length;    /* Data buffer length */
-            uint8_t typ_len_ext;        /* */
-            uint8_t cmd;        /* */
-        } flags;
-    } lower;
-    union {
-        uint32_t data;
-        struct {
-            uint8_t status;     /* Descriptor status */
-            uint8_t popts;      /* Packet Options */
-            uint16_t special;   /* */
-        } fields;
-    } upper;
+	u64 buffer_addr;   /* Address of the descriptor's buffer address */
+	union {
+		u32 data;
+		struct {
+			u16 length;    /* Data buffer length */
+			u8 typ_len_ext;
+			u8 cmd;
+		} flags;
+	} lower;
+	union {
+		u32 data;
+		struct {
+			u8 status;     /* Descriptor status */
+			u8 popts;      /* Packet Options */
+			u16 special;   /* */
+		} fields;
+	} upper;
 };
 
-/* Filters */
-#define E1000_NUM_UNICAST          16   /* Unicast filter entries */
-#define E1000_MC_TBL_SIZE          128  /* Multicast Filter Table (4096 bits) */
-#define E1000_VLAN_FILTER_TBL_SIZE 128  /* VLAN Filter Table (4096 bits) */
+/* Statistics counters collected by the MAC */
+struct e1000_hw_stats {
+	u64 crcerrs;
+	u64 algnerrc;
+	u64 symerrs;
+	u64 rxerrc;
+	u64 mpc;
+	u64 scc;
+	u64 ecol;
+	u64 mcc;
+	u64 latecol;
+	u64 colc;
+	u64 dc;
+	u64 tncrs;
+	u64 sec;
+	u64 cexterr;
+	u64 rlec;
+	u64 xonrxc;
+	u64 xontxc;
+	u64 xoffrxc;
+	u64 xofftxc;
+	u64 fcruc;
+	u64 prc64;
+	u64 prc127;
+	u64 prc255;
+	u64 prc511;
+	u64 prc1023;
+	u64 prc1522;
+	u64 gprc;
+	u64 bprc;
+	u64 mprc;
+	u64 gptc;
+	u64 gorcl;
+	u64 gorch;
+	u64 gotcl;
+	u64 gotch;
+	u64 rnbc;
+	u64 ruc;
+	u64 rfc;
+	u64 roc;
+	u64 rjc;
+	u64 mgprc;
+	u64 mgpdc;
+	u64 mgptc;
+	u64 torl;
+	u64 torh;
+	u64 totl;
+	u64 toth;
+	u64 tpr;
+	u64 tpt;
+	u64 ptc64;
+	u64 ptc127;
+	u64 ptc255;
+	u64 ptc511;
+	u64 ptc1023;
+	u64 ptc1522;
+	u64 mptc;
+	u64 bptc;
+	u64 tsctc;
+	u64 tsctfc;
+	u64 iac;
+	u64 icrxptc;
+	u64 icrxatc;
+	u64 ictxptc;
+	u64 ictxatc;
+	u64 ictxqec;
+	u64 ictxqmtc;
+	u64 icrxdmtc;
+	u64 icrxoc;
+};
 
+struct e1000_phy_stats {
+	u32 idle_errors;
+	u32 receive_errors;
+};
 
-/* Receive Address Register */
-struct e1000_rar {
-    volatile uint32_t low;      /* receive address low */
-    volatile uint32_t high;     /* receive address high */
+struct e1000_host_mng_dhcp_cookie {
+	u32 signature;
+	u8  status;
+	u8  reserved0;
+	u16 vlan_id;
+	u32 reserved1;
+	u16 reserved2;
+	u8  reserved3;
+	u8  checksum;
+};
+
+/* Host Interface "Rev 1" */
+struct e1000_host_command_header {
+	u8 command_id;
+	u8 command_length;
+	u8 command_options;
+	u8 checksum;
+};
+
+#define E1000_HI_MAX_DATA_LENGTH     252
+struct e1000_host_command_info {
+	struct e1000_host_command_header command_header;
+	u8 command_data[E1000_HI_MAX_DATA_LENGTH];
+};
+
+/* Host Interface "Rev 2" */
+struct e1000_host_mng_command_header {
+	u8  command_id;
+	u8  checksum;
+	u16 reserved1;
+	u16 reserved2;
+	u16 command_length;
+};
+
+#define E1000_HI_MAX_MNG_DATA_LENGTH 0x6F8
+struct e1000_host_mng_command_info {
+	struct e1000_host_mng_command_header command_header;
+	u8 command_data[E1000_HI_MAX_MNG_DATA_LENGTH];
+};
+
+#include "e1000_mac.h"
+#include "e1000_phy.h"
+#include "e1000_nvm.h"
+#include "e1000_manage.h"
+
+struct e1000_functions {
+	/* Function pointers for the MAC. */
+	s32       (*init_mac_params)(struct e1000_hw *);
+	s32       (*blink_led)(struct e1000_hw *);
+	s32       (*check_for_link)(struct e1000_hw *);
+	boolean_t (*check_mng_mode)(struct e1000_hw *hw);
+	s32       (*cleanup_led)(struct e1000_hw *);
+	void      (*clear_hw_cntrs)(struct e1000_hw *);
+	void      (*clear_vfta)(struct e1000_hw *);
+	s32       (*get_bus_info)(struct e1000_hw *);
+	s32       (*get_link_up_info)(struct e1000_hw *, u16 *, u16 *);
+	s32       (*led_on)(struct e1000_hw *);
+	s32       (*led_off)(struct e1000_hw *);
+	void      (*mc_addr_list_update)(struct e1000_hw *, u8 *, u32, u32,
+	                                 u32);
+	void      (*remove_device)(struct e1000_hw *);
+	s32       (*reset_hw)(struct e1000_hw *);
+	s32       (*init_hw)(struct e1000_hw *);
+	s32       (*setup_link)(struct e1000_hw *);
+	s32       (*setup_physical_interface)(struct e1000_hw *);
+	s32       (*setup_led)(struct e1000_hw *);
+	void      (*write_vfta)(struct e1000_hw *, u32, u32);
+	void      (*mta_set)(struct e1000_hw *, u32);
+	void      (*config_collision_dist)(struct e1000_hw*);
+	void      (*rar_set)(struct e1000_hw*, u8*, u32);
+	s32       (*validate_mdi_setting)(struct e1000_hw*);
+	s32       (*mng_host_if_write)(struct e1000_hw*, u8*, u16, u16, u8*);
+	s32       (*mng_write_cmd_header)(struct e1000_hw *hw,
+                           struct e1000_host_mng_command_header*);
+	s32       (*mng_enable_host_if)(struct e1000_hw*);
+	s32       (*wait_autoneg)(struct e1000_hw*);
+
+	/* Function pointers for the PHY. */
+	s32       (*init_phy_params)(struct e1000_hw *);
+	s32       (*acquire_phy)(struct e1000_hw *);
+	s32       (*check_polarity)(struct e1000_hw *);
+	s32       (*check_reset_block)(struct e1000_hw *);
+	s32       (*commit_phy)(struct e1000_hw *);
+	s32       (*force_speed_duplex)(struct e1000_hw *);
+	s32       (*get_cfg_done)(struct e1000_hw *hw);
+	s32       (*get_cable_length)(struct e1000_hw *);
+	s32       (*get_phy_info)(struct e1000_hw *);
+	s32       (*read_phy_reg)(struct e1000_hw *, u32, u16 *);
+	void      (*release_phy)(struct e1000_hw *);
+	s32       (*reset_phy)(struct e1000_hw *);
+	s32       (*set_d0_lplu_state)(struct e1000_hw *, boolean_t);
+	s32       (*set_d3_lplu_state)(struct e1000_hw *, boolean_t);
+	s32       (*write_phy_reg)(struct e1000_hw *, u32, u16);
+
+	/* Function pointers for the NVM. */
+	s32       (*init_nvm_params)(struct e1000_hw *);
+	s32       (*acquire_nvm)(struct e1000_hw *);
+	s32       (*read_nvm)(struct e1000_hw *, u16, u16, u16 *);
+	void      (*release_nvm)(struct e1000_hw *);
+	void      (*reload_nvm)(struct e1000_hw *);
+	s32       (*update_nvm)(struct e1000_hw *);
+	s32       (*valid_led_default)(struct e1000_hw *, u16 *);
+	s32       (*validate_nvm)(struct e1000_hw *);
+	s32       (*write_nvm)(struct e1000_hw *, u16, u16, u16 *);
+};
+
+struct e1000_mac_info {
+	u8 addr[6];
+	u8 perm_addr[6];
+
+	e1000_mac_type type;
+	e1000_fc_mode  fc;
+	e1000_fc_mode  original_fc;
+
+	u32 collision_delta;
+	u32 ledctl_default;
+	u32 ledctl_mode1;
+	u32 ledctl_mode2;
+	u32 max_frame_size;
+	u32 mc_filter_type;
+	u32 min_frame_size;
+	u32 tx_packet_delta;
+	u32 txcw;
+
+	u16 current_ifs_val;
+	u16 ifs_max_val;
+	u16 ifs_min_val;
+	u16 ifs_ratio;
+	u16 ifs_step_size;
+	u16 mta_reg_count;
+	u16 rar_entry_count;
+	u16 fc_high_water;
+	u16 fc_low_water;
+	u16 fc_pause_time;
+
+	u8  forced_speed_duplex;
+
+	boolean_t adaptive_ifs;
+	boolean_t arc_subsystem_valid;
+	boolean_t asf_firmware_present;
+	boolean_t autoneg;
+	boolean_t autoneg_failed;
+	boolean_t disable_av;
+	boolean_t disable_hw_init_bits;
+	boolean_t fc_send_xon;
+	boolean_t fc_strict_ieee;
+	boolean_t get_link_status;
+	boolean_t ifs_params_forced;
+	boolean_t in_ifs_mode;
+	boolean_t report_tx_early;
+	boolean_t serdes_has_link;
+	boolean_t tx_pkt_filtering;
 };
 
-/* Number of entries in the Multicast Table Array (MTA). */
-#define E1000_NUM_MTA_REGISTERS 128
+struct e1000_phy_info {
+	e1000_phy_type type;
 
-/* IPv4 Address Table Entry */
-struct e1000_ipv4_at_entry {
-    volatile uint32_t ipv4_addr;        /* IP Address (RW) */
-    volatile uint32_t reserved;
-};
+	e1000_1000t_rx_status local_rx;
+	e1000_1000t_rx_status remote_rx;
+	e1000_ms_type ms_type;
+	e1000_ms_type original_ms_type;
+	e1000_rev_polarity cable_polarity;
+	e1000_smart_speed smart_speed;
+
+	u32 addr;
+	u32 id;
+	u32 reset_delay_us; /* in usec */
+	u32 revision;
+
+	u16 autoneg_advertised;
+	u16 autoneg_mask;
+	u16 cable_length;
+	u16 max_cable_length;
+	u16 min_cable_length;
+
+	u8 mdix;
+
+	boolean_t disable_polarity_correction;
+	boolean_t is_mdix;
+	boolean_t polarity_correction;
+	boolean_t reset_disable;
+	boolean_t speed_downgraded;
+	boolean_t wait_for_link;
+};
+
+struct e1000_nvm_info {
+	e1000_nvm_type type;
+	e1000_nvm_override override;
+
+	u32 flash_bank_size;
+	u32 flash_base_addr;
+
+	u16 word_size;
+	u16 delay_usec;
+	u16 address_bits;
+	u16 opcode_bits;
+	u16 page_size;
+};
+
+struct e1000_bus_info {
+	e1000_bus_type type;
+	e1000_bus_speed speed;
+	e1000_bus_width width;
 
-/* Four wakeup IP addresses are supported */
-#define E1000_WAKEUP_IP_ADDRESS_COUNT_MAX 4
-#define E1000_IP4AT_SIZE                  E1000_WAKEUP_IP_ADDRESS_COUNT_MAX
-#define E1000_IP6AT_SIZE                  1
-
-/* IPv6 Address Table Entry */
-struct e1000_ipv6_at_entry {
-    volatile uint8_t ipv6_addr[16];
-};
+	u32 snoop;
 
-/* Flexible Filter Length Table Entry */
-struct e1000_fflt_entry {
-    volatile uint32_t length;   /* Flexible Filter Length (RW) */
-    volatile uint32_t reserved;
+	u16 func;
+	u16 pci_cmd_word;
 };
 
-/* Flexible Filter Mask Table Entry */
-struct e1000_ffmt_entry {
-    volatile uint32_t mask;     /* Flexible Filter Mask (RW) */
-    volatile uint32_t reserved;
-};
+struct e1000_hw {
+	void *back;
+	void *dev_spec;
 
-/* Flexible Filter Value Table Entry */
-struct e1000_ffvt_entry {
-    volatile uint32_t value;    /* Flexible Filter Value (RW) */
-    volatile uint32_t reserved;
-};
+	u8 *hw_addr;
+	u8 *flash_address;
+	unsigned long io_base;
 
-/* Four Flexible Filters are supported */
-#define E1000_FLEXIBLE_FILTER_COUNT_MAX 4
+	struct e1000_functions func;
+	struct e1000_mac_info  mac;
+	struct e1000_phy_info  phy;
+	struct e1000_nvm_info  nvm;
+	struct e1000_bus_info  bus;
+	struct e1000_host_mng_dhcp_cookie mng_cookie;
 
-/* Each Flexible Filter is at most 128 (0x80) bytes in length */
-#define E1000_FLEXIBLE_FILTER_SIZE_MAX  128
+	e1000_media_type media_type;
 
-#define E1000_FFLT_SIZE E1000_FLEXIBLE_FILTER_COUNT_MAX
-#define E1000_FFMT_SIZE E1000_FLEXIBLE_FILTER_SIZE_MAX
-#define E1000_FFVT_SIZE E1000_FLEXIBLE_FILTER_SIZE_MAX
-
-/* Register Set. (82543, 82544)
- *
- * Registers are defined to be 32 bits and  should be accessed as 32 bit values.
- * These registers are physically located on the NIC, but are mapped into the
- * host memory address space.
- *
- * RW - register is both readable and writable
- * RO - register is read only
- * WO - register is write only
- * R/clr - register is read only and is cleared when read
- * A - register array
- */
-#define E1000_CTRL     0x00000  /* Device Control - RW */
-#define E1000_CTRL_DUP 0x00004  /* Device Control Duplicate (Shadow) - RW */
-#define E1000_STATUS   0x00008  /* Device Status - RO */
-#define E1000_EECD     0x00010  /* EEPROM/Flash Control - RW */
-#define E1000_EERD     0x00014  /* EEPROM Read - RW */
-#define E1000_CTRL_EXT 0x00018  /* Extended Device Control - RW */
-#define E1000_FLA      0x0001C  /* Flash Access - RW */
-#define E1000_MDIC     0x00020  /* MDI Control - RW */
-#define E1000_FCAL     0x00028  /* Flow Control Address Low - RW */
-#define E1000_FCAH     0x0002C  /* Flow Control Address High -RW */
-#define E1000_FCT      0x00030  /* Flow Control Type - RW */
-#define E1000_VET      0x00038  /* VLAN Ether Type - RW */
-#define E1000_ICR      0x000C0  /* Interrupt Cause Read - R/clr */
-#define E1000_ITR      0x000C4  /* Interrupt Throttling Rate - RW */
-#define E1000_ICS      0x000C8  /* Interrupt Cause Set - WO */
-#define E1000_IMS      0x000D0  /* Interrupt Mask Set - RW */
-#define E1000_IMC      0x000D8  /* Interrupt Mask Clear - WO */
-#define E1000_RCTL     0x00100  /* RX Control - RW */
-#define E1000_FCTTV    0x00170  /* Flow Control Transmit Timer Value - RW */
-#define E1000_TXCW     0x00178  /* TX Configuration Word - RW */
-#define E1000_RXCW     0x00180  /* RX Configuration Word - RO */
-#define E1000_TCTL     0x00400  /* TX Control - RW */
-#define E1000_TIPG     0x00410  /* TX Inter-packet gap -RW */
-#define E1000_TBT      0x00448  /* TX Burst Timer - RW */
-#define E1000_AIT      0x00458  /* Adaptive Interframe Spacing Throttle - RW */
-#define E1000_LEDCTL   0x00E00  /* LED Control - RW */
-#define E1000_PBA      0x01000  /* Packet Buffer Allocation - RW */
-#define E1000_FCRTL    0x02160  /* Flow Control Receive Threshold Low - RW */
-#define E1000_FCRTH    0x02168  /* Flow Control Receive Threshold High - RW */
-#define E1000_RDBAL    0x02800  /* RX Descriptor Base Address Low - RW */
-#define E1000_RDBAH    0x02804  /* RX Descriptor Base Address High - RW */
-#define E1000_RDLEN    0x02808  /* RX Descriptor Length - RW */
-#define E1000_RDH      0x02810  /* RX Descriptor Head - RW */
-#define E1000_RDT      0x02818  /* RX Descriptor Tail - RW */
-#define E1000_RDTR     0x02820  /* RX Delay Timer - RW */
-#define E1000_RXDCTL   0x02828  /* RX Descriptor Control - RW */
-#define E1000_RADV     0x0282C  /* RX Interrupt Absolute Delay Timer - RW */
-#define E1000_RSRPD    0x02C00  /* RX Small Packet Detect - RW */
-#define E1000_TXDMAC   0x03000  /* TX DMA Control - RW */
-#define E1000_TDFH     0x03410  /* TX Data FIFO Head - RW */
-#define E1000_TDFT     0x03418  /* TX Data FIFO Tail - RW */
-#define E1000_TDFHS    0x03420  /* TX Data FIFO Head Saved - RW */
-#define E1000_TDFTS    0x03428  /* TX Data FIFO Tail Saved - RW */
-#define E1000_TDFPC    0x03430  /* TX Data FIFO Packet Count - RW */
-#define E1000_TDBAL    0x03800  /* TX Descriptor Base Address Low - RW */
-#define E1000_TDBAH    0x03804  /* TX Descriptor Base Address High - RW */
-#define E1000_TDLEN    0x03808  /* TX Descriptor Length - RW */
-#define E1000_TDH      0x03810  /* TX Descriptor Head - RW */
-#define E1000_TDT      0x03818  /* TX Descripotr Tail - RW */
-#define E1000_TIDV     0x03820  /* TX Interrupt Delay Value - RW */
-#define E1000_TXDCTL   0x03828  /* TX Descriptor Control - RW */
-#define E1000_TADV     0x0382C  /* TX Interrupt Absolute Delay Val - RW */
-#define E1000_TSPMT    0x03830  /* TCP Segmentation PAD & Min Threshold - RW */
-#define E1000_CRCERRS  0x04000  /* CRC Error Count - R/clr */
-#define E1000_ALGNERRC 0x04004  /* Alignment Error Count - R/clr */
-#define E1000_SYMERRS  0x04008  /* Symbol Error Count - R/clr */
-#define E1000_RXERRC   0x0400C  /* Receive Error Count - R/clr */
-#define E1000_MPC      0x04010  /* Missed Packet Count - R/clr */
-#define E1000_SCC      0x04014  /* Single Collision Count - R/clr */
-#define E1000_ECOL     0x04018  /* Excessive Collision Count - R/clr */
-#define E1000_MCC      0x0401C  /* Multiple Collision Count - R/clr */
-#define E1000_LATECOL  0x04020  /* Late Collision Count - R/clr */
-#define E1000_COLC     0x04028  /* Collision Count - R/clr */
-#define E1000_DC       0x04030  /* Defer Count - R/clr */
-#define E1000_TNCRS    0x04034  /* TX-No CRS - R/clr */
-#define E1000_SEC      0x04038  /* Sequence Error Count - R/clr */
-#define E1000_CEXTERR  0x0403C  /* Carrier Extension Error Count - R/clr */
-#define E1000_RLEC     0x04040  /* Receive Length Error Count - R/clr */
-#define E1000_XONRXC   0x04048  /* XON RX Count - R/clr */
-#define E1000_XONTXC   0x0404C  /* XON TX Count - R/clr */
-#define E1000_XOFFRXC  0x04050  /* XOFF RX Count - R/clr */
-#define E1000_XOFFTXC  0x04054  /* XOFF TX Count - R/clr */
-#define E1000_FCRUC    0x04058  /* Flow Control RX Unsupported Count- R/clr */
-#define E1000_PRC64    0x0405C  /* Packets RX (64 bytes) - R/clr */
-#define E1000_PRC127   0x04060  /* Packets RX (65-127 bytes) - R/clr */
-#define E1000_PRC255   0x04064  /* Packets RX (128-255 bytes) - R/clr */
-#define E1000_PRC511   0x04068  /* Packets RX (255-511 bytes) - R/clr */
-#define E1000_PRC1023  0x0406C  /* Packets RX (512-1023 bytes) - R/clr */
-#define E1000_PRC1522  0x04070  /* Packets RX (1024-1522 bytes) - R/clr */
-#define E1000_GPRC     0x04074  /* Good Packets RX Count - R/clr */
-#define E1000_BPRC     0x04078  /* Broadcast Packets RX Count - R/clr */
-#define E1000_MPRC     0x0407C  /* Multicast Packets RX Count - R/clr */
-#define E1000_GPTC     0x04080  /* Good Packets TX Count - R/clr */
-#define E1000_GORCL    0x04088  /* Good Octets RX Count Low - R/clr */
-#define E1000_GORCH    0x0408C  /* Good Octets RX Count High - R/clr */
-#define E1000_GOTCL    0x04090  /* Good Octets TX Count Low - R/clr */
-#define E1000_GOTCH    0x04094  /* Good Octets TX Count High - R/clr */
-#define E1000_RNBC     0x040A0  /* RX No Buffers Count - R/clr */
-#define E1000_RUC      0x040A4  /* RX Undersize Count - R/clr */
-#define E1000_RFC      0x040A8  /* RX Fragment Count - R/clr */
-#define E1000_ROC      0x040AC  /* RX Oversize Count - R/clr */
-#define E1000_RJC      0x040B0  /* RX Jabber Count - R/clr */
-#define E1000_MGTPRC   0x040B4  /* Management Packets RX Count - R/clr */
-#define E1000_MGTPDC   0x040B8  /* Management Packets Dropped Count - R/clr */
-#define E1000_MGTPTC   0x040BC  /* Management Packets TX Count - R/clr */
-#define E1000_TORL     0x040C0  /* Total Octets RX Low - R/clr */
-#define E1000_TORH     0x040C4  /* Total Octets RX High - R/clr */
-#define E1000_TOTL     0x040C8  /* Total Octets TX Low - R/clr */
-#define E1000_TOTH     0x040CC  /* Total Octets TX High - R/clr */
-#define E1000_TPR      0x040D0  /* Total Packets RX - R/clr */
-#define E1000_TPT      0x040D4  /* Total Packets TX - R/clr */
-#define E1000_PTC64    0x040D8  /* Packets TX (64 bytes) - R/clr */
-#define E1000_PTC127   0x040DC  /* Packets TX (65-127 bytes) - R/clr */
-#define E1000_PTC255   0x040E0  /* Packets TX (128-255 bytes) - R/clr */
-#define E1000_PTC511   0x040E4  /* Packets TX (256-511 bytes) - R/clr */
-#define E1000_PTC1023  0x040E8  /* Packets TX (512-1023 bytes) - R/clr */
-#define E1000_PTC1522  0x040EC  /* Packets TX (1024-1522 Bytes) - R/clr */
-#define E1000_MPTC     0x040F0  /* Multicast Packets TX Count - R/clr */
-#define E1000_BPTC     0x040F4  /* Broadcast Packets TX Count - R/clr */
-#define E1000_TSCTC    0x040F8  /* TCP Segmentation Context TX - R/clr */
-#define E1000_TSCTFC   0x040FC  /* TCP Segmentation Context TX Fail - R/clr */
-#define E1000_RXCSUM   0x05000  /* RX Checksum Control - RW */
-#define E1000_MTA      0x05200  /* Multicast Table Array - RW Array */
-#define E1000_RA       0x05400  /* Receive Address - RW Array */
-#define E1000_VFTA     0x05600  /* VLAN Filter Table Array - RW Array */
-#define E1000_WUC      0x05800  /* Wakeup Control - RW */
-#define E1000_WUFC     0x05808  /* Wakeup Filter Control - RW */
-#define E1000_WUS      0x05810  /* Wakeup Status - RO */
-#define E1000_MANC     0x05820  /* Management Control - RW */
-#define E1000_IPAV     0x05838  /* IP Address Valid - RW */
-#define E1000_IP4AT    0x05840  /* IPv4 Address Table - RW Array */
-#define E1000_IP6AT    0x05880  /* IPv6 Address Table - RW Array */
-#define E1000_WUPL     0x05900  /* Wakeup Packet Length - RW */
-#define E1000_WUPM     0x05A00  /* Wakeup Packet Memory - RO A */
-#define E1000_FFLT     0x05F00  /* Flexible Filter Length Table - RW Array */
-#define E1000_HOST_IF  0x08800  /* Host Interface */
-#define E1000_FFMT     0x09000  /* Flexible Filter Mask Table - RW Array */
-#define E1000_FFVT     0x09800  /* Flexible Filter Value Table - RW Array */
-
-/* Register Set (82542)
- *
- * Some of the 82542 registers are located at different offsets than they are
- * in more current versions of the 8254x. Despite the difference in location,
- * the registers function in the same manner.
- */
-#define E1000_82542_CTRL     E1000_CTRL
-#define E1000_82542_CTRL_DUP E1000_CTRL_DUP
-#define E1000_82542_STATUS   E1000_STATUS
-#define E1000_82542_EECD     E1000_EECD
-#define E1000_82542_EERD     E1000_EERD
-#define E1000_82542_CTRL_EXT E1000_CTRL_EXT
-#define E1000_82542_FLA      E1000_FLA
-#define E1000_82542_MDIC     E1000_MDIC
-#define E1000_82542_FCAL     E1000_FCAL
-#define E1000_82542_FCAH     E1000_FCAH
-#define E1000_82542_FCT      E1000_FCT
-#define E1000_82542_VET      E1000_VET
-#define E1000_82542_RA       0x00040
-#define E1000_82542_ICR      E1000_ICR
-#define E1000_82542_ITR      E1000_ITR
-#define E1000_82542_ICS      E1000_ICS
-#define E1000_82542_IMS      E1000_IMS
-#define E1000_82542_IMC      E1000_IMC
-#define E1000_82542_RCTL     E1000_RCTL
-#define E1000_82542_RDTR     0x00108
-#define E1000_82542_RDBAL    0x00110
-#define E1000_82542_RDBAH    0x00114
-#define E1000_82542_RDLEN    0x00118
-#define E1000_82542_RDH      0x00120
-#define E1000_82542_RDT      0x00128
-#define E1000_82542_FCRTH    0x00160
-#define E1000_82542_FCRTL    0x00168
-#define E1000_82542_FCTTV    E1000_FCTTV
-#define E1000_82542_TXCW     E1000_TXCW
-#define E1000_82542_RXCW     E1000_RXCW
-#define E1000_82542_MTA      0x00200
-#define E1000_82542_TCTL     E1000_TCTL
-#define E1000_82542_TIPG     E1000_TIPG
-#define E1000_82542_TDBAL    0x00420
-#define E1000_82542_TDBAH    0x00424
-#define E1000_82542_TDLEN    0x00428
-#define E1000_82542_TDH      0x00430
-#define E1000_82542_TDT      0x00438
-#define E1000_82542_TIDV     0x00440
-#define E1000_82542_TBT      E1000_TBT
-#define E1000_82542_AIT      E1000_AIT
-#define E1000_82542_VFTA     0x00600
-#define E1000_82542_LEDCTL   E1000_LEDCTL
-#define E1000_82542_PBA      E1000_PBA
-#define E1000_82542_RXDCTL   E1000_RXDCTL
-#define E1000_82542_RADV     E1000_RADV
-#define E1000_82542_RSRPD    E1000_RSRPD
-#define E1000_82542_TXDMAC   E1000_TXDMAC
-#define E1000_82542_TDFHS    E1000_TDFHS
-#define E1000_82542_TDFTS    E1000_TDFTS
-#define E1000_82542_TDFPC    E1000_TDFPC
-#define E1000_82542_TXDCTL   E1000_TXDCTL
-#define E1000_82542_TADV     E1000_TADV
-#define E1000_82542_TSPMT    E1000_TSPMT
-#define E1000_82542_CRCERRS  E1000_CRCERRS
-#define E1000_82542_ALGNERRC E1000_ALGNERRC
-#define E1000_82542_SYMERRS  E1000_SYMERRS
-#define E1000_82542_RXERRC   E1000_RXERRC
-#define E1000_82542_MPC      E1000_MPC
-#define E1000_82542_SCC      E1000_SCC
-#define E1000_82542_ECOL     E1000_ECOL
-#define E1000_82542_MCC      E1000_MCC
-#define E1000_82542_LATECOL  E1000_LATECOL
-#define E1000_82542_COLC     E1000_COLC
-#define E1000_82542_DC       E1000_DC
-#define E1000_82542_TNCRS    E1000_TNCRS
-#define E1000_82542_SEC      E1000_SEC
-#define E1000_82542_CEXTERR  E1000_CEXTERR
-#define E1000_82542_RLEC     E1000_RLEC
-#define E1000_82542_XONRXC   E1000_XONRXC
-#define E1000_82542_XONTXC   E1000_XONTXC
-#define E1000_82542_XOFFRXC  E1000_XOFFRXC
-#define E1000_82542_XOFFTXC  E1000_XOFFTXC
-#define E1000_82542_FCRUC    E1000_FCRUC
-#define E1000_82542_PRC64    E1000_PRC64
-#define E1000_82542_PRC127   E1000_PRC127
-#define E1000_82542_PRC255   E1000_PRC255
-#define E1000_82542_PRC511   E1000_PRC511
-#define E1000_82542_PRC1023  E1000_PRC1023
-#define E1000_82542_PRC1522  E1000_PRC1522
-#define E1000_82542_GPRC     E1000_GPRC
-#define E1000_82542_BPRC     E1000_BPRC
-#define E1000_82542_MPRC     E1000_MPRC
-#define E1000_82542_GPTC     E1000_GPTC
-#define E1000_82542_GORCL    E1000_GORCL
-#define E1000_82542_GORCH    E1000_GORCH
-#define E1000_82542_GOTCL    E1000_GOTCL
-#define E1000_82542_GOTCH    E1000_GOTCH
-#define E1000_82542_RNBC     E1000_RNBC
-#define E1000_82542_RUC      E1000_RUC
-#define E1000_82542_RFC      E1000_RFC
-#define E1000_82542_ROC      E1000_ROC
-#define E1000_82542_RJC      E1000_RJC
-#define E1000_82542_MGTPRC   E1000_MGTPRC
-#define E1000_82542_MGTPDC   E1000_MGTPDC
-#define E1000_82542_MGTPTC   E1000_MGTPTC
-#define E1000_82542_TORL     E1000_TORL
-#define E1000_82542_TORH     E1000_TORH
-#define E1000_82542_TOTL     E1000_TOTL
-#define E1000_82542_TOTH     E1000_TOTH
-#define E1000_82542_TPR      E1000_TPR
-#define E1000_82542_TPT      E1000_TPT
-#define E1000_82542_PTC64    E1000_PTC64
-#define E1000_82542_PTC127   E1000_PTC127
-#define E1000_82542_PTC255   E1000_PTC255
-#define E1000_82542_PTC511   E1000_PTC511
-#define E1000_82542_PTC1023  E1000_PTC1023
-#define E1000_82542_PTC1522  E1000_PTC1522
-#define E1000_82542_MPTC     E1000_MPTC
-#define E1000_82542_BPTC     E1000_BPTC
-#define E1000_82542_TSCTC    E1000_TSCTC
-#define E1000_82542_TSCTFC   E1000_TSCTFC
-#define E1000_82542_RXCSUM   E1000_RXCSUM
-#define E1000_82542_WUC      E1000_WUC
-#define E1000_82542_WUFC     E1000_WUFC
-#define E1000_82542_WUS      E1000_WUS
-#define E1000_82542_MANC     E1000_MANC
-#define E1000_82542_IPAV     E1000_IPAV
-#define E1000_82542_IP4AT    E1000_IP4AT
-#define E1000_82542_IP6AT    E1000_IP6AT
-#define E1000_82542_WUPL     E1000_WUPL
-#define E1000_82542_WUPM     E1000_WUPM
-#define E1000_82542_FFLT     E1000_FFLT
-#define E1000_82542_TDFH     0x08010
-#define E1000_82542_TDFT     0x08018
-#define E1000_82542_FFMT     E1000_FFMT
-#define E1000_82542_FFVT     E1000_FFVT
-#define E1000_82542_HOST_IF  E1000_HOST_IF
+	u32 dev_spec_size;
 
-/* Statistics counters collected by the MAC */
-struct e1000_hw_stats {
-    uint64_t crcerrs;
-    uint64_t algnerrc;
-    uint64_t symerrs;
-    uint64_t rxerrc;
-    uint64_t mpc;
-    uint64_t scc;
-    uint64_t ecol;
-    uint64_t mcc;
-    uint64_t latecol;
-    uint64_t colc;
-    uint64_t dc;
-    uint64_t tncrs;
-    uint64_t sec;
-    uint64_t cexterr;
-    uint64_t rlec;
-    uint64_t xonrxc;
-    uint64_t xontxc;
-    uint64_t xoffrxc;
-    uint64_t xofftxc;
-    uint64_t fcruc;
-    uint64_t prc64;
-    uint64_t prc127;
-    uint64_t prc255;
-    uint64_t prc511;
-    uint64_t prc1023;
-    uint64_t prc1522;
-    uint64_t gprc;
-    uint64_t bprc;
-    uint64_t mprc;
-    uint64_t gptc;
-    uint64_t gorcl;
-    uint64_t gorch;
-    uint64_t gotcl;
-    uint64_t gotch;
-    uint64_t rnbc;
-    uint64_t ruc;
-    uint64_t rfc;
-    uint64_t roc;
-    uint64_t rjc;
-    uint64_t mgprc;
-    uint64_t mgpdc;
-    uint64_t mgptc;
-    uint64_t torl;
-    uint64_t torh;
-    uint64_t totl;
-    uint64_t toth;
-    uint64_t tpr;
-    uint64_t tpt;
-    uint64_t ptc64;
-    uint64_t ptc127;
-    uint64_t ptc255;
-    uint64_t ptc511;
-    uint64_t ptc1023;
-    uint64_t ptc1522;
-    uint64_t mptc;
-    uint64_t bptc;
-    uint64_t tsctc;
-    uint64_t tsctfc;
-};
+	u16 device_id;
+	u16 subsystem_vendor_id;
+	u16 subsystem_device_id;
+	u16 vendor_id;
 
-/* Structure containing variables used by the shared code (e1000_hw.c) */
-struct e1000_hw {
-    uint8_t *hw_addr;
-    e1000_mac_type mac_type;
-    e1000_phy_type phy_type;
-    uint32_t phy_init_script;
-    e1000_media_type media_type;
-    void *back;
-    e1000_fc_type fc;
-    e1000_bus_speed bus_speed;
-    e1000_bus_width bus_width;
-    e1000_bus_type bus_type;
-    struct e1000_eeprom_info eeprom;
-    e1000_ms_type master_slave;
-    e1000_ms_type original_master_slave;
-    e1000_ffe_config ffe_config_state;
-    uint32_t asf_firmware_present;
-    unsigned long io_base;
-    uint32_t phy_id;
-    uint32_t phy_revision;
-    uint32_t phy_addr;
-    uint32_t original_fc;
-    uint32_t txcw;
-    uint32_t autoneg_failed;
-    uint32_t max_frame_size;
-    uint32_t min_frame_size;
-    uint32_t mc_filter_type;
-    uint32_t num_mc_addrs;
-    uint32_t collision_delta;
-    uint32_t tx_packet_delta;
-    uint32_t ledctl_default;
-    uint32_t ledctl_mode1;
-    uint32_t ledctl_mode2;
-    uint16_t phy_spd_default;
-    uint16_t autoneg_advertised;
-    uint16_t pci_cmd_word;
-    uint16_t fc_high_water;
-    uint16_t fc_low_water;
-    uint16_t fc_pause_time;
-    uint16_t current_ifs_val;
-    uint16_t ifs_min_val;
-    uint16_t ifs_max_val;
-    uint16_t ifs_step_size;
-    uint16_t ifs_ratio;
-    uint16_t device_id;
-    uint16_t vendor_id;
-    uint16_t subsystem_id;
-    uint16_t subsystem_vendor_id;
-    uint8_t revision_id;
-    uint8_t autoneg;
-    uint8_t mdix;
-    uint8_t forced_speed_duplex;
-    uint8_t wait_autoneg_complete;
-    uint8_t dma_fairness;
-    uint8_t mac_addr[NODE_ADDRESS_SIZE];
-    uint8_t perm_mac_addr[NODE_ADDRESS_SIZE];
-    boolean_t disable_polarity_correction;
-    boolean_t speed_downgraded;
-    e1000_dsp_config dsp_config_state;
-    boolean_t get_link_status;
-    boolean_t serdes_link_down;
-    boolean_t tbi_compatibility_en;
-    boolean_t tbi_compatibility_on;
-    boolean_t phy_reset_disable;
-    boolean_t fc_send_xon;
-    boolean_t fc_strict_ieee;
-    boolean_t report_tx_early;
-    boolean_t adaptive_ifs;
-    boolean_t ifs_params_forced;
-    boolean_t in_ifs_mode;
+	u8  revision_id;
 };
 
+/* These functions must be implemented by drivers */
+void e1000_pci_clear_mwi(struct e1000_hw *hw);
+void e1000_pci_set_mwi(struct e1000_hw *hw);
+s32  e1000_alloc_zeroed_dev_spec_struct(struct e1000_hw *hw, u32 size);
+s32  e1000_read_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value);
+void e1000_free_dev_spec_struct(struct e1000_hw *hw);
+void e1000_read_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value);
+void e1000_write_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value);
 
-#define E1000_EEPROM_SWDPIN0   0x0001   /* SWDPIN 0 EEPROM Value */
-#define E1000_EEPROM_LED_LOGIC 0x0020   /* Led Logic Word */
-/* Register Bit Masks */
-/* Device Control */
-#define E1000_CTRL_FD       0x00000001  /* Full duplex.0=half; 1=full */
-#define E1000_CTRL_BEM      0x00000002  /* Endian Mode.0=little,1=big */
-#define E1000_CTRL_PRIOR    0x00000004  /* Priority on PCI. 0=rx,1=fair */
-#define E1000_CTRL_LRST     0x00000008  /* Link reset. 0=normal,1=reset */
-#define E1000_CTRL_TME      0x00000010  /* Test mode. 0=normal,1=test */
-#define E1000_CTRL_SLE      0x00000020  /* Serial Link on 0=dis,1=en */
-#define E1000_CTRL_ASDE     0x00000020  /* Auto-speed detect enable */
-#define E1000_CTRL_SLU      0x00000040  /* Set link up (Force Link) */
-#define E1000_CTRL_ILOS     0x00000080  /* Invert Loss-Of Signal */
-#define E1000_CTRL_SPD_SEL  0x00000300  /* Speed Select Mask */
-#define E1000_CTRL_SPD_10   0x00000000  /* Force 10Mb */
-#define E1000_CTRL_SPD_100  0x00000100  /* Force 100Mb */
-#define E1000_CTRL_SPD_1000 0x00000200  /* Force 1Gb */
-#define E1000_CTRL_BEM32    0x00000400  /* Big Endian 32 mode */
-#define E1000_CTRL_FRCSPD   0x00000800  /* Force Speed */
-#define E1000_CTRL_FRCDPX   0x00001000  /* Force Duplex */
-#define E1000_CTRL_SWDPIN0  0x00040000  /* SWDPIN 0 value */
-#define E1000_CTRL_SWDPIN1  0x00080000  /* SWDPIN 1 value */
-#define E1000_CTRL_SWDPIN2  0x00100000  /* SWDPIN 2 value */
-#define E1000_CTRL_SWDPIN3  0x00200000  /* SWDPIN 3 value */
-#define E1000_CTRL_SWDPIO0  0x00400000  /* SWDPIN 0 Input or output */
-#define E1000_CTRL_SWDPIO1  0x00800000  /* SWDPIN 1 input or output */
-#define E1000_CTRL_SWDPIO2  0x01000000  /* SWDPIN 2 input or output */
-#define E1000_CTRL_SWDPIO3  0x02000000  /* SWDPIN 3 input or output */
-#define E1000_CTRL_RST      0x04000000  /* Global reset */
-#define E1000_CTRL_RFCE     0x08000000  /* Receive Flow Control enable */
-#define E1000_CTRL_TFCE     0x10000000  /* Transmit flow control enable */
-#define E1000_CTRL_RTE      0x20000000  /* Routing tag enable */
-#define E1000_CTRL_VME      0x40000000  /* IEEE VLAN mode enable */
-#define E1000_CTRL_PHY_RST  0x80000000  /* PHY Reset */
-
-/* Device Status */
-#define E1000_STATUS_FD         0x00000001      /* Full duplex.0=half,1=full */
-#define E1000_STATUS_LU         0x00000002      /* Link up.0=no,1=link */
-#define E1000_STATUS_FUNC_MASK  0x0000000C      /* PCI Function Mask */
-#define E1000_STATUS_FUNC_0     0x00000000      /* Function 0 */
-#define E1000_STATUS_FUNC_1     0x00000004      /* Function 1 */
-#define E1000_STATUS_TXOFF      0x00000010      /* transmission paused */
-#define E1000_STATUS_TBIMODE    0x00000020      /* TBI mode */
-#define E1000_STATUS_SPEED_MASK 0x000000C0
-#define E1000_STATUS_SPEED_10   0x00000000      /* Speed 10Mb/s */
-#define E1000_STATUS_SPEED_100  0x00000040      /* Speed 100Mb/s */
-#define E1000_STATUS_SPEED_1000 0x00000080      /* Speed 1000Mb/s */
-#define E1000_STATUS_ASDV       0x00000300      /* Auto speed detect value */
-#define E1000_STATUS_MTXCKOK    0x00000400      /* MTX clock running OK */
-#define E1000_STATUS_PCI66      0x00000800      /* In 66Mhz slot */
-#define E1000_STATUS_BUS64      0x00001000      /* In 64 bit slot */
-#define E1000_STATUS_PCIX_MODE  0x00002000      /* PCI-X mode */
-#define E1000_STATUS_PCIX_SPEED 0x0000C000      /* PCI-X bus speed */
-
-/* Constants used to intrepret the masked PCI-X bus speed. */
-#define E1000_STATUS_PCIX_SPEED_66  0x00000000 /* PCI-X bus speed  50-66 MHz */
-#define E1000_STATUS_PCIX_SPEED_100 0x00004000 /* PCI-X bus speed  66-100 MHz */
-#define E1000_STATUS_PCIX_SPEED_133 0x00008000 /* PCI-X bus speed 100-133 MHz */
-
-/* EEPROM/Flash Control */
-#define E1000_EECD_SK        0x00000001 /* EEPROM Clock */
-#define E1000_EECD_CS        0x00000002 /* EEPROM Chip Select */
-#define E1000_EECD_DI        0x00000004 /* EEPROM Data In */
-#define E1000_EECD_DO        0x00000008 /* EEPROM Data Out */
-#define E1000_EECD_FWE_MASK  0x00000030
-#define E1000_EECD_FWE_DIS   0x00000010 /* Disable FLASH writes */
-#define E1000_EECD_FWE_EN    0x00000020 /* Enable FLASH writes */
-#define E1000_EECD_FWE_SHIFT 4
-#define E1000_EECD_REQ       0x00000040 /* EEPROM Access Request */
-#define E1000_EECD_GNT       0x00000080 /* EEPROM Access Grant */
-#define E1000_EECD_PRES      0x00000100 /* EEPROM Present */
-#define E1000_EECD_SIZE      0x00000200 /* EEPROM Size (0=64 word 1=256 word) */
-#define E1000_EECD_ADDR_BITS 0x00000400 /* EEPROM Addressing bits based on type
-                                         * (0-small, 1-large) */
-#define E1000_EECD_TYPE      0x00002000 /* EEPROM Type (1-SPI, 0-Microwire) */
-#ifndef E1000_EEPROM_GRANT_ATTEMPTS
-#define E1000_EEPROM_GRANT_ATTEMPTS 1000 /* EEPROM # attempts to gain grant */
 #endif
-
-/* EEPROM Read */
-#define E1000_EERD_START      0x00000001 /* Start Read */
-#define E1000_EERD_DONE       0x00000010 /* Read Done */
-#define E1000_EERD_ADDR_SHIFT 8
-#define E1000_EERD_ADDR_MASK  0x0000FF00 /* Read Address */
-#define E1000_EERD_DATA_SHIFT 16
-#define E1000_EERD_DATA_MASK  0xFFFF0000 /* Read Data */
-
-/* SPI EEPROM Status Register */
-#define EEPROM_STATUS_RDY_SPI  0x01
-#define EEPROM_STATUS_WEN_SPI  0x02
-#define EEPROM_STATUS_BP0_SPI  0x04
-#define EEPROM_STATUS_BP1_SPI  0x08
-#define EEPROM_STATUS_WPEN_SPI 0x80
-
-/* Extended Device Control */
-#define E1000_CTRL_EXT_GPI0_EN   0x00000001 /* Maps SDP4 to GPI0 */
-#define E1000_CTRL_EXT_GPI1_EN   0x00000002 /* Maps SDP5 to GPI1 */
-#define E1000_CTRL_EXT_PHYINT_EN E1000_CTRL_EXT_GPI1_EN
-#define E1000_CTRL_EXT_GPI2_EN   0x00000004 /* Maps SDP6 to GPI2 */
-#define E1000_CTRL_EXT_GPI3_EN   0x00000008 /* Maps SDP7 to GPI3 */
-#define E1000_CTRL_EXT_SDP4_DATA 0x00000010 /* Value of SW Defineable Pin 4 */
-#define E1000_CTRL_EXT_SDP5_DATA 0x00000020 /* Value of SW Defineable Pin 5 */
-#define E1000_CTRL_EXT_PHY_INT   E1000_CTRL_EXT_SDP5_DATA
-#define E1000_CTRL_EXT_SDP6_DATA 0x00000040 /* Value of SW Defineable Pin 6 */
-#define E1000_CTRL_EXT_SDP7_DATA 0x00000080 /* Value of SW Defineable Pin 7 */
-#define E1000_CTRL_EXT_SDP4_DIR  0x00000100 /* Direction of SDP4 0=in 1=out */
-#define E1000_CTRL_EXT_SDP5_DIR  0x00000200 /* Direction of SDP5 0=in 1=out */
-#define E1000_CTRL_EXT_SDP6_DIR  0x00000400 /* Direction of SDP6 0=in 1=out */
-#define E1000_CTRL_EXT_SDP7_DIR  0x00000800 /* Direction of SDP7 0=in 1=out */
-#define E1000_CTRL_EXT_ASDCHK    0x00001000 /* Initiate an ASD sequence */
-#define E1000_CTRL_EXT_EE_RST    0x00002000 /* Reinitialize from EEPROM */
-#define E1000_CTRL_EXT_IPS       0x00004000 /* Invert Power State */
-#define E1000_CTRL_EXT_SPD_BYPS  0x00008000 /* Speed Select Bypass */
-#define E1000_CTRL_EXT_LINK_MODE_MASK 0x00C00000
-#define E1000_CTRL_EXT_LINK_MODE_GMII 0x00000000
-#define E1000_CTRL_EXT_LINK_MODE_TBI  0x00C00000
-#define E1000_CTRL_EXT_WR_WMARK_MASK  0x03000000
-#define E1000_CTRL_EXT_WR_WMARK_256   0x00000000
-#define E1000_CTRL_EXT_WR_WMARK_320   0x01000000
-#define E1000_CTRL_EXT_WR_WMARK_384   0x02000000
-#define E1000_CTRL_EXT_WR_WMARK_448   0x03000000
-
-/* MDI Control */
-#define E1000_MDIC_DATA_MASK 0x0000FFFF
-#define E1000_MDIC_REG_MASK  0x001F0000
-#define E1000_MDIC_REG_SHIFT 16
-#define E1000_MDIC_PHY_MASK  0x03E00000
-#define E1000_MDIC_PHY_SHIFT 21
-#define E1000_MDIC_OP_WRITE  0x04000000
-#define E1000_MDIC_OP_READ   0x08000000
-#define E1000_MDIC_READY     0x10000000
-#define E1000_MDIC_INT_EN    0x20000000
-#define E1000_MDIC_ERROR     0x40000000
-
-/* LED Control */
-#define E1000_LEDCTL_LED0_MODE_MASK       0x0000000F
-#define E1000_LEDCTL_LED0_MODE_SHIFT      0
-#define E1000_LEDCTL_LED0_IVRT            0x00000040
-#define E1000_LEDCTL_LED0_BLINK           0x00000080
-#define E1000_LEDCTL_LED1_MODE_MASK       0x00000F00
-#define E1000_LEDCTL_LED1_MODE_SHIFT      8
-#define E1000_LEDCTL_LED1_IVRT            0x00004000
-#define E1000_LEDCTL_LED1_BLINK           0x00008000
-#define E1000_LEDCTL_LED2_MODE_MASK       0x000F0000
-#define E1000_LEDCTL_LED2_MODE_SHIFT      16
-#define E1000_LEDCTL_LED2_IVRT            0x00400000
-#define E1000_LEDCTL_LED2_BLINK           0x00800000
-#define E1000_LEDCTL_LED3_MODE_MASK       0x0F000000
-#define E1000_LEDCTL_LED3_MODE_SHIFT      24
-#define E1000_LEDCTL_LED3_IVRT            0x40000000
-#define E1000_LEDCTL_LED3_BLINK           0x80000000
-
-#define E1000_LEDCTL_MODE_LINK_10_1000  0x0
-#define E1000_LEDCTL_MODE_LINK_100_1000 0x1
-#define E1000_LEDCTL_MODE_LINK_UP       0x2
-#define E1000_LEDCTL_MODE_ACTIVITY      0x3
-#define E1000_LEDCTL_MODE_LINK_ACTIVITY 0x4
-#define E1000_LEDCTL_MODE_LINK_10       0x5
-#define E1000_LEDCTL_MODE_LINK_100      0x6
-#define E1000_LEDCTL_MODE_LINK_1000     0x7
-#define E1000_LEDCTL_MODE_PCIX_MODE     0x8
-#define E1000_LEDCTL_MODE_FULL_DUPLEX   0x9
-#define E1000_LEDCTL_MODE_COLLISION     0xA
-#define E1000_LEDCTL_MODE_BUS_SPEED     0xB
-#define E1000_LEDCTL_MODE_BUS_SIZE      0xC
-#define E1000_LEDCTL_MODE_PAUSED        0xD
-#define E1000_LEDCTL_MODE_LED_ON        0xE
-#define E1000_LEDCTL_MODE_LED_OFF       0xF
-
-/* Receive Address */
-#define E1000_RAH_AV  0x80000000        /* Receive descriptor valid */
-
-/* Interrupt Cause Read */
-#define E1000_ICR_TXDW          0x00000001 /* Transmit desc written back */
-#define E1000_ICR_TXQE          0x00000002 /* Transmit Queue empty */
-#define E1000_ICR_LSC           0x00000004 /* Link Status Change */
-#define E1000_ICR_RXSEQ         0x00000008 /* rx sequence error */
-#define E1000_ICR_RXDMT0        0x00000010 /* rx desc min. threshold (0) */
-#define E1000_ICR_RXO           0x00000040 /* rx overrun */
-#define E1000_ICR_RXT0          0x00000080 /* rx timer intr (ring 0) */
-#define E1000_ICR_MDAC          0x00000200 /* MDIO access complete */
-#define E1000_ICR_RXCFG         0x00000400 /* RX /c/ ordered set */
-#define E1000_ICR_GPI_EN0       0x00000800 /* GP Int 0 */
-#define E1000_ICR_GPI_EN1       0x00001000 /* GP Int 1 */
-#define E1000_ICR_GPI_EN2       0x00002000 /* GP Int 2 */
-#define E1000_ICR_GPI_EN3       0x00004000 /* GP Int 3 */
-#define E1000_ICR_TXD_LOW       0x00008000
-#define E1000_ICR_SRPD          0x00010000
-
-/* Interrupt Cause Set */
-#define E1000_ICS_TXDW      E1000_ICR_TXDW      /* Transmit desc written back */
-#define E1000_ICS_TXQE      E1000_ICR_TXQE      /* Transmit Queue empty */
-#define E1000_ICS_LSC       E1000_ICR_LSC       /* Link Status Change */
-#define E1000_ICS_RXSEQ     E1000_ICR_RXSEQ     /* rx sequence error */
-#define E1000_ICS_RXDMT0    E1000_ICR_RXDMT0    /* rx desc min. threshold */
-#define E1000_ICS_RXO       E1000_ICR_RXO       /* rx overrun */
-#define E1000_ICS_RXT0      E1000_ICR_RXT0      /* rx timer intr */
-#define E1000_ICS_MDAC      E1000_ICR_MDAC      /* MDIO access complete */
-#define E1000_ICS_RXCFG     E1000_ICR_RXCFG     /* RX /c/ ordered set */
-#define E1000_ICS_GPI_EN0   E1000_ICR_GPI_EN0   /* GP Int 0 */
-#define E1000_ICS_GPI_EN1   E1000_ICR_GPI_EN1   /* GP Int 1 */
-#define E1000_ICS_GPI_EN2   E1000_ICR_GPI_EN2   /* GP Int 2 */
-#define E1000_ICS_GPI_EN3   E1000_ICR_GPI_EN3   /* GP Int 3 */
-#define E1000_ICS_TXD_LOW   E1000_ICR_TXD_LOW
-#define E1000_ICS_SRPD      E1000_ICR_SRPD
-
-/* Interrupt Mask Set */
-#define E1000_IMS_TXDW      E1000_ICR_TXDW      /* Transmit desc written back */
-#define E1000_IMS_TXQE      E1000_ICR_TXQE      /* Transmit Queue empty */
-#define E1000_IMS_LSC       E1000_ICR_LSC       /* Link Status Change */
-#define E1000_IMS_RXSEQ     E1000_ICR_RXSEQ     /* rx sequence error */
-#define E1000_IMS_RXDMT0    E1000_ICR_RXDMT0    /* rx desc min. threshold */
-#define E1000_IMS_RXO       E1000_ICR_RXO       /* rx overrun */
-#define E1000_IMS_RXT0      E1000_ICR_RXT0      /* rx timer intr */
-#define E1000_IMS_MDAC      E1000_ICR_MDAC      /* MDIO access complete */
-#define E1000_IMS_RXCFG     E1000_ICR_RXCFG     /* RX /c/ ordered set */
-#define E1000_IMS_GPI_EN0   E1000_ICR_GPI_EN0   /* GP Int 0 */
-#define E1000_IMS_GPI_EN1   E1000_ICR_GPI_EN1   /* GP Int 1 */
-#define E1000_IMS_GPI_EN2   E1000_ICR_GPI_EN2   /* GP Int 2 */
-#define E1000_IMS_GPI_EN3   E1000_ICR_GPI_EN3   /* GP Int 3 */
-#define E1000_IMS_TXD_LOW   E1000_ICR_TXD_LOW
-#define E1000_IMS_SRPD      E1000_ICR_SRPD
-
-/* Interrupt Mask Clear */
-#define E1000_IMC_TXDW      E1000_ICR_TXDW      /* Transmit desc written back */
-#define E1000_IMC_TXQE      E1000_ICR_TXQE      /* Transmit Queue empty */
-#define E1000_IMC_LSC       E1000_ICR_LSC       /* Link Status Change */
-#define E1000_IMC_RXSEQ     E1000_ICR_RXSEQ     /* rx sequence error */
-#define E1000_IMC_RXDMT0    E1000_ICR_RXDMT0    /* rx desc min. threshold */
-#define E1000_IMC_RXO       E1000_ICR_RXO       /* rx overrun */
-#define E1000_IMC_RXT0      E1000_ICR_RXT0      /* rx timer intr */
-#define E1000_IMC_MDAC      E1000_ICR_MDAC      /* MDIO access complete */
-#define E1000_IMC_RXCFG     E1000_ICR_RXCFG     /* RX /c/ ordered set */
-#define E1000_IMC_GPI_EN0   E1000_ICR_GPI_EN0   /* GP Int 0 */
-#define E1000_IMC_GPI_EN1   E1000_ICR_GPI_EN1   /* GP Int 1 */
-#define E1000_IMC_GPI_EN2   E1000_ICR_GPI_EN2   /* GP Int 2 */
-#define E1000_IMC_GPI_EN3   E1000_ICR_GPI_EN3   /* GP Int 3 */
-#define E1000_IMC_TXD_LOW   E1000_ICR_TXD_LOW
-#define E1000_IMC_SRPD      E1000_ICR_SRPD
-
-/* Receive Control */
-#define E1000_RCTL_RST            0x00000001    /* Software reset */
-#define E1000_RCTL_EN             0x00000002    /* enable */
-#define E1000_RCTL_SBP            0x00000004    /* store bad packet */
-#define E1000_RCTL_UPE            0x00000008    /* unicast promiscuous enable */
-#define E1000_RCTL_MPE            0x00000010    /* multicast promiscuous enab */
-#define E1000_RCTL_LPE            0x00000020    /* long packet enable */
-#define E1000_RCTL_LBM_NO         0x00000000    /* no loopback mode */
-#define E1000_RCTL_LBM_MAC        0x00000040    /* MAC loopback mode */
-#define E1000_RCTL_LBM_SLP        0x00000080    /* serial link loopback mode */
-#define E1000_RCTL_LBM_TCVR       0x000000C0    /* tcvr loopback mode */
-#define E1000_RCTL_RDMTS_HALF     0x00000000    /* rx desc min threshold size */
-#define E1000_RCTL_RDMTS_QUAT     0x00000100    /* rx desc min threshold size */
-#define E1000_RCTL_RDMTS_EIGTH    0x00000200    /* rx desc min threshold size */
-#define E1000_RCTL_MO_SHIFT       12            /* multicast offset shift */
-#define E1000_RCTL_MO_0           0x00000000    /* multicast offset 11:0 */
-#define E1000_RCTL_MO_1           0x00001000    /* multicast offset 12:1 */
-#define E1000_RCTL_MO_2           0x00002000    /* multicast offset 13:2 */
-#define E1000_RCTL_MO_3           0x00003000    /* multicast offset 15:4 */
-#define E1000_RCTL_MDR            0x00004000    /* multicast desc ring 0 */
-#define E1000_RCTL_BAM            0x00008000    /* broadcast enable */
-/* these buffer sizes are valid if E1000_RCTL_BSEX is 0 */
-#define E1000_RCTL_SZ_2048        0x00000000    /* rx buffer size 2048 */
-#define E1000_RCTL_SZ_1024        0x00010000    /* rx buffer size 1024 */
-#define E1000_RCTL_SZ_512         0x00020000    /* rx buffer size 512 */
-#define E1000_RCTL_SZ_256         0x00030000    /* rx buffer size 256 */
-/* these buffer sizes are valid if E1000_RCTL_BSEX is 1 */
-#define E1000_RCTL_SZ_16384       0x00010000    /* rx buffer size 16384 */
-#define E1000_RCTL_SZ_8192        0x00020000    /* rx buffer size 8192 */
-#define E1000_RCTL_SZ_4096        0x00030000    /* rx buffer size 4096 */
-#define E1000_RCTL_VFE            0x00040000    /* vlan filter enable */
-#define E1000_RCTL_CFIEN          0x00080000    /* canonical form enable */
-#define E1000_RCTL_CFI            0x00100000    /* canonical form indicator */
-#define E1000_RCTL_DPF            0x00400000    /* discard pause frames */
-#define E1000_RCTL_PMCF           0x00800000    /* pass MAC control frames */
-#define E1000_RCTL_BSEX           0x02000000    /* Buffer size extension */
-#define E1000_RCTL_SECRC          0x04000000    /* Strip Ethernet CRC */
-
-/* Receive Descriptor */
-#define E1000_RDT_DELAY 0x0000ffff      /* Delay timer (1=1024us) */
-#define E1000_RDT_FPDB  0x80000000      /* Flush descriptor block */
-#define E1000_RDLEN_LEN 0x0007ff80      /* descriptor length */
-#define E1000_RDH_RDH   0x0000ffff      /* receive descriptor head */
-#define E1000_RDT_RDT   0x0000ffff      /* receive descriptor tail */
-
-/* Flow Control */
-#define E1000_FCRTH_RTH  0x0000FFF8     /* Mask Bits[15:3] for RTH */
-#define E1000_FCRTH_XFCE 0x80000000     /* External Flow Control Enable */
-#define E1000_FCRTL_RTL  0x0000FFF8     /* Mask Bits[15:3] for RTL */
-#define E1000_FCRTL_XONE 0x80000000     /* Enable XON frame transmission */
-
-/* Receive Descriptor Control */
-#define E1000_RXDCTL_PTHRESH 0x0000003F /* RXDCTL Prefetch Threshold */
-#define E1000_RXDCTL_HTHRESH 0x00003F00 /* RXDCTL Host Threshold */
-#define E1000_RXDCTL_WTHRESH 0x003F0000 /* RXDCTL Writeback Threshold */
-#define E1000_RXDCTL_GRAN    0x01000000 /* RXDCTL Granularity */
-
-/* Transmit Descriptor Control */
-#define E1000_TXDCTL_PTHRESH 0x000000FF /* TXDCTL Prefetch Threshold */
-#define E1000_TXDCTL_HTHRESH 0x0000FF00 /* TXDCTL Host Threshold */
-#define E1000_TXDCTL_WTHRESH 0x00FF0000 /* TXDCTL Writeback Threshold */
-#define E1000_TXDCTL_GRAN    0x01000000 /* TXDCTL Granularity */
-#define E1000_TXDCTL_LWTHRESH 0xFE000000 /* TXDCTL Low Threshold */
-#define E1000_TXDCTL_FULL_TX_DESC_WB 0x01010000 /* GRAN=1, WTHRESH=1 */
-
-/* Transmit Configuration Word */
-#define E1000_TXCW_FD         0x00000020        /* TXCW full duplex */
-#define E1000_TXCW_HD         0x00000040        /* TXCW half duplex */
-#define E1000_TXCW_PAUSE      0x00000080        /* TXCW sym pause request */
-#define E1000_TXCW_ASM_DIR    0x00000100        /* TXCW astm pause direction */
-#define E1000_TXCW_PAUSE_MASK 0x00000180        /* TXCW pause request mask */
-#define E1000_TXCW_RF         0x00003000        /* TXCW remote fault */
-#define E1000_TXCW_NP         0x00008000        /* TXCW next page */
-#define E1000_TXCW_CW         0x0000ffff        /* TxConfigWord mask */
-#define E1000_TXCW_TXC        0x40000000        /* Transmit Config control */
-#define E1000_TXCW_ANE        0x80000000        /* Auto-neg enable */
-
-/* Receive Configuration Word */
-#define E1000_RXCW_CW    0x0000ffff     /* RxConfigWord mask */
-#define E1000_RXCW_NC    0x04000000     /* Receive config no carrier */
-#define E1000_RXCW_IV    0x08000000     /* Receive config invalid */
-#define E1000_RXCW_CC    0x10000000     /* Receive config change */
-#define E1000_RXCW_C     0x20000000     /* Receive config */
-#define E1000_RXCW_SYNCH 0x40000000     /* Receive config synch */
-#define E1000_RXCW_ANC   0x80000000     /* Auto-neg complete */
-
-/* Transmit Control */
-#define E1000_TCTL_RST    0x00000001    /* software reset */
-#define E1000_TCTL_EN     0x00000002    /* enable tx */
-#define E1000_TCTL_BCE    0x00000004    /* busy check enable */
-#define E1000_TCTL_PSP    0x00000008    /* pad short packets */
-#define E1000_TCTL_CT     0x00000ff0    /* collision threshold */
-#define E1000_TCTL_COLD   0x003ff000    /* collision distance */
-#define E1000_TCTL_SWXOFF 0x00400000    /* SW Xoff transmission */
-#define E1000_TCTL_PBE    0x00800000    /* Packet Burst Enable */
-#define E1000_TCTL_RTLC   0x01000000    /* Re-transmit on late collision */
-#define E1000_TCTL_NRTU   0x02000000    /* No Re-transmit on underrun */
-
-/* Receive Checksum Control */
-#define E1000_RXCSUM_PCSS_MASK 0x000000FF   /* Packet Checksum Start */
-#define E1000_RXCSUM_IPOFL     0x00000100   /* IPv4 checksum offload */
-#define E1000_RXCSUM_TUOFL     0x00000200   /* TCP / UDP checksum offload */
-#define E1000_RXCSUM_IPV6OFL   0x00000400   /* IPv6 checksum offload */
-
-/* Definitions for power management and wakeup registers */
-/* Wake Up Control */
-#define E1000_WUC_APME       0x00000001 /* APM Enable */
-#define E1000_WUC_PME_EN     0x00000002 /* PME Enable */
-#define E1000_WUC_PME_STATUS 0x00000004 /* PME Status */
-#define E1000_WUC_APMPME     0x00000008 /* Assert PME on APM Wakeup */
-#define E1000_WUC_SPM        0x80000000 /* Enable SPM */
-
-/* Wake Up Filter Control */
-#define E1000_WUFC_LNKC 0x00000001 /* Link Status Change Wakeup Enable */
-#define E1000_WUFC_MAG  0x00000002 /* Magic Packet Wakeup Enable */
-#define E1000_WUFC_EX   0x00000004 /* Directed Exact Wakeup Enable */
-#define E1000_WUFC_MC   0x00000008 /* Directed Multicast Wakeup Enable */
-#define E1000_WUFC_BC   0x00000010 /* Broadcast Wakeup Enable */
-#define E1000_WUFC_ARP  0x00000020 /* ARP Request Packet Wakeup Enable */
-#define E1000_WUFC_IPV4 0x00000040 /* Directed IPv4 Packet Wakeup Enable */
-#define E1000_WUFC_IPV6 0x00000080 /* Directed IPv6 Packet Wakeup Enable */
-#define E1000_WUFC_FLX0 0x00010000 /* Flexible Filter 0 Enable */
-#define E1000_WUFC_FLX1 0x00020000 /* Flexible Filter 1 Enable */
-#define E1000_WUFC_FLX2 0x00040000 /* Flexible Filter 2 Enable */
-#define E1000_WUFC_FLX3 0x00080000 /* Flexible Filter 3 Enable */
-#define E1000_WUFC_ALL_FILTERS 0x000F00FF /* Mask for all wakeup filters */
-#define E1000_WUFC_FLX_OFFSET 16       /* Offset to the Flexible Filters bits */
-#define E1000_WUFC_FLX_FILTERS 0x000F0000 /* Mask for the 4 flexible filters */
-
-/* Wake Up Status */
-#define E1000_WUS_LNKC 0x00000001 /* Link Status Changed */
-#define E1000_WUS_MAG  0x00000002 /* Magic Packet Received */
-#define E1000_WUS_EX   0x00000004 /* Directed Exact Received */
-#define E1000_WUS_MC   0x00000008 /* Directed Multicast Received */
-#define E1000_WUS_BC   0x00000010 /* Broadcast Received */
-#define E1000_WUS_ARP  0x00000020 /* ARP Request Packet Received */
-#define E1000_WUS_IPV4 0x00000040 /* Directed IPv4 Packet Wakeup Received */
-#define E1000_WUS_IPV6 0x00000080 /* Directed IPv6 Packet Wakeup Received */
-#define E1000_WUS_FLX0 0x00010000 /* Flexible Filter 0 Match */
-#define E1000_WUS_FLX1 0x00020000 /* Flexible Filter 1 Match */
-#define E1000_WUS_FLX2 0x00040000 /* Flexible Filter 2 Match */
-#define E1000_WUS_FLX3 0x00080000 /* Flexible Filter 3 Match */
-#define E1000_WUS_FLX_FILTERS 0x000F0000 /* Mask for the 4 flexible filters */
-
-/* Management Control */
-#define E1000_MANC_SMBUS_EN      0x00000001 /* SMBus Enabled - RO */
-#define E1000_MANC_ASF_EN        0x00000002 /* ASF Enabled - RO */
-#define E1000_MANC_R_ON_FORCE    0x00000004 /* Reset on Force TCO - RO */
-#define E1000_MANC_RMCP_EN       0x00000100 /* Enable RCMP 026Fh Filtering */
-#define E1000_MANC_0298_EN       0x00000200 /* Enable RCMP 0298h Filtering */
-#define E1000_MANC_IPV4_EN       0x00000400 /* Enable IPv4 */
-#define E1000_MANC_IPV6_EN       0x00000800 /* Enable IPv6 */
-#define E1000_MANC_SNAP_EN       0x00001000 /* Accept LLC/SNAP */
-#define E1000_MANC_ARP_EN        0x00002000 /* Enable ARP Request Filtering */
-#define E1000_MANC_NEIGHBOR_EN   0x00004000 /* Enable Neighbor Discovery
-                                             * Filtering */
-#define E1000_MANC_TCO_RESET     0x00010000 /* TCO Reset Occurred */
-#define E1000_MANC_RCV_TCO_EN    0x00020000 /* Receive TCO Packets Enabled */
-#define E1000_MANC_REPORT_STATUS 0x00040000 /* Status Reporting Enabled */
-#define E1000_MANC_EN_MAC_ADDR_FILTER   0x00100000 /* Enable MAC address
-                                                    * filtering */
-#define E1000_MANC_EN_MNG2HOST   0x00200000 /* Enable MNG packets to host
-                                             * memory */
-#define E1000_MANC_SMB_REQ       0x01000000 /* SMBus Request */
-#define E1000_MANC_SMB_GNT       0x02000000 /* SMBus Grant */
-#define E1000_MANC_SMB_CLK_IN    0x04000000 /* SMBus Clock In */
-#define E1000_MANC_SMB_DATA_IN   0x08000000 /* SMBus Data In */
-#define E1000_MANC_SMB_DATA_OUT  0x10000000 /* SMBus Data Out */
-#define E1000_MANC_SMB_CLK_OUT   0x20000000 /* SMBus Clock Out */
-
-#define E1000_MANC_SMB_DATA_OUT_SHIFT  28 /* SMBus Data Out Shift */
-#define E1000_MANC_SMB_CLK_OUT_SHIFT   29 /* SMBus Clock Out Shift */
-
-/* Wake Up Packet Length */
-#define E1000_WUPL_LENGTH_MASK 0x0FFF   /* Only the lower 12 bits are valid */
-
-#define E1000_MDALIGN          4096
-
-/* EEPROM Commands - Microwire */
-#define EEPROM_READ_OPCODE_MICROWIRE  0x6  /* EEPROM read opcode */
-#define EEPROM_WRITE_OPCODE_MICROWIRE 0x5  /* EEPROM write opcode */
-#define EEPROM_ERASE_OPCODE_MICROWIRE 0x7  /* EEPROM erase opcode */
-#define EEPROM_EWEN_OPCODE_MICROWIRE  0x13 /* EEPROM erase/write enable */
-#define EEPROM_EWDS_OPCODE_MICROWIRE  0x10 /* EEPROM erast/write disable */
-
-/* EEPROM Commands - SPI */
-#define EEPROM_MAX_RETRY_SPI    5000 /* Max wait of 5ms, for RDY signal */
-#define EEPROM_READ_OPCODE_SPI  0x3  /* EEPROM read opcode */
-#define EEPROM_WRITE_OPCODE_SPI 0x2  /* EEPROM write opcode */
-#define EEPROM_A8_OPCODE_SPI    0x8  /* opcode bit-3 = address bit-8 */
-#define EEPROM_WREN_OPCODE_SPI  0x6  /* EEPROM set Write Enable latch */
-#define EEPROM_WRDI_OPCODE_SPI  0x4  /* EEPROM reset Write Enable latch */
-#define EEPROM_RDSR_OPCODE_SPI  0x5  /* EEPROM read Status register */
-#define EEPROM_WRSR_OPCODE_SPI  0x1  /* EEPROM write Status register */
-
-/* EEPROM Size definitions */
-#define EEPROM_SIZE_16KB        0x1800
-#define EEPROM_SIZE_8KB         0x1400
-#define EEPROM_SIZE_4KB         0x1000
-#define EEPROM_SIZE_2KB         0x0C00
-#define EEPROM_SIZE_1KB         0x0800
-#define EEPROM_SIZE_512B        0x0400
-#define EEPROM_SIZE_128B        0x0000
-#define EEPROM_SIZE_MASK        0x1C00
-
-/* EEPROM Word Offsets */
-#define EEPROM_COMPAT                 0x0003
-#define EEPROM_ID_LED_SETTINGS        0x0004
-#define EEPROM_SERDES_AMPLITUDE       0x0006 /* For SERDES output amplitude adjustment. */
-#define EEPROM_PHY_CLASS_WORD         0x0007
-#define EEPROM_INIT_CONTROL1_REG      0x000A
-#define EEPROM_INIT_CONTROL2_REG      0x000F
-#define EEPROM_INIT_CONTROL3_PORT_B   0x0014
-#define EEPROM_INIT_CONTROL3_PORT_A   0x0024
-#define EEPROM_CFG                    0x0012
-#define EEPROM_FLASH_VERSION          0x0032
-#define EEPROM_CHECKSUM_REG           0x003F
-
-/* Word definitions for ID LED Settings */
-#define ID_LED_RESERVED_0000 0x0000
-#define ID_LED_RESERVED_FFFF 0xFFFF
-#define ID_LED_DEFAULT       ((ID_LED_OFF1_ON2 << 12) | \
-                              (ID_LED_OFF1_OFF2 << 8) | \
-                              (ID_LED_DEF1_DEF2 << 4) | \
-                              (ID_LED_DEF1_DEF2))
-#define ID_LED_DEF1_DEF2     0x1
-#define ID_LED_DEF1_ON2      0x2
-#define ID_LED_DEF1_OFF2     0x3
-#define ID_LED_ON1_DEF2      0x4
-#define ID_LED_ON1_ON2       0x5
-#define ID_LED_ON1_OFF2      0x6
-#define ID_LED_OFF1_DEF2     0x7
-#define ID_LED_OFF1_ON2      0x8
-#define ID_LED_OFF1_OFF2     0x9
-
-#define IGP_ACTIVITY_LED_MASK   0xFFFFF0FF
-#define IGP_ACTIVITY_LED_ENABLE 0x0300
-#define IGP_LED3_MODE           0x07000000
-
-
-/* Mask bits for SERDES amplitude adjustment in Word 6 of the EEPROM */
-#define EEPROM_SERDES_AMPLITUDE_MASK  0x000F
-
-/* Mask bit for PHY class in Word 7 of the EEPROM */
-#define EEPROM_PHY_CLASS_A   0x8000
-
-/* Mask bits for fields in Word 0x0a of the EEPROM */
-#define EEPROM_WORD0A_ILOS   0x0010
-#define EEPROM_WORD0A_SWDPIO 0x01E0
-#define EEPROM_WORD0A_LRST   0x0200
-#define EEPROM_WORD0A_FD     0x0400
-#define EEPROM_WORD0A_66MHZ  0x0800
-
-/* Mask bits for fields in Word 0x0f of the EEPROM */
-#define EEPROM_WORD0F_PAUSE_MASK 0x3000
-#define EEPROM_WORD0F_PAUSE      0x1000
-#define EEPROM_WORD0F_ASM_DIR    0x2000
-#define EEPROM_WORD0F_ANE        0x0800
-#define EEPROM_WORD0F_SWPDIO_EXT 0x00F0
-
-/* For checksumming, the sum of all words in the EEPROM should equal 0xBABA. */
-#define EEPROM_SUM 0xBABA
-
-/* EEPROM Map defines (WORD OFFSETS)*/
-#define EEPROM_NODE_ADDRESS_BYTE_0 0
-#define EEPROM_PBA_BYTE_1          8
-
-#define EEPROM_RESERVED_WORD          0xFFFF
-
-/* EEPROM Map Sizes (Byte Counts) */
-#define PBA_SIZE 4
-
-/* Collision related configuration parameters */
-#define E1000_COLLISION_THRESHOLD       15
-#define E1000_CT_SHIFT                  4
-#define E1000_COLLISION_DISTANCE        64
-#define E1000_FDX_COLLISION_DISTANCE    E1000_COLLISION_DISTANCE
-#define E1000_HDX_COLLISION_DISTANCE    E1000_COLLISION_DISTANCE
-#define E1000_COLD_SHIFT                12
-
-/* Number of Transmit and Receive Descriptors must be a multiple of 8 */
-#define REQ_TX_DESCRIPTOR_MULTIPLE  8
-#define REQ_RX_DESCRIPTOR_MULTIPLE  8
-
-/* Default values for the transmit IPG register */
-#define DEFAULT_82542_TIPG_IPGT        10
-#define DEFAULT_82543_TIPG_IPGT_FIBER  9
-#define DEFAULT_82543_TIPG_IPGT_COPPER 8
-
-#define E1000_TIPG_IPGT_MASK  0x000003FF
-#define E1000_TIPG_IPGR1_MASK 0x000FFC00
-#define E1000_TIPG_IPGR2_MASK 0x3FF00000
-
-#define DEFAULT_82542_TIPG_IPGR1 2
-#define DEFAULT_82543_TIPG_IPGR1 8
-#define E1000_TIPG_IPGR1_SHIFT  10
-
-#define DEFAULT_82542_TIPG_IPGR2 10
-#define DEFAULT_82543_TIPG_IPGR2 6
-#define E1000_TIPG_IPGR2_SHIFT  20
-
-#define E1000_TXDMAC_DPP 0x00000001
-
-/* Adaptive IFS defines */
-#define TX_THRESHOLD_START     8
-#define TX_THRESHOLD_INCREMENT 10
-#define TX_THRESHOLD_DECREMENT 1
-#define TX_THRESHOLD_STOP      190
-#define TX_THRESHOLD_DISABLE   0
-#define TX_THRESHOLD_TIMER_MS  10000
-#define MIN_NUM_XMITS          1000
-#define IFS_MAX                80
-#define IFS_STEP               10
-#define IFS_MIN                40
-#define IFS_RATIO              4
-
-/* PBA constants */
-#define E1000_PBA_16K 0x0010    /* 16KB, default TX allocation */
-#define E1000_PBA_22K 0x0016
-#define E1000_PBA_24K 0x0018
-#define E1000_PBA_30K 0x001E
-#define E1000_PBA_40K 0x0028
-#define E1000_PBA_48K 0x0030    /* 48KB, default RX allocation */
-
-/* Flow Control Constants */
-#define FLOW_CONTROL_ADDRESS_LOW  0x00C28001
-#define FLOW_CONTROL_ADDRESS_HIGH 0x00000100
-#define FLOW_CONTROL_TYPE         0x8808
-
-/* The historical defaults for the flow control values are given below. */
-#define FC_DEFAULT_HI_THRESH        (0x8000)    /* 32KB */
-#define FC_DEFAULT_LO_THRESH        (0x4000)    /* 16KB */
-#define FC_DEFAULT_TX_TIMER         (0x100)     /* ~130 us */
-
-/* PCIX Config space */
-#define PCIX_COMMAND_REGISTER    0xE6
-#define PCIX_STATUS_REGISTER_LO  0xE8
-#define PCIX_STATUS_REGISTER_HI  0xEA
-
-#define PCIX_COMMAND_MMRBC_MASK      0x000C
-#define PCIX_COMMAND_MMRBC_SHIFT     0x2
-#define PCIX_STATUS_HI_MMRBC_MASK    0x0060
-#define PCIX_STATUS_HI_MMRBC_SHIFT   0x5
-#define PCIX_STATUS_HI_MMRBC_4K      0x3
-#define PCIX_STATUS_HI_MMRBC_2K      0x2
-
-
-/* Number of bits required to shift right the "pause" bits from the
- * EEPROM (bits 13:12) to the "pause" (bits 8:7) field in the TXCW register.
- */
-#define PAUSE_SHIFT 5
-
-/* Number of bits required to shift left the "SWDPIO" bits from the
- * EEPROM (bits 8:5) to the "SWDPIO" (bits 25:22) field in the CTRL register.
- */
-#define SWDPIO_SHIFT 17
-
-/* Number of bits required to shift left the "SWDPIO_EXT" bits from the
- * EEPROM word F (bits 7:4) to the bits 11:8 of The Extended CTRL register.
- */
-#define SWDPIO__EXT_SHIFT 4
-
-/* Number of bits required to shift left the "ILOS" bit from the EEPROM
- * (bit 4) to the "ILOS" (bit 7) field in the CTRL register.
- */
-#define ILOS_SHIFT  3
-
-
-#define RECEIVE_BUFFER_ALIGN_SIZE  (256)
-
-/* Number of milliseconds we wait for auto-negotiation to complete */
-#define LINK_UP_TIMEOUT             500
-
-#define E1000_TX_BUFFER_SIZE ((uint32_t)1514)
-
-/* The carrier extension symbol, as received by the NIC. */
-#define CARRIER_EXTENSION   0x0F
-
-/* TBI_ACCEPT macro definition:
- *
- * This macro requires:
- *      adapter = a pointer to struct e1000_hw
- *      status = the 8 bit status field of the RX descriptor with EOP set
- *      error = the 8 bit error field of the RX descriptor with EOP set
- *      length = the sum of all the length fields of the RX descriptors that
- *               make up the current frame
- *      last_byte = the last byte of the frame DMAed by the hardware
- *      max_frame_length = the maximum frame length we want to accept.
- *      min_frame_length = the minimum frame length we want to accept.
- *
- * This macro is a conditional that should be used in the interrupt
- * handler's Rx processing routine when RxErrors have been detected.
- *
- * Typical use:
- *  ...
- *  if (TBI_ACCEPT) {
- *      accept_frame = TRUE;
- *      e1000_tbi_adjust_stats(adapter, MacAddress);
- *      frame_length--;
- *  } else {
- *      accept_frame = FALSE;
- *  }
- *  ...
- */
-
-#define TBI_ACCEPT(adapter, status, errors, length, last_byte) \
-    ((adapter)->tbi_compatibility_on && \
-     (((errors) & E1000_RXD_ERR_FRAME_ERR_MASK) == E1000_RXD_ERR_CE) && \
-     ((last_byte) == CARRIER_EXTENSION) && \
-     (((status) & E1000_RXD_STAT_VP) ? \
-          (((length) > ((adapter)->min_frame_size - VLAN_TAG_SIZE)) && \
-           ((length) <= ((adapter)->max_frame_size + 1))) : \
-          (((length) > (adapter)->min_frame_size) && \
-           ((length) <= ((adapter)->max_frame_size + VLAN_TAG_SIZE + 1)))))
-
-
-/* Structures, enums, and macros for the PHY */
-
-/* Bit definitions for the Management Data IO (MDIO) and Management Data
- * Clock (MDC) pins in the Device Control Register.
- */
-#define E1000_CTRL_PHY_RESET_DIR  E1000_CTRL_SWDPIO0
-#define E1000_CTRL_PHY_RESET      E1000_CTRL_SWDPIN0
-#define E1000_CTRL_MDIO_DIR       E1000_CTRL_SWDPIO2
-#define E1000_CTRL_MDIO           E1000_CTRL_SWDPIN2
-#define E1000_CTRL_MDC_DIR        E1000_CTRL_SWDPIO3
-#define E1000_CTRL_MDC            E1000_CTRL_SWDPIN3
-#define E1000_CTRL_PHY_RESET_DIR4 E1000_CTRL_EXT_SDP4_DIR
-#define E1000_CTRL_PHY_RESET4     E1000_CTRL_EXT_SDP4_DATA
-
-/* PHY 1000 MII Register/Bit Definitions */
-/* PHY Registers defined by IEEE */
-#define PHY_CTRL         0x00 /* Control Register */
-#define PHY_STATUS       0x01 /* Status Regiser */
-#define PHY_ID1          0x02 /* Phy Id Reg (word 1) */
-#define PHY_ID2          0x03 /* Phy Id Reg (word 2) */
-#define PHY_AUTONEG_ADV  0x04 /* Autoneg Advertisement */
-#define PHY_LP_ABILITY   0x05 /* Link Partner Ability (Base Page) */
-#define PHY_AUTONEG_EXP  0x06 /* Autoneg Expansion Reg */
-#define PHY_NEXT_PAGE_TX 0x07 /* Next Page TX */
-#define PHY_LP_NEXT_PAGE 0x08 /* Link Partner Next Page */
-#define PHY_1000T_CTRL   0x09 /* 1000Base-T Control Reg */
-#define PHY_1000T_STATUS 0x0A /* 1000Base-T Status Reg */
-#define PHY_EXT_STATUS   0x0F /* Extended Status Reg */
-
-/* M88E1000 Specific Registers */
-#define M88E1000_PHY_SPEC_CTRL     0x10  /* PHY Specific Control Register */
-#define M88E1000_PHY_SPEC_STATUS   0x11  /* PHY Specific Status Register */
-#define M88E1000_INT_ENABLE        0x12  /* Interrupt Enable Register */
-#define M88E1000_INT_STATUS        0x13  /* Interrupt Status Register */
-#define M88E1000_EXT_PHY_SPEC_CTRL 0x14  /* Extended PHY Specific Control */
-#define M88E1000_RX_ERR_CNTR       0x15  /* Receive Error Counter */
-
-#define M88E1000_PHY_EXT_CTRL      0x1A  /* PHY extend control register */
-#define M88E1000_PHY_PAGE_SELECT   0x1D  /* Reg 29 for page number setting */
-#define M88E1000_PHY_GEN_CONTROL   0x1E  /* Its meaning depends on reg 29 */
-#define M88E1000_PHY_VCO_REG_BIT8  0x100 /* Bits 8 & 11 are adjusted for */
-#define M88E1000_PHY_VCO_REG_BIT11 0x800    /* improved BER performance */
-
-#define IGP01E1000_IEEE_REGS_PAGE  0x0000
-#define IGP01E1000_IEEE_RESTART_AUTONEG 0x3300
-#define IGP01E1000_IEEE_FORCE_GIGA      0x0140
-
-/* IGP01E1000 Specific Registers */
-#define IGP01E1000_PHY_PORT_CONFIG 0x10 /* PHY Specific Port Config Register */
-#define IGP01E1000_PHY_PORT_STATUS 0x11 /* PHY Specific Status Register */
-#define IGP01E1000_PHY_PORT_CTRL   0x12 /* PHY Specific Control Register */
-#define IGP01E1000_PHY_LINK_HEALTH 0x13 /* PHY Link Health Register */
-#define IGP01E1000_GMII_FIFO       0x14 /* GMII FIFO Register */
-#define IGP01E1000_PHY_CHANNEL_QUALITY 0x15 /* PHY Channel Quality Register */
-#define IGP01E1000_PHY_PAGE_SELECT     0x1F /* PHY Page Select Core Register */
-
-/* IGP01E1000 AGC Registers - stores the cable length values*/
-#define IGP01E1000_PHY_AGC_A        0x1172
-#define IGP01E1000_PHY_AGC_B        0x1272
-#define IGP01E1000_PHY_AGC_C        0x1472
-#define IGP01E1000_PHY_AGC_D        0x1872
-
-/* IGP01E1000 DSP Reset Register */
-#define IGP01E1000_PHY_DSP_RESET   0x1F33
-#define IGP01E1000_PHY_DSP_SET     0x1F71
-#define IGP01E1000_PHY_DSP_FFE     0x1F35
-
-#define IGP01E1000_PHY_CHANNEL_NUM    4
-#define IGP01E1000_PHY_AGC_PARAM_A    0x1171
-#define IGP01E1000_PHY_AGC_PARAM_B    0x1271
-#define IGP01E1000_PHY_AGC_PARAM_C    0x1471
-#define IGP01E1000_PHY_AGC_PARAM_D    0x1871
-
-#define IGP01E1000_PHY_EDAC_MU_INDEX        0xC000
-#define IGP01E1000_PHY_EDAC_SIGN_EXT_9_BITS 0x8000
-
-#define IGP01E1000_PHY_ANALOG_TX_STATE      0x2890
-#define IGP01E1000_PHY_ANALOG_CLASS_A       0x2000
-#define IGP01E1000_PHY_FORCE_ANALOG_ENABLE  0x0004
-#define IGP01E1000_PHY_DSP_FFE_CM_CP        0x0069
-
-#define IGP01E1000_PHY_DSP_FFE_DEFAULT      0x002A
-/* IGP01E1000 PCS Initialization register - stores the polarity status when
- * speed = 1000 Mbps. */
-#define IGP01E1000_PHY_PCS_INIT_REG  0x00B4
-#define IGP01E1000_PHY_PCS_CTRL_REG  0x00B5
-
-#define IGP01E1000_ANALOG_REGS_PAGE  0x20C0
-
-#define MAX_PHY_REG_ADDRESS 0x1F        /* 5 bit address bus (0-0x1F) */
-#define MAX_PHY_MULTI_PAGE_REG  0xF     /*Registers that are equal on all pages*/
-/* PHY Control Register */
-#define MII_CR_SPEED_SELECT_MSB 0x0040  /* bits 6,13: 10=1000, 01=100, 00=10 */
-#define MII_CR_COLL_TEST_ENABLE 0x0080  /* Collision test enable */
-#define MII_CR_FULL_DUPLEX      0x0100  /* FDX =1, half duplex =0 */
-#define MII_CR_RESTART_AUTO_NEG 0x0200  /* Restart auto negotiation */
-#define MII_CR_ISOLATE          0x0400  /* Isolate PHY from MII */
-#define MII_CR_POWER_DOWN       0x0800  /* Power down */
-#define MII_CR_AUTO_NEG_EN      0x1000  /* Auto Neg Enable */
-#define MII_CR_SPEED_SELECT_LSB 0x2000  /* bits 6,13: 10=1000, 01=100, 00=10 */
-#define MII_CR_LOOPBACK         0x4000  /* 0 = normal, 1 = loopback */
-#define MII_CR_RESET            0x8000  /* 0 = normal, 1 = PHY reset */
-
-/* PHY Status Register */
-#define MII_SR_EXTENDED_CAPS     0x0001 /* Extended register capabilities */
-#define MII_SR_JABBER_DETECT     0x0002 /* Jabber Detected */
-#define MII_SR_LINK_STATUS       0x0004 /* Link Status 1 = link */
-#define MII_SR_AUTONEG_CAPS      0x0008 /* Auto Neg Capable */
-#define MII_SR_REMOTE_FAULT      0x0010 /* Remote Fault Detect */
-#define MII_SR_AUTONEG_COMPLETE  0x0020 /* Auto Neg Complete */
-#define MII_SR_PREAMBLE_SUPPRESS 0x0040 /* Preamble may be suppressed */
-#define MII_SR_EXTENDED_STATUS   0x0100 /* Ext. status info in Reg 0x0F */
-#define MII_SR_100T2_HD_CAPS     0x0200 /* 100T2 Half Duplex Capable */
-#define MII_SR_100T2_FD_CAPS     0x0400 /* 100T2 Full Duplex Capable */
-#define MII_SR_10T_HD_CAPS       0x0800 /* 10T   Half Duplex Capable */
-#define MII_SR_10T_FD_CAPS       0x1000 /* 10T   Full Duplex Capable */
-#define MII_SR_100X_HD_CAPS      0x2000 /* 100X  Half Duplex Capable */
-#define MII_SR_100X_FD_CAPS      0x4000 /* 100X  Full Duplex Capable */
-#define MII_SR_100T4_CAPS        0x8000 /* 100T4 Capable */
-
-/* Autoneg Advertisement Register */
-#define NWAY_AR_SELECTOR_FIELD 0x0001   /* indicates IEEE 802.3 CSMA/CD */
-#define NWAY_AR_10T_HD_CAPS    0x0020   /* 10T   Half Duplex Capable */
-#define NWAY_AR_10T_FD_CAPS    0x0040   /* 10T   Full Duplex Capable */
-#define NWAY_AR_100TX_HD_CAPS  0x0080   /* 100TX Half Duplex Capable */
-#define NWAY_AR_100TX_FD_CAPS  0x0100   /* 100TX Full Duplex Capable */
-#define NWAY_AR_100T4_CAPS     0x0200   /* 100T4 Capable */
-#define NWAY_AR_PAUSE          0x0400   /* Pause operation desired */
-#define NWAY_AR_ASM_DIR        0x0800   /* Asymmetric Pause Direction bit */
-#define NWAY_AR_REMOTE_FAULT   0x2000   /* Remote Fault detected */
-#define NWAY_AR_NEXT_PAGE      0x8000   /* Next Page ability supported */
-
-/* Link Partner Ability Register (Base Page) */
-#define NWAY_LPAR_SELECTOR_FIELD 0x0000 /* LP protocol selector field */
-#define NWAY_LPAR_10T_HD_CAPS    0x0020 /* LP is 10T   Half Duplex Capable */
-#define NWAY_LPAR_10T_FD_CAPS    0x0040 /* LP is 10T   Full Duplex Capable */
-#define NWAY_LPAR_100TX_HD_CAPS  0x0080 /* LP is 100TX Half Duplex Capable */
-#define NWAY_LPAR_100TX_FD_CAPS  0x0100 /* LP is 100TX Full Duplex Capable */
-#define NWAY_LPAR_100T4_CAPS     0x0200 /* LP is 100T4 Capable */
-#define NWAY_LPAR_PAUSE          0x0400 /* LP Pause operation desired */
-#define NWAY_LPAR_ASM_DIR        0x0800 /* LP Asymmetric Pause Direction bit */
-#define NWAY_LPAR_REMOTE_FAULT   0x2000 /* LP has detected Remote Fault */
-#define NWAY_LPAR_ACKNOWLEDGE    0x4000 /* LP has rx'd link code word */
-#define NWAY_LPAR_NEXT_PAGE      0x8000 /* Next Page ability supported */
-
-/* Autoneg Expansion Register */
-#define NWAY_ER_LP_NWAY_CAPS      0x0001 /* LP has Auto Neg Capability */
-#define NWAY_ER_PAGE_RXD          0x0002 /* LP is 10T   Half Duplex Capable */
-#define NWAY_ER_NEXT_PAGE_CAPS    0x0004 /* LP is 10T   Full Duplex Capable */
-#define NWAY_ER_LP_NEXT_PAGE_CAPS 0x0008 /* LP is 100TX Half Duplex Capable */
-#define NWAY_ER_PAR_DETECT_FAULT  0x0010 /* LP is 100TX Full Duplex Capable */
-
-/* Next Page TX Register */
-#define NPTX_MSG_CODE_FIELD 0x0001 /* NP msg code or unformatted data */
-#define NPTX_TOGGLE         0x0800 /* Toggles between exchanges
-                                    * of different NP
-                                    */
-#define NPTX_ACKNOWLDGE2    0x1000 /* 1 = will comply with msg
-                                    * 0 = cannot comply with msg
-                                    */
-#define NPTX_MSG_PAGE       0x2000 /* formatted(1)/unformatted(0) pg */
-#define NPTX_NEXT_PAGE      0x8000 /* 1 = addition NP will follow
-                                    * 0 = sending last NP
-                                    */
-
-/* Link Partner Next Page Register */
-#define LP_RNPR_MSG_CODE_FIELD 0x0001 /* NP msg code or unformatted data */
-#define LP_RNPR_TOGGLE         0x0800 /* Toggles between exchanges
-                                       * of different NP
-                                       */
-#define LP_RNPR_ACKNOWLDGE2    0x1000 /* 1 = will comply with msg
-                                       * 0 = cannot comply with msg
-                                       */
-#define LP_RNPR_MSG_PAGE       0x2000  /* formatted(1)/unformatted(0) pg */
-#define LP_RNPR_ACKNOWLDGE     0x4000  /* 1 = ACK / 0 = NO ACK */
-#define LP_RNPR_NEXT_PAGE      0x8000  /* 1 = addition NP will follow
-                                        * 0 = sending last NP
-                                        */
-
-/* 1000BASE-T Control Register */
-#define CR_1000T_ASYM_PAUSE      0x0080 /* Advertise asymmetric pause bit */
-#define CR_1000T_HD_CAPS         0x0100 /* Advertise 1000T HD capability */
-#define CR_1000T_FD_CAPS         0x0200 /* Advertise 1000T FD capability  */
-#define CR_1000T_REPEATER_DTE    0x0400 /* 1=Repeater/switch device port */
-                                        /* 0=DTE device */
-#define CR_1000T_MS_VALUE        0x0800 /* 1=Configure PHY as Master */
-                                        /* 0=Configure PHY as Slave */
-#define CR_1000T_MS_ENABLE       0x1000 /* 1=Master/Slave manual config value */
-                                        /* 0=Automatic Master/Slave config */
-#define CR_1000T_TEST_MODE_NORMAL 0x0000 /* Normal Operation */
-#define CR_1000T_TEST_MODE_1     0x2000 /* Transmit Waveform test */
-#define CR_1000T_TEST_MODE_2     0x4000 /* Master Transmit Jitter test */
-#define CR_1000T_TEST_MODE_3     0x6000 /* Slave Transmit Jitter test */
-#define CR_1000T_TEST_MODE_4     0x8000 /* Transmitter Distortion test */
-
-/* 1000BASE-T Status Register */
-#define SR_1000T_IDLE_ERROR_CNT   0x00FF /* Num idle errors since last read */
-#define SR_1000T_ASYM_PAUSE_DIR   0x0100 /* LP asymmetric pause direction bit */
-#define SR_1000T_LP_HD_CAPS       0x0400 /* LP is 1000T HD capable */
-#define SR_1000T_LP_FD_CAPS       0x0800 /* LP is 1000T FD capable */
-#define SR_1000T_REMOTE_RX_STATUS 0x1000 /* Remote receiver OK */
-#define SR_1000T_LOCAL_RX_STATUS  0x2000 /* Local receiver OK */
-#define SR_1000T_MS_CONFIG_RES    0x4000 /* 1=Local TX is Master, 0=Slave */
-#define SR_1000T_MS_CONFIG_FAULT  0x8000 /* Master/Slave config fault */
-#define SR_1000T_REMOTE_RX_STATUS_SHIFT          12
-#define SR_1000T_LOCAL_RX_STATUS_SHIFT           13
-#define SR_1000T_PHY_EXCESSIVE_IDLE_ERR_COUNT    5
-#define FFE_IDLE_ERR_COUNT_TIMEOUT_20            20
-#define FFE_IDLE_ERR_COUNT_TIMEOUT_100           100
-
-/* Extended Status Register */
-#define IEEE_ESR_1000T_HD_CAPS 0x1000 /* 1000T HD capable */
-#define IEEE_ESR_1000T_FD_CAPS 0x2000 /* 1000T FD capable */
-#define IEEE_ESR_1000X_HD_CAPS 0x4000 /* 1000X HD capable */
-#define IEEE_ESR_1000X_FD_CAPS 0x8000 /* 1000X FD capable */
-
-#define PHY_TX_POLARITY_MASK   0x0100 /* register 10h bit 8 (polarity bit) */
-#define PHY_TX_NORMAL_POLARITY 0      /* register 10h bit 8 (normal polarity) */
-
-#define AUTO_POLARITY_DISABLE  0x0010 /* register 11h bit 4 */
-                                      /* (0=enable, 1=disable) */
-
-/* M88E1000 PHY Specific Control Register */
-#define M88E1000_PSCR_JABBER_DISABLE    0x0001 /* 1=Jabber Function disabled */
-#define M88E1000_PSCR_POLARITY_REVERSAL 0x0002 /* 1=Polarity Reversal enabled */
-#define M88E1000_PSCR_SQE_TEST          0x0004 /* 1=SQE Test enabled */
-#define M88E1000_PSCR_CLK125_DISABLE    0x0010 /* 1=CLK125 low,
-                                                * 0=CLK125 toggling
-                                                */
-#define M88E1000_PSCR_MDI_MANUAL_MODE  0x0000  /* MDI Crossover Mode bits 6:5 */
-                                               /* Manual MDI configuration */
-#define M88E1000_PSCR_MDIX_MANUAL_MODE 0x0020  /* Manual MDIX configuration */
-#define M88E1000_PSCR_AUTO_X_1000T     0x0040  /* 1000BASE-T: Auto crossover,
-                                                *  100BASE-TX/10BASE-T:
-                                                *  MDI Mode
-                                                */
-#define M88E1000_PSCR_AUTO_X_MODE      0x0060  /* Auto crossover enabled
-                                                * all speeds.
-                                                */
-#define M88E1000_PSCR_10BT_EXT_DIST_ENABLE 0x0080
-                                        /* 1=Enable Extended 10BASE-T distance
-                                         * (Lower 10BASE-T RX Threshold)
-                                         * 0=Normal 10BASE-T RX Threshold */
-#define M88E1000_PSCR_MII_5BIT_ENABLE      0x0100
-                                        /* 1=5-Bit interface in 100BASE-TX
-                                         * 0=MII interface in 100BASE-TX */
-#define M88E1000_PSCR_SCRAMBLER_DISABLE    0x0200 /* 1=Scrambler disable */
-#define M88E1000_PSCR_FORCE_LINK_GOOD      0x0400 /* 1=Force link good */
-#define M88E1000_PSCR_ASSERT_CRS_ON_TX     0x0800 /* 1=Assert CRS on Transmit */
-
-#define M88E1000_PSCR_POLARITY_REVERSAL_SHIFT    1
-#define M88E1000_PSCR_AUTO_X_MODE_SHIFT          5
-#define M88E1000_PSCR_10BT_EXT_DIST_ENABLE_SHIFT 7
-
-/* M88E1000 PHY Specific Status Register */
-#define M88E1000_PSSR_JABBER             0x0001 /* 1=Jabber */
-#define M88E1000_PSSR_REV_POLARITY       0x0002 /* 1=Polarity reversed */
-#define M88E1000_PSSR_DOWNSHIFT          0x0020 /* 1=Downshifted */
-#define M88E1000_PSSR_MDIX               0x0040 /* 1=MDIX; 0=MDI */
-#define M88E1000_PSSR_CABLE_LENGTH       0x0380 /* 0=<50M;1=50-80M;2=80-110M;
-                                            * 3=110-140M;4=>140M */
-#define M88E1000_PSSR_LINK               0x0400 /* 1=Link up, 0=Link down */
-#define M88E1000_PSSR_SPD_DPLX_RESOLVED  0x0800 /* 1=Speed & Duplex resolved */
-#define M88E1000_PSSR_PAGE_RCVD          0x1000 /* 1=Page received */
-#define M88E1000_PSSR_DPLX               0x2000 /* 1=Duplex 0=Half Duplex */
-#define M88E1000_PSSR_SPEED              0xC000 /* Speed, bits 14:15 */
-#define M88E1000_PSSR_10MBS              0x0000 /* 00=10Mbs */
-#define M88E1000_PSSR_100MBS             0x4000 /* 01=100Mbs */
-#define M88E1000_PSSR_1000MBS            0x8000 /* 10=1000Mbs */
-
-#define M88E1000_PSSR_REV_POLARITY_SHIFT 1
-#define M88E1000_PSSR_DOWNSHIFT_SHIFT    5
-#define M88E1000_PSSR_MDIX_SHIFT         6
-#define M88E1000_PSSR_CABLE_LENGTH_SHIFT 7
-
-/* M88E1000 Extended PHY Specific Control Register */
-#define M88E1000_EPSCR_FIBER_LOOPBACK 0x4000 /* 1=Fiber loopback */
-#define M88E1000_EPSCR_DOWN_NO_IDLE   0x8000 /* 1=Lost lock detect enabled.
-                                              * Will assert lost lock and bring
-                                              * link down if idle not seen
-                                              * within 1ms in 1000BASE-T
-                                              */
-/* Number of times we will attempt to autonegotiate before downshifting if we
- * are the master */
-#define M88E1000_EPSCR_MASTER_DOWNSHIFT_MASK 0x0C00
-#define M88E1000_EPSCR_MASTER_DOWNSHIFT_1X   0x0000
-#define M88E1000_EPSCR_MASTER_DOWNSHIFT_2X   0x0400
-#define M88E1000_EPSCR_MASTER_DOWNSHIFT_3X   0x0800
-#define M88E1000_EPSCR_MASTER_DOWNSHIFT_4X   0x0C00
-/* Number of times we will attempt to autonegotiate before downshifting if we
- * are the slave */
-#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_MASK  0x0300
-#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_DIS   0x0000
-#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_1X    0x0100
-#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_2X    0x0200
-#define M88E1000_EPSCR_SLAVE_DOWNSHIFT_3X    0x0300
-#define M88E1000_EPSCR_TX_CLK_2_5     0x0060 /* 2.5 MHz TX_CLK */
-#define M88E1000_EPSCR_TX_CLK_25      0x0070 /* 25  MHz TX_CLK */
-#define M88E1000_EPSCR_TX_CLK_0       0x0000 /* NO  TX_CLK */
-
-/* IGP01E1000 Specific Port Config Register - R/W */
-#define IGP01E1000_PSCFR_AUTO_MDIX_PAR_DETECT  0x0010
-#define IGP01E1000_PSCFR_PRE_EN                0x0020
-#define IGP01E1000_PSCFR_SMART_SPEED           0x0080
-#define IGP01E1000_PSCFR_DISABLE_TPLOOPBACK    0x0100
-#define IGP01E1000_PSCFR_DISABLE_JABBER        0x0400
-#define IGP01E1000_PSCFR_DISABLE_TRANSMIT      0x2000
-
-/* IGP01E1000 Specific Port Status Register - R/O */
-#define IGP01E1000_PSSR_AUTONEG_FAILED         0x0001 /* RO LH SC */
-#define IGP01E1000_PSSR_POLARITY_REVERSED      0x0002
-#define IGP01E1000_PSSR_CABLE_LENGTH           0x007C
-#define IGP01E1000_PSSR_FULL_DUPLEX            0x0200
-#define IGP01E1000_PSSR_LINK_UP                0x0400
-#define IGP01E1000_PSSR_MDIX                   0x0800
-#define IGP01E1000_PSSR_SPEED_MASK             0xC000 /* speed bits mask */
-#define IGP01E1000_PSSR_SPEED_10MBPS           0x4000
-#define IGP01E1000_PSSR_SPEED_100MBPS          0x8000
-#define IGP01E1000_PSSR_SPEED_1000MBPS         0xC000
-#define IGP01E1000_PSSR_CABLE_LENGTH_SHIFT     0x0002 /* shift right 2 */
-#define IGP01E1000_PSSR_MDIX_SHIFT             0x000B /* shift right 11 */
-
-/* IGP01E1000 Specific Port Control Register - R/W */
-#define IGP01E1000_PSCR_TP_LOOPBACK            0x0010
-#define IGP01E1000_PSCR_CORRECT_NC_SCMBLR      0x0200
-#define IGP01E1000_PSCR_TEN_CRS_SELECT         0x0400
-#define IGP01E1000_PSCR_FLIP_CHIP              0x0800
-#define IGP01E1000_PSCR_AUTO_MDIX              0x1000
-#define IGP01E1000_PSCR_FORCE_MDI_MDIX         0x2000 /* 0-MDI, 1-MDIX */
-
-/* IGP01E1000 Specific Port Link Health Register */
-#define IGP01E1000_PLHR_SS_DOWNGRADE           0x8000
-#define IGP01E1000_PLHR_GIG_SCRAMBLER_ERROR    0x4000
-#define IGP01E1000_PLHR_MASTER_FAULT           0x2000
-#define IGP01E1000_PLHR_MASTER_RESOLUTION      0x1000
-#define IGP01E1000_PLHR_GIG_REM_RCVR_NOK       0x0800 /* LH */
-#define IGP01E1000_PLHR_IDLE_ERROR_CNT_OFLOW   0x0400 /* LH */
-#define IGP01E1000_PLHR_DATA_ERR_1             0x0200 /* LH */
-#define IGP01E1000_PLHR_DATA_ERR_0             0x0100
-#define IGP01E1000_PLHR_AUTONEG_FAULT          0x0040
-#define IGP01E1000_PLHR_AUTONEG_ACTIVE         0x0010
-#define IGP01E1000_PLHR_VALID_CHANNEL_D        0x0008
-#define IGP01E1000_PLHR_VALID_CHANNEL_C        0x0004
-#define IGP01E1000_PLHR_VALID_CHANNEL_B        0x0002
-#define IGP01E1000_PLHR_VALID_CHANNEL_A        0x0001
-
-/* IGP01E1000 Channel Quality Register */
-#define IGP01E1000_MSE_CHANNEL_D        0x000F
-#define IGP01E1000_MSE_CHANNEL_C        0x00F0
-#define IGP01E1000_MSE_CHANNEL_B        0x0F00
-#define IGP01E1000_MSE_CHANNEL_A        0xF000
-
-/* IGP01E1000 DSP reset macros */
-#define DSP_RESET_ENABLE     0x0
-#define DSP_RESET_DISABLE    0x2
-#define E1000_MAX_DSP_RESETS 10
-
-/* IGP01E1000 AGC Registers */
-
-#define IGP01E1000_AGC_LENGTH_SHIFT 7         /* Coarse - 13:11, Fine - 10:7 */
-
-/* 7 bits (3 Coarse + 4 Fine) --> 128 optional values */
-#define IGP01E1000_AGC_LENGTH_TABLE_SIZE 128
-
-/* The precision of the length is +/- 10 meters */
-#define IGP01E1000_AGC_RANGE    10
-
-/* IGP01E1000 PCS Initialization register */
-/* bits 3:6 in the PCS registers stores the channels polarity */
-#define IGP01E1000_PHY_POLARITY_MASK    0x0078
-
-/* IGP01E1000 GMII FIFO Register */
-#define IGP01E1000_GMII_FLEX_SPD               0x10 /* Enable flexible speed
-                                                     * on Link-Up */
-#define IGP01E1000_GMII_SPD                    0x20 /* Enable SPD */
-
-/* IGP01E1000 Analog Register */
-#define IGP01E1000_ANALOG_SPARE_FUSE_STATUS       0x20D1
-#define IGP01E1000_ANALOG_FUSE_STATUS             0x20D0
-#define IGP01E1000_ANALOG_FUSE_CONTROL            0x20DC
-#define IGP01E1000_ANALOG_FUSE_BYPASS             0x20DE
-
-#define IGP01E1000_ANALOG_FUSE_POLY_MASK            0xF000
-#define IGP01E1000_ANALOG_FUSE_FINE_MASK            0x0F80
-#define IGP01E1000_ANALOG_FUSE_COARSE_MASK          0x0070
-#define IGP01E1000_ANALOG_SPARE_FUSE_ENABLED        0x0100
-#define IGP01E1000_ANALOG_FUSE_ENABLE_SW_CONTROL    0x0002
-
-#define IGP01E1000_ANALOG_FUSE_COARSE_THRESH        0x0040
-#define IGP01E1000_ANALOG_FUSE_COARSE_10            0x0010
-#define IGP01E1000_ANALOG_FUSE_FINE_1               0x0080
-#define IGP01E1000_ANALOG_FUSE_FINE_10              0x0500
-
-/* Bit definitions for valid PHY IDs. */
-#define M88E1000_E_PHY_ID  0x01410C50
-#define M88E1000_I_PHY_ID  0x01410C30
-#define M88E1011_I_PHY_ID  0x01410C20
-#define IGP01E1000_I_PHY_ID  0x02A80380
-#define M88E1000_12_PHY_ID M88E1000_E_PHY_ID
-#define M88E1000_14_PHY_ID M88E1000_E_PHY_ID
-#define M88E1011_I_REV_4   0x04
-
-/* Miscellaneous PHY bit definitions. */
-#define PHY_PREAMBLE        0xFFFFFFFF
-#define PHY_SOF             0x01
-#define PHY_OP_READ         0x02
-#define PHY_OP_WRITE        0x01
-#define PHY_TURNAROUND      0x02
-#define PHY_PREAMBLE_SIZE   32
-#define MII_CR_SPEED_1000   0x0040
-#define MII_CR_SPEED_100    0x2000
-#define MII_CR_SPEED_10     0x0000
-#define E1000_PHY_ADDRESS   0x01
-#define PHY_AUTO_NEG_TIME   45  /* 4.5 Seconds */
-#define PHY_FORCE_TIME      20  /* 2.0 Seconds */
-#define PHY_REVISION_MASK   0xFFFFFFF0
-#define DEVICE_SPEED_MASK   0x00000300  /* Device Ctrl Reg Speed Mask */
-#define REG4_SPEED_MASK     0x01E0
-#define REG9_SPEED_MASK     0x0300
-#define ADVERTISE_10_HALF   0x0001
-#define ADVERTISE_10_FULL   0x0002
-#define ADVERTISE_100_HALF  0x0004
-#define ADVERTISE_100_FULL  0x0008
-#define ADVERTISE_1000_HALF 0x0010
-#define ADVERTISE_1000_FULL 0x0020
-#define AUTONEG_ADVERTISE_SPEED_DEFAULT 0x002F  /* Everything but 1000-Half */
-#define AUTONEG_ADVERTISE_10_100_ALL    0x000F /* All 10/100 speeds*/
-#define AUTONEG_ADVERTISE_10_ALL        0x0003 /* 10Mbps Full & Half speeds*/
-
-#endif /* _E1000_HW_H_ */
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_ich8lan.c linux-2.6.9/drivers/net/e1000/e1000_ich8lan.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_ich8lan.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_ich8lan.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,2425 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/* e1000_ich8lan
+ * e1000_ich9lan
+ */
+
+#include "e1000_api.h"
+#include "e1000_ich8lan.h"
+
+void e1000_init_function_pointers_ich8lan(struct e1000_hw *hw);
+
+static s32       e1000_init_phy_params_ich8lan(struct e1000_hw *hw);
+static s32       e1000_init_nvm_params_ich8lan(struct e1000_hw *hw);
+static s32       e1000_init_mac_params_ich8lan(struct e1000_hw *hw);
+static s32       e1000_acquire_swflag_ich8lan(struct e1000_hw *hw);
+static void      e1000_release_swflag_ich8lan(struct e1000_hw *hw);
+static boolean_t e1000_check_mng_mode_ich8lan(struct e1000_hw *hw);
+static s32       e1000_check_polarity_ife_ich8lan(struct e1000_hw *hw);
+static s32       e1000_check_reset_block_ich8lan(struct e1000_hw *hw);
+static s32       e1000_phy_force_speed_duplex_ich8lan(struct e1000_hw *hw);
+static s32       e1000_phy_hw_reset_ich8lan(struct e1000_hw *hw);
+static s32       e1000_get_phy_info_ich8lan(struct e1000_hw *hw);
+static s32       e1000_set_d0_lplu_state_ich8lan(struct e1000_hw *hw,
+                                                 boolean_t active);
+static s32       e1000_set_d3_lplu_state_ich8lan(struct e1000_hw *hw,
+                                                 boolean_t active);
+static s32       e1000_read_nvm_ich8lan(struct e1000_hw *hw, u16 offset,
+                                        u16 words, u16 *data);
+static s32       e1000_write_nvm_ich8lan(struct e1000_hw *hw, u16 offset,
+                                         u16 words, u16 *data);
+static s32       e1000_validate_nvm_checksum_ich8lan(struct e1000_hw *hw);
+static s32       e1000_update_nvm_checksum_ich8lan(struct e1000_hw *hw);
+static s32       e1000_valid_led_default_ich8lan(struct e1000_hw *hw,
+                                                 u16 *data);
+static s32       e1000_get_bus_info_ich8lan(struct e1000_hw *hw);
+static s32       e1000_reset_hw_ich8lan(struct e1000_hw *hw);
+static s32       e1000_init_hw_ich8lan(struct e1000_hw *hw);
+static s32       e1000_setup_link_ich8lan(struct e1000_hw *hw);
+static s32       e1000_setup_copper_link_ich8lan(struct e1000_hw *hw);
+static s32       e1000_get_link_up_info_ich8lan(struct e1000_hw *hw,
+                                                u16 *speed, u16 *duplex);
+static s32       e1000_cleanup_led_ich8lan(struct e1000_hw *hw);
+static s32       e1000_led_on_ich8lan(struct e1000_hw *hw);
+static s32       e1000_led_off_ich8lan(struct e1000_hw *hw);
+static void      e1000_clear_hw_cntrs_ich8lan(struct e1000_hw *hw);
+static s32       e1000_erase_flash_bank_ich8lan(struct e1000_hw *hw, u32 bank);
+static s32       e1000_flash_cycle_ich8lan(struct e1000_hw *hw, u32 timeout);
+static s32       e1000_flash_cycle_init_ich8lan(struct e1000_hw *hw);
+static s32       e1000_get_phy_info_ife_ich8lan(struct e1000_hw *hw);
+static void      e1000_initialize_hw_bits_ich8lan(struct e1000_hw *hw);
+static s32       e1000_kmrn_lock_loss_workaround_ich8lan(struct e1000_hw *hw);
+static s32       e1000_read_flash_data_ich8lan(struct e1000_hw *hw, u32 offset,
+                                               u8 size, u16* data);
+static s32       e1000_read_flash_word_ich8lan(struct e1000_hw *hw,
+                                               u32 offset, u16 *data);
+static s32       e1000_retry_write_flash_byte_ich8lan(struct e1000_hw *hw,
+                                                      u32 offset, u8 byte);
+static s32       e1000_write_flash_byte_ich8lan(struct e1000_hw *hw,
+                                                u32 offset, u8 data);
+static s32       e1000_write_flash_data_ich8lan(struct e1000_hw *hw, u32 offset,
+                                                u8 size, u16 data);
+
+/* ICH GbE Flash Hardware Sequencing Flash Status Register bit breakdown */
+/* Offset 04h HSFSTS */
+union ich8_hws_flash_status {
+	struct ich8_hsfsts {
+		u16 flcdone    :1; /* bit 0 Flash Cycle Done */
+		u16 flcerr     :1; /* bit 1 Flash Cycle Error */
+		u16 dael       :1; /* bit 2 Direct Access error Log */
+		u16 berasesz   :2; /* bit 4:3 Sector Erase Size */
+		u16 flcinprog  :1; /* bit 5 flash cycle in Progress */
+		u16 reserved1  :2; /* bit 13:6 Reserved */
+		u16 reserved2  :6; /* bit 13:6 Reserved */
+		u16 fldesvalid :1; /* bit 14 Flash Descriptor Valid */
+		u16 flockdn    :1; /* bit 15 Flash Config Lock-Down */
+	} hsf_status;
+	u16 regval;
+};
+
+/* ICH GbE Flash Hardware Sequencing Flash control Register bit breakdown */
+/* Offset 06h FLCTL */
+union ich8_hws_flash_ctrl {
+	struct ich8_hsflctl {
+		u16 flcgo      :1;   /* 0 Flash Cycle Go */
+		u16 flcycle    :2;   /* 2:1 Flash Cycle */
+		u16 reserved   :5;   /* 7:3 Reserved  */
+		u16 fldbcount  :2;   /* 9:8 Flash Data Byte Count */
+		u16 flockdn    :6;   /* 15:10 Reserved */
+	} hsf_ctrl;
+	u16 regval;
+};
+
+/* ICH Flash Region Access Permissions */
+union ich8_hws_flash_regacc {
+	struct ich8_flracc {
+		u32 grra      :8; /* 0:7 GbE region Read Access */
+		u32 grwa      :8; /* 8:15 GbE region Write Access */
+		u32 gmrag     :8; /* 23:16 GbE Master Read Access Grant */
+		u32 gmwag     :8; /* 31:24 GbE Master Write Access Grant */
+	} hsf_flregacc;
+	u16 regval;
+};
+
+struct e1000_shadow_ram {
+	u16  value;
+	boolean_t modified;
+};
+
+struct e1000_dev_spec_ich8lan {
+	boolean_t kmrn_lock_loss_workaround_enabled;
+	struct e1000_shadow_ram shadow_ram[E1000_SHADOW_RAM_WORDS];
+};
+
+/**
+ *  e1000_init_phy_params_ich8lan - Initialize PHY function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  Initialize family-specific PHY parameters and function pointers.
+ **/
+static s32 e1000_init_phy_params_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i = 0;
+
+	DEBUGFUNC("e1000_init_phy_params_ich8lan");
+
+	phy->addr                       = 1;
+	phy->reset_delay_us             = 100;
+
+	func->acquire_phy               = e1000_acquire_swflag_ich8lan;
+	func->check_polarity            = e1000_check_polarity_ife_ich8lan;
+	func->check_reset_block         = e1000_check_reset_block_ich8lan;
+	func->force_speed_duplex        = e1000_phy_force_speed_duplex_ich8lan;
+	func->get_cable_length          = e1000_get_cable_length_igp_2;
+	func->get_cfg_done              = e1000_get_cfg_done_generic;
+	func->get_phy_info              = e1000_get_phy_info_ich8lan;
+	func->read_phy_reg              = e1000_read_phy_reg_igp;
+	func->release_phy               = e1000_release_swflag_ich8lan;
+	func->reset_phy                 = e1000_phy_hw_reset_ich8lan;
+	func->set_d0_lplu_state         = e1000_set_d0_lplu_state_ich8lan;
+	func->set_d3_lplu_state         = e1000_set_d3_lplu_state_ich8lan;
+	func->write_phy_reg             = e1000_write_phy_reg_igp;
+
+
+	phy->id = 0;
+	while ((e1000_phy_unknown == e1000_get_phy_type_from_id(phy->id)) &&
+	       (i++ < 100)) {
+		msec_delay(1);
+		ret_val = e1000_get_phy_id(hw);
+		if (ret_val)
+			goto out;
+	}
+
+	/* Verify phy id */
+	switch (phy->id) {
+	case IGP03E1000_E_PHY_ID:
+		phy->type = e1000_phy_igp_3;
+		phy->autoneg_mask = AUTONEG_ADVERTISE_SPEED_DEFAULT;
+		break;
+	case IFE_E_PHY_ID:
+	case IFE_PLUS_E_PHY_ID:
+	case IFE_C_E_PHY_ID:
+		phy->type = e1000_phy_ife;
+		phy->autoneg_mask = E1000_ALL_NOT_GIG;
+		break;
+	default:
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_nvm_params_ich8lan - Initialize NVM function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  Initialize family-specific NVM parameters and function
+ *  pointers.
+ **/
+static s32 e1000_init_nvm_params_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_functions *func = &hw->func;
+	struct e1000_dev_spec_ich8lan *dev_spec;
+	u32 gfpreg, sector_base_addr, sector_end_addr;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_nvm_params_ich8lan");
+
+	/* Can't read flash registers if the register set isn't mapped.
+	 */
+	if (!hw->flash_address) {
+		DEBUGOUT("ERROR: Flash registers not mapped\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	nvm->type               = e1000_nvm_flash_sw;
+
+	gfpreg = E1000_READ_FLASH_REG(hw, ICH_FLASH_GFPREG);
+
+	/* sector_X_addr is a "sector"-aligned address (4096 bytes)
+	 * Add 1 to sector_end_addr since this sector is included in
+	 * the overall size. */
+	sector_base_addr = gfpreg & FLASH_GFPREG_BASE_MASK;
+	sector_end_addr = ((gfpreg >> 16) & FLASH_GFPREG_BASE_MASK) + 1;
+
+	/* flash_base_addr is byte-aligned */
+	nvm->flash_base_addr    = sector_base_addr << FLASH_SECTOR_ADDR_SHIFT;
+
+	/* find total size of the NVM, then cut in half since the total
+	 * size represents two separate NVM banks. */
+	nvm->flash_bank_size    = (sector_end_addr - sector_base_addr)
+	                          << FLASH_SECTOR_ADDR_SHIFT;
+	nvm->flash_bank_size    /= 2;
+	/* Adjust to word count */
+	nvm->flash_bank_size    /= sizeof(u16);
+
+	nvm->word_size          = E1000_SHADOW_RAM_WORDS;
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	/* Clear shadow ram */
+	for (i = 0; i < nvm->word_size; i++) {
+		dev_spec->shadow_ram[i].modified = FALSE;
+		dev_spec->shadow_ram[i].value    = 0xFFFF;
+	}
+
+	/* Function Pointers */
+	func->acquire_nvm       = e1000_acquire_swflag_ich8lan;
+	func->read_nvm          = e1000_read_nvm_ich8lan;
+	func->release_nvm       = e1000_release_swflag_ich8lan;
+	func->update_nvm        = e1000_update_nvm_checksum_ich8lan;
+	func->valid_led_default = e1000_valid_led_default_ich8lan;
+	func->validate_nvm      = e1000_validate_nvm_checksum_ich8lan;
+	func->write_nvm         = e1000_write_nvm_ich8lan;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_mac_params_ich8lan - Initialize MAC function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  Initialize family-specific MAC parameters and function
+ *  pointers.
+ **/
+static s32 e1000_init_mac_params_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_init_mac_params_ich8lan");
+
+	/* Set media type function pointer */
+	hw->media_type = e1000_media_type_copper;
+
+	/* Set mta register count */
+	mac->mta_reg_count = 32;
+	/* Set rar entry count */
+	mac->rar_entry_count = E1000_ICH_RAR_ENTRIES;
+	if (mac->type == e1000_ich8lan)
+		mac->rar_entry_count--;
+	/* Set if part includes ASF firmware */
+	mac->asf_firmware_present = TRUE;
+	/* Set if manageability features are enabled. */
+	mac->arc_subsystem_valid = TRUE;
+
+	/* Function pointers */
+
+	/* bus type/speed/width */
+	func->get_bus_info = e1000_get_bus_info_ich8lan;
+	/* reset */
+	func->reset_hw = e1000_reset_hw_ich8lan;
+	/* hw initialization */
+	func->init_hw = e1000_init_hw_ich8lan;
+	/* link setup */
+	func->setup_link = e1000_setup_link_ich8lan;
+	/* physical interface setup */
+	func->setup_physical_interface = e1000_setup_copper_link_ich8lan;
+	/* check for link */
+	func->check_for_link = e1000_check_for_copper_link_generic;
+	/* check management mode */
+	func->check_mng_mode = e1000_check_mng_mode_ich8lan;
+	/* link info */
+	func->get_link_up_info = e1000_get_link_up_info_ich8lan;
+	/* multicast address update */
+	func->mc_addr_list_update = e1000_mc_addr_list_update_generic;
+	/* setting MTA */
+	func->mta_set = e1000_mta_set_generic;
+	/* blink LED */
+	func->blink_led = e1000_blink_led_generic;
+	/* setup LED */
+	func->setup_led = e1000_setup_led_generic;
+	/* cleanup LED */
+	func->cleanup_led = e1000_cleanup_led_ich8lan;
+	/* turn on/off LED */
+	func->led_on = e1000_led_on_ich8lan;
+	func->led_off = e1000_led_off_ich8lan;
+	/* remove device */
+	func->remove_device = e1000_remove_device_generic;
+	/* clear hardware counters */
+	func->clear_hw_cntrs = e1000_clear_hw_cntrs_ich8lan;
+
+	hw->dev_spec_size = sizeof(struct e1000_dev_spec_ich8lan);
+
+	/* Device-specific structure allocation */
+	ret_val = e1000_alloc_zeroed_dev_spec_struct(hw, hw->dev_spec_size);
+	if (ret_val)
+		goto out;
+
+	/* Enable PCS Lock-loss workaround for ICH8 */
+	if (mac->type == e1000_ich8lan)
+		e1000_set_kmrn_lock_loss_workaround_ich8lan(hw, TRUE);
+
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_init_function_pointers_ich8lan - Initialize ICH8 function pointers
+ *  @hw: pointer to the HW structure
+ *
+ *  Initialize family-specific function pointers for PHY, MAC, and NVM.
+ **/
+void e1000_init_function_pointers_ich8lan(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_init_function_pointers_ich8lan");
+
+	hw->func.init_mac_params = e1000_init_mac_params_ich8lan;
+	hw->func.init_nvm_params = e1000_init_nvm_params_ich8lan;
+	hw->func.init_phy_params = e1000_init_phy_params_ich8lan;
+}
+
+/**
+ *  e1000_acquire_swflag_ich8lan - Acquire software control flag
+ *  @hw: pointer to the HW structure
+ *
+ *  Acquires the software control flag for performing NVM and PHY
+ *  operations.  This is a function pointer entry point only called by
+ *  read/write routines for the PHY and NVM parts.
+ **/
+static s32 e1000_acquire_swflag_ich8lan(struct e1000_hw *hw)
+{
+	u32 extcnf_ctrl, timeout = PHY_CFG_TIMEOUT;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_acquire_swflag_ich8lan");
+
+	while (timeout) {
+		extcnf_ctrl = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+		extcnf_ctrl |= E1000_EXTCNF_CTRL_SWFLAG;
+		E1000_WRITE_REG(hw, E1000_EXTCNF_CTRL, extcnf_ctrl);
+
+		extcnf_ctrl = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+		if (extcnf_ctrl & E1000_EXTCNF_CTRL_SWFLAG)
+			break;
+		msec_delay_irq(1);
+		timeout--;
+	}
+
+	if (!timeout) {
+		DEBUGOUT("FW or HW has locked the resource for too long.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_release_swflag_ich8lan - Release software control flag
+ *  @hw: pointer to the HW structure
+ *
+ *  Releases the software control flag for performing NVM and PHY operations.
+ *  This is a function pointer entry point only called by read/write
+ *  routines for the PHY and NVM parts.
+ **/
+static void e1000_release_swflag_ich8lan(struct e1000_hw *hw)
+{
+	u32 extcnf_ctrl;
+
+	DEBUGFUNC("e1000_release_swflag_ich8lan");
+
+	extcnf_ctrl = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+	extcnf_ctrl &= ~E1000_EXTCNF_CTRL_SWFLAG;
+	E1000_WRITE_REG(hw, E1000_EXTCNF_CTRL, extcnf_ctrl);
+
+	return;
+}
+
+/**
+ *  e1000_check_mng_mode_ich8lan - Checks management mode
+ *  @hw: pointer to the HW structure
+ *
+ *  This checks if the adapter has manageability enabled.
+ *  This is a function pointer entry point only called by read/write
+ *  routines for the PHY and NVM parts.
+ **/
+static boolean_t e1000_check_mng_mode_ich8lan(struct e1000_hw *hw)
+{
+	u32 fwsm;
+
+	DEBUGFUNC("e1000_check_mng_mode_ich8lan");
+
+	fwsm = E1000_READ_REG(hw, E1000_FWSM);
+
+	return ((fwsm & E1000_FWSM_MODE_MASK) ==
+	        (E1000_ICH_MNG_IAMT_MODE << E1000_FWSM_MODE_SHIFT));
+}
+
+/**
+ *  e1000_check_reset_block_ich8lan - Check if PHY reset is blocked
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks if firmware is blocking the reset of the PHY.
+ *  This is a function pointer entry point only called by
+ *  reset routines.
+ **/
+static s32 e1000_check_reset_block_ich8lan(struct e1000_hw *hw)
+{
+	u32 fwsm;
+
+	DEBUGFUNC("e1000_check_reset_block_ich8lan");
+
+	fwsm = E1000_READ_REG(hw, E1000_FWSM);
+
+	return (fwsm & E1000_ICH_FWSM_RSPCIPHY) ? E1000_SUCCESS
+	                                        : E1000_BLK_PHY_RESET;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_ich8lan - Force PHY speed & duplex
+ *  @hw: pointer to the HW structure
+ *
+ *  Forces the speed and duplex settings of the PHY.
+ *  This is a function pointer entry point only called by
+ *  PHY setup routines.
+ **/
+static s32 e1000_phy_force_speed_duplex_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_ich8lan");
+
+	if (phy->type != e1000_phy_ife) {
+		ret_val = e1000_phy_force_speed_duplex_igp(hw);
+		goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &data);
+	if (ret_val)
+		goto out;
+
+	e1000_phy_force_speed_duplex_setup(hw, &data);
+
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, data);
+	if (ret_val)
+		goto out;
+
+	/* Disable MDI-X support for 10/100 */
+	ret_val = e1000_read_phy_reg(hw, IFE_PHY_MDIX_CONTROL, &data);
+	if (ret_val)
+		goto out;
+
+	data &= ~IFE_PMC_AUTO_MDIX;
+	data &= ~IFE_PMC_FORCE_MDIX;
+
+	ret_val = e1000_write_phy_reg(hw, IFE_PHY_MDIX_CONTROL, data);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT1("IFE PMC: %X\n", data);
+
+	usec_delay(1);
+
+	if (phy->wait_for_link) {
+		DEBUGOUT("Waiting for forced speed/duplex link on IFE phy.\n");
+
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+
+		if (!link) {
+			DEBUGOUT("Link taking longer than expected.\n");
+		}
+
+		/* Try once more */
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_hw_reset_ich8lan - Performs a PHY reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Resets the PHY
+ *  This is a function pointer entry point called by drivers
+ *  or other shared routines.
+ **/
+static s32 e1000_phy_hw_reset_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	u32 i, data, cnf_size, cnf_base_addr, sw_cfg_mask;
+	s32 ret_val;
+	u16 loop = E1000_ICH8_LAN_INIT_TIMEOUT;
+	u16 word_addr, reg_data, reg_addr, phy_page = 0;
+
+	DEBUGFUNC("e1000_phy_hw_reset_ich8lan");
+
+	ret_val = e1000_phy_hw_reset_generic(hw);
+	if (ret_val)
+		goto out;
+
+	/* Initialize the PHY from the NVM on ICH platforms.  This
+	 * is needed due to an issue where the NVM configuration is
+	 * not properly autoloaded after power transitions.
+	 * Therefore, after each PHY reset, we will load the
+	 * configuration data out of the NVM manually.
+	 */
+	if (hw->mac.type == e1000_ich8lan && phy->type == e1000_phy_igp_3) {
+		/* Check if SW needs configure the PHY */
+		if ((hw->device_id == E1000_DEV_ID_ICH8_IGP_M_AMT) ||
+		    (hw->device_id == E1000_DEV_ID_ICH8_IGP_M))
+			sw_cfg_mask = E1000_FEXTNVM_SW_CONFIG_ICH8M;
+		else
+			sw_cfg_mask = E1000_FEXTNVM_SW_CONFIG;
+
+		data = E1000_READ_REG(hw, E1000_FEXTNVM);
+		if (!(data & sw_cfg_mask))
+			goto out;
+
+		/* Wait for basic configuration completes before proceeding*/
+		do {
+			data = E1000_READ_REG(hw, E1000_STATUS);
+			data &= E1000_STATUS_LAN_INIT_DONE;
+			usec_delay(100);
+		} while ((!data) && --loop);
+
+		/* If basic configuration is incomplete before the above loop
+		 * count reaches 0, loading the configuration from NVM will
+		 * leave the PHY in a bad state possibly resulting in no link.
+		 */
+		if (loop == 0) {
+			DEBUGOUT("LAN_INIT_DONE not set, increase timeout\n");
+		}
+
+		/* Clear the Init Done bit for the next init event */
+		data = E1000_READ_REG(hw, E1000_STATUS);
+		data &= ~E1000_STATUS_LAN_INIT_DONE;
+		E1000_WRITE_REG(hw, E1000_STATUS, data);
+
+		/* Make sure HW does not configure LCD from PHY
+		 * extended configuration before SW configuration */
+		data = E1000_READ_REG(hw, E1000_EXTCNF_CTRL);
+		if (data & E1000_EXTCNF_CTRL_LCD_WRITE_ENABLE)
+			goto out;
+
+		cnf_size = E1000_READ_REG(hw, E1000_EXTCNF_SIZE);
+		cnf_size &= E1000_EXTCNF_SIZE_EXT_PCIE_LENGTH_MASK;
+		cnf_size >>= E1000_EXTCNF_SIZE_EXT_PCIE_LENGTH_SHIFT;
+		if (!cnf_size)
+			goto out;
+
+		cnf_base_addr = data & E1000_EXTCNF_CTRL_EXT_CNF_POINTER_MASK;
+		cnf_base_addr >>= E1000_EXTCNF_CTRL_EXT_CNF_POINTER_SHIFT;
+
+		/* Configure LCD from extended configuration
+		 * region. */
+
+		/* cnf_base_addr is in DWORD */
+		word_addr = (u16)(cnf_base_addr << 1);
+
+		for (i = 0; i < cnf_size; i++) {
+			ret_val = e1000_read_nvm(hw,
+			                        (word_addr + i * 2),
+			                        1,
+			                        &reg_data);
+			if (ret_val)
+				goto out;
+
+			ret_val = e1000_read_nvm(hw,
+			                        (word_addr + i * 2 + 1),
+			                        1,
+			                        &reg_addr);
+			if (ret_val)
+				goto out;
+
+			/* Save off the PHY page for future writes. */
+			if (reg_addr == IGP01E1000_PHY_PAGE_SELECT) {
+				phy_page = reg_data;
+				continue;
+			}
+
+			reg_addr |= phy_page;
+
+			ret_val = e1000_write_phy_reg(hw,
+			                             (u32)reg_addr,
+			                             reg_data);
+			if (ret_val)
+				goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_phy_info_ich8lan - Calls appropriate PHY type get_phy_info
+ *  @hw: pointer to the HW structure
+ *
+ *  Wrapper for calling the get_phy_info routines for the appropriate phy type.
+ *  This is a function pointer entry point called by drivers
+ *  or other shared routines.
+ **/
+static s32 e1000_get_phy_info_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = -E1000_ERR_PHY_TYPE;
+
+	DEBUGFUNC("e1000_get_phy_info_ich8lan");
+
+	switch (hw->phy.type) {
+	case e1000_phy_ife:
+		ret_val = e1000_get_phy_info_ife_ich8lan(hw);
+		break;
+	case e1000_phy_igp_3:
+		ret_val = e1000_get_phy_info_igp(hw);
+		break;
+	default:
+		break;
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_get_phy_info_ife_ich8lan - Retrieves various IFE PHY states
+ *  @hw: pointer to the HW structure
+ *
+ *  Populates "phy" structure with various feature states.
+ *  This function is only called by other family-specific
+ *  routines.
+ **/
+static s32 e1000_get_phy_info_ife_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_get_phy_info_ife_ich8lan");
+
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link) {
+		DEBUGOUT("Phy info is only valid if link is up\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, IFE_PHY_SPECIAL_CONTROL, &data);
+	if (ret_val)
+		goto out;
+	phy->polarity_correction = (data & IFE_PSC_AUTO_POLARITY_DISABLE)
+	                           ? FALSE : TRUE;
+
+	if (phy->polarity_correction) {
+		ret_val = e1000_check_polarity_ife_ich8lan(hw);
+		if (ret_val)
+			goto out;
+	} else {
+		/* Polarity is forced */
+		phy->cable_polarity = (data & IFE_PSC_FORCE_POLARITY)
+		                      ? e1000_rev_polarity_reversed
+		                      : e1000_rev_polarity_normal;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, IFE_PHY_MDIX_CONTROL, &data);
+	if (ret_val)
+		goto out;
+
+	phy->is_mdix = (data & IFE_PMC_MDIX_STATUS) ? TRUE : FALSE;
+
+	/* The following parameters are undefined for 10/100 operation. */
+	phy->cable_length = E1000_CABLE_LENGTH_UNDEFINED;
+	phy->local_rx = e1000_1000t_rx_status_undefined;
+	phy->remote_rx = e1000_1000t_rx_status_undefined;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_polarity_ife_ich8lan - Check cable polarity for IFE PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Polarity is determined on the polarity reveral feature being enabled.
+ *  This function is only called by other family-specific
+ *  routines.
+ **/
+static s32 e1000_check_polarity_ife_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data, offset, mask;
+
+	DEBUGFUNC("e1000_check_polarity_ife_ich8lan");
+
+	/* Polarity is determined based on the reversal feature
+	 * being enabled.
+	 */
+	if (phy->polarity_correction) {
+		offset	= IFE_PHY_EXTENDED_STATUS_CONTROL;
+		mask	= IFE_PESC_POLARITY_REVERSED;
+	} else {
+		offset	= IFE_PHY_SPECIAL_CONTROL;
+		mask	= IFE_PSC_FORCE_POLARITY;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, offset, &phy_data);
+
+	if (!ret_val)
+		phy->cable_polarity = (phy_data & mask)
+		                      ? e1000_rev_polarity_reversed
+		                      : e1000_rev_polarity_normal;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_set_d0_lplu_state_ich8lan - Set Low Power Linkup D0 state
+ *  @hw: pointer to the HW structure
+ *  @active: TRUE to enable LPLU, FALSE to disable
+ *
+ *  Sets the LPLU D0 state according to the active flag.  When
+ *  activating LPLU this function also disables smart speed
+ *  and vice versa.  LPLU will not be activated unless the
+ *  device autonegotiation advertisement meets standards of
+ *  either 10 or 10/100 or 10/100/1000 at all duplexes.
+ *  This is a function pointer entry point only called by
+ *  PHY setup routines.
+ **/
+static s32 e1000_set_d0_lplu_state_ich8lan(struct e1000_hw *hw,
+                                           boolean_t active)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	u32 phy_ctrl;
+	s32 ret_val = E1000_SUCCESS;
+	u16 data;
+
+	DEBUGFUNC("e1000_set_d0_lplu_state_ich8lan");
+
+	if (phy->type != e1000_phy_igp_3)
+		goto out;
+
+	phy_ctrl = E1000_READ_REG(hw, E1000_PHY_CTRL);
+
+	if (active) {
+		phy_ctrl |= E1000_PHY_CTRL_D0A_LPLU;
+		E1000_WRITE_REG(hw, E1000_PHY_CTRL, phy_ctrl);
+
+		/* Call gig speed drop workaround on LPLU before accessing
+		 * any PHY registers */
+		if ((hw->mac.type == e1000_ich8lan) &&
+		    (hw->phy.type == e1000_phy_igp_3))
+			e1000_gig_downshift_workaround_ich8lan(hw);
+
+		/* When LPLU is enabled, we should disable SmartSpeed */
+		ret_val = e1000_read_phy_reg(hw,
+		                            IGP01E1000_PHY_PORT_CONFIG,
+		                            &data);
+		data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+		ret_val = e1000_write_phy_reg(hw,
+		                             IGP01E1000_PHY_PORT_CONFIG,
+		                             data);
+		if (ret_val)
+			goto out;
+	} else {
+		phy_ctrl &= ~E1000_PHY_CTRL_D0A_LPLU;
+		E1000_WRITE_REG(hw, E1000_PHY_CTRL, phy_ctrl);
+
+		/* LPLU and SmartSpeed are mutually exclusive.  LPLU is used
+		 * during Dx states where the power conservation is most
+		 * important.  During driver activity we should enable
+		 * SmartSpeed, so performance is maintained. */
+		if (phy->smart_speed == e1000_smart_speed_on) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data |= IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		} else if (phy->smart_speed == e1000_smart_speed_off) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_d3_lplu_state_ich8lan - Set Low Power Linkup D3 state
+ *  @hw: pointer to the HW structure
+ *  @active: TRUE to enable LPLU, FALSE to disable
+ *
+ *  Sets the LPLU D3 state according to the active flag.  When
+ *  activating LPLU this function also disables smart speed
+ *  and vice versa.  LPLU will not be activated unless the
+ *  device autonegotiation advertisement meets standards of
+ *  either 10 or 10/100 or 10/100/1000 at all duplexes.
+ *  This is a function pointer entry point only called by
+ *  PHY setup routines.
+ **/
+static s32 e1000_set_d3_lplu_state_ich8lan(struct e1000_hw *hw,
+                                           boolean_t active)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	u32 phy_ctrl;
+	s32 ret_val = E1000_SUCCESS;
+	u16 data;
+
+	DEBUGFUNC("e1000_set_d3_lplu_state_ich8lan");
+
+	phy_ctrl = E1000_READ_REG(hw, E1000_PHY_CTRL);
+
+	if (!active) {
+		phy_ctrl &= ~E1000_PHY_CTRL_NOND0A_LPLU;
+		E1000_WRITE_REG(hw, E1000_PHY_CTRL, phy_ctrl);
+		/* LPLU and SmartSpeed are mutually exclusive.  LPLU is used
+		 * during Dx states where the power conservation is most
+		 * important.  During driver activity we should enable
+		 * SmartSpeed, so performance is maintained. */
+		if (phy->smart_speed == e1000_smart_speed_on) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data |= IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		} else if (phy->smart_speed == e1000_smart_speed_off) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		}
+	} else if ((phy->autoneg_advertised == E1000_ALL_SPEED_DUPLEX) ||
+	           (phy->autoneg_advertised == E1000_ALL_NOT_GIG) ||
+	           (phy->autoneg_advertised == E1000_ALL_10_SPEED)) {
+		phy_ctrl |= E1000_PHY_CTRL_NOND0A_LPLU;
+		E1000_WRITE_REG(hw, E1000_PHY_CTRL, phy_ctrl);
+
+		/* Call gig speed drop workaround on LPLU before accessing
+		 * any PHY registers */
+		if ((hw->mac.type == e1000_ich8lan) &&
+		    (hw->phy.type == e1000_phy_igp_3))
+			e1000_gig_downshift_workaround_ich8lan(hw);
+
+		/* When LPLU is enabled, we should disable SmartSpeed */
+		ret_val = e1000_read_phy_reg(hw,
+		                            IGP01E1000_PHY_PORT_CONFIG,
+		                            &data);
+		if (ret_val)
+			goto out;
+
+		data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+		ret_val = e1000_write_phy_reg(hw,
+		                             IGP01E1000_PHY_PORT_CONFIG,
+		                             data);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_nvm_ich8lan - Read word(s) from the NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The offset (in bytes) of the word(s) to read.
+ *  @words: Size of data to read in words
+ *  @data: Pointer to the word(s) to read at offset.
+ *
+ *  Reads a word(s) from the NVM using the flash access registers.
+ **/
+static s32 e1000_read_nvm_ich8lan(struct e1000_hw *hw, u16 offset, u16 words,
+                                  u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_dev_spec_ich8lan *dev_spec;
+	u32 act_offset;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i, word;
+
+	DEBUGFUNC("e1000_read_nvm_ich8lan");
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	if ((offset >= nvm->word_size) || (words > nvm->word_size - offset) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	/* Start with the bank offset, then add the relative offset. */
+	act_offset = (E1000_READ_REG(hw, E1000_EECD) & E1000_EECD_SEC1VAL)
+		     ? nvm->flash_bank_size
+		     : 0;
+	act_offset += offset;
+
+	for (i = 0; i < words; i++) {
+		if ((dev_spec->shadow_ram != NULL) &&
+		    (dev_spec->shadow_ram[offset+i].modified == TRUE)) {
+			data[i] = dev_spec->shadow_ram[offset+i].value;
+		} else {
+			ret_val = e1000_read_flash_word_ich8lan(hw,
+			                                        act_offset + i,
+			                                        &word);
+			if (ret_val)
+				break;
+			data[i] = word;
+		}
+	}
+
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_flash_cycle_init_ich8lan - Initialize flash
+ *  @hw: pointer to the HW structure
+ *
+ *  This function does initial flash setup so that a new read/write/erase cycle
+ *  can be started.
+ **/
+static s32 e1000_flash_cycle_init_ich8lan(struct e1000_hw *hw)
+{
+	union ich8_hws_flash_status hsfsts;
+	s32 ret_val = -E1000_ERR_NVM;
+	s32 i = 0;
+
+	DEBUGFUNC("e1000_flash_cycle_init_ich8lan");
+
+	hsfsts.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFSTS);
+
+	/* Check if the flash descriptor is valid */
+	if (hsfsts.hsf_status.fldesvalid == 0) {
+		DEBUGOUT("Flash descriptor invalid.  "
+		         "SW Sequencing must be used.");
+		goto out;
+	}
+
+	/* Clear FCERR and DAEL in hw status by writing 1 */
+	hsfsts.hsf_status.flcerr = 1;
+	hsfsts.hsf_status.dael = 1;
+
+	E1000_WRITE_FLASH_REG16(hw, ICH_FLASH_HSFSTS, hsfsts.regval);
+
+	/* Either we should have a hardware SPI cycle in progress
+	 * bit to check against, in order to start a new cycle or
+	 * FDONE bit should be changed in the hardware so that it
+	 * is 1 after harware reset, which can then be used as an
+	 * indication whether a cycle is in progress or has been
+	 * completed.
+	 */
+
+	if (hsfsts.hsf_status.flcinprog == 0) {
+		/* There is no cycle running at present,
+		 * so we can start a cycle */
+		/* Begin by setting Flash Cycle Done. */
+		hsfsts.hsf_status.flcdone = 1;
+		E1000_WRITE_FLASH_REG16(hw, ICH_FLASH_HSFSTS, hsfsts.regval);
+		ret_val = E1000_SUCCESS;
+	} else {
+		/* otherwise poll for sometime so the current
+		 * cycle has a chance to end before giving up. */
+		for (i = 0; i < ICH_FLASH_READ_COMMAND_TIMEOUT; i++) {
+			hsfsts.regval = E1000_READ_FLASH_REG16(hw,
+			                                      ICH_FLASH_HSFSTS);
+			if (hsfsts.hsf_status.flcinprog == 0) {
+				ret_val = E1000_SUCCESS;
+				break;
+			}
+			usec_delay(1);
+		}
+		if (ret_val == E1000_SUCCESS) {
+			/* Successful in waiting for previous cycle to timeout,
+			 * now set the Flash Cycle Done. */
+			hsfsts.hsf_status.flcdone = 1;
+			E1000_WRITE_FLASH_REG16(hw,
+			                        ICH_FLASH_HSFSTS,
+			                        hsfsts.regval);
+		} else {
+			DEBUGOUT("Flash controller busy, cannot get access");
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_flash_cycle_ich8lan - Starts flash cycle (read/write/erase)
+ *  @hw: pointer to the HW structure
+ *  @timeout: maximum time to wait for completion
+ *
+ *  This function starts a flash cycle and waits for its completion.
+ **/
+static s32 e1000_flash_cycle_ich8lan(struct e1000_hw *hw, u32 timeout)
+{
+	union ich8_hws_flash_ctrl hsflctl;
+	union ich8_hws_flash_status hsfsts;
+	s32 ret_val = -E1000_ERR_NVM;
+	u32 i = 0;
+
+	DEBUGFUNC("e1000_flash_cycle_ich8lan");
+
+	/* Start a cycle by writing 1 in Flash Cycle Go in Hw Flash Control */
+	hsflctl.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFCTL);
+	hsflctl.hsf_ctrl.flcgo = 1;
+	E1000_WRITE_FLASH_REG16(hw, ICH_FLASH_HSFCTL, hsflctl.regval);
+
+	/* wait till FDONE bit is set to 1 */
+	do {
+		hsfsts.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFSTS);
+		if (hsfsts.hsf_status.flcdone == 1)
+			break;
+		usec_delay(1);
+	} while (i++ < timeout);
+
+	if (hsfsts.hsf_status.flcdone == 1 && hsfsts.hsf_status.flcerr == 0)
+		ret_val = E1000_SUCCESS;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_read_flash_word_ich8lan - Read word from flash
+ *  @hw: pointer to the HW structure
+ *  @offset: offset to data location
+ *  @data: pointer to the location for storing the data
+ *
+ *  Reads the flash word at offset into data.  Offset is converted
+ *  to bytes before read.
+ **/
+static s32 e1000_read_flash_word_ich8lan(struct e1000_hw *hw, u32 offset,
+                                         u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_read_flash_word_ich8lan");
+
+	if (data == NULL) {
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	/* Must convert offset into bytes. */
+	offset <<= 1;
+
+	ret_val = e1000_read_flash_data_ich8lan(hw, offset, 2, data);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_flash_data_ich8lan - Read byte or word from NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The offset (in bytes) of the byte or word to read.
+ *  @size: Size of data to read, 1=byte 2=word
+ *  @data: Pointer to the word to store the value read.
+ *
+ *  Reads a byte or word from the NVM using the flash access registers.
+ **/
+static s32 e1000_read_flash_data_ich8lan(struct e1000_hw *hw, u32 offset,
+                                         u8 size, u16* data)
+{
+	union ich8_hws_flash_status hsfsts;
+	union ich8_hws_flash_ctrl hsflctl;
+	u32 flash_linear_addr;
+	u32 flash_data = 0;
+	s32 ret_val = -E1000_ERR_NVM;
+	u8 count = 0;
+
+	DEBUGFUNC("e1000_read_flash_data_ich8lan");
+
+	if (size < 1  || size > 2 || data == 0x0 ||
+	    offset > ICH_FLASH_LINEAR_ADDR_MASK)
+		goto out;
+
+	flash_linear_addr = (ICH_FLASH_LINEAR_ADDR_MASK & offset) +
+	                    hw->nvm.flash_base_addr;
+
+	do {
+		usec_delay(1);
+		/* Steps */
+		ret_val = e1000_flash_cycle_init_ich8lan(hw);
+		if (ret_val != E1000_SUCCESS)
+			break;
+
+		hsflctl.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFCTL);
+		/* 0b/1b corresponds to 1 or 2 byte size, respectively. */
+		hsflctl.hsf_ctrl.fldbcount = size - 1;
+		hsflctl.hsf_ctrl.flcycle = ICH_CYCLE_READ;
+		E1000_WRITE_FLASH_REG16(hw, ICH_FLASH_HSFCTL, hsflctl.regval);
+
+		E1000_WRITE_FLASH_REG(hw, ICH_FLASH_FADDR, flash_linear_addr);
+
+		ret_val = e1000_flash_cycle_ich8lan(hw,
+		                                ICH_FLASH_READ_COMMAND_TIMEOUT);
+
+		/* Check if FCERR is set to 1, if set to 1, clear it
+		 * and try the whole sequence a few more times, else
+		 * read in (shift in) the Flash Data0, the order is
+		 * least significant byte first msb to lsb */
+		if (ret_val == E1000_SUCCESS) {
+			flash_data = E1000_READ_FLASH_REG(hw, ICH_FLASH_FDATA0);
+			if (size == 1) {
+				*data = (u8)(flash_data & 0x000000FF);
+			} else if (size == 2) {
+				*data = (u16)(flash_data & 0x0000FFFF);
+			}
+			break;
+		} else {
+			/* If we've gotten here, then things are probably
+			 * completely hosed, but if the error condition is
+			 * detected, it won't hurt to give it another try...
+			 * ICH_FLASH_CYCLE_REPEAT_COUNT times.
+			 */
+			hsfsts.regval = E1000_READ_FLASH_REG16(hw,
+			                                      ICH_FLASH_HSFSTS);
+			if (hsfsts.hsf_status.flcerr == 1) {
+				/* Repeat for some time before giving up. */
+				continue;
+			} else if (hsfsts.hsf_status.flcdone == 0) {
+				DEBUGOUT("Timeout error - flash cycle "
+				         "did not complete.");
+				break;
+			}
+		}
+	} while (count++ < ICH_FLASH_CYCLE_REPEAT_COUNT);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_nvm_ich8lan - Write word(s) to the NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The offset (in bytes) of the word(s) to write.
+ *  @words: Size of data to write in words
+ *  @data: Pointer to the word(s) to write at offset.
+ *
+ *  Writes a byte or word to the NVM using the flash access registers.
+ **/
+static s32 e1000_write_nvm_ich8lan(struct e1000_hw *hw, u16 offset, u16 words,
+                                   u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_dev_spec_ich8lan *dev_spec;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i;
+
+	DEBUGFUNC("e1000_write_nvm_ich8lan");
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	if ((offset >= nvm->word_size) || (words > nvm->word_size - offset) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	for (i = 0; i < words; i++) {
+		dev_spec->shadow_ram[offset+i].modified = TRUE;
+		dev_spec->shadow_ram[offset+i].value = data[i];
+	}
+
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_update_nvm_checksum_ich8lan - Update the checksum for NVM
+ *  @hw: pointer to the HW structure
+ *
+ *  The NVM checksum is updated by calling the generic update_nvm_checksum,
+ *  which writes the checksum to the shadow ram.  The changes in the shadow
+ *  ram are then committed to the EEPROM by processing each bank at a time
+ *  checking for the modified bit and writing only the pending changes.
+ *  After a succesful commit, the shadow ram is cleared and is ready for
+ *  future writes.
+ **/
+static s32 e1000_update_nvm_checksum_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	struct e1000_dev_spec_ich8lan *dev_spec;
+	u32 i, act_offset, new_bank_offset, old_bank_offset;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_update_nvm_checksum_ich8lan");
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	ret_val = e1000_update_nvm_checksum_generic(hw);
+	if (ret_val)
+		goto out;
+
+	if (nvm->type != e1000_nvm_flash_sw)
+		goto out;
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	/* We're writing to the opposite bank so if we're on bank 1,
+	 * write to bank 0 etc.  We also need to erase the segment that
+	 * is going to be written */
+	if (!(E1000_READ_REG(hw, E1000_EECD) & E1000_EECD_SEC1VAL)) {
+		new_bank_offset = nvm->flash_bank_size;
+		old_bank_offset = 0;
+		e1000_erase_flash_bank_ich8lan(hw, 1);
+	} else {
+		old_bank_offset = nvm->flash_bank_size;
+		new_bank_offset = 0;
+		e1000_erase_flash_bank_ich8lan(hw, 0);
+	}
+
+	for (i = 0; i < E1000_SHADOW_RAM_WORDS; i++) {
+		/* Determine whether to write the value stored
+		 * in the other NVM bank or a modified value stored
+		 * in the shadow RAM */
+		if (dev_spec->shadow_ram[i].modified == TRUE) {
+			data = dev_spec->shadow_ram[i].value;
+		} else {
+			e1000_read_flash_word_ich8lan(hw,
+			                              i + old_bank_offset,
+			                              &data);
+		}
+
+		/* If the word is 0x13, then make sure the signature bits
+		 * (15:14) are 11b until the commit has completed.
+		 * This will allow us to write 10b which indicates the
+		 * signature is valid.  We want to do this after the write
+		 * has completed so that we don't mark the segment valid
+		 * while the write is still in progress */
+		if (i == E1000_ICH_NVM_SIG_WORD)
+			data |= E1000_ICH_NVM_SIG_MASK;
+
+		/* Convert offset to bytes. */
+		act_offset = (i + new_bank_offset) << 1;
+
+		usec_delay(100);
+		/* Write the bytes to the new bank. */
+		ret_val = e1000_retry_write_flash_byte_ich8lan(hw,
+		                                               act_offset,
+		                                               (u8)data);
+		if (ret_val)
+			break;
+
+		usec_delay(100);
+		ret_val = e1000_retry_write_flash_byte_ich8lan(hw,
+		                                          act_offset + 1,
+		                                          (u8)(data >> 8));
+		if (ret_val)
+			break;
+	}
+
+	/* Don't bother writing the segment valid bits if sector
+	 * programming failed. */
+	if (ret_val) {
+		DEBUGOUT("Flash commit failed.\n");
+		e1000_release_nvm(hw);
+		goto out;
+	}
+
+	/* Finally validate the new segment by setting bit 15:14
+	 * to 10b in word 0x13 , this can be done without an
+	 * erase as well since these bits are 11 to start with
+	 * and we need to change bit 14 to 0b */
+	act_offset = new_bank_offset + E1000_ICH_NVM_SIG_WORD;
+	e1000_read_flash_word_ich8lan(hw, act_offset, &data);
+	data &= 0xBFFF;
+	ret_val = e1000_retry_write_flash_byte_ich8lan(hw,
+	                                               act_offset * 2 + 1,
+	                                               (u8)(data >> 8));
+	if (ret_val) {
+		e1000_release_nvm(hw);
+		goto out;
+	}
+
+	/* And invalidate the previously valid segment by setting
+	 * its signature word (0x13) high_byte to 0b. This can be
+	 * done without an erase because flash erase sets all bits
+	 * to 1's. We can write 1's to 0's without an erase */
+	act_offset = (old_bank_offset + E1000_ICH_NVM_SIG_WORD) * 2 + 1;
+	ret_val = e1000_retry_write_flash_byte_ich8lan(hw, act_offset, 0);
+	if (ret_val) {
+		e1000_release_nvm(hw);
+		goto out;
+	}
+
+	/* Great!  Everything worked, we can now clear the cached entries. */
+	for (i = 0; i < E1000_SHADOW_RAM_WORDS; i++) {
+		dev_spec->shadow_ram[i].modified = FALSE;
+		dev_spec->shadow_ram[i].value = 0xFFFF;
+	}
+
+	e1000_release_nvm(hw);
+
+	/* Reload the EEPROM, or else modifications will not appear
+	 * until after the next adapter reset.
+	 */
+	e1000_reload_nvm(hw);
+	msec_delay(10);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_validate_nvm_checksum_ich8lan - Validate EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Check to see if checksum needs to be fixed by reading bit 6 in word 0x19.
+ *  If the bit is 0, that the EEPROM had been modified, but the checksum was not
+ *  calculated, in which case we need to calculate the checksum and set bit 6.
+ **/
+static s32 e1000_validate_nvm_checksum_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 data;
+
+	DEBUGFUNC("e1000_validate_nvm_checksum_ich8lan");
+
+	/* Read 0x19 and check bit 6.  If this bit is 0, the checksum
+	 * needs to be fixed.  This bit is an indication that the NVM
+	 * was prepared by OEM software and did not calculate the
+	 * checksum...a likely scenario.
+	 */
+	ret_val = e1000_read_nvm(hw, 0x19, 1, &data);
+	if (ret_val)
+		goto out;
+
+	if ((data & 0x40) == 0) {
+		data |= 0x40;
+		ret_val = e1000_write_nvm(hw, 0x19, 1, &data);
+		if (ret_val)
+			goto out;
+		ret_val = e1000_update_nvm_checksum(hw);
+		if (ret_val)
+			goto out;
+	}
+
+	ret_val = e1000_validate_nvm_checksum_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_flash_data_ich8lan - Writes bytes to the NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The offset (in bytes) of the byte/word to read.
+ *  @size: Size of data to read, 1=byte 2=word
+ *  @data: The byte(s) to write to the NVM.
+ *
+ *  Writes one/two bytes to the NVM using the flash access registers.
+ **/
+static s32 e1000_write_flash_data_ich8lan(struct e1000_hw *hw, u32 offset,
+                                          u8 size, u16 data)
+{
+	union ich8_hws_flash_status hsfsts;
+	union ich8_hws_flash_ctrl hsflctl;
+	u32 flash_linear_addr;
+	u32 flash_data = 0;
+	s32 ret_val = -E1000_ERR_NVM;
+	u8 count = 0;
+
+	DEBUGFUNC("e1000_write_ich8_data");
+
+	if (size < 1 || size > 2 || data > size * 0xff ||
+	    offset > ICH_FLASH_LINEAR_ADDR_MASK)
+		goto out;
+
+	flash_linear_addr = (ICH_FLASH_LINEAR_ADDR_MASK & offset) +
+	                    hw->nvm.flash_base_addr;
+
+	do {
+		usec_delay(1);
+		/* Steps */
+		ret_val = e1000_flash_cycle_init_ich8lan(hw);
+		if (ret_val != E1000_SUCCESS)
+			break;
+
+		hsflctl.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFCTL);
+		/* 0b/1b corresponds to 1 or 2 byte size, respectively. */
+		hsflctl.hsf_ctrl.fldbcount = size -1;
+		hsflctl.hsf_ctrl.flcycle = ICH_CYCLE_WRITE;
+		E1000_WRITE_FLASH_REG16(hw, ICH_FLASH_HSFCTL, hsflctl.regval);
+
+		E1000_WRITE_FLASH_REG(hw, ICH_FLASH_FADDR, flash_linear_addr);
+
+		if (size == 1)
+			flash_data = (u32)data & 0x00FF;
+		else
+			flash_data = (u32)data;
+
+		E1000_WRITE_FLASH_REG(hw, ICH_FLASH_FDATA0, flash_data);
+
+		/* check if FCERR is set to 1 , if set to 1, clear it
+		 * and try the whole sequence a few more times else done */
+		ret_val = e1000_flash_cycle_ich8lan(hw,
+		                               ICH_FLASH_WRITE_COMMAND_TIMEOUT);
+		if (ret_val == E1000_SUCCESS) {
+			break;
+		} else {
+			/* If we're here, then things are most likely
+			 * completely hosed, but if the error condition
+			 * is detected, it won't hurt to give it another
+			 * try...ICH_FLASH_CYCLE_REPEAT_COUNT times.
+			 */
+			hsfsts.regval = E1000_READ_FLASH_REG16(hw,
+			                                      ICH_FLASH_HSFSTS);
+			if (hsfsts.hsf_status.flcerr == 1) {
+				/* Repeat for some time before giving up. */
+				continue;
+			} else if (hsfsts.hsf_status.flcdone == 0) {
+				DEBUGOUT("Timeout error - flash cycle "
+				         "did not complete.");
+				break;
+			}
+		}
+	} while (count++ < ICH_FLASH_CYCLE_REPEAT_COUNT);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_flash_byte_ich8lan - Write a single byte to NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The index of the byte to read.
+ *  @data: The byte to write to the NVM.
+ *
+ *  Writes a single byte to the NVM using the flash access registers.
+ **/
+static s32 e1000_write_flash_byte_ich8lan(struct e1000_hw *hw, u32 offset,
+                                          u8 data)
+{
+	u16 word = (u16)data;
+
+	DEBUGFUNC("e1000_write_flash_byte_ich8lan");
+
+	return e1000_write_flash_data_ich8lan(hw, offset, 1, word);
+}
+
+/**
+ *  e1000_retry_write_flash_byte_ich8lan - Writes a single byte to NVM
+ *  @hw: pointer to the HW structure
+ *  @offset: The offset of the byte to write.
+ *  @byte: The byte to write to the NVM.
+ *
+ *  Writes a single byte to the NVM using the flash access registers.
+ *  Goes through a retry algorithm before giving up.
+ **/
+static s32 e1000_retry_write_flash_byte_ich8lan(struct e1000_hw *hw, u32 offset,
+                                                u8 byte)
+{
+	s32 ret_val;
+	u16 program_retries;
+
+	DEBUGFUNC("e1000_retry_write_flash_byte_ich8lan");
+
+	ret_val = e1000_write_flash_byte_ich8lan(hw, offset, byte);
+	if (ret_val)
+		goto out;
+
+	usec_delay(100);
+
+	for (program_retries = 0; program_retries < 100; program_retries++) {
+		DEBUGOUT2("Retrying Byte %2.2X at offset %u\n", byte, offset);
+		usec_delay(100);
+		ret_val = e1000_write_flash_byte_ich8lan(hw, offset, byte);
+		if (ret_val == E1000_SUCCESS)
+			break;
+	}
+	if (program_retries == 100) {
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_erase_flash_bank_ich8lan - Erase a bank (4k) from NVM
+ *  @hw: pointer to the HW structure
+ *  @bank: 0 for first bank, 1 for second bank, etc.
+ *
+ *  Erases the bank specified. Each bank is a 4k block. Banks are 0 based.
+ *  bank N is 4096 * N + flash_reg_addr.
+ **/
+static s32 e1000_erase_flash_bank_ich8lan(struct e1000_hw *hw, u32 bank)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	union ich8_hws_flash_status hsfsts;
+	union ich8_hws_flash_ctrl hsflctl;
+	u32 flash_linear_addr;
+	/* bank size is in 16bit words - adjust to bytes */
+	u32 flash_bank_size = nvm->flash_bank_size * 2;
+	s32  ret_val = E1000_SUCCESS;
+	s32  count = 0;
+	s32  j, iteration, sector_size;
+
+	DEBUGFUNC("e1000_erase_flash_bank_ich8lan");
+
+	hsfsts.regval = E1000_READ_FLASH_REG16(hw, ICH_FLASH_HSFSTS);
+
+	/* Determine HW Sector size: Read BERASE bits of hw flash status
+	 * register */
+	/* 00: The Hw sector is 256 bytes, hence we need to erase 16
+	 *     consecutive sectors.  The start index for the nth Hw sector
+	 *     can be calculated as = bank * 4096 + n * 256
+	 * 01: The Hw sector is 4K bytes, hence we need to erase 1 sector.
+	 *     The start index for the nth Hw sector can be calculated
+	 *     as = bank * 4096
+	 * 10: The Hw sector is 8K bytes, nth sector = bank * 8192
+	 *     (ich9 only, otherwise error condition)
+	 * 11: The Hw sector is 64K bytes, nth sector = bank * 65536
+	 */
+	switch (hsfsts.hsf_status.berasesz) {
+	case 0:
+		/* Hw sector size 256 */
+		sector_size = ICH_FLASH_SEG_SIZE_256;
+		iteration = flash_bank_size / ICH_FLASH_SEG_SIZE_256;
+		break;
+	case 1:
+		sector_size = ICH_FLASH_SEG_SIZE_4K;
+		iteration = flash_bank_size / ICH_FLASH_SEG_SIZE_4K;
+		break;
+	case 2:
+		if (hw->mac.type == e1000_ich9lan) {
+			sector_size = ICH_FLASH_SEG_SIZE_8K;
+			iteration = flash_bank_size / ICH_FLASH_SEG_SIZE_8K;
+		} else {
+			ret_val = -E1000_ERR_NVM;
+			goto out;
+		}
+		break;
+	case 3:
+		sector_size = ICH_FLASH_SEG_SIZE_64K;
+		iteration = flash_bank_size / ICH_FLASH_SEG_SIZE_64K;
+		break;
+	default:
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	/* Start with the base address, then add the sector offset. */
+	flash_linear_addr = hw->nvm.flash_base_addr;
+	flash_linear_addr += (bank) ? (sector_size * iteration) : 0;
+
+	for (j = 0; j < iteration ; j++) {
+		do {
+			/* Steps */
+			ret_val = e1000_flash_cycle_init_ich8lan(hw);
+			if (ret_val)
+				goto out;
+
+			/* Write a value 11 (block Erase) in Flash
+			 * Cycle field in hw flash control */
+			hsflctl.regval = E1000_READ_FLASH_REG16(hw,
+			                                      ICH_FLASH_HSFCTL);
+			hsflctl.hsf_ctrl.flcycle = ICH_CYCLE_ERASE;
+			E1000_WRITE_FLASH_REG16(hw,
+			                        ICH_FLASH_HSFCTL,
+			                        hsflctl.regval);
+
+			/* Write the last 24 bits of an index within the
+			 * block into Flash Linear address field in Flash
+			 * Address.
+			 */
+			flash_linear_addr += (j * sector_size);
+			E1000_WRITE_FLASH_REG(hw,
+			                      ICH_FLASH_FADDR,
+			                      flash_linear_addr);
+
+			ret_val = e1000_flash_cycle_ich8lan(hw,
+			                       ICH_FLASH_ERASE_COMMAND_TIMEOUT);
+			if (ret_val == E1000_SUCCESS) {
+				break;
+			} else {
+				/* Check if FCERR is set to 1.  If 1,
+				 * clear it and try the whole sequence
+				 * a few more times else Done */
+				hsfsts.regval = E1000_READ_FLASH_REG16(hw,
+				                              ICH_FLASH_HSFSTS);
+				if (hsfsts.hsf_status.flcerr == 1) {
+					/* repeat for some time before
+					 * giving up */
+					continue;
+				} else if (hsfsts.hsf_status.flcdone == 0)
+					goto out;
+			}
+		} while (++count < ICH_FLASH_CYCLE_REPEAT_COUNT);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_valid_led_default_ich8lan - Set the default LED settings
+ *  @hw: pointer to the HW structure
+ *  @data: Pointer to the LED settings
+ *
+ *  Reads the LED default settings from the NVM to data.  If the NVM LED
+ *  settings is all 0's or F's, set the LED default to a valid LED default
+ *  setting.
+ **/
+static s32 e1000_valid_led_default_ich8lan(struct e1000_hw *hw, u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_valid_led_default_ich8lan");
+
+	ret_val = e1000_read_nvm(hw, NVM_ID_LED_SETTINGS, 1, data);
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+
+	if (*data == ID_LED_RESERVED_0000 ||
+	    *data == ID_LED_RESERVED_FFFF)
+		*data = ID_LED_DEFAULT_ICH8LAN;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_bus_info_ich8lan - Get/Set the bus type and width
+ *  @hw: pointer to the HW structure
+ *
+ *  ICH8 use the PCI Express bus, but does not contain a PCI Express Capability
+ *  register, so the the bus width is hard coded.
+ **/
+static s32 e1000_get_bus_info_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_bus_info *bus = &hw->bus;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_get_bus_info_ich8lan");
+
+	ret_val = e1000_get_bus_info_pcie_generic(hw);
+
+	/* ICH devices are "PCI Express"-ish.  They have
+	 * a configuration space, but do not contain
+	 * PCI Express Capability registers, so bus width
+	 * must be hardcoded.
+	 */
+	if (bus->width == e1000_bus_width_unknown)
+		bus->width = e1000_bus_width_pcie_x1;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_reset_hw_ich8lan - Reset the hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  Does a full reset of the hardware which includes a reset of the PHY and
+ *  MAC.
+ **/
+static s32 e1000_reset_hw_ich8lan(struct e1000_hw *hw)
+{
+	u32 ctrl, icr, kab;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_reset_hw_ich8lan");
+
+	/* Prevent the PCI-E bus from sticking if there is no TLP connection
+	 * on the last TLP read/write transaction when MAC is reset.
+	 */
+	ret_val = e1000_disable_pcie_master_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("PCI-E Master disable polling has failed.\n");
+	}
+
+	DEBUGOUT("Masking off all interrupts\n");
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+
+	/* Disable the Transmit and Receive units.  Then delay to allow
+	 * any pending transactions to complete before we hit the MAC
+	 * with the global reset.
+	 */
+	E1000_WRITE_REG(hw, E1000_RCTL, 0);
+	E1000_WRITE_REG(hw, E1000_TCTL, E1000_TCTL_PSP);
+	E1000_WRITE_FLUSH(hw);
+
+	msec_delay(10);
+
+	/* Workaround for ICH8 bit corruption issue in FIFO memory */
+	if (hw->mac.type == e1000_ich8lan) {
+		/* Set Tx and Rx buffer allocation to 8k apiece. */
+		E1000_WRITE_REG(hw, E1000_PBA, E1000_PBA_8K);
+		/* Set Packet Buffer Size to 16k. */
+		E1000_WRITE_REG(hw, E1000_PBS, E1000_PBS_16K);
+	}
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	if (!e1000_check_reset_block(hw) && !hw->phy.reset_disable) {
+		/* PHY HW reset requires MAC CORE reset at the same
+		 * time to make sure the interface between MAC and the
+		 * external PHY is reset.
+		 */
+		ctrl |= E1000_CTRL_PHY_RST;
+	}
+	ret_val = e1000_acquire_swflag_ich8lan(hw);
+	DEBUGOUT("Issuing a global reset to ich8lan");
+	E1000_WRITE_REG(hw, E1000_CTRL, (ctrl | E1000_CTRL_RST));
+	msec_delay(20);
+
+	ret_val = e1000_get_auto_rd_done_generic(hw);
+	if (ret_val) {
+		/*
+		 * When auto config read does not complete, do not
+		 * return with an error. This can happen in situations
+		 * where there is no eeprom and prevents getting link.
+		 */
+		DEBUGOUT("Auto Read Done did not complete\n");
+	}
+
+	E1000_WRITE_REG(hw, E1000_IMC, 0xffffffff);
+	icr = E1000_READ_REG(hw, E1000_ICR);
+
+	kab = E1000_READ_REG(hw, E1000_KABGTXD);
+	kab |= E1000_KABGTXD_BGSQLBIAS;
+	E1000_WRITE_REG(hw, E1000_KABGTXD, kab);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_init_hw_ich8lan - Initialize the hardware
+ *  @hw: pointer to the HW structure
+ *
+ *  Prepares the hardware for transmit and receive by doing the following:
+ *   - initialize hardware bits
+ *   - initialize LED identification
+ *   - setup receive address registers
+ *   - setup flow control
+ *   - setup transmit discriptors
+ *   - clear statistics
+ **/
+static s32 e1000_init_hw_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 ctrl_ext, txdctl, snoop;
+	s32 ret_val;
+	u16 i;
+
+	DEBUGFUNC("e1000_init_hw_ich8lan");
+
+	e1000_initialize_hw_bits_ich8lan(hw);
+
+	/* Initialize identification LED */
+	ret_val = e1000_id_led_init_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error initializing identification LED\n");
+		goto out;
+	}
+
+	/* Setup the receive address. */
+	e1000_init_rx_addrs_generic(hw, mac->rar_entry_count);
+
+	/* Zero out the Multicast HASH table */
+	DEBUGOUT("Zeroing the MTA\n");
+	for (i = 0; i < mac->mta_reg_count; i++)
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+
+	/* Setup link and flow control */
+	ret_val = e1000_setup_link(hw);
+
+	/* Set the transmit descriptor write-back policy for both queues */
+	txdctl = E1000_READ_REG(hw, E1000_TXDCTL);
+	txdctl = (txdctl & ~E1000_TXDCTL_WTHRESH) |
+		 E1000_TXDCTL_FULL_TX_DESC_WB;
+	txdctl = (txdctl & ~E1000_TXDCTL_PTHRESH) |
+	         E1000_TXDCTL_MAX_TX_DESC_PREFETCH;
+	E1000_WRITE_REG(hw, E1000_TXDCTL, txdctl);
+	txdctl = E1000_READ_REG(hw, E1000_TXDCTL1);
+	txdctl = (txdctl & ~E1000_TXDCTL_WTHRESH) |
+		 E1000_TXDCTL_FULL_TX_DESC_WB;
+	txdctl = (txdctl & ~E1000_TXDCTL_PTHRESH) |
+	         E1000_TXDCTL_MAX_TX_DESC_PREFETCH;
+	E1000_WRITE_REG(hw, E1000_TXDCTL1, txdctl);
+
+	/* ICH8 has opposite polarity of no_snoop bits.
+	 * By default, we should use snoop behavior. */
+	if (mac->type == e1000_ich8lan)
+		snoop = PCIE_ICH8_SNOOP_ALL;
+	else
+		snoop = (u32)~(PCIE_NO_SNOOP_ALL);
+	e1000_set_pcie_no_snoop_generic(hw, snoop);
+
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	ctrl_ext |= E1000_CTRL_EXT_RO_DIS;
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+
+	/* Clear all of the statistics registers (clear on read).  It is
+	 * important that we do this after we have tried to establish link
+	 * because the symbol error count will increment wildly if there
+	 * is no link.
+	 */
+	e1000_clear_hw_cntrs_ich8lan(hw);
+
+out:
+	return ret_val;
+}
+/**
+ *  e1000_initialize_hw_bits_ich8lan - Initialize required hardware bits
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets/Clears required hardware bits necessary for correctly setting up the
+ *  hardware for transmit and receive.
+ **/
+static void e1000_initialize_hw_bits_ich8lan(struct e1000_hw *hw)
+{
+	u32 reg;
+
+	DEBUGFUNC("e1000_initialize_hw_bits_ich8lan");
+
+	if (hw->mac.disable_hw_init_bits)
+		goto out;
+
+	/* Extended Device Control */
+	reg = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, reg);
+
+	/* Transmit Descriptor Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL, reg);
+
+	/* Transmit Descriptor Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TXDCTL1);
+	reg |= (1 << 22);
+	E1000_WRITE_REG(hw, E1000_TXDCTL1, reg);
+
+	/* Transmit Arbitration Control 0 */
+	reg = E1000_READ_REG(hw, E1000_TARC0);
+	if (hw->mac.type == e1000_ich8lan)
+		reg |= (1 << 28) | (1 << 29);
+	reg |= (1 << 23) | (1 << 24) | (1 << 26) | (1 << 27);
+	E1000_WRITE_REG(hw, E1000_TARC0, reg);
+
+	/* Transmit Arbitration Control 1 */
+	reg = E1000_READ_REG(hw, E1000_TARC1);
+	if (E1000_READ_REG(hw, E1000_TCTL) & E1000_TCTL_MULR)
+		reg &= ~(1 << 28);
+	else
+		reg |= (1 << 28);
+	reg |= (1 << 24) | (1 << 26) | (1 << 30);
+	E1000_WRITE_REG(hw, E1000_TARC1, reg);
+
+	/* Device Status */
+	if (hw->mac.type == e1000_ich8lan) {
+		reg = E1000_READ_REG(hw, E1000_STATUS);
+		reg &= ~(1 << 31);
+		E1000_WRITE_REG(hw, E1000_STATUS, reg);
+	}
+
+out:
+	return;
+}
+
+/**
+ *  e1000_setup_link_ich8lan - Setup flow control and link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines which flow control settings to use, then configures flow
+ *  control.  Calls the appropriate media-specific link configuration
+ *  function.  Assuming the adapter has a valid link partner, a valid link
+ *  should be established.  Assumes the hardware has previously been reset
+ *  and the transmitter and receiver are not enabled.
+ **/
+static s32 e1000_setup_link_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_link_ich8lan");
+
+	if (e1000_check_reset_block(hw))
+		goto out;
+
+	/* ICH parts do not have a word in the NVM to determine
+	 * the default flow control setting, so we explicitly
+	 * set it to full.
+	 */
+	if (mac->fc == e1000_fc_default)
+		mac->fc = e1000_fc_full;
+
+	mac->original_fc = mac->fc;
+
+	DEBUGOUT1("After fix-ups FlowControl is now = %x\n", mac->fc);
+
+	/* Continue to configure the copper link. */
+	ret_val = func->setup_physical_interface(hw);
+	if (ret_val)
+		goto out;
+
+	E1000_WRITE_REG(hw, E1000_FCTTV, mac->fc_pause_time);
+
+	ret_val = e1000_set_fc_watermarks_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_ich8lan - Configure MAC/PHY interface
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures the kumeran interface to the PHY to wait the appropriate time
+ *  when polling the PHY, then call the generic setup_copper_link to finish
+ *  configuring the copper link.
+ **/
+static s32 e1000_setup_copper_link_ich8lan(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val;
+	u16 reg_data;
+
+	DEBUGFUNC("e1000_setup_copper_link_ich8lan");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_SLU;
+	ctrl &= ~(E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	/* Set the mac to wait the maximum time between each iteration
+	 * and increase the max iterations when polling the phy;
+	 * this fixes erroneous timeouts at 10Mbps. */
+	ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 4), 0xFFFF);
+	if (ret_val)
+		goto out;
+	ret_val = e1000_read_kmrn_reg(hw, GG82563_REG(0x34, 9), &reg_data);
+	if (ret_val)
+		goto out;
+	reg_data |= 0x3F;
+	ret_val = e1000_write_kmrn_reg(hw, GG82563_REG(0x34, 9), reg_data);
+	if (ret_val)
+		goto out;
+
+	if (hw->phy.type == e1000_phy_igp_3) {
+		ret_val = e1000_copper_link_setup_igp(hw);
+		if (ret_val)
+			goto out;
+	}
+
+	ret_val = e1000_setup_copper_link_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_link_up_info_ich8lan - Get current link speed and duplex
+ *  @hw: pointer to the HW structure
+ *  @speed: pointer to store current link speed
+ *  @duplex: pointer to store the current link duplex
+ *
+ *  Calls the generic get_speed_and_duplex to retreive the current link
+ *  information and then calls the Kumeran lock loss workaround for links at
+ *  gigabit speeds.
+ **/
+static s32 e1000_get_link_up_info_ich8lan(struct e1000_hw *hw, u16 *speed,
+                                          u16 *duplex)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_get_link_up_info_ich8lan");
+
+	ret_val = e1000_get_speed_and_duplex_copper_generic(hw, speed, duplex);
+	if (ret_val)
+		goto out;
+
+	if ((hw->mac.type == e1000_ich8lan) &&
+	    (hw->phy.type == e1000_phy_igp_3) &&
+	    (*speed == SPEED_1000)) {
+		ret_val = e1000_kmrn_lock_loss_workaround_ich8lan(hw);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_kmrn_lock_loss_workaround_ich8lan - Kumeran workaround
+ *  @hw: pointer to the HW structure
+ *
+ *  Work-around for 82566 Kumeran PCS lock loss:
+ *  On link status change (i.e. PCI reset, speed change) and link is up and
+ *  speed is gigabit-
+ *    0) if workaround is optionally disabled do nothing
+ *    1) wait 1ms for Kumeran link to come up
+ *    2) check Kumeran Diagnostic register PCS lock loss bit
+ *    3) if not set the link is locked (all is good), otherwise...
+ *    4) reset the PHY
+ *    5) repeat up to 10 times
+ *  Note: this is only called for IGP3 copper when speed is 1gb.
+ **/
+static s32 e1000_kmrn_lock_loss_workaround_ich8lan(struct e1000_hw *hw)
+{
+	struct e1000_dev_spec_ich8lan *dev_spec;
+	u32 phy_ctrl;
+	s32 ret_val = E1000_SUCCESS;
+	u16 i, data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_kmrn_lock_loss_workaround_ich8lan");
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	if (dev_spec->kmrn_lock_loss_workaround_enabled == FALSE)
+		goto out;
+
+	/* Make sure link is up before proceeding.  If not just return.
+	 * Attempting this while link is negotiating fouled up link
+	 * stability */
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (!link) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	for (i = 0; i < 10; i++) {
+		/* read once to clear */
+		ret_val = e1000_read_phy_reg(hw, IGP3_KMRN_DIAG, &data);
+		if (ret_val)
+			goto out;
+		/* and again to get new status */
+		ret_val = e1000_read_phy_reg(hw, IGP3_KMRN_DIAG, &data);
+		if (ret_val)
+			goto out;
+
+		/* check for PCS lock */
+		if (!(data & IGP3_KMRN_DIAG_PCS_LOCK_LOSS)) {
+			ret_val = E1000_SUCCESS;
+			goto out;
+		}
+
+		/* Issue PHY reset */
+		e1000_phy_hw_reset(hw);
+		msec_delay_irq(5);
+	}
+	/* Disable GigE link negotiation */
+	phy_ctrl = E1000_READ_REG(hw, E1000_PHY_CTRL);
+	phy_ctrl |= (E1000_PHY_CTRL_GBE_DISABLE |
+	             E1000_PHY_CTRL_NOND0A_GBE_DISABLE);
+	E1000_WRITE_REG(hw, E1000_PHY_CTRL, phy_ctrl);
+
+	/* Call gig speed drop workaround on Giga disable before accessing
+	 * any PHY registers */
+	e1000_gig_downshift_workaround_ich8lan(hw);
+
+	/* unable to acquire PCS lock */
+	ret_val = -E1000_ERR_PHY;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_kmrn_lock_loss_workaound_ich8lan - Set Kumeran workaround state
+ *  @hw: pointer to the HW structure
+ *  @state: boolean value used to set the current Kumaran workaround state
+ *
+ *  If ICH8, set the current Kumeran workaround state (enabled - TRUE
+ *  /disabled - FALSE).
+ **/
+void e1000_set_kmrn_lock_loss_workaround_ich8lan(struct e1000_hw *hw,
+                                                 boolean_t state)
+{
+	struct e1000_dev_spec_ich8lan *dev_spec;
+
+	DEBUGFUNC("e1000_set_kmrn_lock_loss_workaround_ich8lan");
+
+	if (hw->mac.type != e1000_ich8lan) {
+		DEBUGOUT("Workaround applies to ICH8 only.\n");
+		goto out;
+	}
+
+	dev_spec = (struct e1000_dev_spec_ich8lan *)hw->dev_spec;
+
+	if (dev_spec == NULL) {
+		DEBUGOUT("dev_spec pointer is set to NULL.\n");
+		goto out;
+	}
+
+	dev_spec->kmrn_lock_loss_workaround_enabled = state;
+
+out:
+	return;
+}
+
+/**
+ *  e1000_ipg3_phy_powerdown_workaround_ich8lan - Power down workaround on D3
+ *  @hw: pointer to the HW structure
+ *
+ *  Workaround for 82566 power-down on D3 entry:
+ *    1) disable gigabit link
+ *    2) write VR power-down enable
+ *    3) read it back
+ *  Continue if successful, else issue LCD reset and repeat
+ **/
+void e1000_igp3_phy_powerdown_workaround_ich8lan(struct e1000_hw *hw)
+{
+	u32 reg;
+	u16 data;
+	u8  retry = 0;
+
+	DEBUGFUNC("e1000_igp3_phy_powerdown_workaround_ich8lan");
+
+	if (hw->phy.type != e1000_phy_igp_3)
+		goto out;
+
+	/* Try the workaround twice (if needed) */
+	do {
+		/* Disable link */
+		reg = E1000_READ_REG(hw, E1000_PHY_CTRL);
+		reg |= (E1000_PHY_CTRL_GBE_DISABLE |
+		        E1000_PHY_CTRL_NOND0A_GBE_DISABLE);
+		E1000_WRITE_REG(hw, E1000_PHY_CTRL, reg);
+
+		/* Call gig speed drop workaround on Giga disable before
+		 * accessing any PHY registers */
+		if (hw->mac.type == e1000_ich8lan)
+			e1000_gig_downshift_workaround_ich8lan(hw);
+
+		/* Write VR power-down enable */
+		e1000_read_phy_reg(hw, IGP3_VR_CTRL, &data);
+		data &= ~IGP3_VR_CTRL_DEV_POWERDOWN_MODE_MASK;
+		e1000_write_phy_reg(hw,
+		                   IGP3_VR_CTRL,
+		                   data | IGP3_VR_CTRL_MODE_SHUTDOWN);
+
+		/* Read it back and test */
+		e1000_read_phy_reg(hw, IGP3_VR_CTRL, &data);
+		data &= IGP3_VR_CTRL_DEV_POWERDOWN_MODE_MASK;
+		if ((data == IGP3_VR_CTRL_MODE_SHUTDOWN) || retry)
+			break;
+
+		/* Issue PHY reset and repeat at most one more time */
+		reg = E1000_READ_REG(hw, E1000_CTRL);
+		E1000_WRITE_REG(hw, E1000_CTRL, reg | E1000_CTRL_PHY_RST);
+		retry++;
+	} while (retry);
+
+out:
+	return;
+}
+
+/**
+ *  e1000_gig_downshift_workaround_ich8lan - WoL from S5 stops working
+ *  @hw: pointer to the HW structure
+ *
+ *  Steps to take when dropping from 1Gb/s (eg. link cable removal (LSC),
+ *  LPLU, Giga disable, MDIC PHY reset):
+ *    1) Set Kumeran Near-end loopback
+ *    2) Clear Kumeran Near-end loopback
+ *  Should only be called for ICH8[m] devices with IGP_3 Phy.
+ **/
+void e1000_gig_downshift_workaround_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 reg_data;
+
+	DEBUGFUNC("e1000_gig_downshift_workaround_ich8lan");
+
+	if ((hw->mac.type != e1000_ich8lan) ||
+	    (hw->phy.type != e1000_phy_igp_3))
+		goto out;
+
+	ret_val = e1000_read_kmrn_reg(hw, E1000_KMRNCTRLSTA_DIAG_OFFSET,
+				      &reg_data);
+	if (ret_val)
+		goto out;
+	reg_data |= E1000_KMRNCTRLSTA_DIAG_NELPBK;
+	ret_val = e1000_write_kmrn_reg(hw, E1000_KMRNCTRLSTA_DIAG_OFFSET,
+	                               reg_data);
+	if (ret_val)
+		goto out;
+	reg_data &= ~E1000_KMRNCTRLSTA_DIAG_NELPBK;
+	ret_val = e1000_write_kmrn_reg(hw, E1000_KMRNCTRLSTA_DIAG_OFFSET,
+				       reg_data);
+out:
+	return;
+}
+
+/**
+ *  e1000_cleanup_led_ich8lan - Restore the default LED operation
+ *  @hw: pointer to the HW structure
+ *
+ *  Return the LED back to the default configuration.
+ **/
+static s32 e1000_cleanup_led_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_cleanup_led_ich8lan");
+
+	if (hw->phy.type == e1000_phy_ife)
+		ret_val = e1000_write_phy_reg(hw,
+		                                IFE_PHY_SPECIAL_CONTROL_LED,
+		                                0);
+	else
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_default);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_led_on_ich8lan - Turn LED's on
+ *  @hw: pointer to the HW structure
+ *
+ *  Turn on the LED's.
+ **/
+static s32 e1000_led_on_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_led_on_ich8lan");
+
+	if (hw->phy.type == e1000_phy_ife)
+		ret_val = e1000_write_phy_reg(hw,
+		                IFE_PHY_SPECIAL_CONTROL_LED,
+		                (IFE_PSCL_PROBE_MODE | IFE_PSCL_PROBE_LEDS_ON));
+	else
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode2);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_led_off_ich8lan - Turn LED's off
+ *  @hw: pointer to the HW structure
+ *
+ *  Turn off the LED's.
+ **/
+static s32 e1000_led_off_ich8lan(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_led_off_ich8lan");
+
+	if (hw->phy.type == e1000_phy_ife)
+		ret_val = e1000_write_phy_reg(hw,
+		               IFE_PHY_SPECIAL_CONTROL_LED,
+		               (IFE_PSCL_PROBE_MODE | IFE_PSCL_PROBE_LEDS_OFF));
+	else
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode1);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_clear_hw_cntrs_ich8lan - Clear statistical counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears hardware counters specific to the silicon family and calls
+ *  clear_hw_cntrs_generic to clear all general purpose counters.
+ **/
+static void e1000_clear_hw_cntrs_ich8lan(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_ich8lan");
+
+	e1000_clear_hw_cntrs_base_generic(hw);
+
+	temp = E1000_READ_REG(hw, E1000_ALGNERRC);
+	temp = E1000_READ_REG(hw, E1000_RXERRC);
+	temp = E1000_READ_REG(hw, E1000_TNCRS);
+	temp = E1000_READ_REG(hw, E1000_CEXTERR);
+	temp = E1000_READ_REG(hw, E1000_TSCTC);
+	temp = E1000_READ_REG(hw, E1000_TSCTFC);
+
+	temp = E1000_READ_REG(hw, E1000_MGTPRC);
+	temp = E1000_READ_REG(hw, E1000_MGTPDC);
+	temp = E1000_READ_REG(hw, E1000_MGTPTC);
+
+	temp = E1000_READ_REG(hw, E1000_IAC);
+	temp = E1000_READ_REG(hw, E1000_ICRXOC);
+}
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_ich8lan.h linux-2.6.9/drivers/net/e1000/e1000_ich8lan.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_ich8lan.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_ich8lan.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,108 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_ICH8LAN_H_
+#define _E1000_ICH8LAN_H_
+
+#define ICH_FLASH_GFPREG                 0x0000
+#define ICH_FLASH_HSFSTS                 0x0004
+#define ICH_FLASH_HSFCTL                 0x0006
+#define ICH_FLASH_FADDR                  0x0008
+#define ICH_FLASH_FDATA0                 0x0010
+
+#define ICH_FLASH_READ_COMMAND_TIMEOUT   500
+#define ICH_FLASH_WRITE_COMMAND_TIMEOUT  500
+#define ICH_FLASH_ERASE_COMMAND_TIMEOUT  3000000
+#define ICH_FLASH_LINEAR_ADDR_MASK       0x00FFFFFF
+#define ICH_FLASH_CYCLE_REPEAT_COUNT     10
+
+#define ICH_CYCLE_READ                   0
+#define ICH_CYCLE_WRITE                  2
+#define ICH_CYCLE_ERASE                  3
+
+#define FLASH_GFPREG_BASE_MASK           0x1FFF
+#define FLASH_SECTOR_ADDR_SHIFT          12
+
+#define E1000_SHADOW_RAM_WORDS           2048
+
+#define ICH_FLASH_SEG_SIZE_256           256
+#define ICH_FLASH_SEG_SIZE_4K            4096
+#define ICH_FLASH_SEG_SIZE_8K            8192
+#define ICH_FLASH_SEG_SIZE_64K           65536
+#define ICH_FLASH_SECTOR_SIZE            4096
+
+#define ICH_FLASH_REG_MAPSIZE            0x00A0
+
+#define E1000_ICH_FWSM_RSPCIPHY          0x00000040 /* Reset PHY on PCI Reset */
+#define E1000_ICH_FWSM_DISSW             0x10000000 /* FW Disables SW Writes */
+/* FW established a valid mode */
+#define E1000_ICH_FWSM_FW_VALID          0x00008000
+
+#define E1000_ICH_MNG_IAMT_MODE          0x2
+
+#define ID_LED_DEFAULT_ICH8LAN  ((ID_LED_DEF1_DEF2 << 12) | \
+                                 (ID_LED_DEF1_OFF2 <<  8) | \
+                                 (ID_LED_DEF1_ON2  <<  4) | \
+                                 (ID_LED_DEF1_DEF2))
+
+#define E1000_ICH_NVM_SIG_WORD           0x13
+#define E1000_ICH_NVM_SIG_MASK           0xC000
+
+#define E1000_ICH8_LAN_INIT_TIMEOUT      1500
+
+#define E1000_FEXTNVM_SW_CONFIG        1
+#define E1000_FEXTNVM_SW_CONFIG_ICH8M (1 << 27) /* Bit redefined for ICH8M :/ */
+
+#define PCIE_ICH8_SNOOP_ALL   PCIE_NO_SNOOP_ALL
+
+#define E1000_ICH_RAR_ENTRIES            7
+
+#define PHY_PAGE_SHIFT 5
+#define PHY_REG(page, reg) (((page) << PHY_PAGE_SHIFT) | \
+                           ((reg) & MAX_PHY_REG_ADDRESS))
+#define IGP3_KMRN_DIAG  PHY_REG(770, 19) /* KMRN Diagnostic */
+#define IGP3_VR_CTRL    PHY_REG(776, 18) /* Voltage Regulator Control */
+#define IGP3_CAPABILITY PHY_REG(776, 19) /* Capability */
+#define IGP3_PM_CTRL    PHY_REG(769, 20) /* Power Management Control */
+
+#define IGP3_KMRN_DIAG_PCS_LOCK_LOSS         0x0002
+#define IGP3_VR_CTRL_DEV_POWERDOWN_MODE_MASK 0x0300
+#define IGP3_VR_CTRL_MODE_SHUTDOWN           0x0200
+#define IGP3_PM_CTRL_FORCE_PWR_DOWN          0x0020
+
+/* Additional interrupts need to be handled for ICH family:
+ *  DSW = The FW changed the status of the DISSW bit in FWSM
+ *  PHYINT = The LAN connected device generates an interrupt
+ *  EPRST = Manageability reset event
+ */
+#define IMS_ICH_ENABLE_MASK (\
+    E1000_IMS_DSW   | \
+    E1000_IMS_PHYINT | \
+    E1000_IMS_EPRST)
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_mac.c linux-2.6.9/drivers/net/e1000/e1000_mac.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_mac.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_mac.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1915 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "e1000_api.h"
+#include "e1000_mac.h"
+
+/**
+ *  e1000_remove_device_generic - Free device specific structure
+ *  @hw: pointer to the HW structure
+ *
+ *  If a device specific structure was allocated, this function will
+ *  free it.
+ **/
+void e1000_remove_device_generic(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_remove_device_generic");
+
+	/* Freeing the dev_spec member of e1000_hw structure */
+	e1000_free_dev_spec_struct(hw);
+}
+
+/**
+ *  e1000_get_bus_info_pci_generic - Get PCI(x) bus information
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines and stores the system bus information for a particular
+ *  network interface.  The following bus information is determined and stored:
+ *  bus speed, bus width, type (PCI/PCIx), and PCI(-x) function.
+ **/
+s32 e1000_get_bus_info_pci_generic(struct e1000_hw *hw)
+{
+	struct e1000_bus_info *bus = &hw->bus;
+	u32 status = E1000_READ_REG(hw, E1000_STATUS);
+	s32 ret_val = E1000_SUCCESS;
+	u16 pci_header_type;
+
+	DEBUGFUNC("e1000_get_bus_info_pci_generic");
+
+	/* PCI or PCI-X? */
+	bus->type = (status & E1000_STATUS_PCIX_MODE)
+			? e1000_bus_type_pcix
+			: e1000_bus_type_pci;
+
+	/* Bus speed */
+	if (bus->type == e1000_bus_type_pci) {
+		bus->speed = (status & E1000_STATUS_PCI66)
+		             ? e1000_bus_speed_66
+		             : e1000_bus_speed_33;
+	} else {
+		switch (status & E1000_STATUS_PCIX_SPEED) {
+		case E1000_STATUS_PCIX_SPEED_66:
+			bus->speed = e1000_bus_speed_66;
+			break;
+		case E1000_STATUS_PCIX_SPEED_100:
+			bus->speed = e1000_bus_speed_100;
+			break;
+		case E1000_STATUS_PCIX_SPEED_133:
+			bus->speed = e1000_bus_speed_133;
+			break;
+		default:
+			bus->speed = e1000_bus_speed_reserved;
+			break;
+		}
+	}
+
+	/* Bus width */
+	bus->width = (status & E1000_STATUS_BUS64)
+	             ? e1000_bus_width_64
+	             : e1000_bus_width_32;
+
+	/* Which PCI(-X) function? */
+	e1000_read_pci_cfg(hw, PCI_HEADER_TYPE_REGISTER, &pci_header_type);
+	if (pci_header_type & PCI_HEADER_TYPE_MULTIFUNC)
+		bus->func = (status & E1000_STATUS_FUNC_MASK)
+		            >> E1000_STATUS_FUNC_SHIFT;
+	else
+		bus->func = 0;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_get_bus_info_pcie_generic - Get PCIe bus information
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines and stores the system bus information for a particular
+ *  network interface.  The following bus information is determined and stored:
+ *  bus speed, bus width, type (PCIe), and PCIe function.
+ **/
+s32 e1000_get_bus_info_pcie_generic(struct e1000_hw *hw)
+{
+	struct e1000_bus_info *bus = &hw->bus;
+	s32 ret_val;
+	u32 status;
+	u16 pcie_link_status, pci_header_type;
+
+	DEBUGFUNC("e1000_get_bus_info_pcie_generic");
+
+	bus->type = e1000_bus_type_pci_express;
+	bus->speed = e1000_bus_speed_2500;
+
+	ret_val = e1000_read_pcie_cap_reg(hw,
+	                                  PCIE_LINK_STATUS,
+	                                  &pcie_link_status);
+	if (ret_val)
+		bus->width = e1000_bus_width_unknown;
+	else
+		bus->width = (e1000_bus_width)((pcie_link_status &
+		                                PCIE_LINK_WIDTH_MASK) >>
+		                               PCIE_LINK_WIDTH_SHIFT);
+
+	e1000_read_pci_cfg(hw, PCI_HEADER_TYPE_REGISTER, &pci_header_type);
+	if (pci_header_type & PCI_HEADER_TYPE_MULTIFUNC) {
+		status = E1000_READ_REG(hw, E1000_STATUS);
+		bus->func = (status & E1000_STATUS_FUNC_MASK)
+		            >> E1000_STATUS_FUNC_SHIFT;
+	} else {
+		bus->func = 0;
+	}
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_clear_vfta_generic - Clear VLAN filter table
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the register array which contains the VLAN filter table by
+ *  setting all the values to 0.
+ **/
+void e1000_clear_vfta_generic(struct e1000_hw *hw)
+{
+	u32 offset;
+
+	DEBUGFUNC("e1000_clear_vfta_generic");
+
+	for (offset = 0; offset < E1000_VLAN_FILTER_TBL_SIZE; offset++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_VFTA, offset, 0);
+		E1000_WRITE_FLUSH(hw);
+	}
+}
+
+/**
+ *  e1000_write_vfta_generic - Write value to VLAN filter table
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset in VLAN filter table
+ *  @value: register value written to VLAN filter table
+ *
+ *  Writes value at the given offset in the register array which stores
+ *  the VLAN filter table.
+ **/
+void e1000_write_vfta_generic(struct e1000_hw *hw, u32 offset, u32 value)
+{
+	DEBUGFUNC("e1000_write_vfta_generic");
+
+	E1000_WRITE_REG_ARRAY(hw, E1000_VFTA, offset, value);
+	E1000_WRITE_FLUSH(hw);
+}
+
+/**
+ *  e1000_init_rx_addrs_generic - Initialize receive address's
+ *  @hw: pointer to the HW structure
+ *  @rar_count: receive address registers
+ *
+ *  Setups the receive address registers by setting the base receive address
+ *  register to the devices MAC address and clearing all the other receive
+ *  address registers to 0.
+ **/
+void e1000_init_rx_addrs_generic(struct e1000_hw *hw, u16 rar_count)
+{
+	u32 i;
+
+	DEBUGFUNC("e1000_init_rx_addrs_generic");
+
+	/* Setup the receive address */
+	DEBUGOUT("Programming MAC Address into RAR[0]\n");
+
+	e1000_rar_set_generic(hw, hw->mac.addr, 0);
+
+	/* Zero out the other (rar_entry_count - 1) receive addresses */
+	DEBUGOUT1("Clearing RAR[1-%u]\n", rar_count-1);
+	for (i = 1; i < rar_count; i++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_RA, (i << 1), 0);
+		E1000_WRITE_FLUSH(hw);
+		E1000_WRITE_REG_ARRAY(hw, E1000_RA, ((i << 1) + 1), 0);
+		E1000_WRITE_FLUSH(hw);
+	}
+}
+
+/**
+ *  e1000_rar_set_generic - Set receive address register
+ *  @hw: pointer to the HW structure
+ *  @addr: pointer to the receive address
+ *  @index: receive address array register
+ *
+ *  Sets the receive address array register at index to the address passed
+ *  in by addr.
+ **/
+void e1000_rar_set_generic(struct e1000_hw *hw, u8 *addr, u32 index)
+{
+	u32 rar_low, rar_high;
+
+	DEBUGFUNC("e1000_rar_set_generic");
+
+	/* HW expects these in little endian so we reverse the byte order
+	 * from network order (big endian) to little endian
+	 */
+	rar_low = ((u32) addr[0] |
+		   ((u32) addr[1] << 8) |
+		    ((u32) addr[2] << 16) | ((u32) addr[3] << 24));
+
+	rar_high = ((u32) addr[4] | ((u32) addr[5] << 8));
+
+	if (!hw->mac.disable_av)
+		rar_high |= E1000_RAH_AV;
+
+	E1000_WRITE_REG_ARRAY(hw, E1000_RA, (index << 1), rar_low);
+	E1000_WRITE_REG_ARRAY(hw, E1000_RA, ((index << 1) + 1), rar_high);
+}
+
+/**
+ *  e1000_mta_set_generic - Set multicast filter table address
+ *  @hw: pointer to the HW structure
+ *  @hash_value: determines the MTA register and bit to set
+ *
+ *  The multicast table address is a register array of 32-bit registers.
+ *  The hash_value is used to determine what register the bit is in, the
+ *  current value is read, the new bit is OR'd in and the new value is
+ *  written back into the register.
+ **/
+void e1000_mta_set_generic(struct e1000_hw *hw, u32 hash_value)
+{
+	u32 hash_bit, hash_reg, mta;
+
+	DEBUGFUNC("e1000_mta_set_generic");
+	/* The MTA is a register array of 32-bit registers. It is
+	 * treated like an array of (32*mta_reg_count) bits.  We want to
+	 * set bit BitArray[hash_value]. So we figure out what register
+	 * the bit is in, read it, OR in the new bit, then write
+	 * back the new value.  The (hw->mac.mta_reg_count - 1) serves as a
+	 * mask to bits 31:5 of the hash value which gives us the
+	 * register we're modifying.  The hash bit within that register
+	 * is determined by the lower 5 bits of the hash value.
+	 */
+	hash_reg = (hash_value >> 5) & (hw->mac.mta_reg_count - 1);
+	hash_bit = hash_value & 0x1F;
+
+	mta = E1000_READ_REG_ARRAY(hw, E1000_MTA, hash_reg);
+
+	mta |= (1 << hash_bit);
+
+	E1000_WRITE_REG_ARRAY(hw, E1000_MTA, hash_reg, mta);
+	E1000_WRITE_FLUSH(hw);
+}
+
+/**
+ *  e1000_mc_addr_list_update_generic - Update Multicast addresses
+ *  @hw: pointer to the HW structure
+ *  @mc_addr_list: array of multicast addresses to program
+ *  @mc_addr_count: number of multicast addresses to program
+ *  @rar_used_count: the first RAR register free to program
+ *  @rar_count: total number of supported Receive Address Registers
+ *
+ *  Updates the Receive Address Registers and Multicast Table Array.
+ *  The caller must have a packed mc_addr_list of multicast addresses.
+ *  The parameter rar_count will usually be hw->mac.rar_entry_count
+ *  unless there are workarounds that change this.
+ **/
+void e1000_mc_addr_list_update_generic(struct e1000_hw *hw,
+                                       u8 *mc_addr_list, u32 mc_addr_count,
+                                       u32 rar_used_count, u32 rar_count)
+{
+	u32 hash_value;
+	u32 i;
+
+	DEBUGFUNC("e1000_mc_addr_list_update_generic");
+
+	/* Load the first set of multicast addresses into the exact
+	 * filters (RAR).  If there are not enough to fill the RAR
+	 * array, clear the filters.
+	 */
+	for (i = rar_used_count; i < rar_count; i++) {
+		if (mc_addr_count) {
+			e1000_rar_set(hw, mc_addr_list, i);
+			mc_addr_count--;
+			mc_addr_list += ETH_ADDR_LEN;
+		} else {
+			E1000_WRITE_REG_ARRAY(hw, E1000_RA, i << 1, 0);
+			E1000_WRITE_FLUSH(hw);
+			E1000_WRITE_REG_ARRAY(hw, E1000_RA, (i << 1) + 1, 0);
+			E1000_WRITE_FLUSH(hw);
+		}
+	}
+
+	/* Clear the old settings from the MTA */
+	DEBUGOUT("Clearing MTA\n");
+	for (i = 0; i < hw->mac.mta_reg_count; i++) {
+		E1000_WRITE_REG_ARRAY(hw, E1000_MTA, i, 0);
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	/* Load any remaining multicast addresses into the hash table. */
+	for (; mc_addr_count > 0; mc_addr_count--) {
+		hash_value = e1000_hash_mc_addr(hw, mc_addr_list);
+		DEBUGOUT1("Hash value = 0x%03X\n", hash_value);
+		e1000_mta_set(hw, hash_value);
+		mc_addr_list += ETH_ADDR_LEN;
+	}
+}
+
+/**
+ *  e1000_hash_mc_addr_generic - Generate a multicast hash value
+ *  @hw: pointer to the HW structure
+ *  @mc_addr: pointer to a multicast address
+ *
+ *  Generates a multicast address hash value which is used to determine
+ *  the multicast filter table array address and new table value.  See
+ *  e1000_mta_set_generic()
+ **/
+u32 e1000_hash_mc_addr_generic(struct e1000_hw *hw, u8 *mc_addr)
+{
+	u32 hash_value, hash_mask;
+	u8 bit_shift = 0;
+
+	DEBUGFUNC("e1000_hash_mc_addr_generic");
+
+	/* Register count multiplied by bits per register */
+	hash_mask = (hw->mac.mta_reg_count * 32) - 1;
+
+	/* For a mc_filter_type of 0, bit_shift is the number of left-shifts
+	 * where 0xFF would still fall within the hash mask. */
+	while (hash_mask >> bit_shift != 0xFF)
+		bit_shift++;
+
+	/* The portion of the address that is used for the hash table
+	 * is determined by the mc_filter_type setting.
+	 * The algorithm is such that there is a total of 8 bits of shifting.
+	 * The bit_shift for a mc_filter_type of 0 represents the number of
+	 * left-shifts where the MSB of mc_addr[5] would still fall within
+	 * the hash_mask.  Case 0 does this exactly.  Since there are a total
+	 * of 8 bits of shifting, then mc_addr[4] will shift right the
+	 * remaining number of bits. Thus 8 - bit_shift.  The rest of the
+	 * cases are a variation of this algorithm...essentially raising the
+	 * number of bits to shift mc_addr[5] left, while still keeping the
+	 * 8-bit shifting total.
+	 */
+	/* For example, given the following Destination MAC Address and an
+	 * mta register count of 128 (thus a 4096-bit vector and 0xFFF mask),
+	 * we can see that the bit_shift for case 0 is 4.  These are the hash
+	 * values resulting from each mc_filter_type...
+	 * [0] [1] [2] [3] [4] [5]
+	 * 01  AA  00  12  34  56
+	 * LSB                 MSB
+	 *
+	 * case 0: hash_value = ((0x34 >> 4) | (0x56 << 4)) & 0xFFF = 0x563
+	 * case 1: hash_value = ((0x34 >> 3) | (0x56 << 5)) & 0xFFF = 0xAC6
+	 * case 2: hash_value = ((0x34 >> 2) | (0x56 << 6)) & 0xFFF = 0x163
+	 * case 3: hash_value = ((0x34 >> 0) | (0x56 << 8)) & 0xFFF = 0x634
+	 */
+	switch (hw->mac.mc_filter_type) {
+		default:
+		case 0:
+			break;
+		case 1:
+			bit_shift += 1;
+			break;
+		case 2:
+			bit_shift += 2;
+			break;
+		case 3:
+			bit_shift += 4;
+			break;
+	}
+
+	hash_value = hash_mask & (((mc_addr[4] >> (8 - bit_shift)) |
+	                          (((u16) mc_addr[5]) << bit_shift)));
+
+	return hash_value;
+}
+
+/**
+ *  e1000_pcix_mmrbc_workaround_generic - Fix incorrect MMRBC value
+ *  @hw: pointer to the HW structure
+ *
+ *  In certain situations, a system BIOS may report that the PCIx maximum
+ *  memory read byte count (MMRBC) value is higher than than the actual
+ *  value. We check the PCIx command regsiter with the current PCIx status
+ *  regsiter.
+ **/
+void e1000_pcix_mmrbc_workaround_generic(struct e1000_hw *hw)
+{
+	u16 cmd_mmrbc;
+	u16 pcix_cmd;
+	u16 pcix_stat_hi_word;
+	u16 stat_mmrbc;
+
+	DEBUGFUNC("e1000_pcix_mmrbc_workaround_generic");
+
+	/* Workaround for PCI-X issue when BIOS sets MMRBC incorrectly */
+	if (hw->bus.type != e1000_bus_type_pcix)
+		return;
+
+	e1000_read_pci_cfg(hw, PCIX_COMMAND_REGISTER, &pcix_cmd);
+	e1000_read_pci_cfg(hw, PCIX_STATUS_REGISTER_HI, &pcix_stat_hi_word);
+	cmd_mmrbc = (pcix_cmd & PCIX_COMMAND_MMRBC_MASK) >>
+		    PCIX_COMMAND_MMRBC_SHIFT;
+	stat_mmrbc = (pcix_stat_hi_word & PCIX_STATUS_HI_MMRBC_MASK) >>
+		     PCIX_STATUS_HI_MMRBC_SHIFT;
+	if (stat_mmrbc == PCIX_STATUS_HI_MMRBC_4K)
+		stat_mmrbc = PCIX_STATUS_HI_MMRBC_2K;
+	if (cmd_mmrbc > stat_mmrbc) {
+		pcix_cmd &= ~PCIX_COMMAND_MMRBC_MASK;
+		pcix_cmd |= stat_mmrbc << PCIX_COMMAND_MMRBC_SHIFT;
+		e1000_write_pci_cfg(hw, PCIX_COMMAND_REGISTER, &pcix_cmd);
+	}
+}
+
+/**
+ *  e1000_clear_hw_cntrs_base_generic - Clear base hardware counters
+ *  @hw: pointer to the HW structure
+ *
+ *  Clears the base hardware counters by reading the counter registers.
+ **/
+void e1000_clear_hw_cntrs_base_generic(struct e1000_hw *hw)
+{
+	volatile u32 temp;
+
+	DEBUGFUNC("e1000_clear_hw_cntrs_base_generic");
+
+	temp = E1000_READ_REG(hw, E1000_CRCERRS);
+	temp = E1000_READ_REG(hw, E1000_SYMERRS);
+	temp = E1000_READ_REG(hw, E1000_MPC);
+	temp = E1000_READ_REG(hw, E1000_SCC);
+	temp = E1000_READ_REG(hw, E1000_ECOL);
+	temp = E1000_READ_REG(hw, E1000_MCC);
+	temp = E1000_READ_REG(hw, E1000_LATECOL);
+	temp = E1000_READ_REG(hw, E1000_COLC);
+	temp = E1000_READ_REG(hw, E1000_DC);
+	temp = E1000_READ_REG(hw, E1000_SEC);
+	temp = E1000_READ_REG(hw, E1000_RLEC);
+	temp = E1000_READ_REG(hw, E1000_XONRXC);
+	temp = E1000_READ_REG(hw, E1000_XONTXC);
+	temp = E1000_READ_REG(hw, E1000_XOFFRXC);
+	temp = E1000_READ_REG(hw, E1000_XOFFTXC);
+	temp = E1000_READ_REG(hw, E1000_FCRUC);
+	temp = E1000_READ_REG(hw, E1000_GPRC);
+	temp = E1000_READ_REG(hw, E1000_BPRC);
+	temp = E1000_READ_REG(hw, E1000_MPRC);
+	temp = E1000_READ_REG(hw, E1000_GPTC);
+	temp = E1000_READ_REG(hw, E1000_GORCL);
+	temp = E1000_READ_REG(hw, E1000_GORCH);
+	temp = E1000_READ_REG(hw, E1000_GOTCL);
+	temp = E1000_READ_REG(hw, E1000_GOTCH);
+	temp = E1000_READ_REG(hw, E1000_RNBC);
+	temp = E1000_READ_REG(hw, E1000_RUC);
+	temp = E1000_READ_REG(hw, E1000_RFC);
+	temp = E1000_READ_REG(hw, E1000_ROC);
+	temp = E1000_READ_REG(hw, E1000_RJC);
+	temp = E1000_READ_REG(hw, E1000_TORL);
+	temp = E1000_READ_REG(hw, E1000_TORH);
+	temp = E1000_READ_REG(hw, E1000_TOTL);
+	temp = E1000_READ_REG(hw, E1000_TOTH);
+	temp = E1000_READ_REG(hw, E1000_TPR);
+	temp = E1000_READ_REG(hw, E1000_TPT);
+	temp = E1000_READ_REG(hw, E1000_MPTC);
+	temp = E1000_READ_REG(hw, E1000_BPTC);
+}
+
+/**
+ *  e1000_check_for_copper_link_generic - Check for link (Copper)
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks to see of the link status of the hardware has changed.  If a
+ *  change in link status has been detected, then we read the PHY registers
+ *  to get the current speed/duplex if link exists.
+ **/
+s32 e1000_check_for_copper_link_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_check_for_copper_link");
+
+	/* We only want to go out to the PHY registers to see if Auto-Neg
+	 * has completed and/or if our link status has changed.  The
+	 * get_link_status flag is set upon receiving a Link Status
+	 * Change or Rx Sequence Error interrupt.
+	 */
+	if (!mac->get_link_status) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	/* First we want to see if the MII Status Register reports
+	 * link.  If so, then we want to get the current speed/duplex
+	 * of the PHY.
+	 */
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link)
+		goto out; /* No link detected */
+
+	mac->get_link_status = FALSE;
+
+	/* Check if there was DownShift, must be checked
+	 * immediately after link-up */
+	e1000_check_downshift_generic(hw);
+
+	/* If we are forcing speed/duplex, then we simply return since
+	 * we have already determined whether we have link or not.
+	 */
+	if (!mac->autoneg) {
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	/* Auto-Neg is enabled.  Auto Speed Detection takes care
+	 * of MAC speed/duplex configuration.  So we only need to
+	 * configure Collision Distance in the MAC.
+	 */
+	e1000_config_collision_dist_generic(hw);
+
+	/* Configure Flow Control now that Auto-Neg has completed.
+	 * First, we need to restore the desired flow control
+	 * settings because we may have had to re-autoneg with a
+	 * different link partner.
+	 */
+	ret_val = e1000_config_fc_after_link_up_generic(hw);
+	if (ret_val) {
+		DEBUGOUT("Error configuring flow control\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_for_fiber_link_generic - Check for link (Fiber)
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks for link up on the hardware.  If link is not up and we have
+ *  a signal, then we need to force link up.
+ **/
+s32 e1000_check_for_fiber_link_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 rxcw;
+	u32 ctrl;
+	u32 status;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_check_for_fiber_link_generic");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	status = E1000_READ_REG(hw, E1000_STATUS);
+	rxcw = E1000_READ_REG(hw, E1000_RXCW);
+
+	/* If we don't have link (auto-negotiation failed or link partner
+	 * cannot auto-negotiate), the cable is plugged in (we have signal),
+	 * and our link partner is not trying to auto-negotiate with us (we
+	 * are receiving idles or data), we need to force link up. We also
+	 * need to give auto-negotiation time to complete, in case the cable
+	 * was just plugged in. The autoneg_failed flag does this.
+	 */
+	/* (ctrl & E1000_CTRL_SWDPIN1) == 1 == have signal */
+	if ((ctrl & E1000_CTRL_SWDPIN1) && (!(status & E1000_STATUS_LU)) &&
+	    (!(rxcw & E1000_RXCW_C))) {
+		if (mac->autoneg_failed == 0) {
+			mac->autoneg_failed = 1;
+			goto out;
+		}
+		DEBUGOUT("NOT RXing /C/, disable AutoNeg and force link.\n");
+
+		/* Disable auto-negotiation in the TXCW register */
+		E1000_WRITE_REG(hw, E1000_TXCW, (mac->txcw & ~E1000_TXCW_ANE));
+
+		/* Force link-up and also force full-duplex. */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= (E1000_CTRL_SLU | E1000_CTRL_FD);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+		/* Configure Flow Control after forcing link up. */
+		ret_val = e1000_config_fc_after_link_up_generic(hw);
+		if (ret_val) {
+			DEBUGOUT("Error configuring flow control\n");
+			goto out;
+		}
+	} else if ((ctrl & E1000_CTRL_SLU) && (rxcw & E1000_RXCW_C)) {
+		/* If we are forcing link and we are receiving /C/ ordered
+		 * sets, re-enable auto-negotiation in the TXCW register
+		 * and disable forced link in the Device Control register
+		 * in an attempt to auto-negotiate with our link partner.
+		 */
+		DEBUGOUT("RXing /C/, enable AutoNeg and stop forcing link.\n");
+		E1000_WRITE_REG(hw, E1000_TXCW, mac->txcw);
+		E1000_WRITE_REG(hw, E1000_CTRL, (ctrl & ~E1000_CTRL_SLU));
+
+		mac->serdes_has_link = TRUE;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_for_serdes_link_generic - Check for link (Serdes)
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks for link up on the hardware.  If link is not up and we have
+ *  a signal, then we need to force link up.
+ **/
+s32 e1000_check_for_serdes_link_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 rxcw;
+	u32 ctrl;
+	u32 status;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_check_for_serdes_link_generic");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	status = E1000_READ_REG(hw, E1000_STATUS);
+	rxcw = E1000_READ_REG(hw, E1000_RXCW);
+
+	/* If we don't have link (auto-negotiation failed or link partner
+	 * cannot auto-negotiate), and our link partner is not trying to
+	 * auto-negotiate with us (we are receiving idles or data),
+	 * we need to force link up. We also need to give auto-negotiation
+	 * time to complete.
+	 */
+	/* (ctrl & E1000_CTRL_SWDPIN1) == 1 == have signal */
+	if ((!(status & E1000_STATUS_LU)) && (!(rxcw & E1000_RXCW_C))) {
+		if (mac->autoneg_failed == 0) {
+			mac->autoneg_failed = 1;
+			goto out;
+		}
+		DEBUGOUT("NOT RXing /C/, disable AutoNeg and force link.\n");
+
+		/* Disable auto-negotiation in the TXCW register */
+		E1000_WRITE_REG(hw, E1000_TXCW, (mac->txcw & ~E1000_TXCW_ANE));
+
+		/* Force link-up and also force full-duplex. */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= (E1000_CTRL_SLU | E1000_CTRL_FD);
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+		/* Configure Flow Control after forcing link up. */
+		ret_val = e1000_config_fc_after_link_up_generic(hw);
+		if (ret_val) {
+			DEBUGOUT("Error configuring flow control\n");
+			goto out;
+		}
+	} else if ((ctrl & E1000_CTRL_SLU) && (rxcw & E1000_RXCW_C)) {
+		/* If we are forcing link and we are receiving /C/ ordered
+		 * sets, re-enable auto-negotiation in the TXCW register
+		 * and disable forced link in the Device Control register
+		 * in an attempt to auto-negotiate with our link partner.
+		 */
+		DEBUGOUT("RXing /C/, enable AutoNeg and stop forcing link.\n");
+		E1000_WRITE_REG(hw, E1000_TXCW, mac->txcw);
+		E1000_WRITE_REG(hw, E1000_CTRL, (ctrl & ~E1000_CTRL_SLU));
+
+		mac->serdes_has_link = TRUE;
+	} else if (!(E1000_TXCW_ANE & E1000_READ_REG(hw, E1000_TXCW))) {
+		/* If we force link for non-auto-negotiation switch, check
+		 * link status based on MAC synchronization for internal
+		 * serdes media type.
+		 */
+		/* SYNCH bit and IV bit are sticky. */
+		usec_delay(10);
+		if (E1000_RXCW_SYNCH & E1000_READ_REG(hw, E1000_RXCW)) {
+			if (!(rxcw & E1000_RXCW_IV)) {
+				mac->serdes_has_link = TRUE;
+				DEBUGOUT("SERDES: Link is up.\n");
+			}
+		} else {
+			mac->serdes_has_link = FALSE;
+			DEBUGOUT("SERDES: Link is down.\n");
+		}
+	}
+
+	if (E1000_TXCW_ANE & E1000_READ_REG(hw, E1000_TXCW)) {
+		status = E1000_READ_REG(hw, E1000_STATUS);
+		mac->serdes_has_link = (status & E1000_STATUS_LU)
+					? TRUE
+					: FALSE;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_link_generic - Setup flow control and link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Determines which flow control settings to use, then configures flow
+ *  control.  Calls the appropriate media-specific link configuration
+ *  function.  Assuming the adapter has a valid link partner, a valid link
+ *  should be established.  Assumes the hardware has previously been reset
+ *  and the transmitter and receiver are not enabled.
+ **/
+s32 e1000_setup_link_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_functions *func = &hw->func;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_link_generic");
+
+	/* In the case of the phy reset being blocked, we already have a link.
+	 * We do not need to set it up again.
+	 */
+	if (e1000_check_reset_block(hw))
+		goto out;
+
+	ret_val = e1000_set_default_fc_generic(hw);
+	if (ret_val)
+		goto out;
+
+	/* We want to save off the original Flow Control configuration just
+	 * in case we get disconnected and then reconnected into a different
+	 * hub or switch with different Flow Control capabilities.
+	 */
+	mac->original_fc = mac->fc;
+
+	DEBUGOUT1("After fix-ups FlowControl is now = %x\n", mac->fc);
+
+	/* Call the necessary media_type subroutine to configure the link. */
+	ret_val = func->setup_physical_interface(hw);
+	if (ret_val)
+		goto out;
+
+	/* Initialize the flow control address, type, and PAUSE timer
+	 * registers to their default values.  This is done even if flow
+	 * control is disabled, because it does not hurt anything to
+	 * initialize these registers.
+	 */
+	DEBUGOUT("Initializing the Flow Control address, type and timer regs\n");
+	E1000_WRITE_REG(hw, E1000_FCT, FLOW_CONTROL_TYPE);
+	E1000_WRITE_REG(hw, E1000_FCAH, FLOW_CONTROL_ADDRESS_HIGH);
+	E1000_WRITE_REG(hw, E1000_FCAL, FLOW_CONTROL_ADDRESS_LOW);
+
+	E1000_WRITE_REG(hw, E1000_FCTTV, mac->fc_pause_time);
+
+	ret_val = e1000_set_fc_watermarks_generic(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_fiber_serdes_link_generic - Setup link for fiber/serdes
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures collision distance and flow control for fiber and serdes
+ *  links.  Upon successful setup, poll for link.
+ **/
+s32 e1000_setup_fiber_serdes_link_generic(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_fiber_serdes_link_generic");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Take the link out of reset */
+	ctrl &= ~E1000_CTRL_LRST;
+
+	e1000_config_collision_dist_generic(hw);
+
+	ret_val = e1000_commit_fc_settings_generic(hw);
+	if (ret_val)
+		goto out;
+
+	/* Since auto-negotiation is enabled, take the link out of reset (the
+	 * link will be in reset, because we previously reset the chip). This
+	 * will restart auto-negotiation.  If auto-negotiation is successful
+	 * then the link-up status bit will be set and the flow control enable
+	 * bits (RFCE and TFCE) will be set according to their negotiated value.
+	 */
+	DEBUGOUT("Auto-negotiation enabled\n");
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	E1000_WRITE_FLUSH(hw);
+	msec_delay(1);
+
+	/* For these adapters, the SW defineable pin 1 is set when the optics
+	 * detect a signal.  If we have a signal, then poll for a "Link-Up"
+	 * indication.
+	 */
+	if (hw->media_type == e1000_media_type_internal_serdes ||
+	    (E1000_READ_REG(hw, E1000_CTRL) & E1000_CTRL_SWDPIN1)) {
+		ret_val = e1000_poll_fiber_serdes_link_generic(hw);
+	} else {
+		DEBUGOUT("No signal detected\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_config_collision_dist_generic - Configure collision distance
+ *  @hw: pointer to the HW structure
+ *
+ *  Configures the collision distance to the default value and is used
+ *  during link setup. Currently no func pointer exists and all
+ *  implementations are handled in the generic version of this function.
+ **/
+void e1000_config_collision_dist_generic(struct e1000_hw *hw)
+{
+	u32 tctl;
+
+	DEBUGFUNC("e1000_config_collision_dist_generic");
+
+	tctl = E1000_READ_REG(hw, E1000_TCTL);
+
+	tctl &= ~E1000_TCTL_COLD;
+	tctl |= E1000_COLLISION_DISTANCE << E1000_COLD_SHIFT;
+
+	E1000_WRITE_REG(hw, E1000_TCTL, tctl);
+	E1000_WRITE_FLUSH(hw);
+}
+
+/**
+ *  e1000_poll_fiber_serdes_link_generic - Poll for link up
+ *  @hw: pointer to the HW structure
+ *
+ *  Polls for link up by reading the status register, if link fails to come
+ *  up with auto-negotiation, then the link is forced if a signal is detected.
+ **/
+s32 e1000_poll_fiber_serdes_link_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 i, status;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_poll_fiber_serdes_link_generic");
+
+	/* If we have a signal (the cable is plugged in, or assumed true for
+	 * serdes media) then poll for a "Link-Up" indication in the Device
+	 * Status Register.  Time-out if a link isn't seen in 500 milliseconds
+	 * seconds (Auto-negotiation should complete in less than 500
+	 * milliseconds even if the other end is doing it in SW).
+	 */
+	for (i = 0; i < FIBER_LINK_UP_LIMIT; i++) {
+		msec_delay(10);
+		status = E1000_READ_REG(hw, E1000_STATUS);
+		if (status & E1000_STATUS_LU)
+			break;
+	}
+	if (i == FIBER_LINK_UP_LIMIT) {
+		DEBUGOUT("Never got a valid link from auto-neg!!!\n");
+		mac->autoneg_failed = 1;
+		/* AutoNeg failed to achieve a link, so we'll call
+		 * mac->check_for_link. This routine will force the
+		 * link up if we detect a signal. This will allow us to
+		 * communicate with non-autonegotiating link partners.
+		 */
+		ret_val = e1000_check_for_link(hw);
+		if (ret_val) {
+			DEBUGOUT("Error while checking for link\n");
+			goto out;
+		}
+		mac->autoneg_failed = 0;
+	} else {
+		mac->autoneg_failed = 0;
+		DEBUGOUT("Valid Link Found\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_commit_fc_settings_generic - Configure flow control
+ *  @hw: pointer to the HW structure
+ *
+ *  Write the flow control settings to the Transmit Config Word Register (TXCW)
+ *  base on the flow control settings in e1000_mac_info.
+ **/
+s32 e1000_commit_fc_settings_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 txcw;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_commit_fc_settings_generic");
+
+	/* Check for a software override of the flow control settings, and
+	 * setup the device accordingly.  If auto-negotiation is enabled, then
+	 * software will have to set the "PAUSE" bits to the correct value in
+	 * the Transmit Config Word Register (TXCW) and re-start auto-
+	 * negotiation.  However, if auto-negotiation is disabled, then
+	 * software will have to manually configure the two flow control enable
+	 * bits in the CTRL register.
+	 *
+	 * The possible values of the "fc" parameter are:
+	 *      0:  Flow control is completely disabled
+	 *      1:  Rx flow control is enabled (we can receive pause frames,
+	 *          but not send pause frames).
+	 *      2:  Tx flow control is enabled (we can send pause frames but we
+	 *          do not support receiving pause frames).
+	 *      3:  Both Rx and TX flow control (symmetric) are enabled.
+	 */
+	switch (mac->fc) {
+	case e1000_fc_none:
+		/* Flow control completely disabled by a software over-ride. */
+		txcw = (E1000_TXCW_ANE | E1000_TXCW_FD);
+		break;
+	case e1000_fc_rx_pause:
+		/* RX Flow control is enabled and TX Flow control is disabled
+		 * by a software over-ride. Since there really isn't a way to
+		 * advertise that we are capable of RX Pause ONLY, we will
+		 * advertise that we support both symmetric and asymmetric RX
+		 * PAUSE.  Later, we will disable the adapter's ability to send
+		 * PAUSE frames.
+		 */
+		txcw = (E1000_TXCW_ANE | E1000_TXCW_FD | E1000_TXCW_PAUSE_MASK);
+		break;
+	case e1000_fc_tx_pause:
+		/* TX Flow control is enabled, and RX Flow control is disabled,
+		 * by a software over-ride.
+		 */
+		txcw = (E1000_TXCW_ANE | E1000_TXCW_FD | E1000_TXCW_ASM_DIR);
+		break;
+	case e1000_fc_full:
+		/* Flow control (both RX and TX) is enabled by a software
+		 * over-ride.
+		 */
+		txcw = (E1000_TXCW_ANE | E1000_TXCW_FD | E1000_TXCW_PAUSE_MASK);
+		break;
+	default:
+		DEBUGOUT("Flow control param set incorrectly\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+		break;
+	}
+
+	E1000_WRITE_REG(hw, E1000_TXCW, txcw);
+	mac->txcw = txcw;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_set_fc_watermarks_generic - Set flow control high/low watermarks
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets the flow control high/low threshold (watermark) registers.  If
+ *  flow control XON frame transmission is enabled, then set XON frame
+ *  tansmission as well.
+ **/
+s32 e1000_set_fc_watermarks_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val = E1000_SUCCESS;
+	u32 fcrtl = 0, fcrth = 0;
+
+	DEBUGFUNC("e1000_set_fc_watermarks_generic");
+
+	/* Set the flow control receive threshold registers.  Normally,
+	 * these registers will be set to a default threshold that may be
+	 * adjusted later by the driver's runtime code.  However, if the
+	 * ability to transmit pause frames is not enabled, then these
+	 * registers will be set to 0.
+	 */
+	if (mac->fc & e1000_fc_tx_pause) {
+		/* We need to set up the Receive Threshold high and low water
+		 * marks as well as (optionally) enabling the transmission of
+		 * XON frames.
+		 */
+		fcrtl = mac->fc_low_water;
+		if (mac->fc_send_xon)
+			fcrtl |= E1000_FCRTL_XONE;
+
+		fcrth = mac->fc_high_water;
+	}
+	E1000_WRITE_REG(hw, E1000_FCRTL, fcrtl);
+	E1000_WRITE_REG(hw, E1000_FCRTH, fcrth);
+
+	return ret_val;
+}
+
+/**
+ *  e1000_set_default_fc_generic - Set flow control default values
+ *  @hw: pointer to the HW structure
+ *
+ *  Read the EEPROM for the default values for flow control and store the
+ *  values.
+ **/
+s32 e1000_set_default_fc_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val = E1000_SUCCESS;
+	u16 nvm_data;
+
+	DEBUGFUNC("e1000_set_default_fc_generic");
+
+	if (mac->fc != e1000_fc_default)
+		goto out;
+
+	/* Read and store word 0x0F of the EEPROM. This word contains bits
+	 * that determine the hardware's default PAUSE (flow control) mode,
+	 * a bit that determines whether the HW defaults to enabling or
+	 * disabling auto-negotiation, and the direction of the
+	 * SW defined pins. If there is no SW over-ride of the flow
+	 * control setting, then the variable hw->fc will
+	 * be initialized based on a value in the EEPROM.
+	 */
+	ret_val = e1000_read_nvm(hw, NVM_INIT_CONTROL2_REG, 1, &nvm_data);
+
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+
+	if ((nvm_data & NVM_WORD0F_PAUSE_MASK) == 0)
+		mac->fc = e1000_fc_none;
+	else if ((nvm_data & NVM_WORD0F_PAUSE_MASK) ==
+		 NVM_WORD0F_ASM_DIR)
+		mac->fc = e1000_fc_tx_pause;
+	else
+		mac->fc = e1000_fc_full;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_force_mac_fc_generic - Force the MAC's flow control settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Force the MAC's flow control settings.  Sets the TFCE and RFCE bits in the
+ *  device control register to reflect the adapter settings.  TFCE and RFCE
+ *  need to be explicitly set by software when a copper PHY is used because
+ *  autonegotiation is managed by the PHY rather than the MAC.  Software must
+ *  also configure these bits when link is forced on a fiber connection.
+ **/
+s32 e1000_force_mac_fc_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 ctrl;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_force_mac_fc_generic");
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+
+	/* Because we didn't get link via the internal auto-negotiation
+	 * mechanism (we either forced link or we got link via PHY
+	 * auto-neg), we have to manually enable/disable transmit an
+	 * receive flow control.
+	 *
+	 * The "Case" statement below enables/disable flow control
+	 * according to the "mac->fc" parameter.
+	 *
+	 * The possible values of the "fc" parameter are:
+	 *      0:  Flow control is completely disabled
+	 *      1:  Rx flow control is enabled (we can receive pause
+	 *          frames but not send pause frames).
+	 *      2:  Tx flow control is enabled (we can send pause frames
+	 *          frames but we do not receive pause frames).
+	 *      3:  Both Rx and TX flow control (symmetric) is enabled.
+	 *  other:  No other values should be possible at this point.
+	 */
+	DEBUGOUT1("mac->fc = %u\n", mac->fc);
+
+	switch (mac->fc) {
+	case e1000_fc_none:
+		ctrl &= (~(E1000_CTRL_TFCE | E1000_CTRL_RFCE));
+		break;
+	case e1000_fc_rx_pause:
+		ctrl &= (~E1000_CTRL_TFCE);
+		ctrl |= E1000_CTRL_RFCE;
+		break;
+	case e1000_fc_tx_pause:
+		ctrl &= (~E1000_CTRL_RFCE);
+		ctrl |= E1000_CTRL_TFCE;
+		break;
+	case e1000_fc_full:
+		ctrl |= (E1000_CTRL_TFCE | E1000_CTRL_RFCE);
+		break;
+	default:
+		DEBUGOUT("Flow control param set incorrectly\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_config_fc_after_link_up_generic - Configures flow control after link
+ *  @hw: pointer to the HW structure
+ *
+ *  Checks the status of auto-negotiation after link up to ensure that the
+ *  speed and duplex were not forced.  If the link needed to be forced, then
+ *  flow control needs to be forced also.  If auto-negotiation is enabled
+ *  and did not fail, then we configure flow control based on our link
+ *  partner.
+ **/
+s32 e1000_config_fc_after_link_up_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val = E1000_SUCCESS;
+	u16 mii_status_reg, mii_nway_adv_reg, mii_nway_lp_ability_reg;
+	u16 speed, duplex;
+
+	DEBUGFUNC("e1000_config_fc_after_link_up_generic");
+
+	/* Check for the case where we have fiber media and auto-neg failed
+	 * so we had to force link.  In this case, we need to force the
+	 * configuration of the MAC to match the "fc" parameter.
+	 */
+	if (mac->autoneg_failed) {
+		if (hw->media_type == e1000_media_type_fiber ||
+		    hw->media_type == e1000_media_type_internal_serdes)
+			ret_val = e1000_force_mac_fc_generic(hw);
+	} else {
+		if (hw->media_type == e1000_media_type_copper)
+			ret_val = e1000_force_mac_fc_generic(hw);
+	}
+
+	if (ret_val) {
+		DEBUGOUT("Error forcing flow control settings\n");
+		goto out;
+	}
+
+	/* Check for the case where we have copper media and auto-neg is
+	 * enabled.  In this case, we need to check and see if Auto-Neg
+	 * has completed, and if so, how the PHY and link partner has
+	 * flow control configured.
+	 */
+	if ((hw->media_type == e1000_media_type_copper) && mac->autoneg) {
+		/* Read the MII Status Register and check to see if AutoNeg
+		 * has completed.  We read this twice because this reg has
+		 * some "sticky" (latched) bits.
+		 */
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+		if (ret_val)
+			goto out;
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &mii_status_reg);
+		if (ret_val)
+			goto out;
+
+		if (!(mii_status_reg & MII_SR_AUTONEG_COMPLETE)) {
+			DEBUGOUT("Copper PHY and Auto Neg "
+			         "has not completed.\n");
+			goto out;
+		}
+
+		/* The AutoNeg process has completed, so we now need to
+		 * read both the Auto Negotiation Advertisement
+		 * Register (Address 4) and the Auto_Negotiation Base
+		 * Page Ability Register (Address 5) to determine how
+		 * flow control was negotiated.
+		 */
+		ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_ADV,
+					    &mii_nway_adv_reg);
+		if (ret_val)
+			goto out;
+		ret_val = e1000_read_phy_reg(hw, PHY_LP_ABILITY,
+					    &mii_nway_lp_ability_reg);
+		if (ret_val)
+			goto out;
+
+		/* Two bits in the Auto Negotiation Advertisement Register
+		 * (Address 4) and two bits in the Auto Negotiation Base
+		 * Page Ability Register (Address 5) determine flow control
+		 * for both the PHY and the link partner.  The following
+		 * table, taken out of the IEEE 802.3ab/D6.0 dated March 25,
+		 * 1999, describes these PAUSE resolution bits and how flow
+		 * control is determined based upon these settings.
+		 * NOTE:  DC = Don't Care
+		 *
+		 *   LOCAL DEVICE  |   LINK PARTNER
+		 * PAUSE | ASM_DIR | PAUSE | ASM_DIR | NIC Resolution
+		 *-------|---------|-------|---------|--------------------
+		 *   0   |    0    |  DC   |   DC    | e1000_fc_none
+		 *   0   |    1    |   0   |   DC    | e1000_fc_none
+		 *   0   |    1    |   1   |    0    | e1000_fc_none
+		 *   0   |    1    |   1   |    1    | e1000_fc_tx_pause
+		 *   1   |    0    |   0   |   DC    | e1000_fc_none
+		 *   1   |   DC    |   1   |   DC    | e1000_fc_full
+		 *   1   |    1    |   0   |    0    | e1000_fc_none
+		 *   1   |    1    |   0   |    1    | e1000_fc_rx_pause
+		 *
+		 */
+		/* Are both PAUSE bits set to 1?  If so, this implies
+		 * Symmetric Flow Control is enabled at both ends.  The
+		 * ASM_DIR bits are irrelevant per the spec.
+		 *
+		 * For Symmetric Flow Control:
+		 *
+		 *   LOCAL DEVICE  |   LINK PARTNER
+		 * PAUSE | ASM_DIR | PAUSE | ASM_DIR | Result
+		 *-------|---------|-------|---------|--------------------
+		 *   1   |   DC    |   1   |   DC    | E1000_fc_full
+		 *
+		 */
+		if ((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+		    (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE)) {
+			/* Now we need to check if the user selected RX ONLY
+			 * of pause frames.  In this case, we had to advertise
+			 * FULL flow control because we could not advertise RX
+			 * ONLY. Hence, we must now check to see if we need to
+			 * turn OFF  the TRANSMISSION of PAUSE frames.
+			 */
+			if (mac->original_fc == e1000_fc_full) {
+				mac->fc = e1000_fc_full;
+				DEBUGOUT("Flow Control = FULL.\r\n");
+			} else {
+				mac->fc = e1000_fc_rx_pause;
+				DEBUGOUT("Flow Control = "
+				         "RX PAUSE frames only.\r\n");
+			}
+		}
+		/* For receiving PAUSE frames ONLY.
+		 *
+		 *   LOCAL DEVICE  |   LINK PARTNER
+		 * PAUSE | ASM_DIR | PAUSE | ASM_DIR | Result
+		 *-------|---------|-------|---------|--------------------
+		 *   0   |    1    |   1   |    1    | e1000_fc_tx_pause
+		 *
+		 */
+		else if (!(mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+		          (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
+		          (mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
+		          (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
+			mac->fc = e1000_fc_tx_pause;
+			DEBUGOUT("Flow Control = TX PAUSE frames only.\r\n");
+		}
+		/* For transmitting PAUSE frames ONLY.
+		 *
+		 *   LOCAL DEVICE  |   LINK PARTNER
+		 * PAUSE | ASM_DIR | PAUSE | ASM_DIR | Result
+		 *-------|---------|-------|---------|--------------------
+		 *   1   |    1    |   0   |    1    | e1000_fc_rx_pause
+		 *
+		 */
+		else if ((mii_nway_adv_reg & NWAY_AR_PAUSE) &&
+		         (mii_nway_adv_reg & NWAY_AR_ASM_DIR) &&
+		         !(mii_nway_lp_ability_reg & NWAY_LPAR_PAUSE) &&
+		         (mii_nway_lp_ability_reg & NWAY_LPAR_ASM_DIR)) {
+			mac->fc = e1000_fc_rx_pause;
+			DEBUGOUT("Flow Control = RX PAUSE frames only.\r\n");
+		}
+		/* Per the IEEE spec, at this point flow control should be
+		 * disabled.  However, we want to consider that we could
+		 * be connected to a legacy switch that doesn't advertise
+		 * desired flow control, but can be forced on the link
+		 * partner.  So if we advertised no flow control, that is
+		 * what we will resolve to.  If we advertised some kind of
+		 * receive capability (Rx Pause Only or Full Flow Control)
+		 * and the link partner advertised none, we will configure
+		 * ourselves to enable Rx Flow Control only.  We can do
+		 * this safely for two reasons:  If the link partner really
+		 * didn't want flow control enabled, and we enable Rx, no
+		 * harm done since we won't be receiving any PAUSE frames
+		 * anyway.  If the intent on the link partner was to have
+		 * flow control enabled, then by us enabling RX only, we
+		 * can at least receive pause frames and process them.
+		 * This is a good idea because in most cases, since we are
+		 * predominantly a server NIC, more times than not we will
+		 * be asked to delay transmission of packets than asking
+		 * our link partner to pause transmission of frames.
+		 */
+		else if ((mac->original_fc == e1000_fc_none ||
+		          mac->original_fc == e1000_fc_tx_pause) ||
+		         mac->fc_strict_ieee) {
+			mac->fc = e1000_fc_none;
+			DEBUGOUT("Flow Control = NONE.\r\n");
+		} else {
+			mac->fc = e1000_fc_rx_pause;
+			DEBUGOUT("Flow Control = RX PAUSE frames only.\r\n");
+		}
+
+		/* Now we need to do one last check...  If we auto-
+		 * negotiated to HALF DUPLEX, flow control should not be
+		 * enabled per IEEE 802.3 spec.
+		 */
+		ret_val = e1000_get_speed_and_duplex(hw, &speed, &duplex);
+		if (ret_val) {
+			DEBUGOUT("Error getting link speed and duplex\n");
+			goto out;
+		}
+
+		if (duplex == HALF_DUPLEX)
+			mac->fc = e1000_fc_none;
+
+		/* Now we call a subroutine to actually force the MAC
+		 * controller to use the correct flow control settings.
+		 */
+		ret_val = e1000_force_mac_fc_generic(hw);
+		if (ret_val) {
+			DEBUGOUT("Error forcing flow control settings\n");
+			goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_speed_and_duplex_copper_generic - Retreive current speed/duplex
+ *  @hw: pointer to the HW structure
+ *  @speed: stores the current speed
+ *  @duplex: stores the current duplex
+ *
+ *  Read the status register for the current speed/duplex and store the current
+ *  speed and duplex for copper connections.
+ **/
+s32 e1000_get_speed_and_duplex_copper_generic(struct e1000_hw *hw, u16 *speed,
+                                              u16 *duplex)
+{
+	u32 status;
+
+	DEBUGFUNC("e1000_get_speed_and_duplex_copper_generic");
+
+	status = E1000_READ_REG(hw, E1000_STATUS);
+	if (status & E1000_STATUS_SPEED_1000) {
+		*speed = SPEED_1000;
+		DEBUGOUT("1000 Mbs, ");
+	} else if (status & E1000_STATUS_SPEED_100) {
+		*speed = SPEED_100;
+		DEBUGOUT("100 Mbs, ");
+	} else {
+		*speed = SPEED_10;
+		DEBUGOUT("10 Mbs, ");
+	}
+
+	if (status & E1000_STATUS_FD) {
+		*duplex = FULL_DUPLEX;
+		DEBUGOUT("Full Duplex\n");
+	} else {
+		*duplex = HALF_DUPLEX;
+		DEBUGOUT("Half Duplex\n");
+	}
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_get_speed_and_duplex_fiber_generic - Retreive current speed/duplex
+ *  @hw: pointer to the HW structure
+ *  @speed: stores the current speed
+ *  @duplex: stores the current duplex
+ *
+ *  Sets the speed and duplex to gigabit full duplex (the only possible option)
+ *  for fiber/serdes links.
+ **/
+s32 e1000_get_speed_and_duplex_fiber_serdes_generic(struct e1000_hw *hw,
+                                                    u16 *speed, u16 *duplex)
+{
+	DEBUGFUNC("e1000_get_speed_and_duplex_fiber_serdes_generic");
+
+	*speed = SPEED_1000;
+	*duplex = FULL_DUPLEX;
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_get_hw_semaphore_generic - Acquire hardware semaphore
+ *  @hw: pointer to the HW structure
+ *
+ *  Acquire the HW semaphore to access the PHY or NVM
+ **/
+s32 e1000_get_hw_semaphore_generic(struct e1000_hw *hw)
+{
+	u32 swsm;
+	s32 ret_val = E1000_SUCCESS;
+	s32 timeout = hw->nvm.word_size + 1;
+	s32 i = 0;
+
+	DEBUGFUNC("e1000_get_hw_semaphore_generic");
+
+	/* Get the SW semaphore */
+	while (i < timeout) {
+		swsm = E1000_READ_REG(hw, E1000_SWSM);
+		if (!(swsm & E1000_SWSM_SMBI))
+			break;
+
+		usec_delay(50);
+		i++;
+	}
+
+	if (i == timeout) {
+		DEBUGOUT("Driver can't access device - SMBI bit is set.\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	/* Get the FW semaphore. */
+	for (i = 0; i < timeout; i++) {
+		swsm = E1000_READ_REG(hw, E1000_SWSM);
+		E1000_WRITE_REG(hw, E1000_SWSM, swsm | E1000_SWSM_SWESMBI);
+
+		/* Semaphore acquired if bit latched */
+		if (E1000_READ_REG(hw, E1000_SWSM) & E1000_SWSM_SWESMBI)
+			break;
+
+		usec_delay(50);
+	}
+
+	if (i == timeout) {
+		/* Release semaphores */
+		e1000_put_hw_semaphore_generic(hw);
+		DEBUGOUT("Driver can't access the NVM\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_put_hw_semaphore_generic - Release hardware semaphore
+ *  @hw: pointer to the HW structure
+ *
+ *  Release hardware semaphore used to access the PHY or NVM
+ **/
+void e1000_put_hw_semaphore_generic(struct e1000_hw *hw)
+{
+	u32 swsm;
+
+	DEBUGFUNC("e1000_put_hw_semaphore_generic");
+
+	swsm = E1000_READ_REG(hw, E1000_SWSM);
+
+	swsm &= ~(E1000_SWSM_SMBI | E1000_SWSM_SWESMBI);
+
+	E1000_WRITE_REG(hw, E1000_SWSM, swsm);
+}
+
+/**
+ *  e1000_get_auto_rd_done_generic - Check for auto read completion
+ *  @hw: pointer to the HW structure
+ *
+ *  Check EEPROM for Auto Read done bit.
+ **/
+s32 e1000_get_auto_rd_done_generic(struct e1000_hw *hw)
+{
+	s32 i = 0;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_get_auto_rd_done_generic");
+
+	while (i < AUTO_READ_DONE_TIMEOUT) {
+		if (E1000_READ_REG(hw, E1000_EECD) & E1000_EECD_AUTO_RD)
+			break;
+		msec_delay(1);
+		i++;
+	}
+
+	if (i == AUTO_READ_DONE_TIMEOUT) {
+		DEBUGOUT("Auto read by HW from NVM has not completed.\n");
+		ret_val = -E1000_ERR_RESET;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_valid_led_default_generic - Verify a valid default LED config
+ *  @hw: pointer to the HW structure
+ *  @data: pointer to the NVM (EEPROM)
+ *
+ *  Read the EEPROM for the current default LED configuration.  If the
+ *  LED configuration is not valid, set to a valid LED configuration.
+ **/
+s32 e1000_valid_led_default_generic(struct e1000_hw *hw, u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_valid_led_default_generic");
+
+	ret_val = e1000_read_nvm(hw, NVM_ID_LED_SETTINGS, 1, data);
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+
+	if (*data == ID_LED_RESERVED_0000 || *data == ID_LED_RESERVED_FFFF)
+		*data = ID_LED_DEFAULT;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_id_led_init_generic -
+ *  @hw: pointer to the HW structure
+ *
+ **/
+s32 e1000_id_led_init_generic(struct e1000_hw * hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	s32 ret_val;
+	const u32 ledctl_mask = 0x000000FF;
+	const u32 ledctl_on = E1000_LEDCTL_MODE_LED_ON;
+	const u32 ledctl_off = E1000_LEDCTL_MODE_LED_OFF;
+	u16 data, i, temp;
+	const u16 led_mask = 0x0F;
+
+	DEBUGFUNC("e1000_id_led_init_generic");
+
+	ret_val = hw->func.valid_led_default(hw, &data);
+	if (ret_val)
+		goto out;
+
+	mac->ledctl_default = E1000_READ_REG(hw, E1000_LEDCTL);
+	mac->ledctl_mode1 = mac->ledctl_default;
+	mac->ledctl_mode2 = mac->ledctl_default;
+
+	for (i = 0; i < 4; i++) {
+		temp = (data >> (i << 2)) & led_mask;
+		switch (temp) {
+		case ID_LED_ON1_DEF2:
+		case ID_LED_ON1_ON2:
+		case ID_LED_ON1_OFF2:
+			mac->ledctl_mode1 &= ~(ledctl_mask << (i << 3));
+			mac->ledctl_mode1 |= ledctl_on << (i << 3);
+			break;
+		case ID_LED_OFF1_DEF2:
+		case ID_LED_OFF1_ON2:
+		case ID_LED_OFF1_OFF2:
+			mac->ledctl_mode1 &= ~(ledctl_mask << (i << 3));
+			mac->ledctl_mode1 |= ledctl_off << (i << 3);
+			break;
+		default:
+			/* Do nothing */
+			break;
+		}
+		switch (temp) {
+		case ID_LED_DEF1_ON2:
+		case ID_LED_ON1_ON2:
+		case ID_LED_OFF1_ON2:
+			mac->ledctl_mode2 &= ~(ledctl_mask << (i << 3));
+			mac->ledctl_mode2 |= ledctl_on << (i << 3);
+			break;
+		case ID_LED_DEF1_OFF2:
+		case ID_LED_ON1_OFF2:
+		case ID_LED_OFF1_OFF2:
+			mac->ledctl_mode2 &= ~(ledctl_mask << (i << 3));
+			mac->ledctl_mode2 |= ledctl_off << (i << 3);
+			break;
+		default:
+			/* Do nothing */
+			break;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_led_generic - Configures SW controllable LED
+ *  @hw: pointer to the HW structure
+ *
+ *  This prepares the SW controllable LED for use and saves the current state
+ *  of the LED so it can be later restored.
+ **/
+s32 e1000_setup_led_generic(struct e1000_hw *hw)
+{
+	u32 ledctl;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_setup_led_generic");
+
+	if (hw->func.setup_led != e1000_setup_led_generic) {
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	if (hw->media_type == e1000_media_type_fiber) {
+		ledctl = E1000_READ_REG(hw, E1000_LEDCTL);
+		hw->mac.ledctl_default = ledctl;
+		/* Turn off LED0 */
+		ledctl &= ~(E1000_LEDCTL_LED0_IVRT |
+		            E1000_LEDCTL_LED0_BLINK |
+		            E1000_LEDCTL_LED0_MODE_MASK);
+		ledctl |= (E1000_LEDCTL_MODE_LED_OFF <<
+		           E1000_LEDCTL_LED0_MODE_SHIFT);
+		E1000_WRITE_REG(hw, E1000_LEDCTL, ledctl);
+	} else if (hw->media_type == e1000_media_type_copper) {
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode1);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_cleanup_led_generic - Set LED config to default operation
+ *  @hw: pointer to the HW structure
+ *
+ *  Remove the current LED configuration and set the LED configuration
+ *  to the default value, saved from the EEPROM.
+ **/
+s32 e1000_cleanup_led_generic(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_cleanup_led_generic");
+
+	if (hw->func.cleanup_led != e1000_cleanup_led_generic) {
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_default);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_blink_led_generic - Blink LED
+ *  @hw: pointer to the HW structure
+ *
+ *  Blink the led's which are set to be on.
+ **/
+s32 e1000_blink_led_generic(struct e1000_hw *hw)
+{
+	u32 ledctl_blink = 0;
+	u32 i;
+
+	DEBUGFUNC("e1000_blink_led_generic");
+
+	if (hw->media_type == e1000_media_type_fiber) {
+		/* always blink LED0 for PCI-E fiber */
+		ledctl_blink = E1000_LEDCTL_LED0_BLINK |
+		     (E1000_LEDCTL_MODE_LED_ON << E1000_LEDCTL_LED0_MODE_SHIFT);
+	} else {
+		/* set the blink bit for each LED that's "on" (0x0E)
+		 * in ledctl_mode2 */
+		ledctl_blink = hw->mac.ledctl_mode2;
+		for (i = 0; i < 4; i++)
+			if (((hw->mac.ledctl_mode2 >> (i * 8)) & 0xFF) ==
+			    E1000_LEDCTL_MODE_LED_ON)
+				ledctl_blink |= (E1000_LEDCTL_LED0_BLINK <<
+				                 (i * 8));
+	}
+
+	E1000_WRITE_REG(hw, E1000_LEDCTL, ledctl_blink);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_on_generic - Turn LED on
+ *  @hw: pointer to the HW structure
+ *
+ *  Turn LED on.
+ **/
+s32 e1000_led_on_generic(struct e1000_hw *hw)
+{
+	u32 ctrl;
+
+	DEBUGFUNC("e1000_led_on_generic");
+
+	switch (hw->media_type) {
+	case e1000_media_type_fiber:
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl &= ~E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+		break;
+	case e1000_media_type_copper:
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode2);
+		break;
+	default:
+		break;
+	}
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_led_off_generic - Turn LED off
+ *  @hw: pointer to the HW structure
+ *
+ *  Turn LED off.
+ **/
+s32 e1000_led_off_generic(struct e1000_hw *hw)
+{
+	u32 ctrl;
+
+	DEBUGFUNC("e1000_led_off_generic");
+
+	switch (hw->media_type) {
+	case e1000_media_type_fiber:
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= E1000_CTRL_SWDPIN0;
+		ctrl |= E1000_CTRL_SWDPIO0;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+		break;
+	case e1000_media_type_copper:
+		E1000_WRITE_REG(hw, E1000_LEDCTL, hw->mac.ledctl_mode1);
+		break;
+	default:
+		break;
+	}
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_set_pcie_no_snoop_generic - Set PCI-express capabilities
+ *  @hw: pointer to the HW structure
+ *  @no_snoop: bitmap of snoop events
+ *
+ *  Set the PCI-express register to snoop for events enabled in 'no_snoop'.
+ **/
+void e1000_set_pcie_no_snoop_generic(struct e1000_hw *hw, u32 no_snoop)
+{
+	u32 gcr;
+
+	DEBUGFUNC("e1000_set_pcie_no_snoop_generic");
+
+	if (hw->bus.type != e1000_bus_type_pci_express)
+		goto out;
+
+	if (no_snoop) {
+		gcr = E1000_READ_REG(hw, E1000_GCR);
+		gcr &= ~(PCIE_NO_SNOOP_ALL);
+		gcr |= no_snoop;
+		E1000_WRITE_REG(hw, E1000_GCR, gcr);
+	}
+out:
+	return;
+}
+
+/**
+ *  e1000_disable_pcie_master_generic - Disables PCI-express master access
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns 0 (E1000_SUCCESS) if successful, else returns -10
+ *  (-E1000_ERR_MASTER_REQUESTS_PENDING) if master disable bit has not casued
+ *  the master requests to be disabled.
+ *
+ *  Disables PCI-Express master access and verifies there are no pending
+ *  requests.
+ **/
+s32 e1000_disable_pcie_master_generic(struct e1000_hw *hw)
+{
+	u32 ctrl;
+	s32 timeout = MASTER_DISABLE_TIMEOUT;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_disable_pcie_master_generic");
+
+	if (hw->bus.type != e1000_bus_type_pci_express)
+		goto out;
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= E1000_CTRL_GIO_MASTER_DISABLE;
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+	while (timeout) {
+		if (!(E1000_READ_REG(hw, E1000_STATUS) &
+		      E1000_STATUS_GIO_MASTER_ENABLE))
+			break;
+		usec_delay(100);
+		timeout--;
+	}
+
+	if (!timeout) {
+		DEBUGOUT("Master requests are pending.\n");
+		ret_val = -E1000_ERR_MASTER_REQUESTS_PENDING;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_reset_adaptive_generic - Reset Adaptive Interframe Spacing
+ *  @hw: pointer to the HW structure
+ *
+ *  Reset the Adaptive Interframe Spacing throttle to default values.
+ **/
+void e1000_reset_adaptive_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+
+	DEBUGFUNC("e1000_reset_adaptive_generic");
+
+	if (!mac->adaptive_ifs) {
+		DEBUGOUT("Not in Adaptive IFS mode!\n");
+		goto out;
+	}
+
+	if (!mac->ifs_params_forced) {
+		mac->current_ifs_val = 0;
+		mac->ifs_min_val = IFS_MIN;
+		mac->ifs_max_val = IFS_MAX;
+		mac->ifs_step_size = IFS_STEP;
+		mac->ifs_ratio = IFS_RATIO;
+	}
+
+	mac->in_ifs_mode = FALSE;
+	E1000_WRITE_REG(hw, E1000_AIT, 0);
+out:
+	return;
+}
+
+/**
+ *  e1000_update_adaptive_generic - Update Adaptive Interframe Spacing
+ *  @hw: pointer to the HW structure
+ *
+ *  Update the Adaptive Interframe Spacing Throttle value based on the
+ *  time between transmitted packets and time between collisions.
+ **/
+void e1000_update_adaptive_generic(struct e1000_hw *hw)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+
+	DEBUGFUNC("e1000_update_adaptive_generic");
+
+	if (!mac->adaptive_ifs) {
+		DEBUGOUT("Not in Adaptive IFS mode!\n");
+		goto out;
+	}
+
+	if ((mac->collision_delta * mac->ifs_ratio) > mac->tx_packet_delta) {
+		if (mac->tx_packet_delta > MIN_NUM_XMITS) {
+			mac->in_ifs_mode = TRUE;
+			if (mac->current_ifs_val < mac->ifs_max_val) {
+				if (!mac->current_ifs_val)
+					mac->current_ifs_val = mac->ifs_min_val;
+				else
+					mac->current_ifs_val +=
+						mac->ifs_step_size;
+				E1000_WRITE_REG(hw, E1000_AIT, mac->current_ifs_val);
+			}
+		}
+	} else {
+		if (mac->in_ifs_mode &&
+		    (mac->tx_packet_delta <= MIN_NUM_XMITS)) {
+			mac->current_ifs_val = 0;
+			mac->in_ifs_mode = FALSE;
+			E1000_WRITE_REG(hw, E1000_AIT, 0);
+		}
+	}
+out:
+	return;
+}
+
+/**
+ *  e1000_validate_mdi_setting_generic - Verify MDI/MDIx settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Verify that when not using auto-negotitation that MDI/MDIx is correctly
+ *  set, which is forced to MDI mode only.
+ **/
+s32 e1000_validate_mdi_setting_generic(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_validate_mdi_setting_generic");
+
+	if (!hw->mac.autoneg && (hw->phy.mdix == 0 || hw->phy.mdix == 3)) {
+		DEBUGOUT("Invalid MDI setting detected\n");
+		hw->phy.mdix = 1;
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_mac.h linux-2.6.9/drivers/net/e1000/e1000_mac.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_mac.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_mac.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,82 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_MAC_H_
+#define _E1000_MAC_H_
+
+/* Functions that should not be called directly from drivers but can be used
+ * by other files in this 'shared code'
+ */
+s32  e1000_blink_led_generic(struct e1000_hw *hw);
+s32  e1000_check_for_copper_link_generic(struct e1000_hw *hw);
+s32  e1000_check_for_fiber_link_generic(struct e1000_hw *hw);
+s32  e1000_check_for_serdes_link_generic(struct e1000_hw *hw);
+s32  e1000_cleanup_led_generic(struct e1000_hw *hw);
+s32  e1000_commit_fc_settings_generic(struct e1000_hw *hw);
+s32  e1000_config_fc_after_link_up_generic(struct e1000_hw *hw);
+s32  e1000_disable_pcie_master_generic(struct e1000_hw *hw);
+s32  e1000_force_mac_fc_generic(struct e1000_hw *hw);
+s32  e1000_get_auto_rd_done_generic(struct e1000_hw *hw);
+s32  e1000_get_bus_info_pci_generic(struct e1000_hw *hw);
+s32  e1000_get_bus_info_pcie_generic(struct e1000_hw *hw);
+s32  e1000_get_hw_semaphore_generic(struct e1000_hw *hw);
+s32  e1000_get_speed_and_duplex_copper_generic(struct e1000_hw *hw, u16 *speed,
+                                               u16 *duplex);
+s32  e1000_get_speed_and_duplex_fiber_serdes_generic(struct e1000_hw *hw,
+                                                     u16 *speed, u16 *duplex);
+s32  e1000_id_led_init_generic(struct e1000_hw *hw);
+s32  e1000_led_on_generic(struct e1000_hw *hw);
+s32  e1000_led_off_generic(struct e1000_hw *hw);
+void e1000_mc_addr_list_update_generic(struct e1000_hw *hw,
+	                               u8 *mc_addr_list, u32 mc_addr_count,
+	                               u32 rar_used_count, u32 rar_count);
+s32  e1000_poll_fiber_serdes_link_generic(struct e1000_hw *hw);
+s32  e1000_set_default_fc_generic(struct e1000_hw *hw);
+s32  e1000_set_fc_watermarks_generic(struct e1000_hw *hw);
+s32  e1000_setup_fiber_serdes_link_generic(struct e1000_hw *hw);
+s32  e1000_setup_led_generic(struct e1000_hw *hw);
+s32  e1000_setup_link_generic(struct e1000_hw *hw);
+s32  e1000_validate_mdi_setting_generic(struct e1000_hw *hw);
+
+u32  e1000_hash_mc_addr_generic(struct e1000_hw *hw, u8 *mc_addr);
+
+void e1000_clear_hw_cntrs_base_generic(struct e1000_hw *hw);
+void e1000_clear_vfta_generic(struct e1000_hw *hw);
+void e1000_config_collision_dist_generic(struct e1000_hw *hw);
+void e1000_init_rx_addrs_generic(struct e1000_hw *hw, u16 rar_count);
+void e1000_mta_set_generic(struct e1000_hw *hw, u32 hash_value);
+void e1000_pcix_mmrbc_workaround_generic(struct e1000_hw *hw);
+void e1000_put_hw_semaphore_generic(struct e1000_hw *hw);
+void e1000_rar_set_generic(struct e1000_hw *hw, u8 *addr, u32 index);
+void e1000_remove_device_generic(struct e1000_hw *hw);
+void e1000_reset_adaptive_generic(struct e1000_hw *hw);
+void e1000_set_pcie_no_snoop_generic(struct e1000_hw *hw, u32 no_snoop);
+void e1000_update_adaptive_generic(struct e1000_hw *hw);
+void e1000_write_vfta_generic(struct e1000_hw *hw, u32 offset, u32 value);
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_main.c linux-2.6.9/drivers/net/e1000/e1000_main.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_main.c	2004-10-18 23:53:50.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_main.c	2007-07-16 13:33:15.000000000 +0200
@@ -1,55 +1,75 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
 
-#include "e1000.h"
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <linux/pagemap.h>
+#include <linux/netdevice.h>
+#include <linux/tcp.h>
+#include <linux/ipv6.h>
+#ifdef NETIF_F_TSO
+#include <net/checksum.h>
+#ifdef NETIF_F_TSO6
+#include <net/ip6_checksum.h>
+#endif
+#endif
+#ifdef SIOCGMIIPHY
+#include <linux/mii.h>
+#endif
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#ifdef NETIF_F_HW_VLAN_TX
+#include <linux/if_vlan.h>
+#endif
+#ifdef CONFIG_E1000_MQ
+#include <linux/cpu.h>
+#include <linux/smp.h>
+#endif
 
-/* Change Log
- * 5.3.12	6/7/04
- * - kcompat NETIF_MSG for older kernels (2.4.9) <sean.p.mcdermott@intel.com>
- * - if_mii support and associated kcompat for older kernels
- * - More errlogging support from Jon Mason <jonmason@us.ibm.com>
- * - Fix TSO issues on PPC64 machines -- Jon Mason <jonmason@us.ibm.com>
- *
- * 5.3.11	6/4/04
- * - ethtool register dump reads MANC register conditionally.
- *
- * 5.3.10	6/1/04
- */
+#include "e1000.h"
 
 char e1000_driver_name[] = "e1000";
-char e1000_driver_string[] = "Intel(R) PRO/1000 Network Driver";
-#ifndef CONFIG_E1000_NAPI
-#define DRIVERNAPI
+static char e1000_driver_string[] = "Intel(R) PRO/1000 Network Driver";
+
+#ifdef CONFIG_E1000_NAPI
+#define DRV_NAPI "-NAPI"
 #else
-#define DRIVERNAPI "-NAPI"
+#define DRV_NAPI
 #endif
-char e1000_driver_version[] = "5.3.19-k2"DRIVERNAPI;
-char e1000_copyright[] = "Copyright (c) 1999-2004 Intel Corporation.";
+
+#define DRV_DEBUG
+
+#define DRV_VERSION "7.6.5" DRV_NAPI DRV_DEBUG
+char e1000_driver_version[] = DRV_VERSION;
+static char e1000_copyright[] = "Copyright (c) 1999-2007 Intel Corporation.";
 
 /* e1000_pci_tbl - PCI Device ID Table
  *
@@ -59,37 +79,71 @@
  *   {PCI_DEVICE(PCI_VENDOR_ID_INTEL, device_id)}
  */
 static struct pci_device_id e1000_pci_tbl[] = {
-	INTEL_E1000_ETHERNET_DEVICE(0x1000),
-	INTEL_E1000_ETHERNET_DEVICE(0x1001),
-	INTEL_E1000_ETHERNET_DEVICE(0x1004),
-	INTEL_E1000_ETHERNET_DEVICE(0x1008),
-	INTEL_E1000_ETHERNET_DEVICE(0x1009),
-	INTEL_E1000_ETHERNET_DEVICE(0x100C),
-	INTEL_E1000_ETHERNET_DEVICE(0x100D),
-	INTEL_E1000_ETHERNET_DEVICE(0x100E),
-	INTEL_E1000_ETHERNET_DEVICE(0x100F),
-	INTEL_E1000_ETHERNET_DEVICE(0x1010),
-	INTEL_E1000_ETHERNET_DEVICE(0x1011),
-	INTEL_E1000_ETHERNET_DEVICE(0x1012),
-	INTEL_E1000_ETHERNET_DEVICE(0x1013),
-	INTEL_E1000_ETHERNET_DEVICE(0x1015),
-	INTEL_E1000_ETHERNET_DEVICE(0x1016),
-	INTEL_E1000_ETHERNET_DEVICE(0x1017),
-	INTEL_E1000_ETHERNET_DEVICE(0x1018),
-	INTEL_E1000_ETHERNET_DEVICE(0x1019),
-	INTEL_E1000_ETHERNET_DEVICE(0x101D),
-	INTEL_E1000_ETHERNET_DEVICE(0x101E),
-	INTEL_E1000_ETHERNET_DEVICE(0x1026),
-	INTEL_E1000_ETHERNET_DEVICE(0x1027),
-	INTEL_E1000_ETHERNET_DEVICE(0x1028),
-	INTEL_E1000_ETHERNET_DEVICE(0x1075),
-	INTEL_E1000_ETHERNET_DEVICE(0x1076),
-	INTEL_E1000_ETHERNET_DEVICE(0x1077),
-	INTEL_E1000_ETHERNET_DEVICE(0x1078),
-	INTEL_E1000_ETHERNET_DEVICE(0x1079),
-	INTEL_E1000_ETHERNET_DEVICE(0x107A),
-	INTEL_E1000_ETHERNET_DEVICE(0x107B),
-	INTEL_E1000_ETHERNET_DEVICE(0x107C),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82542),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82543GC_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82543GC_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82544EI_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82544EI_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82544GC_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82544GC_LOM),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82540EM),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82545EM_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546EB_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82545EM_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546EB_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541EI),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541ER_LOM),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82540EM_LOM),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82540EP_LOM),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82540EP),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541EI_MOBILE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82547EI),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82547EI_MOBILE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546EB_QUAD_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82540EP_LP),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82545GM_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82545GM_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82545GM_SERDES),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IGP_M_AMT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IGP_AMT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IGP_C),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IFE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IGP_M),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_SERDES),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82547GI),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541GI),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541GI_MOBILE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541ER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_SERDES),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82541GI_LF),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82572EI_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82572EI_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82572EI_SERDES),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_PCIE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82573E),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82573E_IAMT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_80003ES2LAN_COPPER_DPT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_80003ES2LAN_SERDES_DPT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_QUAD_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82573L),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_QUAD_COPPER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_QUAD_FIBER),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82572EI),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_80003ES2LAN_COPPER_SPT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_80003ES2LAN_SERDES_SPT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_82571EB_QUAD_COPPER_LP),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IFE_GT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH8_IFE_G),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH9_IGP_AMT),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH9_IGP_C),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH9_IFE),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH9_IFE_G),
+	INTEL_E1000_ETHERNET_DEVICE(E1000_DEV_ID_ICH9_IFE_GT),
 	/* required last entry */
 	{0,}
 };
@@ -98,104 +152,181 @@
 
 int e1000_up(struct e1000_adapter *adapter);
 void e1000_down(struct e1000_adapter *adapter);
+void e1000_reinit_locked(struct e1000_adapter *adapter);
 void e1000_reset(struct e1000_adapter *adapter);
-int e1000_set_spd_dplx(struct e1000_adapter *adapter, uint16_t spddplx);
-int e1000_setup_tx_resources(struct e1000_adapter *adapter);
-int e1000_setup_rx_resources(struct e1000_adapter *adapter);
-void e1000_free_tx_resources(struct e1000_adapter *adapter);
-void e1000_free_rx_resources(struct e1000_adapter *adapter);
+int e1000_set_spd_dplx(struct e1000_adapter *adapter, u16 spddplx);
+int e1000_setup_all_tx_resources(struct e1000_adapter *adapter);
+int e1000_setup_all_rx_resources(struct e1000_adapter *adapter);
+void e1000_free_all_tx_resources(struct e1000_adapter *adapter);
+void e1000_free_all_rx_resources(struct e1000_adapter *adapter);
+static int e1000_setup_tx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring);
+static int e1000_setup_rx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring);
+static void e1000_free_tx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring);
+static void e1000_free_rx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring);
 void e1000_update_stats(struct e1000_adapter *adapter);
 
-/* Local Function Prototypes */
-
 static int e1000_init_module(void);
 static void e1000_exit_module(void);
 static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent);
 static void __devexit e1000_remove(struct pci_dev *pdev);
+static int e1000_alloc_queues(struct e1000_adapter *adapter);
+#ifdef CONFIG_E1000_MQ
+static void e1000_setup_queue_mapping(struct e1000_adapter *adapter);
+#endif
 static int e1000_sw_init(struct e1000_adapter *adapter);
 static int e1000_open(struct net_device *netdev);
 static int e1000_close(struct net_device *netdev);
+static void e1000_configure(struct e1000_adapter *adapter);
 static void e1000_configure_tx(struct e1000_adapter *adapter);
 static void e1000_configure_rx(struct e1000_adapter *adapter);
 static void e1000_setup_rctl(struct e1000_adapter *adapter);
-static void e1000_clean_tx_ring(struct e1000_adapter *adapter);
-static void e1000_clean_rx_ring(struct e1000_adapter *adapter);
+static void e1000_clean_all_tx_rings(struct e1000_adapter *adapter);
+static void e1000_clean_all_rx_rings(struct e1000_adapter *adapter);
+static void e1000_clean_tx_ring(struct e1000_adapter *adapter,
+                                struct e1000_tx_ring *tx_ring);
+static void e1000_clean_rx_ring(struct e1000_adapter *adapter,
+                                struct e1000_rx_ring *rx_ring);
 static void e1000_set_multi(struct net_device *netdev);
 static void e1000_update_phy_info(unsigned long data);
 static void e1000_watchdog(unsigned long data);
+static void e1000_watchdog_task(struct work_struct *work);
 static void e1000_82547_tx_fifo_stall(unsigned long data);
+static int e1000_xmit_frame_ring(struct sk_buff *skb, struct net_device *netdev,
+                                 struct e1000_tx_ring *tx_ring);
 static int e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev);
+#ifdef CONFIG_E1000_MQ
+static int e1000_subqueue_xmit_frame(struct sk_buff *skb,
+                                     struct net_device *netdev, int queue);
+#endif
 static struct net_device_stats * e1000_get_stats(struct net_device *netdev);
 static int e1000_change_mtu(struct net_device *netdev, int new_mtu);
 static int e1000_set_mac(struct net_device *netdev, void *p);
-static void e1000_irq_disable(struct e1000_adapter *adapter);
-static void e1000_irq_enable(struct e1000_adapter *adapter);
-static irqreturn_t e1000_intr(int irq, void *data, struct pt_regs *regs);
-static boolean_t e1000_clean_tx_irq(struct e1000_adapter *adapter);
+static irqreturn_t e1000_intr(int irq, void *data);
+#ifdef CONFIG_PCI_MSI
+static irqreturn_t e1000_intr_msi(int irq, void *data);
+#endif
+static boolean_t e1000_clean_tx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring);
 #ifdef CONFIG_E1000_NAPI
-static int e1000_clean(struct net_device *netdev, int *budget);
+static int e1000_clean(struct net_device *poll_dev, int *budget);
 static boolean_t e1000_clean_rx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring,
                                     int *work_done, int work_to_do);
+static boolean_t e1000_clean_rx_irq_ps(struct e1000_adapter *adapter,
+                                       struct e1000_rx_ring *rx_ring,
+                                       int *work_done, int work_to_do);
+static boolean_t e1000_clean_jumbo_rx_irq(struct e1000_adapter *adapter,
+                                          struct e1000_rx_ring *rx_ring,
+                                          int *work_done, int work_to_do);
+static void e1000_alloc_jumbo_rx_buffers(struct e1000_adapter *adapter,
+                                         struct e1000_rx_ring *rx_ring,
+                                         int cleaned_count);
 #else
-static boolean_t e1000_clean_rx_irq(struct e1000_adapter *adapter);
+static boolean_t e1000_clean_rx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring);
+static boolean_t e1000_clean_rx_irq_ps(struct e1000_adapter *adapter,
+                                       struct e1000_rx_ring *rx_ring);
 #endif
-static void e1000_alloc_rx_buffers(struct e1000_adapter *adapter);
+static void e1000_alloc_rx_buffers(struct e1000_adapter *adapter,
+                                   struct e1000_rx_ring *rx_ring,
+                                   int cleaned_count);
+static void e1000_alloc_rx_buffers_ps(struct e1000_adapter *adapter,
+                                      struct e1000_rx_ring *rx_ring,
+                                      int cleaned_count);
 static int e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd);
+#ifdef SIOCGMIIPHY
 static int e1000_mii_ioctl(struct net_device *netdev, struct ifreq *ifr,
-			   int cmd);
-void set_ethtool_ops(struct net_device *netdev);
+                           int cmd);
+#endif
+void e1000_set_ethtool_ops(struct net_device *netdev);
+#ifdef ETHTOOL_OPS_COMPAT
+extern int ethtool_ioctl(struct ifreq *ifr);
+#endif
 static void e1000_enter_82542_rst(struct e1000_adapter *adapter);
 static void e1000_leave_82542_rst(struct e1000_adapter *adapter);
-static void e1000_rx_checksum(struct e1000_adapter *adapter,
-				struct e1000_rx_desc *rx_desc,
-				struct sk_buff *skb);
 static void e1000_tx_timeout(struct net_device *dev);
-static void e1000_tx_timeout_task(struct net_device *dev);
+static void e1000_reset_task(struct work_struct *work);
 static void e1000_smartspeed(struct e1000_adapter *adapter);
-static inline int e1000_82547_fifo_workaround(struct e1000_adapter *adapter,
-					      struct sk_buff *skb);
+static int e1000_82547_fifo_workaround(struct e1000_adapter *adapter,
+                                       struct sk_buff *skb);
 
-static void e1000_vlan_rx_register(struct net_device *netdev, struct vlan_group *grp);
-static void e1000_vlan_rx_add_vid(struct net_device *netdev, uint16_t vid);
-static void e1000_vlan_rx_kill_vid(struct net_device *netdev, uint16_t vid);
+#ifdef NETIF_F_HW_VLAN_TX
+static void e1000_vlan_rx_register(struct net_device *netdev,
+                                   struct vlan_group *grp);
+static void e1000_vlan_rx_add_vid(struct net_device *netdev, u16 vid);
+static void e1000_vlan_rx_kill_vid(struct net_device *netdev, u16 vid);
 static void e1000_restore_vlan(struct e1000_adapter *adapter);
+#endif
 
-static int e1000_notify_reboot(struct notifier_block *, unsigned long event, void *ptr);
-static int e1000_suspend(struct pci_dev *pdev, uint32_t state);
+static int e1000_suspend(struct pci_dev *pdev, pm_message_t state);
 #ifdef CONFIG_PM
 static int e1000_resume(struct pci_dev *pdev);
 #endif
-
-#ifdef CONFIG_NET_POLL_CONTROLLER
-/* for netdump / net console */
-static void e1000_netpoll (struct net_device *netdev);
-#endif
-
-struct notifier_block e1000_notifier_reboot = {
+#ifndef USE_REBOOT_NOTIFIER
+static void e1000_shutdown(struct pci_dev *pdev);
+#else
+static int e1000_notify_reboot(struct notifier_block *, unsigned long event,
+                               void *ptr);
+static struct notifier_block e1000_notifier_reboot = {
 	.notifier_call	= e1000_notify_reboot,
 	.next		= NULL,
 	.priority	= 0
 };
+#endif
 
-/* Exported from other modules */
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/* for netdump / net console */
+static void e1000_netpoll (struct net_device *netdev);
+#endif
 
 extern void e1000_check_options(struct e1000_adapter *adapter);
 
+#define COPYBREAK_DEFAULT 256
+static unsigned int copybreak __read_mostly = COPYBREAK_DEFAULT;
+module_param(copybreak, uint, 0644);
+MODULE_PARM_DESC(copybreak,
+	"Maximum size of packet that is copied to a new buffer on receive");
+
+
+#ifdef CONFIG_E1000_PCI_ERS
+static pci_ers_result_t e1000_io_error_detected(struct pci_dev *pdev,
+                     pci_channel_state_t state);
+static pci_ers_result_t e1000_io_slot_reset(struct pci_dev *pdev);
+static void e1000_io_resume(struct pci_dev *pdev);
+
+static struct pci_error_handlers e1000_err_handler = {
+	.error_detected = e1000_io_error_detected,
+	.slot_reset = e1000_io_slot_reset,
+	.resume = e1000_io_resume,
+};
+#endif
+
 static struct pci_driver e1000_driver = {
 	.name     = e1000_driver_name,
 	.id_table = e1000_pci_tbl,
 	.probe    = e1000_probe,
 	.remove   = __devexit_p(e1000_remove),
-	/* Power Managment Hooks */
 #ifdef CONFIG_PM
+	/* Power Managment Hooks */
 	.suspend  = e1000_suspend,
-	.resume   = e1000_resume
+	.resume   = e1000_resume,
+#endif
+#ifndef USE_REBOOT_NOTIFIER
+	.shutdown = e1000_shutdown,
+#endif
+#ifdef CONFIG_E1000_PCI_ERS
+	.err_handler = &e1000_err_handler
 #endif
 };
 
 MODULE_AUTHOR("Intel Corporation, <linux.nics@intel.com>");
 MODULE_DESCRIPTION("Intel(R) PRO/1000 Network Driver");
 MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
 
 static int debug = NETIF_MSG_DRV | NETIF_MSG_PROBE;
 module_param(debug, int, 0);
@@ -207,9 +338,7 @@
  * e1000_init_module is the first routine called when the driver is
  * loaded. All it does is register with the PCI subsystem.
  **/
-
-static int __init
-e1000_init_module(void)
+static int __init e1000_init_module(void)
 {
 	int ret;
 	printk(KERN_INFO "%s - version %s\n",
@@ -217,10 +346,19 @@
 
 	printk(KERN_INFO "%s\n", e1000_copyright);
 
-	ret = pci_module_init(&e1000_driver);
-	if(ret >= 0) {
+	ret = pci_register_driver(&e1000_driver);
+#ifdef USE_REBOOT_NOTIFIER
+	if (ret >= 0) {
 		register_reboot_notifier(&e1000_notifier_reboot);
 	}
+#endif
+	if (copybreak != COPYBREAK_DEFAULT) {
+		if (copybreak == 0)
+			printk(KERN_INFO "e1000: copybreak disabled\n");
+		else
+			printk(KERN_INFO "e1000: copybreak enabled for "
+			       "packets <= %u bytes\n", copybreak);
+	}
 	return ret;
 }
 
@@ -232,134 +370,623 @@
  * e1000_exit_module is called just before the driver is removed
  * from memory.
  **/
-
-static void __exit
-e1000_exit_module(void)
+static void __exit e1000_exit_module(void)
 {
+#ifdef USE_REBOOT_NOTIFIER
 	unregister_reboot_notifier(&e1000_notifier_reboot);
+#endif
 	pci_unregister_driver(&e1000_driver);
 }
 
 module_exit(e1000_exit_module);
 
+static int e1000_request_irq(struct e1000_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	int irq_flags, err = 0;
+
+	irq_flags = IRQF_SHARED;
+
+#ifdef CONFIG_PCI_MSI
+	if (adapter->flags.has_msi) {
+		err = pci_enable_msi(adapter->pdev);
+		adapter->flags.msi_enabled = (err ? 0 : 1);
+	}
+	if (adapter->flags.msi_enabled) {
+		irq_flags &= ~IRQF_SHARED;
+		err = request_irq(adapter->pdev->irq, &e1000_intr_msi,
+		                  irq_flags, netdev->name, netdev);
+		if (!err) {
+			return err;
+		} else {
+			irq_flags |= IRQF_SHARED;
+			adapter->flags.msi_enabled = 0;
+			pci_disable_msi(adapter->pdev);
+		}
+	}
+#endif
+	err = request_irq(adapter->pdev->irq, &e1000_intr, irq_flags,
+	                  netdev->name, netdev);
+	if (err)
+		DPRINTK(PROBE, ERR, "Unable to allocate interrupt Error: %d\n",
+		        err);
+
+	return err;
+}
 
-int
-e1000_up(struct e1000_adapter *adapter)
+static void e1000_free_irq(struct e1000_adapter *adapter)
 {
 	struct net_device *netdev = adapter->netdev;
-	int err;
 
-	/* hardware has been reset, we need to reload some things */
+	free_irq(adapter->pdev->irq, netdev);
+
+#ifdef CONFIG_PCI_MSI
+	if (adapter->flags.msi_enabled) {
+		pci_disable_msi(adapter->pdev);
+		adapter->flags.msi_enabled = 0;
+	}
+#endif
+}
+
+/**
+ * e1000_irq_disable - Mask off interrupt generation on the NIC
+ * @adapter: board private structure
+ **/
+static void e1000_irq_disable(struct e1000_adapter *adapter)
+{
+	atomic_inc(&adapter->irq_sem);
+	E1000_WRITE_REG(&adapter->hw, E1000_IMC, ~0);
+	E1000_WRITE_FLUSH(&adapter->hw);
+	synchronize_irq(adapter->pdev->irq);
+}
+
+/**
+ * e1000_irq_enable - Enable default interrupt generation settings
+ * @adapter: board private structure
+ **/
+
+static void e1000_irq_enable(struct e1000_adapter *adapter)
+{
+	if (likely(atomic_dec_and_test(&adapter->irq_sem))) {
+		E1000_WRITE_REG(&adapter->hw, E1000_IMS, IMS_ENABLE_MASK);
+		E1000_WRITE_FLUSH(&adapter->hw);
+	}
+}
+#ifdef NETIF_F_HW_VLAN_TX
+
+static void e1000_update_mng_vlan(struct e1000_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	u16 vid = adapter->hw.mng_cookie.vlan_id;
+	u16 old_vid = adapter->mng_vlan_id;
+	if (adapter->vlgrp) {
+		if (!vlan_group_get_device(adapter->vlgrp, vid)) {
+			if (adapter->hw.mng_cookie.status &
+				E1000_MNG_DHCP_COOKIE_STATUS_VLAN) {
+				e1000_vlan_rx_add_vid(netdev, vid);
+				adapter->mng_vlan_id = vid;
+			} else {
+				adapter->mng_vlan_id = E1000_MNG_VLAN_NONE;
+			}
+
+			if ((old_vid != (u16)E1000_MNG_VLAN_NONE) &&
+					(vid != old_vid) &&
+			    !vlan_group_get_device(adapter->vlgrp, old_vid))
+				e1000_vlan_rx_kill_vid(netdev, old_vid);
+		} else {
+			adapter->mng_vlan_id = vid;
+		}
+	}
+}
+#endif
+
+/**
+ * e1000_release_hw_control - release control of the h/w to f/w
+ * @adapter: address of board private structure
+ *
+ * e1000_release_hw_control resets {CTRL_EXT|FWSM}:DRV_LOAD bit.
+ * For ASF and Pass Through versions of f/w this means that the
+ * driver is no longer loaded. For AMT version (only with 82573) i
+ * of the f/w this means that the network i/f is closed.
+ *
+ **/
+static void e1000_release_hw_control(struct e1000_adapter *adapter)
+{
+	u32 ctrl_ext;
+	u32 swsm;
+
+	/* Let firmware taken over control of h/w */
+	switch (adapter->hw.mac.type) {
+	case e1000_82573:
+		swsm = E1000_READ_REG(&adapter->hw, E1000_SWSM);
+		E1000_WRITE_REG(&adapter->hw, E1000_SWSM,
+				swsm & ~E1000_SWSM_DRV_LOAD);
+		break;
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_80003es2lan:
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		ctrl_ext = E1000_READ_REG(&adapter->hw, E1000_CTRL_EXT);
+		E1000_WRITE_REG(&adapter->hw, E1000_CTRL_EXT,
+				ctrl_ext & ~E1000_CTRL_EXT_DRV_LOAD);
+		break;
+	default:
+		break;
+	}
+}
+
+/**
+ * e1000_get_hw_control - get control of the h/w from f/w
+ * @adapter: address of board private structure
+ *
+ * e1000_get_hw_control sets {CTRL_EXT|FWSM}:DRV_LOAD bit.
+ * For ASF and Pass Through versions of f/w this means that
+ * the driver is loaded. For AMT version (only with 82573)
+ * of the f/w this means that the network i/f is open.
+ *
+ **/
+static void e1000_get_hw_control(struct e1000_adapter *adapter)
+{
+	u32 ctrl_ext;
+	u32 swsm;
+
+	/* Let firmware know the driver has taken over */
+	switch (adapter->hw.mac.type) {
+	case e1000_82573:
+		swsm = E1000_READ_REG(&adapter->hw, E1000_SWSM);
+		E1000_WRITE_REG(&adapter->hw, E1000_SWSM,
+				swsm | E1000_SWSM_DRV_LOAD);
+		break;
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_80003es2lan:
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		ctrl_ext = E1000_READ_REG(&adapter->hw, E1000_CTRL_EXT);
+		E1000_WRITE_REG(&adapter->hw, E1000_CTRL_EXT,
+				ctrl_ext | E1000_CTRL_EXT_DRV_LOAD);
+		break;
+	default:
+		break;
+	}
+}
+
+static void e1000_init_manageability(struct e1000_adapter *adapter)
+{
+	if (adapter->en_mng_pt) {
+		u32 manc = E1000_READ_REG(&adapter->hw, E1000_MANC);
+
+		/* disable hardware interception of ARP */
+		manc &= ~(E1000_MANC_ARP_EN);
+
+		/* enable receiving management packets to the host */
+		/* this will probably generate destination unreachable messages
+		 * from the host OS, but the packets will be handled on SMBUS */
+		if (adapter->flags.has_manc2h) {
+			u32 manc2h = E1000_READ_REG(&adapter->hw, E1000_MANC2H);
+			manc |= E1000_MANC_EN_MNG2HOST;
+#define E1000_MNG2HOST_PORT_623 (1 << 5)
+#define E1000_MNG2HOST_PORT_664 (1 << 6)
+			manc2h |= E1000_MNG2HOST_PORT_623;
+			manc2h |= E1000_MNG2HOST_PORT_664;
+			E1000_WRITE_REG(&adapter->hw, E1000_MANC2H, manc2h);
+		}
+
+		E1000_WRITE_REG(&adapter->hw, E1000_MANC, manc);
+	}
+}
+
+static void e1000_release_manageability(struct e1000_adapter *adapter)
+{
+	if (adapter->en_mng_pt) {
+		u32 manc = E1000_READ_REG(&adapter->hw, E1000_MANC);
+
+		/* re-enable hardware interception of ARP */
+		manc |= E1000_MANC_ARP_EN;
 
-	/* Reset the PHY if it was previously powered down */
-	if(adapter->hw.media_type == e1000_media_type_copper) {
-		uint16_t mii_reg;
-		e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &mii_reg);
-		if(mii_reg & MII_CR_POWER_DOWN)
-			e1000_phy_reset(&adapter->hw);
+		if (adapter->flags.has_manc2h)
+			manc &= ~E1000_MANC_EN_MNG2HOST;
+
+		/* don't explicitly have to mess with MANC2H since
+		 * MANC has an enable disable that gates MANC2H */
+
+		/* XXX stop the hardware watchdog ? */
+
+		E1000_WRITE_REG(&adapter->hw, E1000_MANC, manc);
 	}
+}
+
+/**
+ * e1000_configure - configure the hardware for RX and TX
+ * @adapter: private board structure
+ **/
+static void e1000_configure(struct e1000_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	int i;
 
 	e1000_set_multi(netdev);
 
+#ifdef NETIF_F_HW_VLAN_TX
 	e1000_restore_vlan(adapter);
+#endif
+	e1000_init_manageability(adapter);
 
 	e1000_configure_tx(adapter);
 	e1000_setup_rctl(adapter);
 	e1000_configure_rx(adapter);
-	e1000_alloc_rx_buffers(adapter);
+	/* call E1000_DESC_UNUSED which always leaves
+	 * at least 1 descriptor unused to make sure
+	 * next_to_use != next_to_clean */
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		struct e1000_rx_ring *ring = &adapter->rx_ring[i];
+		adapter->alloc_rx_buf(adapter, ring,
+		                      E1000_DESC_UNUSED(ring));
+	}
 
-	if((err = request_irq(adapter->pdev->irq, &e1000_intr,
-		              SA_SHIRQ | SA_SAMPLE_RANDOM,
-		              netdev->name, netdev)))
-		return err;
+#ifdef CONFIG_E1000_MQ
+	e1000_setup_queue_mapping(adapter);
+#endif
+
+	adapter->tx_queue_len = netdev->tx_queue_len;
+}
 
-	mod_timer(&adapter->watchdog_timer, jiffies);
+int e1000_up(struct e1000_adapter *adapter)
+{
+	/* hardware has been reset, we need to reload some things */
+	e1000_configure(adapter);
+
+	clear_bit(__E1000_DOWN, &adapter->state);
+
+#ifdef CONFIG_E1000_NAPI
+	netif_poll_enable(adapter->netdev);
+#endif
 	e1000_irq_enable(adapter);
 
+	/* fire a link change interrupt to start the watchdog */
+	E1000_WRITE_REG(&adapter->hw, E1000_ICS, E1000_ICS_LSC);
 	return 0;
 }
 
-void
-e1000_down(struct e1000_adapter *adapter)
+/**
+ * e1000_power_up_phy - restore link in case the phy was powered down
+ * @adapter: address of board private structure
+ *
+ * The phy may be powered down to save power and turn off link when the
+ * driver is unloaded and wake on lan is not enabled (among others)
+ * *** this routine MUST be followed by a call to e1000_reset ***
+ *
+ **/
+void e1000_power_up_phy(struct e1000_adapter *adapter)
+{
+	u16 mii_reg = 0;
+
+	/* Just clear the power down bit to wake the phy back up */
+	if (adapter->hw.media_type == e1000_media_type_copper) {
+		/* according to the manual, the phy will retain its
+		 * settings across a power-down/up cycle */
+		e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &mii_reg);
+		mii_reg &= ~MII_CR_POWER_DOWN;
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, mii_reg);
+	}
+
+	e1000_setup_link(&adapter->hw);
+}
+
+static void e1000_power_down_phy(struct e1000_adapter *adapter)
+{
+	/* Power down the PHY so no link is implied when interface is down *
+	 * The PHY cannot be powered down if any of the following is TRUE *
+	 * (a) WoL is enabled
+	 * (b) AMT is active
+	 * (c) SoL/IDER session is active */
+	if (!adapter->wol && adapter->hw.mac.type >= e1000_82540 &&
+	   adapter->hw.media_type == e1000_media_type_copper) {
+		u16 mii_reg = 0;
+
+		switch (adapter->hw.mac.type) {
+		case e1000_82540:
+		case e1000_82545:
+		case e1000_82545_rev_3:
+		case e1000_82546:
+		case e1000_82546_rev_3:
+		case e1000_82541:
+		case e1000_82541_rev_2:
+		case e1000_82547:
+		case e1000_82547_rev_2:
+			if (E1000_READ_REG(&adapter->hw, E1000_MANC) &
+			    E1000_MANC_SMBUS_EN)
+				goto out;
+			break;
+		case e1000_82571:
+		case e1000_82572:
+		case e1000_82573:
+		case e1000_80003es2lan:
+		case e1000_ich8lan:
+		case e1000_ich9lan:
+			if (e1000_check_mng_mode(&adapter->hw) ||
+			    e1000_check_reset_block(&adapter->hw))
+				goto out;
+			break;
+		default:
+			goto out;
+		}
+		e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &mii_reg);
+		mii_reg |= MII_CR_POWER_DOWN;
+		e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, mii_reg);
+		mdelay(1);
+	}
+out:
+	return;
+}
+
+void e1000_down(struct e1000_adapter *adapter)
 {
 	struct net_device *netdev = adapter->netdev;
+	u32 tctl, rctl;
+
+	/* signal that we're down so the interrupt handler does not
+	 * reschedule our watchdog timer */
+	set_bit(__E1000_DOWN, &adapter->state);
+
+	/* disable receives in the hardware */
+	rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+	/* flush and sleep below */
+
+#ifdef NETIF_F_LLTX
+	netif_stop_queue(netdev);
+#else
+	netif_tx_disable(netdev);
+#endif
 
+	/* disable transmits in the hardware */
+	tctl = E1000_READ_REG(&adapter->hw, E1000_TCTL);
+	tctl &= ~E1000_TCTL_EN;
+	E1000_WRITE_REG(&adapter->hw, E1000_TCTL, tctl);
+	/* flush both disables and wait for them to finish */
+	E1000_WRITE_FLUSH(&adapter->hw);
+	msleep(10);
+
+#ifdef CONFIG_E1000_NAPI
+	netif_poll_disable(netdev);
+#endif
 	e1000_irq_disable(adapter);
-	free_irq(adapter->pdev->irq, netdev);
+
 	del_timer_sync(&adapter->tx_fifo_stall_timer);
 	del_timer_sync(&adapter->watchdog_timer);
 	del_timer_sync(&adapter->phy_info_timer);
+
+	netdev->tx_queue_len = adapter->tx_queue_len;
+	netif_carrier_off(netdev);
 	adapter->link_speed = 0;
 	adapter->link_duplex = 0;
-	netif_carrier_off(netdev);
-	netif_stop_queue(netdev);
 
 	e1000_reset(adapter);
-	e1000_clean_tx_ring(adapter);
-	e1000_clean_rx_ring(adapter);
+	e1000_clean_all_tx_rings(adapter);
+	e1000_clean_all_rx_rings(adapter);
+}
 
-	/* If WoL is not enabled
-	 * Power down the PHY so no link is implied when interface is down */
-	if(!adapter->wol && adapter->hw.media_type == e1000_media_type_copper) {
-		uint16_t mii_reg;
-		e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &mii_reg);
-		mii_reg |= MII_CR_POWER_DOWN;
-		e1000_write_phy_reg(&adapter->hw, PHY_CTRL, mii_reg);
-	}
+void e1000_reinit_locked(struct e1000_adapter *adapter)
+{
+	WARN_ON(in_interrupt());
+	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
+		msleep(1);
+	e1000_down(adapter);
+	e1000_up(adapter);
+	clear_bit(__E1000_RESETTING, &adapter->state);
 }
 
-void
-e1000_reset(struct e1000_adapter *adapter)
+void e1000_reset(struct e1000_adapter *adapter)
 {
-	uint32_t pba, manc;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+	u32 pba = 0, tx_space, min_tx_space, min_rx_space;
+	boolean_t legacy_pba_adjust = FALSE;
+	u16 hwm;
+
 	/* Repartition Pba for greater than 9k mtu
 	 * To take effect CTRL.RST is required.
 	 */
 
-	if(adapter->hw.mac_type < e1000_82547) {
-		if(adapter->rx_buffer_len > E1000_RXBUFFER_8192)
-			pba = E1000_PBA_40K;
-		else
-			pba = E1000_PBA_48K;
-	} else {
-		if(adapter->rx_buffer_len > E1000_RXBUFFER_8192)
-			pba = E1000_PBA_22K;
-		else
-			pba = E1000_PBA_30K;
-		adapter->tx_fifo_head = 0;
-		adapter->tx_head_addr = pba << E1000_TX_HEAD_ADDR_SHIFT;
-		adapter->tx_fifo_size =
-			(E1000_PBA_40K - pba) << E1000_PBA_BYTES_SHIFT;
-		atomic_set(&adapter->tx_fifo_stall, 0);
+	switch (mac->type) {
+	case e1000_82542:
+	case e1000_82543:
+	case e1000_82544:
+	case e1000_82540:
+	case e1000_82541:
+	case e1000_82541_rev_2:
+		legacy_pba_adjust = TRUE;
+		pba = E1000_PBA_48K;
+		break;
+	case e1000_82545:
+	case e1000_82545_rev_3:
+	case e1000_82546:
+	case e1000_82546_rev_3:
+		pba = E1000_PBA_48K;
+		break;
+	case e1000_82547:
+	case e1000_82547_rev_2:
+		legacy_pba_adjust = TRUE;
+		pba = E1000_PBA_30K;
+		break;
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_80003es2lan:
+		pba = E1000_PBA_38K;
+		break;
+	case e1000_82573:
+		pba = E1000_PBA_20K;
+		break;
+	case e1000_ich8lan:
+		pba = E1000_PBA_8K;
+		break;
+	case e1000_ich9lan:
+#define E1000_PBA_10K 0x000A
+		pba = E1000_PBA_10K;
+		break;
+	case e1000_undefined:
+	case e1000_num_macs:
+		break;
+	}
+
+	if (legacy_pba_adjust == TRUE) {
+		if (mac->max_frame_size > E1000_RXBUFFER_8192)
+			pba -= 8; /* allocate more FIFO for Tx */
+
+		if (mac->type == e1000_82547) {
+			adapter->tx_fifo_head = 0;
+			adapter->tx_head_addr = pba << E1000_TX_HEAD_ADDR_SHIFT;
+			adapter->tx_fifo_size =
+				(E1000_PBA_40K - pba) << E1000_PBA_BYTES_SHIFT;
+			atomic_set(&adapter->tx_fifo_stall, 0);
+		}
+	} else if (mac->max_frame_size > ETH_FRAME_LEN + ETHERNET_FCS_SIZE) {
+		/* adjust PBA for jumbo frames */
+		E1000_WRITE_REG(&adapter->hw, E1000_PBA, pba);
+
+		/* To maintain wire speed transmits, the Tx FIFO should be
+		 * large enough to accommodate two full transmit packets,
+		 * rounded up to the next 1KB and expressed in KB.  Likewise,
+		 * the Rx FIFO should be large enough to accommodate at least
+		 * one full receive packet and is similarly rounded up and
+		 * expressed in KB. */
+		pba = E1000_READ_REG(&adapter->hw, E1000_PBA);
+		/* upper 16 bits has Tx packet buffer allocation size in KB */
+		tx_space = pba >> 16;
+		/* lower 16 bits has Rx packet buffer allocation size in KB */
+		pba &= 0xffff;
+		/* the tx fifo also stores 16 bytes of information about the tx
+		 * but don't include ethernet FCS because hardware appends it */
+		min_tx_space = (mac->max_frame_size +
+		                sizeof(struct e1000_tx_desc) -
+		                ETHERNET_FCS_SIZE) * 2;
+		min_tx_space = ALIGN(min_tx_space, 1024);
+		min_tx_space >>= 10;
+		/* software strips receive CRC, so leave room for it */
+		min_rx_space = mac->max_frame_size;
+		min_rx_space = ALIGN(min_rx_space, 1024);
+		min_rx_space >>= 10;
+
+		/* If current Tx allocation is less than the min Tx FIFO size,
+		 * and the min Tx FIFO size is less than the current Rx FIFO
+		 * allocation, take space away from current Rx allocation */
+		if (tx_space < min_tx_space &&
+		    ((min_tx_space - tx_space) < pba)) {
+			pba = pba - (min_tx_space - tx_space);
+
+			/* PCI/PCIx hardware has PBA alignment constraints */
+			switch (mac->type) {
+			case e1000_82545 ... e1000_82546_rev_3:
+				pba &= ~(E1000_PBA_8K - 1);
+				break;
+			default:
+				break;
+			}
+
+			/* if short on rx space, rx wins and must trump tx
+			 * adjustment or use Early Receive if available */
+			if (pba < min_rx_space) {
+				switch (mac->type) {
+				case e1000_82573:
+				case e1000_ich9lan:
+					/* ERT enabled in e1000_configure_rx */
+					break;
+				default:
+					pba = min_rx_space;
+					break;
+				}
+			}
+		}
 	}
-	E1000_WRITE_REG(&adapter->hw, PBA, pba);
+
+	E1000_WRITE_REG(&adapter->hw, E1000_PBA, pba);
 
 	/* flow control settings */
-	adapter->hw.fc_high_water = (pba << E1000_PBA_BYTES_SHIFT) -
-				    E1000_FC_HIGH_DIFF;
-	adapter->hw.fc_low_water = (pba << E1000_PBA_BYTES_SHIFT) -
-				   E1000_FC_LOW_DIFF;
-	adapter->hw.fc_pause_time = E1000_FC_PAUSE_TIME;
-	adapter->hw.fc_send_xon = 1;
-	adapter->hw.fc = adapter->hw.original_fc;
+	/* The high water mark must be low enough to fit one full frame
+	 * (or the size used for early receive) above it in the Rx FIFO.
+	 * Set it to the lower of:
+	 * - 90% of the Rx FIFO size, and
+	 * - the full Rx FIFO size minus the early receive size (for parts
+	 *   with ERT support assuming ERT set to E1000_ERT_2048), or
+	 * - the full Rx FIFO size minus one full frame */
+	hwm = min(((pba << 10) * 9 / 10),
+	          ((mac->type == e1000_82573 || mac->type == e1000_ich9lan) ?
+	              (u16)((pba << 10) - (E1000_ERT_2048 << 3)) :
+	              ((pba << 10) - mac->max_frame_size)));
+
+	mac->fc_high_water = hwm & 0xFFF8;	/* 8-byte granularity */
+	mac->fc_low_water = mac->fc_high_water - 8;
 
+	if (mac->type == e1000_80003es2lan)
+		mac->fc_pause_time = 0xFFFF;
+	else
+		mac->fc_pause_time = E1000_FC_PAUSE_TIME;
+	mac->fc_send_xon = 1;
+	mac->fc = mac->original_fc;
+
+	/* Allow time for pending master requests to run */
 	e1000_reset_hw(&adapter->hw);
-	if(adapter->hw.mac_type >= e1000_82544)
-		E1000_WRITE_REG(&adapter->hw, WUC, 0);
-	if(e1000_init_hw(&adapter->hw))
+
+	/* For 82573 and ICHx if AMT is enabled, let the firmware know
+	 * that the network interface is in control */
+	if (((adapter->hw.mac.type == e1000_82573) ||
+	     (adapter->hw.mac.type == e1000_ich8lan) ||
+	     (adapter->hw.mac.type == e1000_ich9lan)) &&
+	    e1000_check_mng_mode(&adapter->hw))
+		e1000_get_hw_control(adapter);
+
+	if (mac->type >= e1000_82544)
+		E1000_WRITE_REG(&adapter->hw, E1000_WUC, 0);
+
+	if (e1000_init_hw(&adapter->hw))
 		DPRINTK(PROBE, ERR, "Hardware Error\n");
+#ifdef NETIF_F_HW_VLAN_TX
+	e1000_update_mng_vlan(adapter);
+#endif
+	/* if (adapter->hwflags & HWFLAGS_PHY_PWR_BIT) { */
+	if (mac->type >= e1000_82544 &&
+	    mac->type <= e1000_82547_rev_2 &&
+	    mac->autoneg == 1 &&
+	    adapter->hw.phy.autoneg_advertised == ADVERTISE_1000_FULL) {
+		u32 ctrl = E1000_READ_REG(&adapter->hw, E1000_CTRL);
+		/* clear phy power management bit if we are in gig only mode,
+		 * which if enabled will attempt negotiation to 100Mb, which
+		 * can cause a loss of link at power off or driver unload */
+		ctrl &= ~E1000_CTRL_SWDPIN3;
+		E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl);
+	}
+
+#if defined(CONFIG_PPC64) || defined(CONFIG_PPC)
+#define E1000_GCR_DISABLE_TIMEOUT_MECHANISM 0x80000000
+	if (adapter->hw.mac.type == e1000_82571) {
+		/* work around pSeries hardware by disabling timeouts */
+		u32 gcr = E1000_READ_REG(&adapter->hw, E1000_GCR);
+		gcr |= E1000_GCR_DISABLE_TIMEOUT_MECHANISM;
+		E1000_WRITE_REG(&adapter->hw, E1000_GCR, gcr);
+	}
+#endif
 
 	/* Enable h/w to recognize an 802.1Q VLAN Ethernet packet */
-	E1000_WRITE_REG(&adapter->hw, VET, ETHERNET_IEEE_VLAN_TYPE);
+	E1000_WRITE_REG(&adapter->hw, E1000_VET, ETHERNET_IEEE_VLAN_TYPE);
 
 	e1000_reset_adaptive(&adapter->hw);
-	e1000_phy_get_info(&adapter->hw, &adapter->phy_info);
+	e1000_get_phy_info(&adapter->hw);
 
-	if(adapter->en_mng_pt) {
-		manc = E1000_READ_REG(&adapter->hw, MANC);
-		manc |= (E1000_MANC_ARP_EN | E1000_MANC_EN_MNG2HOST);
-		E1000_WRITE_REG(&adapter->hw, MANC, manc);
+	if (!adapter->flags.smart_power_down &&
+	    (mac->type == e1000_82571 || mac->type == e1000_82572)) {
+		u16 phy_data = 0;
+		/* speed up time to link by disabling smart power down, ignore
+		 * the return value of this function because there is nothing
+		 * different we would do if it failed */
+		e1000_read_phy_reg(&adapter->hw, IGP02E1000_PHY_POWER_MGMT,
+		                   &phy_data);
+		phy_data &= ~IGP02E1000_PM_SPD;
+		e1000_write_phy_reg(&adapter->hw, IGP02E1000_PHY_POWER_MGMT,
+		                    phy_data);
 	}
+
+	e1000_release_manageability(adapter);
 }
 
 /**
@@ -373,73 +1000,71 @@
  * The OS initialization, configuring of the adapter private structure,
  * and a hardware reset occur.
  **/
-
-static int __devinit
-e1000_probe(struct pci_dev *pdev,
-            const struct pci_device_id *ent)
+static int __devinit e1000_probe(struct pci_dev *pdev,
+                                 const struct pci_device_id *ent)
 {
 	struct net_device *netdev;
 	struct e1000_adapter *adapter;
+	unsigned long mmio_start, mmio_len;
+	unsigned long flash_start, flash_len;
+
 	static int cards_found = 0;
-	unsigned long mmio_start;
-	int mmio_len;
-	int pci_using_dac;
-	int i;
-	int err;
-	uint16_t eeprom_data;
-
-	if((err = pci_enable_device(pdev)))
+	static int global_quad_port_a = 0; /* global ksp3 port a indication */
+	int i, err, pci_using_dac;
+	u16 eeprom_data = 0;
+	u16 eeprom_apme_mask = E1000_EEPROM_APME;
+	if ((err = pci_enable_device(pdev)))
 		return err;
 
-	if(!(err = pci_set_dma_mask(pdev, DMA_64BIT_MASK))) {
+	if (!(err = pci_set_dma_mask(pdev, DMA_64BIT_MASK)) &&
+	    !(err = pci_set_consistent_dma_mask(pdev, DMA_64BIT_MASK))) {
 		pci_using_dac = 1;
 	} else {
-		if((err = pci_set_dma_mask(pdev, DMA_32BIT_MASK))) {
+		if ((err = pci_set_dma_mask(pdev, DMA_32BIT_MASK)) &&
+		    (err = pci_set_consistent_dma_mask(pdev, DMA_32BIT_MASK))) {
 			E1000_ERR("No usable DMA configuration, aborting\n");
-			return err;
+			goto err_dma;
 		}
 		pci_using_dac = 0;
 	}
 
-	if((err = pci_request_regions(pdev, e1000_driver_name)))
-		return err;
+	if ((err = pci_request_regions(pdev, e1000_driver_name)))
+		goto err_pci_reg;
 
 	pci_set_master(pdev);
 
+	err = -ENOMEM;
+#ifdef CONFIG_E1000_MQ
+	netdev = alloc_etherdev(sizeof(struct e1000_adapter) +
+		(sizeof(struct net_device_subqueue) * E1000_MAX_TX_QUEUES));
+#else
 	netdev = alloc_etherdev(sizeof(struct e1000_adapter));
-	if(!netdev) {
-		err = -ENOMEM;
+#endif
+	if (!netdev)
 		goto err_alloc_etherdev;
-	}
 
 	SET_MODULE_OWNER(netdev);
 	SET_NETDEV_DEV(netdev, &pdev->dev);
 
 	pci_set_drvdata(pdev, netdev);
-	adapter = netdev->priv;
+	adapter = netdev_priv(netdev);
 	adapter->netdev = netdev;
 	adapter->pdev = pdev;
 	adapter->hw.back = adapter;
 	adapter->msg_enable = (1 << debug) - 1;
 
-	rtnl_lock();
-	/* we need to set the name early for the DPRINTK macro */
-	if(dev_alloc_name(netdev, netdev->name) < 0)
-		goto err_free_unlock;
-
 	mmio_start = pci_resource_start(pdev, BAR_0);
 	mmio_len = pci_resource_len(pdev, BAR_0);
 
+	err = -EIO;
 	adapter->hw.hw_addr = ioremap(mmio_start, mmio_len);
-	if(!adapter->hw.hw_addr) {
-		err = -EIO;
+	if (!adapter->hw.hw_addr)
 		goto err_ioremap;
-	}
 
-	for(i = BAR_1; i <= BAR_5; i++) {
-		if(pci_resource_len(pdev, i) == 0)
+	for (i = BAR_1; i <= BAR_5; i++) {
+		if (pci_resource_len(pdev, i) == 0)
 			continue;
-		if(pci_resource_flags(pdev, i) & IORESOURCE_IO) {
+		if (pci_resource_flags(pdev, i) & IORESOURCE_IO) {
 			adapter->hw.io_base = pci_resource_start(pdev, i);
 			break;
 		}
@@ -448,24 +1073,32 @@
 	netdev->open = &e1000_open;
 	netdev->stop = &e1000_close;
 	netdev->hard_start_xmit = &e1000_xmit_frame;
+#ifdef CONFIG_E1000_MQ
+	netdev->hard_start_subqueue_xmit = &e1000_subqueue_xmit_frame;
+#endif
 	netdev->get_stats = &e1000_get_stats;
 	netdev->set_multicast_list = &e1000_set_multi;
 	netdev->set_mac_address = &e1000_set_mac;
 	netdev->change_mtu = &e1000_change_mtu;
 	netdev->do_ioctl = &e1000_ioctl;
-	set_ethtool_ops(netdev);
+	e1000_set_ethtool_ops(netdev);
+#ifdef HAVE_TX_TIMEOUT
 	netdev->tx_timeout = &e1000_tx_timeout;
 	netdev->watchdog_timeo = 5 * HZ;
+#endif
 #ifdef CONFIG_E1000_NAPI
 	netdev->poll = &e1000_clean;
 	netdev->weight = 64;
 #endif
+#ifdef NETIF_F_HW_VLAN_TX
 	netdev->vlan_rx_register = e1000_vlan_rx_register;
 	netdev->vlan_rx_add_vid = e1000_vlan_rx_add_vid;
 	netdev->vlan_rx_kill_vid = e1000_vlan_rx_kill_vid;
+#endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	netdev->poll_controller = e1000_netpoll;
 #endif
+	strncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);
 
 	netdev->mem_start = mmio_start;
 	netdev->mem_end = mmio_start + mmio_len;
@@ -475,64 +1108,143 @@
 
 	/* setup the private structure */
 
-	if((err = e1000_sw_init(adapter)))
+	if ((err = e1000_sw_init(adapter)))
 		goto err_sw_init;
 
-	if(adapter->hw.mac_type >= e1000_82543) {
+	err = -EIO;
+	/* Flash BAR mapping must happen after e1000_sw_init
+	 * because it depends on mac.type */
+	if (((adapter->hw.mac.type == e1000_ich8lan) ||
+	     (adapter->hw.mac.type == e1000_ich9lan)) &&
+	   (pci_resource_flags(pdev, 1) & IORESOURCE_MEM)) {
+		flash_start = pci_resource_start(pdev, 1);
+		flash_len = pci_resource_len(pdev, 1);
+		adapter->hw.flash_address = ioremap(flash_start, flash_len);
+		if (!adapter->hw.flash_address)
+			goto err_flashmap;
+	}
+
+	if ((err = e1000_init_mac_params(&adapter->hw)))
+		goto err_hw_init;
+
+	if ((err = e1000_init_nvm_params(&adapter->hw)))
+		goto err_hw_init;
+
+	if ((err = e1000_init_phy_params(&adapter->hw)))
+		goto err_hw_init;
+
+	e1000_get_bus_info(&adapter->hw);
+
+	e1000_init_script_state_82541(&adapter->hw, TRUE);
+	e1000_set_tbi_compatibility_82543(&adapter->hw, TRUE);
+
+	adapter->hw.phy.wait_for_link = FALSE;
+	adapter->hw.mac.adaptive_ifs = TRUE;
+
+	/* Copper options */
+
+	if (adapter->hw.media_type == e1000_media_type_copper) {
+		adapter->hw.phy.mdix = AUTO_ALL_MODES;
+		adapter->hw.phy.disable_polarity_correction = FALSE;
+		adapter->hw.phy.ms_type = e1000_ms_hw_default;
+	}
+
+	if (e1000_check_reset_block(&adapter->hw))
+		DPRINTK(PROBE, INFO, "PHY reset is blocked due to SOL/IDER session.\n");
+
+#ifdef MAX_SKB_FRAGS
+	if (adapter->hw.mac.type >= e1000_82543) {
+#ifdef NETIF_F_HW_VLAN_TX
 		netdev->features = NETIF_F_SG |
 				   NETIF_F_HW_CSUM |
 				   NETIF_F_HW_VLAN_TX |
 				   NETIF_F_HW_VLAN_RX |
 				   NETIF_F_HW_VLAN_FILTER;
-	} else {
-		netdev->features = NETIF_F_SG;
+		if ((adapter->hw.mac.type == e1000_ich8lan) ||
+		    (adapter->hw.mac.type == e1000_ich9lan))
+			netdev->features &= ~NETIF_F_HW_VLAN_FILTER;
+#else
+		netdev->features = NETIF_F_SG | NETIF_F_HW_CSUM;
+#endif
 	}
 
 #ifdef NETIF_F_TSO
-	/* Disbaled for now until root-cause is found for
-	 * hangs reported against non-IA archs.  TSO can be
-	 * enabled using ethtool -K eth<x> tso on */
-	if((adapter->hw.mac_type >= e1000_82544) &&
-	   (adapter->hw.mac_type != e1000_82547))
+	if ((adapter->hw.mac.type >= e1000_82544) &&
+	   (adapter->hw.mac.type != e1000_82547)) {
+		adapter->flags.has_tso = 1;
 		netdev->features |= NETIF_F_TSO;
+	}
+
+#ifdef NETIF_F_TSO6
+	if (adapter->hw.mac.type > e1000_82547_rev_2) {
+		adapter->flags.has_tso6 = 1;
+		netdev->features |= NETIF_F_TSO6;
+	}
 #endif
-	if(pci_using_dac)
+#endif
+	if (pci_using_dac)
 		netdev->features |= NETIF_F_HIGHDMA;
 
- 	/* hard_start_xmit is safe against parallel locking */
- 	netdev->features |= NETIF_F_LLTX; 
- 
+#endif
+#ifdef NETIF_F_LLTX
+	netdev->features |= NETIF_F_LLTX;
+#endif
+
+	/* Hardware features, flags and workarounds */
+	if (adapter->hw.mac.type >= e1000_82571) {
+		adapter->flags.int_assert_auto_mask = 1;
+#ifdef CONFIG_PCI_MSI
+		adapter->flags.has_msi = 1;
+#endif
+		adapter->flags.has_manc2h = 1;
+	}
+
+	if (adapter->hw.mac.type >= e1000_82540) {
+		adapter->flags.has_smbus = 1;
+		adapter->flags.has_intr_moderation = 1;
+	}
+
+	if (adapter->hw.mac.type == e1000_82543)
+		adapter->flags.bad_tx_carrier_stats_fd = 1;
+
+	/* In rare occasions, ESB2 systems would end up started without
+	 * the RX unit being turned on. */
+	if (adapter->hw.mac.type == e1000_80003es2lan)
+		adapter->flags.rx_needs_restart = 1;
+
+
 	adapter->en_mng_pt = e1000_enable_mng_pass_thru(&adapter->hw);
 
-	/* before reading the EEPROM, reset the controller to 
+	/* before reading the NVM, reset the controller to
 	 * put the device in a known good starting state */
-	
+
 	e1000_reset_hw(&adapter->hw);
 
-	/* make sure the EEPROM is good */
+	/* make sure the NVM is good */
 
-	if(e1000_validate_eeprom_checksum(&adapter->hw) < 0) {
-		DPRINTK(PROBE, ERR, "The EEPROM Checksum Is Not Valid\n");
+	if (e1000_validate_nvm_checksum(&adapter->hw) < 0) {
+		DPRINTK(PROBE, ERR, "The NVM Checksum Is Not Valid\n");
 		err = -EIO;
 		goto err_eeprom;
 	}
 
-	/* copy the MAC address out of the EEPROM */
+	/* copy the MAC address out of the NVM */
 
 	if (e1000_read_mac_addr(&adapter->hw))
-		DPRINTK(PROBE, ERR, "EEPROM Read Error\n");
-	memcpy(netdev->dev_addr, adapter->hw.mac_addr, netdev->addr_len);
+		DPRINTK(PROBE, ERR, "NVM Read Error\n");
+	memcpy(netdev->dev_addr, adapter->hw.mac.addr, netdev->addr_len);
+#ifdef ETHTOOL_GPERMADDR
+	memcpy(netdev->perm_addr, adapter->hw.mac.addr, netdev->addr_len);
 
-	if(!is_valid_ether_addr(netdev->dev_addr)) {
+	if (!is_valid_ether_addr(netdev->perm_addr)) {
+#else
+	if (!is_valid_ether_addr(netdev->dev_addr)) {
+#endif
 		DPRINTK(PROBE, ERR, "Invalid MAC Address\n");
 		err = -EIO;
 		goto err_eeprom;
 	}
 
-	e1000_read_part_num(&adapter->hw, &(adapter->part_num));
-
-	e1000_get_bus_info(&adapter->hw);
-
 	init_timer(&adapter->tx_fifo_stall_timer);
 	adapter->tx_fifo_stall_timer.function = &e1000_82547_tx_fifo_stall;
 	adapter->tx_fifo_stall_timer.data = (unsigned long) adapter;
@@ -545,15 +1257,9 @@
 	adapter->phy_info_timer.function = &e1000_update_phy_info;
 	adapter->phy_info_timer.data = (unsigned long) adapter;
 
-	INIT_WORK(&adapter->tx_timeout_task,
-		(void (*)(void *))e1000_tx_timeout_task, netdev);
-
-	/* we're going to reset, so assume we have no link for now */
+	INIT_WORK(&adapter->reset_task, e1000_reset_task);
+	INIT_WORK(&adapter->watchdog_task, e1000_watchdog_task);
 
-	netif_carrier_off(netdev);
-	netif_stop_queue(netdev);
-
-	DPRINTK(PROBE, INFO, "Intel(R) PRO/1000 Network Connection\n");
 	e1000_check_options(adapter);
 
 	/* Initial Wake on LAN setting
@@ -561,49 +1267,154 @@
 	 * enable the ACPI Magic Packet filter
 	 */
 
-	switch(adapter->hw.mac_type) {
-	case e1000_82542_rev2_0:
-	case e1000_82542_rev2_1:
+	switch (adapter->hw.mac.type) {
+	case e1000_82542:
 	case e1000_82543:
 		break;
+	case e1000_82544:
+		e1000_read_nvm(&adapter->hw,
+			NVM_INIT_CONTROL2_REG, 1, &eeprom_data);
+		eeprom_apme_mask = E1000_EEPROM_82544_APM;
+		break;
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		/* APME bit in EEPROM is mapped to WUC.APME */
+		eeprom_data = E1000_READ_REG(&adapter->hw, E1000_WUC);
+		eeprom_apme_mask = E1000_WUC_APME;
+		break;
 	case e1000_82546:
 	case e1000_82546_rev_3:
-		if((E1000_READ_REG(&adapter->hw, STATUS) & E1000_STATUS_FUNC_1)
-		   && (adapter->hw.media_type == e1000_media_type_copper)) {
-			e1000_read_eeprom(&adapter->hw,
-				EEPROM_INIT_CONTROL3_PORT_B, 1, &eeprom_data);
+	case e1000_82571:
+	case e1000_80003es2lan:
+		if (adapter->hw.bus.func == 1) {
+			e1000_read_nvm(&adapter->hw,
+				NVM_INIT_CONTROL3_PORT_B, 1, &eeprom_data);
 			break;
 		}
 		/* Fall Through */
 	default:
-		e1000_read_eeprom(&adapter->hw,
-			EEPROM_INIT_CONTROL3_PORT_A, 1, &eeprom_data);
+		e1000_read_nvm(&adapter->hw,
+			NVM_INIT_CONTROL3_PORT_A, 1, &eeprom_data);
+		break;
+	}
+	if (eeprom_data & eeprom_apme_mask)
+		adapter->eeprom_wol |= E1000_WUFC_MAG;
+
+	/* now that we have the eeprom settings, apply the special cases
+	 * where the eeprom may be wrong or the board simply won't support
+	 * wake on lan on a particular port */
+	switch (pdev->device) {
+	case E1000_DEV_ID_82546GB_PCIE:
+		adapter->eeprom_wol = 0;
+		break;
+	case E1000_DEV_ID_82546EB_FIBER:
+	case E1000_DEV_ID_82546GB_FIBER:
+	case E1000_DEV_ID_82571EB_FIBER:
+		/* Wake events only supported on port A for dual fiber
+		 * regardless of eeprom setting */
+		if (E1000_READ_REG(&adapter->hw, E1000_STATUS) &
+		    E1000_STATUS_FUNC_1)
+			adapter->eeprom_wol = 0;
+		break;
+	case E1000_DEV_ID_82546GB_QUAD_COPPER_KSP3:
+	case E1000_DEV_ID_82571EB_QUAD_COPPER:
+	case E1000_DEV_ID_82571EB_QUAD_FIBER:
+	case E1000_DEV_ID_82571EB_QUAD_COPPER_LP:
+		/* if quad port adapter, disable WoL on all but port A */
+		if (global_quad_port_a != 0)
+			adapter->eeprom_wol = 0;
+		else
+			adapter->flags.quad_port_a = 1;
+		/* Reset for multiple quad port adapters */
+		if (++global_quad_port_a == 4)
+			global_quad_port_a = 0;
 		break;
 	}
-	if(eeprom_data & E1000_EEPROM_APME)
-		adapter->wol |= E1000_WUFC_MAG;
+
+	/* initialize the wol settings based on the eeprom settings */
+	adapter->wol = adapter->eeprom_wol;
+
+	/* print bus type/speed/width info */
+	{
+	struct e1000_hw *hw = &adapter->hw;
+	DPRINTK(PROBE, INFO, "(PCI%s:%s:%s) ",
+		((hw->bus.type == e1000_bus_type_pcix) ? "-X" :
+		 (hw->bus.type == e1000_bus_type_pci_express ? " Express":"")),
+		((hw->bus.speed == e1000_bus_speed_2500) ? "2.5Gb/s" :
+		 (hw->bus.speed == e1000_bus_speed_133) ? "133MHz" :
+		 (hw->bus.speed == e1000_bus_speed_120) ? "120MHz" :
+		 (hw->bus.speed == e1000_bus_speed_100) ? "100MHz" :
+		 (hw->bus.speed == e1000_bus_speed_66) ? "66MHz" : "33MHz"),
+		((hw->bus.width == e1000_bus_width_64) ? "64-bit" :
+		 (hw->bus.width == e1000_bus_width_pcie_x4) ? "Width x4" :
+		 (hw->bus.width == e1000_bus_width_pcie_x1) ? "Width x1" :
+		 "32-bit"));
+	}
+
+	for (i = 0; i < 6; i++)
+		printk("%2.2x%c", netdev->dev_addr[i], i == 5 ? '\n' : ':');
 
 	/* reset the hardware with the new settings */
 	e1000_reset(adapter);
 
-	/* We're already holding the rtnl lock; call the no-lock version */
-	if((err = register_netdevice(netdev)))
+	/* If the controller is 82573 or ICH and f/w is AMT, do not set
+	 * DRV_LOAD until the interface is up.  For all other cases,
+	 * let the f/w know that the h/w is now under the control
+	 * of the driver. */
+	if (((adapter->hw.mac.type != e1000_82573) &&
+	     (adapter->hw.mac.type != e1000_ich8lan) &&
+	     (adapter->hw.mac.type != e1000_ich9lan)) ||
+	    !e1000_check_mng_mode(&adapter->hw))
+		e1000_get_hw_control(adapter);
+
+	/* tell the stack to leave us alone until e1000_open() is called */
+	netif_carrier_off(netdev);
+	netif_stop_queue(netdev);
+#ifdef CONFIG_E1000_NAPI
+	netif_poll_disable(netdev);
+#endif
+
+	strcpy(netdev->name, "eth%d");
+	err = register_netdev(netdev);
+	if (err)
 		goto err_register;
 
+	DPRINTK(PROBE, INFO, "Intel(R) PRO/1000 Network Connection\n");
+
 	cards_found++;
-	rtnl_unlock();
 	return 0;
 
 err_register:
-err_sw_init:
+err_hw_init:
+	e1000_release_hw_control(adapter);
 err_eeprom:
+	if (!e1000_check_reset_block(&adapter->hw))
+		e1000_phy_hw_reset(&adapter->hw);
+
+	if (adapter->hw.flash_address)
+		iounmap(adapter->hw.flash_address);
+
+	e1000_remove_device(&adapter->hw);
+err_flashmap:
+#ifdef CONFIG_E1000_NAPI
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		dev_put(&adapter->polling_netdev[i]);
+#endif
+
+	kfree(adapter->tx_ring);
+	kfree(adapter->rx_ring);
+#ifdef CONFIG_E1000_NAPI
+	kfree(adapter->polling_netdev);
+#endif
+err_sw_init:
 	iounmap(adapter->hw.hw_addr);
 err_ioremap:
-err_free_unlock:
-	rtnl_unlock();
 	free_netdev(netdev);
 err_alloc_etherdev:
 	pci_release_regions(pdev);
+err_pci_reg:
+err_dma:
+	pci_disable_device(pdev);
 	return err;
 }
 
@@ -616,31 +1427,54 @@
  * Hot-Plug event, or because the driver is going to be removed from
  * memory.
  **/
-
-static void __devexit
-e1000_remove(struct pci_dev *pdev)
+static void __devexit e1000_remove(struct pci_dev *pdev)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t manc;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+#ifdef CONFIG_E1000_NAPI
+	int i;
+#endif
 
-	if(adapter->hw.mac_type >= e1000_82540 &&
-	   adapter->hw.media_type == e1000_media_type_copper) {
-		manc = E1000_READ_REG(&adapter->hw, MANC);
-		if(manc & E1000_MANC_SMBUS_EN) {
-			manc |= E1000_MANC_ARP_EN;
-			E1000_WRITE_REG(&adapter->hw, MANC, manc);
-		}
-	}
+	/* flush_scheduled work may reschedule our watchdog task, so
+	 * explicitly disable watchdog tasks from being rescheduled  */
+	set_bit(__E1000_DOWN, &adapter->state);
+	del_timer_sync(&adapter->tx_fifo_stall_timer);
+	del_timer_sync(&adapter->watchdog_timer);
+	del_timer_sync(&adapter->phy_info_timer);
+
+	flush_scheduled_work();
+
+	e1000_release_manageability(adapter);
+
+	/* Release control of h/w to f/w.  If f/w is AMT enabled, this
+	 * would have already happened in close and is redundant. */
+	e1000_release_hw_control(adapter);
 
 	unregister_netdev(netdev);
+#ifdef CONFIG_E1000_NAPI
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		dev_put(&adapter->polling_netdev[i]);
+#endif
+
+	if (!e1000_check_reset_block(&adapter->hw))
+		e1000_phy_hw_reset(&adapter->hw);
 
-	e1000_phy_hw_reset(&adapter->hw);
+	e1000_remove_device(&adapter->hw);
+
+	kfree(adapter->tx_ring);
+	kfree(adapter->rx_ring);
+#ifdef CONFIG_E1000_NAPI
+	kfree(adapter->polling_netdev);
+#endif
 
 	iounmap(adapter->hw.hw_addr);
+	if (adapter->hw.flash_address)
+		iounmap(adapter->hw.flash_address);
 	pci_release_regions(pdev);
 
 	free_netdev(netdev);
+
+	pci_disable_device(pdev);
 }
 
 /**
@@ -651,74 +1485,300 @@
  * Fields are initialized based on PCI device information and
  * OS network device settings (MTU size).
  **/
-
-static int __devinit
-e1000_sw_init(struct e1000_adapter *adapter)
+static int __devinit e1000_sw_init(struct e1000_adapter *adapter)
 {
 	struct e1000_hw *hw = &adapter->hw;
 	struct net_device *netdev = adapter->netdev;
 	struct pci_dev *pdev = adapter->pdev;
+#ifdef CONFIG_E1000_NAPI
+	int i;
+#endif
 
 	/* PCI config space info */
 
 	hw->vendor_id = pdev->vendor;
 	hw->device_id = pdev->device;
 	hw->subsystem_vendor_id = pdev->subsystem_vendor;
-	hw->subsystem_id = pdev->subsystem_device;
+	hw->subsystem_device_id = pdev->subsystem_device;
 
 	pci_read_config_byte(pdev, PCI_REVISION_ID, &hw->revision_id);
 
-	pci_read_config_word(pdev, PCI_COMMAND, &hw->pci_cmd_word);
-
-	adapter->rx_buffer_len = E1000_RXBUFFER_2048;
-	hw->max_frame_size = netdev->mtu +
-			     ENET_HEADER_SIZE + ETHERNET_FCS_SIZE;
-	hw->min_frame_size = MINIMUM_ETHERNET_FRAME_SIZE;
+	pci_read_config_word(pdev, PCI_COMMAND, &hw->bus.pci_cmd_word);
 
-	/* identify the MAC */
-
-	if(e1000_set_mac_type(hw)) {
-		DPRINTK(PROBE, ERR, "Unknown MAC Type\n");
+	adapter->rx_buffer_len = MAXIMUM_ETHERNET_VLAN_SIZE;
+	adapter->rx_ps_bsize0 = E1000_RXBUFFER_128;
+	hw->mac.max_frame_size = netdev->mtu + ETH_HLEN + ETHERNET_FCS_SIZE;
+	hw->mac.min_frame_size = ETH_ZLEN + ETHERNET_FCS_SIZE;
+
+	/* Initialize the hardware-specific values */
+	if (e1000_setup_init_funcs(hw, FALSE)) {
+		DPRINTK(PROBE, ERR, "Hardware Initialization Failure\n");
 		return -EIO;
 	}
 
-	/* initialize eeprom parameters */
-
-	e1000_init_eeprom_params(hw);
-
-	switch(hw->mac_type) {
-	default:
+#ifdef CONFIG_E1000_MQ
+	/* Number of supported queues.
+	 * TODO: It's assumed num_rx_queues >= num_tx_queues, since multi-rx
+	 * queues are much more interesting.  Is it worth coding for the
+	 * possibility (however improbable) of num_tx_queues > num_rx_queues?
+	 */
+	switch (hw->mac.type) {
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_82573:
+	case e1000_80003es2lan:
+		adapter->num_tx_queues = 2;
+		adapter->num_rx_queues = 2;
 		break;
-	case e1000_82541:
-	case e1000_82547:
-	case e1000_82541_rev_2:
-	case e1000_82547_rev_2:
-		hw->phy_init_script = 1;
+	case e1000_ich8lan:
+	case e1000_ich9lan:
+		if ((adapter->hw.device_id == E1000_DEV_ID_ICH8_IGP_AMT) ||
+		    (adapter->hw.device_id == E1000_DEV_ID_ICH8_IGP_M_AMT) ||
+		    (adapter->hw.device_id == E1000_DEV_ID_ICH9_IGP_AMT)) {
+			adapter->num_tx_queues = 2;
+			adapter->num_rx_queues = 2;
+			break;
+		}
+		/* Fall through - remaining ICH SKUs do not support MQ */
+	default:
+		/* All hardware before 82571 only have 1 queue each for Rx/Tx.
+		 * However, the 82571 family does not have MSI-X, so multi-
+		 * queue isn't enabled.
+		 * It'd be wise not to mess with this default case. :) */
+		adapter->num_tx_queues = 1;
+		adapter->num_rx_queues = 1;
+		netdev->egress_subqueue_count = 0;
 		break;
 	}
+	adapter->num_rx_queues = min(adapter->num_rx_queues, num_online_cpus());
+	adapter->num_tx_queues = min(adapter->num_tx_queues, num_online_cpus());
 
-	e1000_set_media_type(hw);
+	if ((adapter->num_tx_queues > 1) || (adapter->num_rx_queues > 1)) {
+		netdev->egress_subqueue = (struct net_device_subqueue *)
+		                           ((void *)adapter +
+		                            sizeof(struct e1000_adapter));
+		netdev->egress_subqueue_count = adapter->num_tx_queues;
+		DPRINTK(DRV, INFO, "Multiqueue Enabled: RX queues = %u, "
+		        "TX queues = %u\n", adapter->num_rx_queues,
+		        adapter->num_tx_queues);
+	}
+#else
+	adapter->num_tx_queues = 1;
+	adapter->num_rx_queues = 1;
+#endif
 
-	hw->wait_autoneg_complete = FALSE;
-	hw->tbi_compatibility_en = TRUE;
-	hw->adaptive_ifs = TRUE;
+	if (e1000_alloc_queues(adapter)) {
+		DPRINTK(PROBE, ERR, "Unable to allocate memory for queues\n");
+		return -ENOMEM;
+	}
 
-	/* Copper options */
+#ifdef CONFIG_E1000_NAPI
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		adapter->polling_netdev[i].priv = adapter;
+		adapter->polling_netdev[i].poll = &e1000_clean;
+		adapter->polling_netdev[i].weight = 64;
+		dev_hold(&adapter->polling_netdev[i]);
+		set_bit(__LINK_STATE_START, &adapter->polling_netdev[i].state);
+	}
+	spin_lock_init(&adapter->tx_queue_lock);
+#ifdef CONFIG_E1000_MQ
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		spin_lock_init(&adapter->tx_ring[i].tx_queue_lock);
+#endif
+#endif
 
-	if(hw->media_type == e1000_media_type_copper) {
-		hw->mdix = AUTO_ALL_MODES;
-		hw->disable_polarity_correction = FALSE;
-		hw->master_slave = E1000_MASTER_SLAVE;
-	}
+	/* Explicitly disable IRQ since the NIC can be in any state. */
+	atomic_set(&adapter->irq_sem, 0);
+	e1000_irq_disable(adapter);
 
-	atomic_set(&adapter->irq_sem, 1);
 	spin_lock_init(&adapter->stats_lock);
-	spin_lock_init(&adapter->tx_lock);
 
+	set_bit(__E1000_DOWN, &adapter->state);
 	return 0;
 }
 
 /**
+ * e1000_alloc_queues - Allocate memory for all rings
+ * @adapter: board private structure to initialize
+ *
+ * We allocate one ring per queue at run-time since we don't know the
+ * number of queues at compile-time.  The polling_netdev array is
+ * intended for Multiqueue, but should work fine with a single queue.
+ **/
+static int __devinit e1000_alloc_queues(struct e1000_adapter *adapter)
+{
+	adapter->tx_ring = kcalloc(adapter->num_tx_queues,
+	                           sizeof(struct e1000_tx_ring), GFP_KERNEL);
+	if (!adapter->tx_ring)
+		return -ENOMEM;
+
+	adapter->rx_ring = kcalloc(adapter->num_rx_queues,
+	                           sizeof(struct e1000_rx_ring), GFP_KERNEL);
+	if (!adapter->rx_ring) {
+		kfree(adapter->tx_ring);
+		return -ENOMEM;
+	}
+
+#ifdef CONFIG_E1000_NAPI
+	adapter->polling_netdev = kcalloc(adapter->num_rx_queues,
+	                                  sizeof(struct net_device),
+	                                  GFP_KERNEL);
+	if (!adapter->polling_netdev) {
+		kfree(adapter->tx_ring);
+		kfree(adapter->rx_ring);
+		return -ENOMEM;
+	}
+#endif
+#ifdef CONFIG_E1000_MQ
+	adapter->cpu_tx_ring = alloc_percpu(struct e1000_tx_ring *);
+#endif
+
+	return E1000_SUCCESS;
+}
+
+#ifdef CONFIG_E1000_MQ
+static void e1000_setup_queue_mapping(struct e1000_adapter *adapter)
+{
+	int i, cpu;
+
+	lock_cpu_hotplug();
+	i = 0;
+	for_each_online_cpu(cpu) {
+		*per_cpu_ptr(adapter->cpu_tx_ring, cpu) =
+		             &adapter->tx_ring[i % adapter->num_tx_queues];
+		i++;
+	}
+	unlock_cpu_hotplug();
+}
+#endif
+
+#ifdef CONFIG_PCI_MSI
+/**
+ * e1000_intr_msi_test - Interrupt Handler
+ * @irq: interrupt number
+ * @data: pointer to a network interface device structure
+ **/
+static irqreturn_t e1000_intr_msi_test(int irq, void *data)
+{
+	struct net_device *netdev = data;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+
+	u32 icr = E1000_READ_REG(&adapter->hw, E1000_ICR);
+	DPRINTK(HW,INFO, "icr is %08X\n", icr);
+	if (icr & E1000_ICR_RXSEQ) {
+		adapter->flags.has_msi = 1;
+		wmb();
+	}
+	
+	return IRQ_HANDLED;
+}
+
+/**
+ * e1000_test_msi_interrupt - Returns 0 for successful test
+ * @adapter: board private struct
+ *
+ * code flow taken from tg3.c
+ **/
+static int e1000_test_msi_interrupt(struct e1000_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	int err;
+
+	/* poll_enable hasn't been called yet, so don't need disable */
+	/* clear any pending events */
+	E1000_READ_REG(&adapter->hw, E1000_ICR);
+	
+	/* free the real vector and request a test handler */
+	e1000_free_irq(adapter);
+
+	pci_enable_msi(adapter->pdev);
+	err = request_irq(adapter->pdev->irq, &e1000_intr_msi_test, 0,
+	                  netdev->name, netdev);
+	if (err) {
+		pci_disable_msi(adapter->pdev);
+		goto msi_test_failed;
+	}
+
+	/* our temporary test variable */
+	adapter->flags.has_msi = 0;
+	wmb();
+
+	e1000_irq_enable(adapter);
+
+	/* fire an unusual interrupt on the test handler */
+	E1000_WRITE_REG(&adapter->hw, E1000_ICS, E1000_ICS_RXSEQ);
+	E1000_WRITE_FLUSH(&adapter->hw);
+	msleep(50);
+
+	e1000_irq_disable(adapter);
+
+	rmb();
+	if (!adapter->flags.has_msi) {
+		adapter->flags.has_msi = 1;
+		err = -EIO;
+		DPRINTK(HW, INFO, "MSI interrupt test failed!\n");
+	}
+
+	free_irq(adapter->pdev->irq, netdev);
+	pci_disable_msi(adapter->pdev);
+
+	if (err == -EIO)
+		goto msi_test_failed;
+	
+	/* okay so the test worked, restore settings */
+	DPRINTK(HW, INFO, "MSI interrupt test succeeded!\n");
+msi_test_failed:
+	/* restore the original vector, even if it failed */
+	e1000_request_irq(adapter);
+	return err;
+}
+
+/**
+ * e1000_test_msi - Returns 0 if MSI test succeeds and INTx mode is restored
+ * @adapter: board private struct
+ *
+ * code flow taken from tg3.c, called with e1000 interrupts disabled.
+ **/
+static int e1000_test_msi(struct e1000_adapter *adapter)
+{
+	int err;
+	u16 pci_cmd;
+
+	if (!adapter->flags.msi_enabled || !adapter->flags.has_msi)
+		return 0;
+
+	/* disable SERR in case the MSI write causes a master abort */
+	pci_read_config_word(adapter->pdev, PCI_COMMAND, &pci_cmd);
+	pci_write_config_word(adapter->pdev, PCI_COMMAND,
+	                      pci_cmd & ~PCI_COMMAND_SERR);
+
+	err = e1000_test_msi_interrupt(adapter);
+
+	/* restore previous setting of command word */
+	pci_write_config_word(adapter->pdev, PCI_COMMAND, pci_cmd);
+
+	/* success ! */
+	if (!err)
+		return 0;
+
+	/* EIO means MSI test failed */
+	if (err != -EIO)
+		return err;
+
+	/* back to INTx mode */
+	DPRINTK(PROBE, WARNING, "MSI interrupt test failed, using legacy "
+	        "interrupt.\n");
+
+	e1000_free_irq(adapter);
+	adapter->flags.has_msi = 0;
+
+	err = e1000_request_irq(adapter);
+
+	return err;
+}
+#endif /* CONFIG_PCI_MSI */
+
+/**
  * e1000_open - Called when a network interface is made active
  * @netdev: network interface device structure
  *
@@ -730,32 +1790,84 @@
  * handler is registered with the OS, the watchdog timer is started,
  * and the stack is notified that the interface is ready.
  **/
-
-static int
-e1000_open(struct net_device *netdev)
+static int e1000_open(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	int err;
 
-	/* allocate transmit descriptors */
+	/* disallow open during test */
+	if (test_bit(__E1000_TESTING, &adapter->state))
+		return -EBUSY;
 
-	if((err = e1000_setup_tx_resources(adapter)))
+	/* allocate transmit descriptors */
+	err = e1000_setup_all_tx_resources(adapter);
+	if (err)
 		goto err_setup_tx;
 
 	/* allocate receive descriptors */
-
-	if((err = e1000_setup_rx_resources(adapter)))
+	err = e1000_setup_all_rx_resources(adapter);
+	if (err)
 		goto err_setup_rx;
 
-	if((err = e1000_up(adapter)))
-		goto err_up;
+	e1000_power_up_phy(adapter);
+
+#ifdef NETIF_F_HW_VLAN_TX
+	adapter->mng_vlan_id = E1000_MNG_VLAN_NONE;
+	if ((adapter->hw.mng_cookie.status &
+	     E1000_MNG_DHCP_COOKIE_STATUS_VLAN)) {
+		e1000_update_mng_vlan(adapter);
+	}
+#endif
+
+	/* For 82573 and ICHx if AMT is enabled, let the firmware know
+	 * that the network interface is now open */
+	if (((adapter->hw.mac.type == e1000_82573) ||
+	     (adapter->hw.mac.type == e1000_ich8lan) ||
+	     (adapter->hw.mac.type == e1000_ich9lan)) &&
+	    e1000_check_mng_mode(&adapter->hw))
+		e1000_get_hw_control(adapter);
+
+	/* before we allocate an interrupt, we must be ready to handle it.
+	 * Setting DEBUG_SHIRQ in the kernel makes it fire an interrupt
+	 * as soon as we call pci_request_irq, so we have to setup our
+	 * clean_rx handler before we do so.  */
+	e1000_configure(adapter);
+
+	err = e1000_request_irq(adapter);
+	if (err)
+		goto err_req_irq;
+
+#ifdef CONFIG_PCI_MSI
+	/* work around PCIe errata with MSI interrupts causing some chipsets to
+	 * ignore e1000 MSI messages, which means we need to test our MSI
+	 * interrupt now */
+	err = e1000_test_msi(adapter);
+	if (err) {
+		DPRINTK(PROBE, ERR, "Interrupt allocation failed\n");
+		goto err_req_irq;
+	}
+#endif
+
+	/* From here on the code is the same as e1000_up() */
+	clear_bit(__E1000_DOWN, &adapter->state);
+
+#ifdef CONFIG_E1000_NAPI
+	netif_poll_enable(netdev);
+#endif
+
+	e1000_irq_enable(adapter);
+
+	/* fire a link status change interrupt to start the watchdog */
+	E1000_WRITE_REG(&adapter->hw, E1000_ICS, E1000_ICS_LSC);
 
 	return E1000_SUCCESS;
 
-err_up:
-	e1000_free_rx_resources(adapter);
+err_req_irq:
+	e1000_release_hw_control(adapter);
+	e1000_power_down_phy(adapter);
+	e1000_free_all_rx_resources(adapter);
 err_setup_rx:
-	e1000_free_tx_resources(adapter);
+	e1000_free_all_tx_resources(adapter);
 err_setup_tx:
 	e1000_reset(adapter);
 
@@ -773,229 +1885,534 @@
  * needs to be disabled.  A global MAC reset is issued to stop the
  * hardware, and all transmit and receive resources are freed.
  **/
-
-static int
-e1000_close(struct net_device *netdev)
+static int e1000_close(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
+	WARN_ON(test_bit(__E1000_RESETTING, &adapter->state));
 	e1000_down(adapter);
+	e1000_power_down_phy(adapter);
+	e1000_free_irq(adapter);
+
+	e1000_free_all_tx_resources(adapter);
+	e1000_free_all_rx_resources(adapter);
 
-	e1000_free_tx_resources(adapter);
-	e1000_free_rx_resources(adapter);
+#ifdef NETIF_F_HW_VLAN_TX
+	/* kill manageability vlan ID if supported, but not if a vlan with
+	 * the same ID is registered on the host OS (let 8021q kill it) */
+	if ((adapter->hw.mng_cookie.status &
+			  E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
+	     !(adapter->vlgrp &&
+	       vlan_group_get_device(adapter->vlgrp, adapter->mng_vlan_id))) {
+		e1000_vlan_rx_kill_vid(netdev, adapter->mng_vlan_id);
+	}
+#endif
+
+	/* For 82573 and ICHx if AMT is enabled, let the firmware know
+	 * that the network interface is now closed */
+	if (((adapter->hw.mac.type == e1000_82573) ||
+	     (adapter->hw.mac.type == e1000_ich8lan) ||
+	     (adapter->hw.mac.type == e1000_ich9lan)) &&
+	    e1000_check_mng_mode(&adapter->hw))
+		e1000_release_hw_control(adapter);
 
 	return 0;
 }
 
 /**
- * e1000_setup_tx_resources - allocate Tx resources (Descriptors)
- * @adapter: board private structure
- *
- * Return 0 on success, negative on failure
+ * e1000_check_64k_bound - check that memory doesn't cross 64kB boundary
+ * @adapter: address of board private structure
+ * @start: address of beginning of memory
+ * @len: length of memory
  **/
-
-int
-e1000_setup_tx_resources(struct e1000_adapter *adapter)
+static boolean_t e1000_check_64k_bound(struct e1000_adapter *adapter,
+                                       void *start, unsigned long len)
 {
-	struct e1000_desc_ring *txdr = &adapter->tx_ring;
-	struct pci_dev *pdev = adapter->pdev;
-	int size;
+	unsigned long begin = (unsigned long) start;
+	unsigned long end = begin + len;
 
-	size = sizeof(struct e1000_buffer) * txdr->count;
-	txdr->buffer_info = vmalloc(size);
-	if(!txdr->buffer_info) {
-		DPRINTK(PROBE, ERR, 
-		"Unble to Allocate Memory for the Transmit descriptor ring\n");
-		return -ENOMEM;
+	/* First rev 82545 and 82546 need to not allow any memory
+	 * write location to cross 64k boundary due to errata 23 */
+	if (adapter->hw.mac.type == e1000_82545 ||
+	    adapter->hw.mac.type == e1000_82546) {
+		return ((begin ^ (end - 1)) >> 16) != 0 ? FALSE : TRUE;
 	}
-	memset(txdr->buffer_info, 0, size);
+
+	return TRUE;
+}
+
+/**
+ * e1000_setup_tx_resources - allocate Tx resources (Descriptors)
+ * @adapter: board private structure
+ * @tx_ring:    tx descriptor ring (for a specific queue) to setup
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int e1000_setup_tx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int size;
+
+	size = sizeof(struct e1000_buffer) * tx_ring->count;
+	tx_ring->buffer_info = vmalloc(size);
+	if (!tx_ring->buffer_info) {
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the transmit descriptor ring\n");
+		return -ENOMEM;
+	}
+	memset(tx_ring->buffer_info, 0, size);
 
 	/* round up to nearest 4K */
 
-	txdr->size = txdr->count * sizeof(struct e1000_tx_desc);
-	E1000_ROUNDUP(txdr->size, 4096);
+	tx_ring->size = tx_ring->count * sizeof(struct e1000_tx_desc);
+	tx_ring->size = ALIGN(tx_ring->size, 4096);
 
-	txdr->desc = pci_alloc_consistent(pdev, txdr->size, &txdr->dma);
-	if(!txdr->desc) {
-		DPRINTK(PROBE, ERR, 
-		"Unble to Allocate Memory for the Transmit descriptor ring\n");
-		vfree(txdr->buffer_info);
+	tx_ring->desc = pci_alloc_consistent(pdev, tx_ring->size,
+	                                     &tx_ring->dma);
+	if (!tx_ring->desc) {
+setup_tx_desc_die:
+		vfree(tx_ring->buffer_info);
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the transmit descriptor ring\n");
 		return -ENOMEM;
 	}
-	memset(txdr->desc, 0, txdr->size);
 
-	txdr->next_to_use = 0;
-	txdr->next_to_clean = 0;
+	/* Fix for errata 23, can't cross 64kB boundary */
+	if (!e1000_check_64k_bound(adapter, tx_ring->desc, tx_ring->size)) {
+		void *olddesc = tx_ring->desc;
+		dma_addr_t olddma = tx_ring->dma;
+		DPRINTK(TX_ERR, ERR, "tx_ring align check failed: %u bytes "
+				     "at %p\n", tx_ring->size, tx_ring->desc);
+		/* Try again, without freeing the previous */
+		tx_ring->desc = pci_alloc_consistent(pdev, tx_ring->size,
+		                                     &tx_ring->dma);
+		/* Failed allocation, critical failure */
+		if (!tx_ring->desc) {
+			pci_free_consistent(pdev, tx_ring->size, olddesc,
+			                    olddma);
+			goto setup_tx_desc_die;
+		}
+
+		if (!e1000_check_64k_bound(adapter, tx_ring->desc,
+		                           tx_ring->size)) {
+			/* give up */
+			pci_free_consistent(pdev, tx_ring->size, tx_ring->desc,
+					    tx_ring->dma);
+			pci_free_consistent(pdev, tx_ring->size, olddesc,
+			                    olddma);
+			DPRINTK(PROBE, ERR,
+				"Unable to allocate aligned memory "
+				"for the transmit descriptor ring\n");
+			vfree(tx_ring->buffer_info);
+			return -ENOMEM;
+		} else {
+			/* Free old allocation, new allocation was successful */
+			pci_free_consistent(pdev, tx_ring->size, olddesc,
+			                    olddma);
+		}
+	}
+	memset(tx_ring->desc, 0, tx_ring->size);
+
+	tx_ring->next_to_use = 0;
+	tx_ring->next_to_clean = 0;
+	spin_lock_init(&tx_ring->tx_lock);
 
 	return 0;
 }
 
 /**
- * e1000_configure_tx - Configure 8254x Transmit Unit after Reset
+ * e1000_setup_all_tx_resources - wrapper to allocate Tx resources
  * @adapter: board private structure
  *
- * Configure the Tx unit of the MAC after a reset.
+ * this allocates tx resources for all queues, return 0 on success, negative
+ * on failure
  **/
-
-static void
-e1000_configure_tx(struct e1000_adapter *adapter)
+int e1000_setup_all_tx_resources(struct e1000_adapter *adapter)
 {
-	uint64_t tdba = adapter->tx_ring.dma;
-	uint32_t tdlen = adapter->tx_ring.count * sizeof(struct e1000_tx_desc);
-	uint32_t tctl, tipg;
+	int i, err = 0;
+
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		err = e1000_setup_tx_resources(adapter, &adapter->tx_ring[i]);
+		if (err) {
+			DPRINTK(PROBE, ERR,
+				"Allocation for Tx Queue %u failed\n", i);
+			for (i-- ; i >= 0; i--)
+				e1000_free_tx_resources(adapter,
+							&adapter->tx_ring[i]);
+			break;
+		}
+	}
 
-	E1000_WRITE_REG(&adapter->hw, TDBAL, (tdba & 0x00000000ffffffffULL));
-	E1000_WRITE_REG(&adapter->hw, TDBAH, (tdba >> 32));
+	return err;
+}
 
-	E1000_WRITE_REG(&adapter->hw, TDLEN, tdlen);
+/**
+ * e1000_configure_tx - Configure 8254x Transmit Unit after Reset
+ * @adapter: board private structure
+ *
+ * Configure the Tx unit of the MAC after a reset.
+ **/
+static void e1000_configure_tx(struct e1000_adapter *adapter)
+{
+	u64 tdba;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 tdlen, tctl, tipg, tarc;
+	u32 ipgr1, ipgr2;
+	int i;
 
 	/* Setup the HW Tx Head and Tail descriptor pointers */
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		tdba = adapter->tx_ring[i].dma;
+		tdlen = adapter->tx_ring[i].count * sizeof(struct e1000_tx_desc);
+		E1000_WRITE_REG(hw, E1000_TDBAL_REG(i), (tdba & 0x00000000ffffffffULL));
+		E1000_WRITE_REG(hw, E1000_TDBAH_REG(i), (tdba >> 32));
+		E1000_WRITE_REG(hw, E1000_TDLEN_REG(i), tdlen);
+		E1000_WRITE_REG(hw, E1000_TDH_REG(i), 0);
+		E1000_WRITE_REG(hw, E1000_TDT_REG(i), 0);
+		adapter->tx_ring[i].tdh = E1000_REGISTER(hw, E1000_TDH_REG(i));
+		adapter->tx_ring[i].tdt = E1000_REGISTER(hw, E1000_TDT_REG(i));
+	}
 
-	E1000_WRITE_REG(&adapter->hw, TDH, 0);
-	E1000_WRITE_REG(&adapter->hw, TDT, 0);
 
 	/* Set the default values for the Tx Inter Packet Gap timer */
+	if (adapter->hw.mac.type <= e1000_82547_rev_2 &&
+	    (hw->media_type == e1000_media_type_fiber ||
+	     hw->media_type == e1000_media_type_internal_serdes))
+		tipg = DEFAULT_82543_TIPG_IPGT_FIBER;
+	else
+		tipg = DEFAULT_82543_TIPG_IPGT_COPPER;
 
-	switch (adapter->hw.mac_type) {
-	case e1000_82542_rev2_0:
-	case e1000_82542_rev2_1:
+	switch (hw->mac.type) {
+	case e1000_82542:
 		tipg = DEFAULT_82542_TIPG_IPGT;
-		tipg |= DEFAULT_82542_TIPG_IPGR1 << E1000_TIPG_IPGR1_SHIFT;
-		tipg |= DEFAULT_82542_TIPG_IPGR2 << E1000_TIPG_IPGR2_SHIFT;
+		ipgr1 = DEFAULT_82542_TIPG_IPGR1;
+		ipgr2 = DEFAULT_82542_TIPG_IPGR2;
+		break;
+	case e1000_80003es2lan:
+		ipgr1 = DEFAULT_82543_TIPG_IPGR1;
+		ipgr2 = DEFAULT_80003ES2LAN_TIPG_IPGR2;
 		break;
 	default:
-		if(adapter->hw.media_type == e1000_media_type_fiber ||
-		   adapter->hw.media_type == e1000_media_type_internal_serdes)
-			tipg = DEFAULT_82543_TIPG_IPGT_FIBER;
-		else
-			tipg = DEFAULT_82543_TIPG_IPGT_COPPER;
-		tipg |= DEFAULT_82543_TIPG_IPGR1 << E1000_TIPG_IPGR1_SHIFT;
-		tipg |= DEFAULT_82543_TIPG_IPGR2 << E1000_TIPG_IPGR2_SHIFT;
+		ipgr1 = DEFAULT_82543_TIPG_IPGR1;
+		ipgr2 = DEFAULT_82543_TIPG_IPGR2;
+		break;
 	}
-	E1000_WRITE_REG(&adapter->hw, TIPG, tipg);
+	tipg |= ipgr1 << E1000_TIPG_IPGR1_SHIFT;
+	tipg |= ipgr2 << E1000_TIPG_IPGR2_SHIFT;
+	E1000_WRITE_REG(hw, E1000_TIPG, tipg);
 
 	/* Set the Tx Interrupt Delay register */
 
-	E1000_WRITE_REG(&adapter->hw, TIDV, adapter->tx_int_delay);
-	if(adapter->hw.mac_type >= e1000_82540)
-		E1000_WRITE_REG(&adapter->hw, TADV, adapter->tx_abs_int_delay);
+	E1000_WRITE_REG(hw, E1000_TIDV, adapter->tx_int_delay);
+	if (adapter->flags.has_intr_moderation)
+		E1000_WRITE_REG(hw, E1000_TADV, adapter->tx_abs_int_delay);
 
 	/* Program the Transmit Control Register */
 
-	tctl = E1000_READ_REG(&adapter->hw, TCTL);
-
+	tctl = E1000_READ_REG(hw, E1000_TCTL);
 	tctl &= ~E1000_TCTL_CT;
-	tctl |= E1000_TCTL_EN | E1000_TCTL_PSP |
+	tctl |= E1000_TCTL_PSP | E1000_TCTL_RTLC |
 		(E1000_COLLISION_THRESHOLD << E1000_CT_SHIFT);
 
-	E1000_WRITE_REG(&adapter->hw, TCTL, tctl);
+	if (hw->mac.type == e1000_82571 || hw->mac.type == e1000_82572) {
+		tarc = E1000_READ_REG(hw, E1000_TARC0);
+		/* set the speed mode bit, we'll clear it if we're not at
+		 * gigabit link later */
+#define SPEED_MODE_BIT (1 << 21)
+		tarc |= SPEED_MODE_BIT;
+		E1000_WRITE_REG(hw, E1000_TARC0, tarc);
+	} else if (hw->mac.type == e1000_80003es2lan) {
+		tarc = E1000_READ_REG(hw, E1000_TARC0);
+		tarc |= 1;
+		E1000_WRITE_REG(hw, E1000_TARC0, tarc);
+		tarc = E1000_READ_REG(hw, E1000_TARC1);
+		tarc |= 1;
+		E1000_WRITE_REG(hw, E1000_TARC1, tarc);
+	}
 
-	e1000_config_collision_dist(&adapter->hw);
+	e1000_config_collision_dist(hw);
 
 	/* Setup Transmit Descriptor Settings for eop descriptor */
-	adapter->txd_cmd = E1000_TXD_CMD_IDE | E1000_TXD_CMD_EOP |
-		E1000_TXD_CMD_IFCS;
+	adapter->txd_cmd = E1000_TXD_CMD_EOP | E1000_TXD_CMD_IFCS;
+
+	/* only set IDE if we are delaying interrupts using the timers */
+	if (adapter->tx_int_delay)
+		adapter->txd_cmd |= E1000_TXD_CMD_IDE;
 
-	if(adapter->hw.mac_type < e1000_82543)
+	if (hw->mac.type < e1000_82543)
 		adapter->txd_cmd |= E1000_TXD_CMD_RPS;
 	else
 		adapter->txd_cmd |= E1000_TXD_CMD_RS;
 
 	/* Cache if we're 82544 running in PCI-X because we'll
 	 * need this to apply a workaround later in the send path. */
-	if(adapter->hw.mac_type == e1000_82544 &&
-	   adapter->hw.bus_type == e1000_bus_type_pcix)
+	if (hw->mac.type == e1000_82544 &&
+	    hw->bus.type == e1000_bus_type_pcix)
 		adapter->pcix_82544 = 1;
+
+	E1000_WRITE_REG(hw, E1000_TCTL, tctl);
+
 }
 
 /**
  * e1000_setup_rx_resources - allocate Rx resources (Descriptors)
  * @adapter: board private structure
+ * @rx_ring:    rx descriptor ring (for a specific queue) to setup
  *
  * Returns 0 on success, negative on failure
  **/
-
-int
-e1000_setup_rx_resources(struct e1000_adapter *adapter)
+static int e1000_setup_rx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring)
 {
-	struct e1000_desc_ring *rxdr = &adapter->rx_ring;
 	struct pci_dev *pdev = adapter->pdev;
-	int size;
+	int size, desc_len;
 
-	size = sizeof(struct e1000_buffer) * rxdr->count;
-	rxdr->buffer_info = vmalloc(size);
-	if(!rxdr->buffer_info) {
-		DPRINTK(PROBE, ERR, 
-		"Unble to Allocate Memory for the Recieve descriptor ring\n");
+	size = sizeof(struct e1000_rx_buffer) * rx_ring->count;
+	rx_ring->buffer_info = vmalloc(size);
+	if (!rx_ring->buffer_info) {
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the receive descriptor ring\n");
 		return -ENOMEM;
 	}
-	memset(rxdr->buffer_info, 0, size);
+	memset(rx_ring->buffer_info, 0, size);
+
+	rx_ring->ps_page = kcalloc(rx_ring->count, sizeof(struct e1000_ps_page),
+	                           GFP_KERNEL);
+	if (!rx_ring->ps_page) {
+		vfree(rx_ring->buffer_info);
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the receive descriptor ring\n");
+		return -ENOMEM;
+	}
+
+	rx_ring->ps_page_dma = kcalloc(rx_ring->count,
+	                               sizeof(struct e1000_ps_page_dma),
+	                               GFP_KERNEL);
+	if (!rx_ring->ps_page_dma) {
+		vfree(rx_ring->buffer_info);
+		kfree(rx_ring->ps_page);
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the receive descriptor ring\n");
+		return -ENOMEM;
+	}
+
+	if (adapter->hw.mac.type <= e1000_82547_rev_2)
+		desc_len = sizeof(struct e1000_rx_desc);
+	else
+		desc_len = sizeof(union e1000_rx_desc_packet_split);
 
 	/* Round up to nearest 4K */
 
-	rxdr->size = rxdr->count * sizeof(struct e1000_rx_desc);
-	E1000_ROUNDUP(rxdr->size, 4096);
+	rx_ring->size = rx_ring->count * desc_len;
+	rx_ring->size = ALIGN(rx_ring->size, 4096);
 
-	rxdr->desc = pci_alloc_consistent(pdev, rxdr->size, &rxdr->dma);
+	rx_ring->desc = pci_alloc_consistent(pdev, rx_ring->size,
+	                                     &rx_ring->dma);
 
-	if(!rxdr->desc) {
-		DPRINTK(PROBE, ERR, 
-		"Unble to Allocate Memory for the Recieve descriptor ring\n");
-		vfree(rxdr->buffer_info);
+	if (!rx_ring->desc) {
+		DPRINTK(PROBE, ERR,
+		"Unable to allocate memory for the receive descriptor ring\n");
+setup_rx_desc_die:
+		vfree(rx_ring->buffer_info);
+		kfree(rx_ring->ps_page);
+		kfree(rx_ring->ps_page_dma);
 		return -ENOMEM;
 	}
-	memset(rxdr->desc, 0, rxdr->size);
 
-	rxdr->next_to_clean = 0;
-	rxdr->next_to_use = 0;
+	/* Fix for errata 23, can't cross 64kB boundary */
+	if (!e1000_check_64k_bound(adapter, rx_ring->desc, rx_ring->size)) {
+		void *olddesc = rx_ring->desc;
+		dma_addr_t olddma = rx_ring->dma;
+		DPRINTK(RX_ERR, ERR, "rx_ring align check failed: %u bytes "
+				     "at %p\n", rx_ring->size, rx_ring->desc);
+		/* Try again, without freeing the previous */
+		rx_ring->desc = pci_alloc_consistent(pdev, rx_ring->size,
+		                                     &rx_ring->dma);
+		/* Failed allocation, critical failure */
+		if (!rx_ring->desc) {
+			pci_free_consistent(pdev, rx_ring->size, olddesc,
+			                    olddma);
+			DPRINTK(PROBE, ERR,
+				"Unable to allocate memory "
+				"for the receive descriptor ring\n");
+			goto setup_rx_desc_die;
+		}
+
+		if (!e1000_check_64k_bound(adapter, rx_ring->desc,
+		                           rx_ring->size)) {
+			/* give up */
+			pci_free_consistent(pdev, rx_ring->size, rx_ring->desc,
+			                    rx_ring->dma);
+			pci_free_consistent(pdev, rx_ring->size, olddesc,
+			                    olddma);
+			DPRINTK(PROBE, ERR,
+				"Unable to allocate aligned memory "
+				"for the receive descriptor ring\n");
+			goto setup_rx_desc_die;
+		} else {
+			/* Free old allocation, new allocation was successful */
+			pci_free_consistent(pdev, rx_ring->size, olddesc,
+			                    olddma);
+		}
+	}
+	memset(rx_ring->desc, 0, rx_ring->size);
+
+	rx_ring->next_to_clean = 0;
+	rx_ring->next_to_use = 0;
+	rx_ring->rx_skb_top = NULL;
 
 	return 0;
 }
 
 /**
- * e1000_setup_rctl - configure the receive control register
- * @adapter: Board private structure
+ * e1000_setup_all_rx_resources - wrapper to allocate Rx resources
+ * @adapter: board private structure
+ *
+ * this allocates rx resources for all queues, return 0 on success, negative
+ * on failure
  **/
+int e1000_setup_all_rx_resources(struct e1000_adapter *adapter)
+{
+	int i, err = 0;
+
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		err = e1000_setup_rx_resources(adapter, &adapter->rx_ring[i]);
+		if (err) {
+			DPRINTK(PROBE, ERR,
+				"Allocation for Rx Queue %u failed\n", i);
+			for (i-- ; i >= 0; i--)
+				e1000_free_rx_resources(adapter,
+							&adapter->rx_ring[i]);
+			break;
+		}
+	}
+
+	return err;
+}
 
-static void
-e1000_setup_rctl(struct e1000_adapter *adapter)
+#define PAGE_USE_COUNT(S) (((S) >> PAGE_SHIFT) + \
+			(((S) & (PAGE_SIZE - 1)) ? 1 : 0))
+/**
+ * e1000_setup_rctl - configure the receive control registers
+ * @adapter: Board private structure
+ **/
+static void e1000_setup_rctl(struct e1000_adapter *adapter)
 {
-	uint32_t rctl;
+	u32 rctl, rfctl;
+	u32 psrctl = 0;
+#ifndef CONFIG_E1000_DISABLE_PACKET_SPLIT
+	u32 pages = 0;
+#endif
 
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
+	rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
 
 	rctl &= ~(3 << E1000_RCTL_MO_SHIFT);
 
 	rctl |= E1000_RCTL_EN | E1000_RCTL_BAM |
 		E1000_RCTL_LBM_NO | E1000_RCTL_RDMTS_HALF |
-		(adapter->hw.mc_filter_type << E1000_RCTL_MO_SHIFT);
+		(adapter->hw.mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
+
+	/* disable the stripping of CRC because it breaks
+	 * BMC firmware connected over SMBUS
+	if (adapter->hw.mac.type > e1000_82543)
+		rctl |= E1000_RCTL_SECRC;
+	*/
 
-	if(adapter->hw.tbi_compatibility_on == 1)
+	if (e1000_tbi_sbp_enabled_82543(&adapter->hw))
 		rctl |= E1000_RCTL_SBP;
 	else
 		rctl &= ~E1000_RCTL_SBP;
 
+	if (adapter->netdev->mtu <= ETH_DATA_LEN)
+		rctl &= ~E1000_RCTL_LPE;
+	else
+		rctl |= E1000_RCTL_LPE;
+
 	/* Setup buffer sizes */
-	rctl &= ~(E1000_RCTL_SZ_4096);
-	rctl |= (E1000_RCTL_BSEX | E1000_RCTL_LPE);
+	rctl &= ~E1000_RCTL_SZ_4096;
+	rctl |= E1000_RCTL_BSEX;
 	switch (adapter->rx_buffer_len) {
-	case E1000_RXBUFFER_2048:
-	default:
-		rctl |= E1000_RCTL_SZ_2048;
-		rctl &= ~(E1000_RCTL_BSEX | E1000_RCTL_LPE);
-		break;
-	case E1000_RXBUFFER_4096:
-		rctl |= E1000_RCTL_SZ_4096;
-		break;
-	case E1000_RXBUFFER_8192:
-		rctl |= E1000_RCTL_SZ_8192;
-		break;
-	case E1000_RXBUFFER_16384:
-		rctl |= E1000_RCTL_SZ_16384;
-		break;
+		case E1000_RXBUFFER_256:
+			rctl |= E1000_RCTL_SZ_256;
+			rctl &= ~E1000_RCTL_BSEX;
+			break;
+		case E1000_RXBUFFER_512:
+			rctl |= E1000_RCTL_SZ_512;
+			rctl &= ~E1000_RCTL_BSEX;
+			break;
+		case E1000_RXBUFFER_1024:
+			rctl |= E1000_RCTL_SZ_1024;
+			rctl &= ~E1000_RCTL_BSEX;
+			break;
+		case E1000_RXBUFFER_2048:
+		default:
+			rctl |= E1000_RCTL_SZ_2048;
+			rctl &= ~E1000_RCTL_BSEX;
+			break;
+		case E1000_RXBUFFER_4096:
+			rctl |= E1000_RCTL_SZ_4096;
+			break;
+		case E1000_RXBUFFER_8192:
+			rctl |= E1000_RCTL_SZ_8192;
+			break;
+		case E1000_RXBUFFER_16384:
+			rctl |= E1000_RCTL_SZ_16384;
+			break;
+	}
+
+#ifndef CONFIG_E1000_DISABLE_PACKET_SPLIT
+	/* 82571 and greater support packet-split where the protocol
+	 * header is placed in skb->data and the packet data is
+	 * placed in pages hanging off of skb_shinfo(skb)->nr_frags.
+	 * In the case of a non-split, skb->data is linearly filled,
+	 * followed by the page buffers.  Therefore, skb->data is
+	 * sized to hold the largest protocol header.
+	 */
+	/* allocations using alloc_page take too long for regular MTU
+	 * so only enable packet split for jumbo frames */
+	pages = PAGE_USE_COUNT(adapter->netdev->mtu);
+	if ((adapter->hw.mac.type >= e1000_82571) && (pages <= 3) &&
+	    PAGE_SIZE <= 16384 && (rctl & E1000_RCTL_LPE))
+		adapter->rx_ps_pages = pages;
+	else
+		adapter->rx_ps_pages = 0;
+#endif
+	if (adapter->rx_ps_pages) {
+		/* Configure extra packet-split registers */
+		rfctl = E1000_READ_REG(&adapter->hw, E1000_RFCTL);
+		rfctl |= E1000_RFCTL_EXTEN;
+		/* disable packet split support for IPv6 extension headers,
+		 * because some malformed IPv6 headers can hang the RX */
+		rfctl |= (E1000_RFCTL_IPV6_EX_DIS |
+		          E1000_RFCTL_NEW_IPV6_EXT_DIS);
+
+		E1000_WRITE_REG(&adapter->hw, E1000_RFCTL, rfctl);
+
+		/* disable the stripping of CRC because it breaks
+		 * BMC firmware connected over SMBUS */
+		rctl |= E1000_RCTL_DTYP_PS /* | E1000_RCTL_SECRC */;
+
+		psrctl |= adapter->rx_ps_bsize0 >>
+			E1000_PSRCTL_BSIZE0_SHIFT;
+
+		switch (adapter->rx_ps_pages) {
+		case 3:
+			psrctl |= PAGE_SIZE <<
+				E1000_PSRCTL_BSIZE3_SHIFT;
+		case 2:
+			psrctl |= PAGE_SIZE <<
+				E1000_PSRCTL_BSIZE2_SHIFT;
+		case 1:
+			psrctl |= PAGE_SIZE >>
+				E1000_PSRCTL_BSIZE1_SHIFT;
+			break;
+		}
+
+		E1000_WRITE_REG(&adapter->hw, E1000_PSRCTL, psrctl);
 	}
 
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
 }
 
 /**
@@ -1004,103 +2421,209 @@
  *
  * Configure the Rx unit of the MAC after a reset.
  **/
-
-static void
-e1000_configure_rx(struct e1000_adapter *adapter)
+static void e1000_configure_rx(struct e1000_adapter *adapter)
 {
-	uint64_t rdba = adapter->rx_ring.dma;
-	uint32_t rdlen = adapter->rx_ring.count * sizeof(struct e1000_rx_desc);
-	uint32_t rctl;
-	uint32_t rxcsum;
+	u64 rdba;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rdlen, rctl, rxcsum, ctrl_ext;
+	int i;
+
+	if (adapter->rx_ps_pages) {
+		/* this is a 32 byte descriptor */
+		rdlen = adapter->rx_ring[0].count *
+			sizeof(union e1000_rx_desc_packet_split);
+		adapter->clean_rx = e1000_clean_rx_irq_ps;
+		adapter->alloc_rx_buf = e1000_alloc_rx_buffers_ps;
+#ifdef CONFIG_E1000_NAPI
+	} else if (adapter->netdev->mtu > MAXIMUM_ETHERNET_VLAN_SIZE) {
+		rdlen = adapter->rx_ring[0].count *
+		        sizeof(struct e1000_rx_desc);
+		adapter->clean_rx = e1000_clean_jumbo_rx_irq;
+		adapter->alloc_rx_buf = e1000_alloc_jumbo_rx_buffers;
+#endif
+	} else {
+		rdlen = adapter->rx_ring[0].count *
+			sizeof(struct e1000_rx_desc);
+		adapter->clean_rx = e1000_clean_rx_irq;
+		adapter->alloc_rx_buf = e1000_alloc_rx_buffers;
+	}
 
 	/* disable receives while setting up the descriptors */
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl & ~E1000_RCTL_EN);
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+	E1000_WRITE_FLUSH(hw);
+	mdelay(10);
 
 	/* set the Receive Delay Timer Register */
-	E1000_WRITE_REG(&adapter->hw, RDTR, adapter->rx_int_delay);
+	E1000_WRITE_REG(hw, E1000_RDTR, adapter->rx_int_delay);
 
-	if(adapter->hw.mac_type >= e1000_82540) {
-		E1000_WRITE_REG(&adapter->hw, RADV, adapter->rx_abs_int_delay);
-		if(adapter->itr > 1)
-			E1000_WRITE_REG(&adapter->hw, ITR,
+	if (adapter->flags.has_intr_moderation) {
+		E1000_WRITE_REG(hw, E1000_RADV, adapter->rx_abs_int_delay);
+		if (adapter->itr_setting != 0)
+			E1000_WRITE_REG(hw, E1000_ITR,
 				1000000000 / (adapter->itr * 256));
 	}
 
-	/* Setup the Base and Length of the Rx Descriptor Ring */
-	E1000_WRITE_REG(&adapter->hw, RDBAL, (rdba & 0x00000000ffffffffULL));
-	E1000_WRITE_REG(&adapter->hw, RDBAH, (rdba >> 32));
-
-	E1000_WRITE_REG(&adapter->hw, RDLEN, rdlen);
-
-	/* Setup the HW Rx Head and Tail Descriptor Pointers */
-	E1000_WRITE_REG(&adapter->hw, RDH, 0);
-	E1000_WRITE_REG(&adapter->hw, RDT, 0);
-
-	/* Enable 82543 Receive Checksum Offload for TCP and UDP */
-	if((adapter->hw.mac_type >= e1000_82543) &&
-	   (adapter->rx_csum == TRUE)) {
-		rxcsum = E1000_READ_REG(&adapter->hw, RXCSUM);
-		rxcsum |= E1000_RXCSUM_TUOFL;
-		E1000_WRITE_REG(&adapter->hw, RXCSUM, rxcsum);
+	if (hw->mac.type >= e1000_82571) {
+		ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		/* Reset delay timers after every interrupt */
+		ctrl_ext |= E1000_CTRL_EXT_INT_TIMER_CLR;
+#ifdef CONFIG_E1000_NAPI
+		/* Auto-Mask interrupts upon ICR access */
+		ctrl_ext |= E1000_CTRL_EXT_IAME;
+		E1000_WRITE_REG(hw, E1000_IAM, 0xffffffff);
+#endif
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	/* Setup the HW Rx Head and Tail Descriptor Pointers and
+	 * the Base and Length of the Rx Descriptor Ring */
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		rdba = adapter->rx_ring[i].dma;
+		E1000_WRITE_REG(hw, E1000_RDBAL_REG(i), (rdba & 0x00000000ffffffffULL));
+		E1000_WRITE_REG(hw, E1000_RDBAH_REG(i), (rdba >> 32));
+		E1000_WRITE_REG(hw, E1000_RDLEN_REG(i), rdlen);
+		E1000_WRITE_REG(hw, E1000_RDH_REG(i), 0);
+		E1000_WRITE_REG(hw, E1000_RDT_REG(i), 0);
+		adapter->rx_ring[i].rdh = E1000_REGISTER(hw, E1000_RDH_REG(i));
+		adapter->rx_ring[i].rdt = E1000_REGISTER(hw, E1000_RDT_REG(i));
+	}
+
+#ifdef CONFIG_E1000_MQ
+	if (adapter->num_rx_queues > 1) {
+		u32 random[10];
+		u32 reta, mrqc;
+		int i;
+
+		get_random_bytes(&random[0], 40);
+
+		switch (adapter->num_rx_queues) {
+		default:
+			reta = 0x00800080;
+			mrqc = E1000_MRQC_ENABLE_RSS_2Q;
+			break;
+		}
+
+		/* Fill out redirection table */
+		for (i = 0; i < 32; i++)
+			E1000_WRITE_REG_ARRAY(hw, E1000_RETA, i, reta);
+		/* Fill out hash function seeds */
+		for (i = 0; i < 10; i++)
+			E1000_WRITE_REG_ARRAY(hw, E1000_RSSRK, i, random[i]);
+
+		mrqc |= (E1000_MRQC_RSS_FIELD_IPV4 |
+			 E1000_MRQC_RSS_FIELD_IPV4_TCP);
+
+		E1000_WRITE_REG(hw, E1000_MRQC, mrqc);
+
+		/* Multiqueue and packet checksumming are mutually exclusive. */
+		rxcsum = E1000_READ_REG(hw, E1000_RXCSUM);
+		rxcsum |= E1000_RXCSUM_PCSD;
+		E1000_WRITE_REG(hw, E1000_RXCSUM, rxcsum);
+	} else if (hw->mac.type >= e1000_82543) {
+#else
+	if (hw->mac.type >= e1000_82543) {
+#endif /* CONFIG_E1000_MQ */
+		/* Enable 82543 Receive Checksum Offload for TCP and UDP */
+		rxcsum = E1000_READ_REG(hw, E1000_RXCSUM);
+		if (adapter->rx_csum == TRUE) {
+			rxcsum |= E1000_RXCSUM_TUOFL;
+
+			/* Enable 82571 IPv4 payload checksum for UDP fragments
+			 * Must be used in conjunction with packet-split. */
+			if ((hw->mac.type >= e1000_82571) &&
+			    (adapter->rx_ps_pages)) {
+				rxcsum |= E1000_RXCSUM_IPPCSE;
+			}
+		} else {
+			rxcsum &= ~E1000_RXCSUM_TUOFL;
+			/* don't need to clear IPPCSE as it defaults to 0 */
+		}
+		E1000_WRITE_REG(hw, E1000_RXCSUM, rxcsum);
 	}
 
+	/* Enable early receives on supported devices, only takes effect when
+	 * packet size is equal or larger than the specified value (in 8 byte
+	 * units), e.g. using jumbo frames when setting to E1000_ERT_2048 */
+	if ((hw->mac.type == e1000_82573 || hw->mac.type == e1000_ich9lan) &&
+	    (adapter->netdev->mtu > ETH_DATA_LEN))
+		E1000_WRITE_REG(hw, E1000_ERT, E1000_ERT_2048);
+
 	/* Enable Receives */
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl);
 }
 
 /**
- * e1000_free_tx_resources - Free Tx Resources
+ * e1000_free_tx_resources - Free Tx Resources per Queue
  * @adapter: board private structure
+ * @tx_ring: Tx descriptor ring for a specific queue
  *
  * Free all transmit software resources
  **/
-
-void
-e1000_free_tx_resources(struct e1000_adapter *adapter)
+static void e1000_free_tx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring)
 {
 	struct pci_dev *pdev = adapter->pdev;
 
-	e1000_clean_tx_ring(adapter);
+	e1000_clean_tx_ring(adapter, tx_ring);
 
-	vfree(adapter->tx_ring.buffer_info);
-	adapter->tx_ring.buffer_info = NULL;
+	vfree(tx_ring->buffer_info);
+	tx_ring->buffer_info = NULL;
 
-	pci_free_consistent(pdev, adapter->tx_ring.size,
-	                    adapter->tx_ring.desc, adapter->tx_ring.dma);
+	pci_free_consistent(pdev, tx_ring->size, tx_ring->desc, tx_ring->dma);
 
-	adapter->tx_ring.desc = NULL;
+	tx_ring->desc = NULL;
 }
 
 /**
- * e1000_clean_tx_ring - Free Tx Buffers
+ * e1000_free_all_tx_resources - Free Tx Resources for All Queues
  * @adapter: board private structure
+ *
+ * Free all transmit software resources
  **/
+void e1000_free_all_tx_resources(struct e1000_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		e1000_free_tx_resources(adapter, &adapter->tx_ring[i]);
+}
+
+static void e1000_unmap_and_free_tx_resource(struct e1000_adapter *adapter,
+                                             struct e1000_buffer *buffer_info)
+{
+	if (buffer_info->dma) {
+		pci_unmap_page(adapter->pdev,
+				buffer_info->dma,
+				buffer_info->length,
+				PCI_DMA_TODEVICE);
+		buffer_info->dma = 0;
+	}
+	if (buffer_info->skb) {
+		dev_kfree_skb_any(buffer_info->skb);
+		buffer_info->skb = NULL;
+	}
+	/* buffer_info must be completely set up in the transmit path */
+}
 
-static void
-e1000_clean_tx_ring(struct e1000_adapter *adapter)
+/**
+ * e1000_clean_tx_ring - Free Tx Buffers
+ * @adapter: board private structure
+ * @tx_ring: ring to be cleaned
+ **/
+static void e1000_clean_tx_ring(struct e1000_adapter *adapter,
+                                struct e1000_tx_ring *tx_ring)
 {
-	struct e1000_desc_ring *tx_ring = &adapter->tx_ring;
 	struct e1000_buffer *buffer_info;
-	struct pci_dev *pdev = adapter->pdev;
 	unsigned long size;
 	unsigned int i;
 
 	/* Free all the Tx ring sk_buffs */
 
-	for(i = 0; i < tx_ring->count; i++) {
+	for (i = 0; i < tx_ring->count; i++) {
 		buffer_info = &tx_ring->buffer_info[i];
-		if(buffer_info->skb) {
-
-			pci_unmap_page(pdev,
-				       buffer_info->dma,
-				       buffer_info->length,
-				       PCI_DMA_TODEVICE);
-
-			dev_kfree_skb(buffer_info->skb);
-
-			buffer_info->skb = NULL;
-		}
+		e1000_unmap_and_free_tx_resource(adapter, buffer_info);
 	}
 
 	size = sizeof(struct e1000_buffer) * tx_ring->count;
@@ -1112,28 +2635,44 @@
 
 	tx_ring->next_to_use = 0;
 	tx_ring->next_to_clean = 0;
+	tx_ring->last_tx_tso = 0;
+
+	writel(0, adapter->hw.hw_addr + tx_ring->tdh);
+	writel(0, adapter->hw.hw_addr + tx_ring->tdt);
+}
+
+/**
+ * e1000_clean_all_tx_rings - Free Tx Buffers for all queues
+ * @adapter: board private structure
+ **/
+static void e1000_clean_all_tx_rings(struct e1000_adapter *adapter)
+{
+	int i;
 
-	E1000_WRITE_REG(&adapter->hw, TDH, 0);
-	E1000_WRITE_REG(&adapter->hw, TDT, 0);
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		e1000_clean_tx_ring(adapter, &adapter->tx_ring[i]);
 }
 
 /**
  * e1000_free_rx_resources - Free Rx Resources
  * @adapter: board private structure
+ * @rx_ring: ring to clean the resources from
  *
  * Free all receive software resources
  **/
-
-void
-e1000_free_rx_resources(struct e1000_adapter *adapter)
+static void e1000_free_rx_resources(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring)
 {
-	struct e1000_desc_ring *rx_ring = &adapter->rx_ring;
 	struct pci_dev *pdev = adapter->pdev;
 
-	e1000_clean_rx_ring(adapter);
+	e1000_clean_rx_ring(adapter, rx_ring);
 
 	vfree(rx_ring->buffer_info);
 	rx_ring->buffer_info = NULL;
+	kfree(rx_ring->ps_page);
+	rx_ring->ps_page = NULL;
+	kfree(rx_ring->ps_page_dma);
+	rx_ring->ps_page_dma = NULL;
 
 	pci_free_consistent(pdev, rx_ring->size, rx_ring->desc, rx_ring->dma);
 
@@ -1141,37 +2680,90 @@
 }
 
 /**
- * e1000_clean_rx_ring - Free Rx Buffers
+ * e1000_free_all_rx_resources - Free Rx Resources for All Queues
  * @adapter: board private structure
+ *
+ * Free all receive software resources
  **/
+void e1000_free_all_rx_resources(struct e1000_adapter *adapter)
+{
+	int i;
 
-static void
-e1000_clean_rx_ring(struct e1000_adapter *adapter)
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		e1000_free_rx_resources(adapter, &adapter->rx_ring[i]);
+}
+
+/**
+ * e1000_clean_rx_ring - Free Rx Buffers per Queue
+ * @adapter: board private structure
+ * @rx_ring: ring to free buffers from
+ **/
+static void e1000_clean_rx_ring(struct e1000_adapter *adapter,
+                                struct e1000_rx_ring *rx_ring)
 {
-	struct e1000_desc_ring *rx_ring = &adapter->rx_ring;
-	struct e1000_buffer *buffer_info;
+	struct e1000_rx_buffer *buffer_info;
+	struct e1000_ps_page *ps_page;
+	struct e1000_ps_page_dma *ps_page_dma;
 	struct pci_dev *pdev = adapter->pdev;
 	unsigned long size;
-	unsigned int i;
+	unsigned int i, j;
 
 	/* Free all the Rx ring sk_buffs */
-
-	for(i = 0; i < rx_ring->count; i++) {
+	for (i = 0; i < rx_ring->count; i++) {
 		buffer_info = &rx_ring->buffer_info[i];
-		if(buffer_info->skb) {
-
-			pci_unmap_single(pdev,
-					 buffer_info->dma,
-					 buffer_info->length,
-					 PCI_DMA_FROMDEVICE);
-
+		if (buffer_info->dma &&
+		    adapter->clean_rx == e1000_clean_rx_irq) {
+			pci_unmap_single(pdev, buffer_info->dma,
+			                 adapter->rx_buffer_len,
+			                 PCI_DMA_FROMDEVICE);
+#ifdef CONFIG_E1000_NAPI
+		} else if (buffer_info->dma &&
+		           adapter->clean_rx == e1000_clean_jumbo_rx_irq) {
+			pci_unmap_page(pdev, buffer_info->dma, PAGE_SIZE,
+			               PCI_DMA_FROMDEVICE);
+#endif
+		} else if (buffer_info->dma &&
+		           adapter->clean_rx == e1000_clean_rx_irq_ps) {
+			pci_unmap_single(pdev, buffer_info->dma,
+			                 adapter->rx_ps_bsize0,
+			                 PCI_DMA_FROMDEVICE);
+		}
+		buffer_info->dma = 0;
+		if (buffer_info->page) {
+			put_page(buffer_info->page);
+			buffer_info->page = NULL;
+		}
+		if (buffer_info->skb) {
 			dev_kfree_skb(buffer_info->skb);
 			buffer_info->skb = NULL;
 		}
+		ps_page = &rx_ring->ps_page[i];
+		ps_page_dma = &rx_ring->ps_page_dma[i];
+		for (j = 0; j < adapter->rx_ps_pages; j++) {
+			if (!ps_page->ps_page[j]) break;
+			pci_unmap_page(pdev,
+				       ps_page_dma->ps_page_dma[j],
+				       PAGE_SIZE, PCI_DMA_FROMDEVICE);
+			ps_page_dma->ps_page_dma[j] = 0;
+			put_page(ps_page->ps_page[j]);
+			ps_page->ps_page[j] = NULL;
+		}
+	}
+
+#ifdef CONFIG_E1000_NAPI
+	/* there also may be some cached data from a chained receive */
+	if (rx_ring->rx_skb_top) {
+		dev_kfree_skb(rx_ring->rx_skb_top);
+		rx_ring->rx_skb_top = NULL;
 	}
+#endif
 
-	size = sizeof(struct e1000_buffer) * rx_ring->count;
+	size = sizeof(struct e1000_rx_buffer) * rx_ring->count;
 	memset(rx_ring->buffer_info, 0, size);
+	size = sizeof(struct e1000_ps_page) * rx_ring->count;
+	memset(rx_ring->ps_page, 0, size);
+	size = sizeof(struct e1000_ps_page_dma) * rx_ring->count;
+	memset(rx_ring->ps_page_dma, 0, size);
 
 	/* Zero out the descriptor ring */
 
@@ -1180,49 +2772,71 @@
 	rx_ring->next_to_clean = 0;
 	rx_ring->next_to_use = 0;
 
-	E1000_WRITE_REG(&adapter->hw, RDH, 0);
-	E1000_WRITE_REG(&adapter->hw, RDT, 0);
+	writel(0, adapter->hw.hw_addr + rx_ring->rdh);
+	writel(0, adapter->hw.hw_addr + rx_ring->rdt);
+}
+
+/**
+ * e1000_clean_all_rx_rings - Free Rx Buffers for all queues
+ * @adapter: board private structure
+ **/
+static void e1000_clean_all_rx_rings(struct e1000_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		e1000_clean_rx_ring(adapter, &adapter->rx_ring[i]);
 }
 
 /* The 82542 2.0 (revision 2) needs to have the receive unit in reset
  * and memory write and invalidate disabled for certain operations
  */
-static void
-e1000_enter_82542_rst(struct e1000_adapter *adapter)
+static void e1000_enter_82542_rst(struct e1000_adapter *adapter)
 {
 	struct net_device *netdev = adapter->netdev;
-	uint32_t rctl;
+	u32 rctl;
+
+	if (adapter->hw.mac.type != e1000_82542)
+		return;
+	if (adapter->hw.revision_id != E1000_REVISION_2)
+		return;
 
 	e1000_pci_clear_mwi(&adapter->hw);
 
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
+	rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
 	rctl |= E1000_RCTL_RST;
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
 	E1000_WRITE_FLUSH(&adapter->hw);
 	mdelay(5);
 
-	if(netif_running(netdev))
-		e1000_clean_rx_ring(adapter);
+	if (netif_running(netdev))
+		e1000_clean_all_rx_rings(adapter);
 }
 
-static void
-e1000_leave_82542_rst(struct e1000_adapter *adapter)
+static void e1000_leave_82542_rst(struct e1000_adapter *adapter)
 {
 	struct net_device *netdev = adapter->netdev;
-	uint32_t rctl;
+	u32 rctl;
 
-	rctl = E1000_READ_REG(&adapter->hw, RCTL);
+	if (adapter->hw.mac.type != e1000_82542)
+		return;
+	if (adapter->hw.revision_id != E1000_REVISION_2)
+		return;
+
+	rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
 	rctl &= ~E1000_RCTL_RST;
-	E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+	E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
 	E1000_WRITE_FLUSH(&adapter->hw);
 	mdelay(5);
 
-	if(adapter->hw.pci_cmd_word & PCI_COMMAND_INVALIDATE)
+	if (adapter->hw.bus.pci_cmd_word & PCI_COMMAND_INVALIDATE)
 		e1000_pci_set_mwi(&adapter->hw);
 
-	if(netif_running(netdev)) {
+	if (netif_running(netdev)) {
+		/* No need to loop, because 82542 supports only 1 queue */
+		struct e1000_rx_ring *ring = &adapter->rx_ring[0];
 		e1000_configure_rx(adapter);
-		e1000_alloc_rx_buffers(adapter);
+		adapter->alloc_rx_buf(adapter, ring, E1000_DESC_UNUSED(ring));
 	}
 }
 
@@ -1233,27 +2847,42 @@
  *
  * Returns 0 on success, negative on failure
  **/
-
-static int
-e1000_set_mac(struct net_device *netdev, void *p)
+static int e1000_set_mac(struct net_device *netdev, void *p)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct sockaddr *addr = p;
 
-	if(!is_valid_ether_addr(addr->sa_data))
+	if (!is_valid_ether_addr(addr->sa_data))
 		return -EADDRNOTAVAIL;
 
 	/* 82542 2.0 needs to be in reset to write receive address registers */
 
-	if(adapter->hw.mac_type == e1000_82542_rev2_0)
+	if (adapter->hw.mac.type == e1000_82542)
 		e1000_enter_82542_rst(adapter);
 
 	memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
-	memcpy(adapter->hw.mac_addr, addr->sa_data, netdev->addr_len);
+	memcpy(adapter->hw.mac.addr, addr->sa_data, netdev->addr_len);
+
+	e1000_rar_set(&adapter->hw, adapter->hw.mac.addr, 0);
 
-	e1000_rar_set(&adapter->hw, adapter->hw.mac_addr, 0);
+	/* With 82571 controllers, LAA may be overwritten (with the default)
+	 * due to controller reset from the other port. */
+	if (adapter->hw.mac.type == e1000_82571) {
+		/* activate the work around */
+		e1000_set_laa_state_82571(&adapter->hw, TRUE);
+
+		/* Hold a copy of the LAA in RAR[14] This is done so that
+		 * between the time RAR[0] gets clobbered  and the time it
+		 * gets fixed (in e1000_watchdog), the actual LAA is in one
+		 * of the RARs and no incoming packets directed to this port
+		 * are dropped. Eventually the LAA will be in RAR[0] and
+		 * RAR[14] */
+		e1000_rar_set(&adapter->hw,
+		              adapter->hw.mac.addr,
+		              adapter->hw.mac.rar_entry_count - 1);
+	}
 
-	if(adapter->hw.mac_type == e1000_82542_rev2_0)
+	if (adapter->hw.mac.type == e1000_82542)
 		e1000_leave_82542_rst(adapter);
 
 	return 0;
@@ -1268,182 +2897,285 @@
  * responsible for configuring the hardware for proper multicast,
  * promiscuous mode, and all-multi behavior.
  **/
-
-static void
-e1000_set_multi(struct net_device *netdev)
+static void e1000_set_multi(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
+	struct e1000_mac_info *mac = &hw->mac;
 	struct dev_mc_list *mc_ptr;
-	uint32_t rctl;
-	uint32_t hash_value;
+	u8  *mta_list;
+	u32 rctl;
 	int i;
-	unsigned long flags;
 
 	/* Check for Promiscuous and All Multicast modes */
 
-	spin_lock_irqsave(&adapter->tx_lock, flags);
-
-	rctl = E1000_READ_REG(hw, RCTL);
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
 
-	if(netdev->flags & IFF_PROMISC) {
+	if (netdev->flags & IFF_PROMISC) {
 		rctl |= (E1000_RCTL_UPE | E1000_RCTL_MPE);
-	} else if(netdev->flags & IFF_ALLMULTI) {
+	} else if (netdev->flags & IFF_ALLMULTI) {
 		rctl |= E1000_RCTL_MPE;
 		rctl &= ~E1000_RCTL_UPE;
 	} else {
 		rctl &= ~(E1000_RCTL_UPE | E1000_RCTL_MPE);
 	}
 
-	E1000_WRITE_REG(hw, RCTL, rctl);
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl);
 
 	/* 82542 2.0 needs to be in reset to write receive address registers */
 
-	if(hw->mac_type == e1000_82542_rev2_0)
+	if (hw->mac.type == e1000_82542)
 		e1000_enter_82542_rst(adapter);
 
-	/* load the first 14 multicast address into the exact filters 1-14
-	 * RAR 0 is used for the station MAC adddress
-	 * if there are not 14 addresses, go ahead and clear the filters
-	 */
+	mta_list = kmalloc(netdev->mc_count * 6, GFP_ATOMIC);
+	if (!mta_list)
+		return;
+
+	/* The shared function expects a packed array of only addresses. */
 	mc_ptr = netdev->mc_list;
 
-	for(i = 1; i < E1000_RAR_ENTRIES; i++) {
-		if(mc_ptr) {
-			e1000_rar_set(hw, mc_ptr->dmi_addr, i);
-			mc_ptr = mc_ptr->next;
-		} else {
-			E1000_WRITE_REG_ARRAY(hw, RA, i << 1, 0);
-			E1000_WRITE_REG_ARRAY(hw, RA, (i << 1) + 1, 0);
-		}
+	for (i = 0; i < netdev->mc_count; i++) {
+		if (!mc_ptr)
+			break;
+		memcpy(mta_list + (i*ETH_ALEN), mc_ptr->dmi_addr, ETH_ALEN);
+		mc_ptr = mc_ptr->next;
 	}
 
-	/* clear the old settings from the multicast hash table */
+	e1000_mc_addr_list_update(hw, mta_list, i, 1, mac->rar_entry_count);
 
-	for(i = 0; i < E1000_NUM_MTA_REGISTERS; i++)
-		E1000_WRITE_REG_ARRAY(hw, MTA, i, 0);
+	kfree(mta_list);
 
-	/* load any remaining addresses into the hash table */
-
-	for(; mc_ptr; mc_ptr = mc_ptr->next) {
-		hash_value = e1000_hash_mc_addr(hw, mc_ptr->dmi_addr);
-		e1000_mta_set(hw, hash_value);
-	}
-
-	if(hw->mac_type == e1000_82542_rev2_0)
+	if (hw->mac.type == e1000_82542)
 		e1000_leave_82542_rst(adapter);
-
-	spin_unlock_irqrestore(&adapter->tx_lock, flags);
 }
 
 /* Need to wait a few seconds after link up to get diagnostic information from
  * the phy */
-
-static void
-e1000_update_phy_info(unsigned long data)
+static void e1000_update_phy_info(unsigned long data)
 {
 	struct e1000_adapter *adapter = (struct e1000_adapter *) data;
-	e1000_phy_get_info(&adapter->hw, &adapter->phy_info);
+	e1000_get_phy_info(&adapter->hw);
 }
 
 /**
  * e1000_82547_tx_fifo_stall - Timer Call-back
  * @data: pointer to adapter cast into an unsigned long
  **/
-
-static void
-e1000_82547_tx_fifo_stall(unsigned long data)
+static void e1000_82547_tx_fifo_stall(unsigned long data)
 {
 	struct e1000_adapter *adapter = (struct e1000_adapter *) data;
 	struct net_device *netdev = adapter->netdev;
-	uint32_t tctl;
+	u32 tctl;
 
-	if(atomic_read(&adapter->tx_fifo_stall)) {
-		if((E1000_READ_REG(&adapter->hw, TDT) ==
-		    E1000_READ_REG(&adapter->hw, TDH)) &&
-		   (E1000_READ_REG(&adapter->hw, TDFT) ==
-		    E1000_READ_REG(&adapter->hw, TDFH)) &&
-		   (E1000_READ_REG(&adapter->hw, TDFTS) ==
-		    E1000_READ_REG(&adapter->hw, TDFHS))) {
-			tctl = E1000_READ_REG(&adapter->hw, TCTL);
-			E1000_WRITE_REG(&adapter->hw, TCTL,
+	if (atomic_read(&adapter->tx_fifo_stall)) {
+		if ((E1000_READ_REG(&adapter->hw, E1000_TDT) ==
+		    E1000_READ_REG(&adapter->hw, E1000_TDH)) &&
+		   (E1000_READ_REG(&adapter->hw, E1000_TDFT) ==
+		    E1000_READ_REG(&adapter->hw, E1000_TDFH)) &&
+		   (E1000_READ_REG(&adapter->hw, E1000_TDFTS) ==
+		    E1000_READ_REG(&adapter->hw, E1000_TDFHS))) {
+			tctl = E1000_READ_REG(&adapter->hw, E1000_TCTL);
+			E1000_WRITE_REG(&adapter->hw, E1000_TCTL,
 					tctl & ~E1000_TCTL_EN);
-			E1000_WRITE_REG(&adapter->hw, TDFT,
+			E1000_WRITE_REG(&adapter->hw, E1000_TDFT,
 					adapter->tx_head_addr);
-			E1000_WRITE_REG(&adapter->hw, TDFH,
+			E1000_WRITE_REG(&adapter->hw, E1000_TDFH,
 					adapter->tx_head_addr);
-			E1000_WRITE_REG(&adapter->hw, TDFTS,
+			E1000_WRITE_REG(&adapter->hw, E1000_TDFTS,
 					adapter->tx_head_addr);
-			E1000_WRITE_REG(&adapter->hw, TDFHS,
+			E1000_WRITE_REG(&adapter->hw, E1000_TDFHS,
 					adapter->tx_head_addr);
-			E1000_WRITE_REG(&adapter->hw, TCTL, tctl);
+			E1000_WRITE_REG(&adapter->hw, E1000_TCTL, tctl);
 			E1000_WRITE_FLUSH(&adapter->hw);
 
 			adapter->tx_fifo_head = 0;
 			atomic_set(&adapter->tx_fifo_stall, 0);
 			netif_wake_queue(netdev);
-		} else {
+		} else if (!test_bit(__E1000_DOWN, &adapter->state))
 			mod_timer(&adapter->tx_fifo_stall_timer, jiffies + 1);
-		}
 	}
 }
 
 /**
  * e1000_watchdog - Timer Call-back
- * @data: pointer to netdev cast into an unsigned long
+ * @data: pointer to adapter cast into an unsigned long
  **/
-
-static void
-e1000_watchdog(unsigned long data)
+static void e1000_watchdog(unsigned long data)
 {
 	struct e1000_adapter *adapter = (struct e1000_adapter *) data;
-	struct net_device *netdev = adapter->netdev;
-	struct e1000_desc_ring *txdr = &adapter->tx_ring;
-	unsigned int i;
-	uint32_t link;
 
-	e1000_check_for_link(&adapter->hw);
+	/* Do the rest outside of interrupt context */
+	schedule_work(&adapter->watchdog_task);
+}
+
+static void e1000_watchdog_task(struct work_struct *work)
+{
+	struct e1000_adapter *adapter = container_of(work,
+	                                struct e1000_adapter, watchdog_task);
+
+	struct net_device *netdev = adapter->netdev;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+	struct e1000_tx_ring *tx_ring;
+	u32 link, tctl;
+	s32 ret_val;
+	int i, tx_pending = 0;
+
+	if ((netif_carrier_ok(netdev)) &&
+	    (E1000_READ_REG(&adapter->hw, E1000_STATUS) & E1000_STATUS_LU))
+		goto link_up;
+
+	ret_val = e1000_check_for_link(&adapter->hw);
+	if ((ret_val == E1000_ERR_PHY) &&
+	    (adapter->hw.phy.type == e1000_phy_igp_3) &&
+	    (E1000_READ_REG(&adapter->hw, E1000_CTRL) & E1000_PHY_CTRL_GBE_DISABLE)) {
+		/* See e1000_kmrn_lock_loss_workaround_ich8lan() */
+		DPRINTK(LINK, INFO,
+			"Gigabit has been disabled, downgrading speed\n");
+	}
+
+	if (mac->type == e1000_82573) {
+		e1000_enable_tx_pkt_filtering(&adapter->hw);
+#ifdef NETIF_F_HW_VLAN_TX
+		if (adapter->mng_vlan_id != adapter->hw.mng_cookie.vlan_id)
+			e1000_update_mng_vlan(adapter);
+#endif
+	}
 
-	if((adapter->hw.media_type == e1000_media_type_internal_serdes) &&
-	   !(E1000_READ_REG(&adapter->hw, TXCW) & E1000_TXCW_ANE))
-		link = !adapter->hw.serdes_link_down;
+	if ((adapter->hw.media_type == e1000_media_type_internal_serdes) &&
+	   !(E1000_READ_REG(&adapter->hw, E1000_TXCW) & E1000_TXCW_ANE))
+		link = adapter->hw.mac.serdes_has_link;
 	else
-		link = E1000_READ_REG(&adapter->hw, STATUS) & E1000_STATUS_LU;
+		link = E1000_READ_REG(&adapter->hw, E1000_STATUS) & E1000_STATUS_LU;
 
-	if(link) {
-		if(!netif_carrier_ok(netdev)) {
+	if (link) {
+		if (!netif_carrier_ok(netdev)) {
+			u32 ctrl;
+			boolean_t txb2b = 1;
 			e1000_get_speed_and_duplex(&adapter->hw,
 			                           &adapter->link_speed,
 			                           &adapter->link_duplex);
 
-			DPRINTK(LINK, INFO, "NIC Link is Up %d Mbps %s\n",
-			       adapter->link_speed,
-			       adapter->link_duplex == FULL_DUPLEX ?
-			       "Full Duplex" : "Half Duplex");
+			ctrl = E1000_READ_REG(&adapter->hw, E1000_CTRL);
+			DPRINTK(LINK, INFO, "NIC Link is Up %d Mbps %s, "
+			        "Flow Control: %s\n",
+			        adapter->link_speed,
+			        adapter->link_duplex == FULL_DUPLEX ?
+			        "Full Duplex" : "Half Duplex",
+			        ((ctrl & E1000_CTRL_TFCE) && (ctrl &
+			        E1000_CTRL_RFCE)) ? "RX/TX" : ((ctrl &
+			        E1000_CTRL_RFCE) ? "RX" : ((ctrl &
+			        E1000_CTRL_TFCE) ? "TX" : "None" )));
+
+			/* tweak tx_queue_len according to speed/duplex
+			 * and adjust the timeout factor */
+			netdev->tx_queue_len = adapter->tx_queue_len;
+			adapter->tx_timeout_factor = 1;
+			switch (adapter->link_speed) {
+			case SPEED_10:
+				txb2b = 0;
+				netdev->tx_queue_len = 10;
+				adapter->tx_timeout_factor = 14;
+				break;
+			case SPEED_100:
+				txb2b = 0;
+				netdev->tx_queue_len = 100;
+				/* maybe add some timeout factor ? */
+				break;
+			}
+
+			if ((mac->type == e1000_82571 ||
+			     mac->type == e1000_82572) &&
+			    txb2b == 0) {
+				u32 tarc0;
+				tarc0 = E1000_READ_REG(&adapter->hw, E1000_TARC0);
+				tarc0 &= ~SPEED_MODE_BIT;
+				E1000_WRITE_REG(&adapter->hw, E1000_TARC0, tarc0);
+			}
+
+#ifdef NETIF_F_TSO
+			/* disable TSO for pcie and 10/100 speeds, to avoid
+			 * some hardware issues */
+			if (!adapter->flags.tso_force &&
+			    adapter->hw.bus.type == e1000_bus_type_pci_express){
+				switch (adapter->link_speed) {
+				case SPEED_10:
+				case SPEED_100:
+					DPRINTK(PROBE,INFO,
+				        "10/100 speed: disabling TSO\n");
+					netdev->features &= ~NETIF_F_TSO;
+#ifdef NETIF_F_TSO6
+					netdev->features &= ~NETIF_F_TSO6;
+#endif
+					break;
+				case SPEED_1000:
+					netdev->features |= NETIF_F_TSO;
+#ifdef NETIF_F_TSO6
+					netdev->features |= NETIF_F_TSO6;
+#endif
+					break;
+				default:
+					/* oops */
+					break;
+				}
+			}
+#endif
+
+			/* enable transmits in the hardware, need to do this
+			 * after setting TARC0 */
+			tctl = E1000_READ_REG(&adapter->hw, E1000_TCTL);
+			tctl |= E1000_TCTL_EN;
+			E1000_WRITE_REG(&adapter->hw, E1000_TCTL, tctl);
 
 			netif_carrier_on(netdev);
 			netif_wake_queue(netdev);
-			mod_timer(&adapter->phy_info_timer, jiffies + 2 * HZ);
-			adapter->smartspeed = 0;
+#ifdef CONFIG_E1000_MQ
+			if (netif_is_multiqueue(netdev))
+				for (i = 0; i < adapter->num_tx_queues; i++)
+					netif_wake_subqueue(netdev, i);
+#endif
+
+			if (!test_bit(__E1000_DOWN, &adapter->state))
+				mod_timer(&adapter->phy_info_timer,
+				          round_jiffies(jiffies + 2 * HZ));
+			adapter->smartspeed = 0;
+		} else {
+			/* make sure the receive unit is started */
+			if (mac->type == e1000_80003es2lan) {
+				struct e1000_hw *hw = &adapter->hw;
+				u32 rctl = E1000_READ_REG(hw, E1000_RCTL);
+				E1000_WRITE_REG(hw, E1000_RCTL, rctl | E1000_RCTL_EN);
+			}
 		}
 	} else {
-		if(netif_carrier_ok(netdev)) {
+		if (netif_carrier_ok(netdev)) {
 			adapter->link_speed = 0;
 			adapter->link_duplex = 0;
 			DPRINTK(LINK, INFO, "NIC Link is Down\n");
 			netif_carrier_off(netdev);
 			netif_stop_queue(netdev);
-			mod_timer(&adapter->phy_info_timer, jiffies + 2 * HZ);
+			if (!test_bit(__E1000_DOWN, &adapter->state))
+				mod_timer(&adapter->phy_info_timer,
+				          round_jiffies(jiffies + 2 * HZ));
+
+			/* 80003ES2LAN workaround--
+			 * For packet buffer work-around on link down event;
+			 * disable receives in the ISR and
+			 * reset device here in the watchdog
+			 */
+			if (adapter->hw.mac.type == e1000_80003es2lan)
+				/* reset device */
+				schedule_work(&adapter->reset_task);
 		}
 
 		e1000_smartspeed(adapter);
 	}
 
+link_up:
 	e1000_update_stats(adapter);
 
-	adapter->hw.tx_packet_delta = adapter->stats.tpt - adapter->tpt_old;
+	mac->tx_packet_delta = adapter->stats.tpt - adapter->tpt_old;
 	adapter->tpt_old = adapter->stats.tpt;
-	adapter->hw.collision_delta = adapter->stats.colc - adapter->colc_old;
+	mac->collision_delta = adapter->stats.colc - adapter->colc_old;
 	adapter->colc_old = adapter->stats.colc;
 
 	adapter->gorcl = adapter->stats.gorcl - adapter->gorcl_old;
@@ -1453,82 +3185,238 @@
 
 	e1000_update_adaptive(&adapter->hw);
 
-	if(!netif_carrier_ok(netdev)) {
-		if(E1000_DESC_UNUSED(txdr) + 1 < txdr->count) {
+	if (!netif_carrier_ok(netdev)) {
+		for (i = 0 ; i < adapter->num_tx_queues ; i++) {
+			tx_ring = &adapter->tx_ring[i];
+			tx_pending |= (E1000_DESC_UNUSED(tx_ring) + 1 <
+			                                       tx_ring->count);
+		}
+		if (tx_pending) {
 			/* We've lost link, so the controller stops DMA,
 			 * but we've got queued Tx work that's never going
 			 * to get done, so reset controller to flush Tx.
 			 * (Do the reset outside of interrupt context). */
-			schedule_work(&adapter->tx_timeout_task);
+			adapter->tx_timeout_count++;
+			schedule_work(&adapter->reset_task);
 		}
 	}
 
-	/* Dynamic mode for Interrupt Throttle Rate (ITR) */
-	if(adapter->hw.mac_type >= e1000_82540 && adapter->itr == 1) {
-		/* Symmetric Tx/Rx gets a reduced ITR=2000; Total
-		 * asymmetrical Tx or Rx gets ITR=8000; everyone
-		 * else is between 2000-8000. */
-		uint32_t goc = (adapter->gotcl + adapter->gorcl) / 10000;
-		uint32_t dif = (adapter->gotcl > adapter->gorcl ? 
-			adapter->gotcl - adapter->gorcl :
-			adapter->gorcl - adapter->gotcl) / 10000;
-		uint32_t itr = goc > 0 ? (dif * 6000 / goc + 2000) : 8000;
-		E1000_WRITE_REG(&adapter->hw, ITR, 1000000000 / (itr * 256));
-	}
-
 	/* Cause software interrupt to ensure rx ring is cleaned */
-	E1000_WRITE_REG(&adapter->hw, ICS, E1000_ICS_RXDMT0);
+	E1000_WRITE_REG(&adapter->hw, E1000_ICS, E1000_ICS_RXDMT0);
 
-	/* Early detection of hung controller */
-	i = txdr->next_to_clean;
-	if(txdr->buffer_info[i].dma &&
-	   time_after(jiffies, txdr->buffer_info[i].time_stamp + HZ) &&
-	   !(E1000_READ_REG(&adapter->hw, STATUS) & E1000_STATUS_TXOFF))
-		netif_stop_queue(netdev);
+	/* Force detection of hung controller every watchdog period */
+	adapter->detect_tx_hung = TRUE;
+
+	/* With 82571 controllers, LAA may be overwritten due to controller
+	 * reset from the other port. Set the appropriate LAA in RAR[0] */
+	if (e1000_get_laa_state_82571(&adapter->hw) == TRUE)
+		e1000_rar_set(&adapter->hw, adapter->hw.mac.addr, 0);
 
 	/* Reset the timer */
-	mod_timer(&adapter->watchdog_timer, jiffies + 2 * HZ);
+	if (!test_bit(__E1000_DOWN, &adapter->state))
+		mod_timer(&adapter->watchdog_timer, round_jiffies(jiffies + 2 * HZ));
+}
+
+enum latency_range {
+	lowest_latency = 0,
+	low_latency = 1,
+	bulk_latency = 2,
+	latency_invalid = 255
+};
+
+/**
+ * e1000_update_itr - update the dynamic ITR value based on statistics
+ * @adapter: pointer to adapter
+ * @itr_setting: current adapter->itr
+ * @packets: the number of packets during this measurement interval
+ * @bytes: the number of bytes during this measurement interval
+ *
+ *      Stores a new ITR value based on packets and byte
+ *      counts during the last interrupt.  The advantage of per interrupt
+ *      computation is faster updates and more accurate ITR for the current
+ *      traffic pattern.  Constants in this function were computed
+ *      based on theoretical maximum wire speed and thresholds were set based
+ *      on testing data as well as attempting to minimize response time
+ *      while increasing bulk throughput.
+ *      this functionality is controlled by the InterruptThrottleRate module
+ *      parameter (see e1000_param.c)
+ **/
+static unsigned int e1000_update_itr(struct e1000_adapter *adapter,
+                                     u16 itr_setting, int packets,
+                                     int bytes)
+{
+	unsigned int retval = itr_setting;
+
+	if (unlikely(!adapter->flags.has_intr_moderation))
+		goto update_itr_done;
+
+	if (packets == 0)
+		goto update_itr_done;
+
+	switch (itr_setting) {
+	case lowest_latency:
+		/* handle TSO and jumbo frames */
+		if (bytes/packets > 8000)
+			retval = bulk_latency;
+		else if ((packets < 5) && (bytes > 512)) {
+			retval = low_latency;
+		}
+		break;
+	case low_latency:  /* 50 usec aka 20000 ints/s */
+		if (bytes > 10000) {
+			/* this if handles the TSO accounting */
+			if (bytes/packets > 8000) {
+				retval = bulk_latency;
+			} else if ((packets < 10) || ((bytes/packets) > 1200)) {
+				retval = bulk_latency;
+			} else if ((packets > 35)) {
+				retval = lowest_latency;
+			}
+		} else if (bytes/packets > 2000) {
+			retval = bulk_latency;
+		} else if (packets <= 2 && bytes < 512) {
+			retval = lowest_latency;
+		}
+		break;
+	case bulk_latency: /* 250 usec aka 4000 ints/s */
+		if (bytes > 25000) {
+			if (packets > 35) {
+				retval = low_latency;
+			}
+		} else if (bytes < 6000) {
+			retval = low_latency;
+		}
+		break;
+	}
+
+update_itr_done:
+	return retval;
+}
+
+static void e1000_set_itr(struct e1000_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u16 current_itr;
+	u32 new_itr = adapter->itr;
+
+	if (unlikely(!adapter->flags.has_intr_moderation))
+		return;
+
+	/* for non-gigabit speeds, just fix the interrupt rate at 4000 */
+	if (unlikely(adapter->link_speed != SPEED_1000)) {
+		current_itr = 0;
+		new_itr = 4000;
+		goto set_itr_now;
+	}
+
+	adapter->tx_itr = e1000_update_itr(adapter,
+	                            adapter->tx_itr,
+	                            adapter->total_tx_packets,
+	                            adapter->total_tx_bytes);
+	/* conservative mode (itr 3) eliminates the lowest_latency setting */
+	if (adapter->itr_setting == 3 && adapter->tx_itr == lowest_latency)
+		adapter->tx_itr = low_latency;
+
+	adapter->rx_itr = e1000_update_itr(adapter,
+	                            adapter->rx_itr,
+	                            adapter->total_rx_packets,
+	                            adapter->total_rx_bytes);
+	/* conservative mode (itr 3) eliminates the lowest_latency setting */
+	if (adapter->itr_setting == 3 && adapter->rx_itr == lowest_latency)
+		adapter->rx_itr = low_latency;
+
+	current_itr = max(adapter->rx_itr, adapter->tx_itr);
+
+	switch (current_itr) {
+	/* counts and packets in update_itr are dependent on these numbers */
+	case lowest_latency:
+		new_itr = 70000;
+		break;
+	case low_latency:
+		new_itr = 20000; /* aka hwitr = ~200 */
+		break;
+	case bulk_latency:
+		new_itr = 4000;
+		break;
+	default:
+		break;
+	}
+
+set_itr_now:
+	if (new_itr != adapter->itr) {
+		/* this attempts to bias the interrupt rate towards Bulk
+		 * by adding intermediate steps when interrupt rate is
+		 * increasing */
+		new_itr = new_itr > adapter->itr ?
+		             min(adapter->itr + (new_itr >> 2), new_itr) :
+		             new_itr;
+		adapter->itr = new_itr;
+		E1000_WRITE_REG(hw, E1000_ITR, 1000000000 / (new_itr * 256));
+	}
+
+	return;
 }
 
 #define E1000_TX_FLAGS_CSUM		0x00000001
 #define E1000_TX_FLAGS_VLAN		0x00000002
 #define E1000_TX_FLAGS_TSO		0x00000004
+#define E1000_TX_FLAGS_IPV4		0x00000008
 #define E1000_TX_FLAGS_VLAN_MASK	0xffff0000
 #define E1000_TX_FLAGS_VLAN_SHIFT	16
 
-static inline boolean_t
-e1000_tso(struct e1000_adapter *adapter, struct sk_buff *skb)
+static int e1000_tso(struct e1000_adapter *adapter,
+                     struct e1000_tx_ring *tx_ring, struct sk_buff *skb)
 {
 #ifdef NETIF_F_TSO
 	struct e1000_context_desc *context_desc;
+	struct e1000_buffer *buffer_info;
 	unsigned int i;
-	uint32_t cmd_length = 0;
-	uint16_t ipcse, tucse, mss;
-	uint8_t ipcss, ipcso, tucss, tucso, hdr_len;
-
-	if(skb_shinfo(skb)->tso_size) {
-		hdr_len = ((skb->h.raw - skb->data) + (skb->h.th->doff << 2));
-		mss = skb_shinfo(skb)->tso_size;
-		skb->nh.iph->tot_len = 0;
-		skb->nh.iph->check = 0;
-		skb->h.th->check = ~csum_tcpudp_magic(skb->nh.iph->saddr,
-		                                      skb->nh.iph->daddr,
-		                                      0,
-		                                      IPPROTO_TCP,
-		                                      0);
-		ipcss = skb->nh.raw - skb->data;
-		ipcso = (void *)&(skb->nh.iph->check) - (void *)skb->data;
-		ipcse = skb->h.raw - skb->data - 1;
-		tucss = skb->h.raw - skb->data;
-		tucso = (void *)&(skb->h.th->check) - (void *)skb->data;
+	u32 cmd_length = 0;
+	u16 ipcse = 0, tucse, mss;
+	u8 ipcss, ipcso, tucss, tucso, hdr_len;
+	int err;
+
+	if (skb_is_gso(skb)) {
+		if (skb_header_cloned(skb)) {
+			err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+			if (err)
+				return err;
+		}
+
+		hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+		mss = skb_shinfo(skb)->gso_size;
+		if (skb->protocol == htons(ETH_P_IP)) {
+			struct iphdr *iph = ip_hdr(skb);
+			iph->tot_len = 0;
+			iph->check = 0;
+			tcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,
+								 iph->daddr, 0,
+								 IPPROTO_TCP,
+								 0);
+			cmd_length = E1000_TXD_CMD_IP;
+			ipcse = skb_transport_offset(skb) - 1;
+#ifdef NETIF_F_TSO6
+		} else if (skb_shinfo(skb)->gso_type == SKB_GSO_TCPV6) {
+			ipv6_hdr(skb)->payload_len = 0;
+			tcp_hdr(skb)->check =
+				~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
+						 &ipv6_hdr(skb)->daddr,
+						 0, IPPROTO_TCP, 0);
+			ipcse = 0;
+#endif
+		}
+		ipcss = skb_network_offset(skb);
+		ipcso = (void *)&(ip_hdr(skb)->check) - (void *)skb->data;
+		tucss = skb_transport_offset(skb);
+		tucso = (void *)&(tcp_hdr(skb)->check) - (void *)skb->data;
 		tucse = 0;
 
 		cmd_length |= (E1000_TXD_CMD_DEXT | E1000_TXD_CMD_TSE |
-			       E1000_TXD_CMD_IP | E1000_TXD_CMD_TCP |
-			       (skb->len - (hdr_len)));
+			       E1000_TXD_CMD_TCP | (skb->len - (hdr_len)));
 
-		i = adapter->tx_ring.next_to_use;
-		context_desc = E1000_CONTEXT_DESC(adapter->tx_ring, i);
+		i = tx_ring->next_to_use;
+		context_desc = E1000_CONTEXT_DESC(*tx_ring, i);
+		buffer_info = &tx_ring->buffer_info[i];
 
 		context_desc->lower_setup.ip_fields.ipcss  = ipcss;
 		context_desc->lower_setup.ip_fields.ipcso  = ipcso;
@@ -1540,8 +3428,11 @@
 		context_desc->tcp_seg_setup.fields.hdr_len = hdr_len;
 		context_desc->cmd_and_length = cpu_to_le32(cmd_length);
 
-		if(++i == adapter->tx_ring.count) i = 0;
-		adapter->tx_ring.next_to_use = i;
+		buffer_info->time_stamp = jiffies;
+		buffer_info->next_to_watch = i;
+
+		if (++i == tx_ring->count) i = 0;
+		tx_ring->next_to_use = i;
 
 		return TRUE;
 	}
@@ -1550,27 +3441,35 @@
 	return FALSE;
 }
 
-static inline boolean_t
-e1000_tx_csum(struct e1000_adapter *adapter, struct sk_buff *skb)
+static boolean_t e1000_tx_csum(struct e1000_adapter *adapter,
+                               struct e1000_tx_ring *tx_ring,
+                               struct sk_buff *skb)
 {
 	struct e1000_context_desc *context_desc;
+	struct e1000_buffer *buffer_info;
 	unsigned int i;
-	uint8_t css;
+	u8 css;
 
-	if(likely(skb->ip_summed == CHECKSUM_HW)) {
-		css = skb->h.raw - skb->data;
+	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
+		css = skb_transport_offset(skb);
 
-		i = adapter->tx_ring.next_to_use;
-		context_desc = E1000_CONTEXT_DESC(adapter->tx_ring, i);
+		i = tx_ring->next_to_use;
+		buffer_info = &tx_ring->buffer_info[i];
+		context_desc = E1000_CONTEXT_DESC(*tx_ring, i);
 
+		context_desc->lower_setup.ip_config = 0;
 		context_desc->upper_setup.tcp_fields.tucss = css;
-		context_desc->upper_setup.tcp_fields.tucso = css + skb->csum;
+		context_desc->upper_setup.tcp_fields.tucso = css +
+		                                             skb->csum_offset;
 		context_desc->upper_setup.tcp_fields.tucse = 0;
 		context_desc->tcp_seg_setup.data = 0;
 		context_desc->cmd_and_length = cpu_to_le32(E1000_TXD_CMD_DEXT);
 
-		if(unlikely(++i == adapter->tx_ring.count)) i = 0;
-		adapter->tx_ring.next_to_use = i;
+		buffer_info->time_stamp = jiffies;
+		buffer_info->next_to_watch = i;
+
+		if (unlikely(++i == tx_ring->count)) i = 0;
+		tx_ring->next_to_use = i;
 
 		return TRUE;
 	}
@@ -1581,89 +3480,115 @@
 #define E1000_MAX_TXD_PWR	12
 #define E1000_MAX_DATA_PER_TXD	(1<<E1000_MAX_TXD_PWR)
 
-static inline int
-e1000_tx_map(struct e1000_adapter *adapter, struct sk_buff *skb,
-	unsigned int first, unsigned int max_per_txd,
-	unsigned int nr_frags, unsigned int mss)
+static int e1000_tx_map(struct e1000_adapter *adapter,
+                        struct e1000_tx_ring *tx_ring,
+                        struct sk_buff *skb, unsigned int first,
+                        unsigned int max_per_txd, unsigned int nr_frags,
+                        unsigned int mss)
 {
-	struct e1000_desc_ring *tx_ring = &adapter->tx_ring;
 	struct e1000_buffer *buffer_info;
 	unsigned int len = skb->len;
 	unsigned int offset = 0, size, count = 0, i;
+#ifdef MAX_SKB_FRAGS
 	unsigned int f;
 	len -= skb->data_len;
+#endif
 
 	i = tx_ring->next_to_use;
 
-	while(len) {
+	while (len) {
 		buffer_info = &tx_ring->buffer_info[i];
 		size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
+		/* Workaround for Controller erratum --
+		 * descriptor for non-tso packet in a linear SKB that follows a
+		 * tso gets written back prematurely before the data is fully
+		 * DMA'd to the controller */
+		if (tx_ring->last_tx_tso && !skb_is_gso(skb)) {
+			tx_ring->last_tx_tso = 0;
+			if (!skb->data_len)
+				size -= 4;
+		}
+
 		/* Workaround for premature desc write-backs
 		 * in TSO mode.  Append 4-byte sentinel desc */
-		if(unlikely(mss && !nr_frags && size == len && size > 8))
+		if (unlikely(mss && !nr_frags && size == len && size > 8))
 			size -= 4;
 #endif
+		/* work-around for errata 10 and it applies
+		 * to all controllers in PCI-X mode
+		 * The fix is to make sure that the first descriptor of a
+		 * packet is smaller than 2048 - 16 - 16 (or 2016) bytes
+		 */
+		if (unlikely((adapter->hw.bus.type == e1000_bus_type_pcix) &&
+		                (size > 2015) && count == 0))
+		        size = 2015;
+
 		/* Workaround for potential 82544 hang in PCI-X.  Avoid
 		 * terminating buffers within evenly-aligned dwords. */
-		if(unlikely(adapter->pcix_82544 &&
+		if (unlikely(adapter->pcix_82544 &&
 		   !((unsigned long)(skb->data + offset + size - 1) & 4) &&
 		   size > 4))
 			size -= 4;
 
 		buffer_info->length = size;
+		/* set time_stamp *before* dma to help avoid a possible race */
+		buffer_info->time_stamp = jiffies;
 		buffer_info->dma =
 			pci_map_single(adapter->pdev,
 				skb->data + offset,
 				size,
 				PCI_DMA_TODEVICE);
-		buffer_info->time_stamp = jiffies;
+		buffer_info->next_to_watch = i;
 
 		len -= size;
 		offset += size;
 		count++;
-		if(unlikely(++i == tx_ring->count)) i = 0;
+		if (unlikely(++i == tx_ring->count)) i = 0;
 	}
 
-	for(f = 0; f < nr_frags; f++) {
+#ifdef MAX_SKB_FRAGS
+	for (f = 0; f < nr_frags; f++) {
 		struct skb_frag_struct *frag;
 
 		frag = &skb_shinfo(skb)->frags[f];
 		len = frag->size;
 		offset = frag->page_offset;
 
-		while(len) {
+		while (len) {
 			buffer_info = &tx_ring->buffer_info[i];
 			size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
 			/* Workaround for premature desc write-backs
 			 * in TSO mode.  Append 4-byte sentinel desc */
-			if(unlikely(mss && f == (nr_frags-1) && size == len && size > 8))
+			if (unlikely(mss && f == (nr_frags-1) && size == len && size > 8))
 				size -= 4;
 #endif
 			/* Workaround for potential 82544 hang in PCI-X.
 			 * Avoid terminating buffers within evenly-aligned
 			 * dwords. */
-			if(unlikely(adapter->pcix_82544 &&
+			if (unlikely(adapter->pcix_82544 &&
 			   !((unsigned long)(frag->page+offset+size-1) & 4) &&
 			   size > 4))
 				size -= 4;
 
 			buffer_info->length = size;
+			buffer_info->time_stamp = jiffies;
 			buffer_info->dma =
 				pci_map_page(adapter->pdev,
 					frag->page,
 					offset,
 					size,
 					PCI_DMA_TODEVICE);
-			buffer_info->time_stamp = jiffies;
+			buffer_info->next_to_watch = i;
 
 			len -= size;
 			offset += size;
 			count++;
-			if(unlikely(++i == tx_ring->count)) i = 0;
+			if (unlikely(++i == tx_ring->count)) i = 0;
 		}
 	}
+#endif
 
 	i = (i == 0) ? tx_ring->count - 1 : i - 1;
 	tx_ring->buffer_info[i].skb = skb;
@@ -1672,41 +3597,44 @@
 	return count;
 }
 
-static inline void
-e1000_tx_queue(struct e1000_adapter *adapter, int count, int tx_flags)
+static void e1000_tx_queue(struct e1000_adapter *adapter,
+                           struct e1000_tx_ring *tx_ring,
+                           int tx_flags, int count)
 {
-	struct e1000_desc_ring *tx_ring = &adapter->tx_ring;
 	struct e1000_tx_desc *tx_desc = NULL;
 	struct e1000_buffer *buffer_info;
-	uint32_t txd_upper = 0, txd_lower = E1000_TXD_CMD_IFCS;
+	u32 txd_upper = 0, txd_lower = E1000_TXD_CMD_IFCS;
 	unsigned int i;
 
-	if(likely(tx_flags & E1000_TX_FLAGS_TSO)) {
+	if (likely(tx_flags & E1000_TX_FLAGS_TSO)) {
 		txd_lower |= E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D |
 		             E1000_TXD_CMD_TSE;
-		txd_upper |= (E1000_TXD_POPTS_IXSM | E1000_TXD_POPTS_TXSM) << 8;
+		txd_upper |= E1000_TXD_POPTS_TXSM << 8;
+
+		if (likely(tx_flags & E1000_TX_FLAGS_IPV4))
+			txd_upper |= E1000_TXD_POPTS_IXSM << 8;
 	}
 
-	if(likely(tx_flags & E1000_TX_FLAGS_CSUM)) {
+	if (likely(tx_flags & E1000_TX_FLAGS_CSUM)) {
 		txd_lower |= E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D;
 		txd_upper |= E1000_TXD_POPTS_TXSM << 8;
 	}
 
-	if(unlikely(tx_flags & E1000_TX_FLAGS_VLAN)) {
+	if (unlikely(tx_flags & E1000_TX_FLAGS_VLAN)) {
 		txd_lower |= E1000_TXD_CMD_VLE;
 		txd_upper |= (tx_flags & E1000_TX_FLAGS_VLAN_MASK);
 	}
 
 	i = tx_ring->next_to_use;
 
-	while(count--) {
+	while (count--) {
 		buffer_info = &tx_ring->buffer_info[i];
 		tx_desc = E1000_TX_DESC(*tx_ring, i);
 		tx_desc->buffer_addr = cpu_to_le64(buffer_info->dma);
 		tx_desc->lower.data =
 			cpu_to_le32(txd_lower | buffer_info->length);
 		tx_desc->upper.data = cpu_to_le32(txd_upper);
-		if(unlikely(++i == tx_ring->count)) i = 0;
+		if (unlikely(++i == tx_ring->count)) i = 0;
 	}
 
 	tx_desc->lower.data |= cpu_to_le32(adapter->txd_cmd);
@@ -1718,9 +3646,15 @@
 	wmb();
 
 	tx_ring->next_to_use = i;
-	E1000_WRITE_REG(&adapter->hw, TDT, i);
+	writel(i, adapter->hw.hw_addr + tx_ring->tdt);
+	/* we need this if more than one processor can write to our tail
+	 * at a time, it synchronizes IO on IA64/Altix systems */
+	mmiowb();
 }
 
+#define E1000_FIFO_HDR			0x10
+#define E1000_82547_PAD_LEN		0x3E0
+
 /**
  * 82547 workaround to avoid controller hang in half-duplex environment.
  * The workaround is to avoid queuing a large packet that would span
@@ -1729,157 +3663,345 @@
  * flush all packets.  When that occurs, we reset the Tx FIFO pointers
  * to the beginning of the Tx FIFO.
  **/
-
-#define E1000_FIFO_HDR			0x10
-#define E1000_82547_PAD_LEN		0x3E0
-
-static inline int
-e1000_82547_fifo_workaround(struct e1000_adapter *adapter, struct sk_buff *skb)
+static int e1000_82547_fifo_workaround(struct e1000_adapter *adapter,
+                                       struct sk_buff *skb)
 {
-	uint32_t fifo_space = adapter->tx_fifo_size - adapter->tx_fifo_head;
-	uint32_t skb_fifo_len = skb->len + E1000_FIFO_HDR;
+	u32 fifo_space = adapter->tx_fifo_size - adapter->tx_fifo_head;
+	u32 skb_fifo_len = skb->len + E1000_FIFO_HDR;
 
-	E1000_ROUNDUP(skb_fifo_len, E1000_FIFO_HDR);
+	skb_fifo_len = ALIGN(skb_fifo_len, E1000_FIFO_HDR);
 
-	if(adapter->link_duplex != HALF_DUPLEX)
+	if (adapter->link_duplex != HALF_DUPLEX)
 		goto no_fifo_stall_required;
 
-	if(atomic_read(&adapter->tx_fifo_stall))
+	if (atomic_read(&adapter->tx_fifo_stall))
 		return 1;
 
-	if(skb_fifo_len >= (E1000_82547_PAD_LEN + fifo_space)) {
+	if (skb_fifo_len >= (E1000_82547_PAD_LEN + fifo_space)) {
 		atomic_set(&adapter->tx_fifo_stall, 1);
 		return 1;
 	}
 
 no_fifo_stall_required:
 	adapter->tx_fifo_head += skb_fifo_len;
-	if(adapter->tx_fifo_head >= adapter->tx_fifo_size)
+	if (adapter->tx_fifo_head >= adapter->tx_fifo_size)
 		adapter->tx_fifo_head -= adapter->tx_fifo_size;
 	return 0;
 }
 
+#define MINIMUM_DHCP_PACKET_SIZE 282
+static int e1000_transfer_dhcp_info(struct e1000_adapter *adapter,
+                                    struct sk_buff *skb)
+{
+	struct e1000_hw *hw =  &adapter->hw;
+	u16 length, offset;
+#ifdef NETIF_F_HW_VLAN_TX
+	if (vlan_tx_tag_present(skb)) {
+		if (!((vlan_tx_tag_get(skb) == adapter->hw.mng_cookie.vlan_id)
+		    && (adapter->hw.mng_cookie.status &
+		        E1000_MNG_DHCP_COOKIE_STATUS_VLAN)))
+			return 0;
+	}
+#endif
+	if (skb->len > MINIMUM_DHCP_PACKET_SIZE) {
+		struct ethhdr *eth = (struct ethhdr *) skb->data;
+		if ((htons(ETH_P_IP) == eth->h_proto)) {
+			const struct iphdr *ip =
+				(struct iphdr *)((u8 *)skb->data+14);
+			if (IPPROTO_UDP == ip->protocol) {
+				struct udphdr *udp =
+					(struct udphdr *)((u8 *)ip +
+						(ip->ihl << 2));
+				if (ntohs(udp->dest) == 67) {
+					offset = (u8 *)udp + 8 - skb->data;
+					length = skb->len - offset;
+
+					return e1000_mng_write_dhcp_info(hw,
+							(u8 *)udp + 8,
+							length);
+				}
+			}
+		}
+	}
+	return 0;
+}
+
+static int __e1000_maybe_stop_tx(struct net_device *netdev,
+                                 struct e1000_tx_ring *tx_ring, int size)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+
+	netif_stop_queue(netdev);
+	/* Herbert's original patch had:
+	 *  smp_mb__after_netif_stop_queue();
+	 * but since that doesn't exist yet, just open code it. */
+	smp_mb();
+
+	/* We need to check again in a case another CPU has just
+	 * made room available. */
+	if (likely(E1000_DESC_UNUSED(tx_ring) < size))
+		return -EBUSY;
+
+	/* A reprieve! */
+	netif_start_queue(netdev);
+	++adapter->restart_queue;
+	return 0;
+}
+
+static int e1000_maybe_stop_tx(struct net_device *netdev,
+                               struct e1000_tx_ring *tx_ring, int size)
+{
+	if (likely(E1000_DESC_UNUSED(tx_ring) >= size))
+		return 0;
+	return __e1000_maybe_stop_tx(netdev, tx_ring, size);
+}
+
 #define TXD_USE_COUNT(S, X) (((S) >> (X)) + 1 )
-static int
-e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
+static int e1000_xmit_frame_ring(struct sk_buff *skb,
+                                 struct net_device *netdev,
+                                 struct e1000_tx_ring *tx_ring)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	unsigned int first, max_per_txd = E1000_MAX_DATA_PER_TXD;
 	unsigned int max_txd_pwr = E1000_MAX_TXD_PWR;
 	unsigned int tx_flags = 0;
 	unsigned int len = skb->len;
-	unsigned long flags;
+	unsigned long irq_flags;
 	unsigned int nr_frags = 0;
 	unsigned int mss = 0;
 	int count = 0;
+	int tso;
+#ifdef MAX_SKB_FRAGS
 	unsigned int f;
-	nr_frags = skb_shinfo(skb)->nr_frags;
 	len -= skb->data_len;
+#endif
+
+	if (test_bit(__E1000_DOWN, &adapter->state)) {
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
 
-	if(unlikely(skb->len <= 0)) {
+	if (unlikely(skb->len <= 0)) {
 		dev_kfree_skb_any(skb);
 		return NETDEV_TX_OK;
 	}
 
+	/* 82571 and newer doesn't need the workaround that limited descriptor
+	 * length to 4kB */
+	if (adapter->hw.mac.type >= e1000_82571)
+		max_per_txd = 8192;
+
 #ifdef NETIF_F_TSO
-	mss = skb_shinfo(skb)->tso_size;
+	mss = skb_shinfo(skb)->gso_size;
 	/* The controller does a simple calculation to
 	 * make sure there is enough room in the FIFO before
 	 * initiating the DMA for each buffer.  The calc is:
 	 * 4 = ceil(buffer len/mss).  To make sure we don't
 	 * overrun the FIFO, adjust the max buffer len if mss
 	 * drops. */
-	if(mss) {
+	if (mss) {
+		u8 hdr_len;
 		max_per_txd = min(mss << 2, max_per_txd);
 		max_txd_pwr = fls(max_per_txd) - 1;
+
+		/* TSO Workaround for 82571/2/3 Controllers -- if skb->data
+		* points to just header, pull a few bytes of payload from
+		* frags into skb->data */
+		hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+		if (skb->data_len && (hdr_len == (skb->len - skb->data_len))) {
+			switch (adapter->hw.mac.type) {
+				unsigned int pull_size;
+			case e1000_82544:
+				/* Make sure we have room to chop off 4 bytes,
+				 * and that the end alignment will work out to
+				 * this hardware's requirements
+				 * NOTE: this is a TSO only workaround
+				 * if end byte alignment not correct move us
+				 * into the next dword */
+				if ((unsigned long)(skb_tail_pointer(skb) - 1) & 4)
+					break;
+				/* fall through */
+			case e1000_82571:
+			case e1000_82572:
+			case e1000_82573:
+			case e1000_ich8lan:
+			case e1000_ich9lan:
+				pull_size = min((unsigned int)4, skb->data_len);
+				if (!__pskb_pull_tail(skb, pull_size)) {
+					DPRINTK(DRV, ERR,
+						"__pskb_pull_tail failed.\n");
+					dev_kfree_skb_any(skb);
+					return NETDEV_TX_OK;
+				}
+				len = skb->len - skb->data_len;
+				break;
+			default:
+				/* do nothing */
+				break;
+			}
+		}
 	}
 
-	if((mss) || (skb->ip_summed == CHECKSUM_HW))
+	/* reserve a descriptor for the offload context */
+	if ((mss) || (skb->ip_summed == CHECKSUM_PARTIAL))
 		count++;
-	count++;	/* for sentinel desc */
+	count++;
 #else
-	if(skb->ip_summed == CHECKSUM_HW)
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
+		count++;
+#endif
+
+#ifdef NETIF_F_TSO
+	/* Controller Erratum workaround */
+	if (!skb->data_len && tx_ring->last_tx_tso && !skb_is_gso(skb))
 		count++;
 #endif
+
 	count += TXD_USE_COUNT(len, max_txd_pwr);
 
-	if(adapter->pcix_82544)
+	if (adapter->pcix_82544)
+		count++;
+
+	/* work-around for errata 10 and it applies to all controllers
+	 * in PCI-X mode, so add one more descriptor to the count
+	 */
+	if (unlikely((adapter->hw.bus.type == e1000_bus_type_pcix) &&
+			(len > 2015)))
 		count++;
 
+#ifdef MAX_SKB_FRAGS
 	nr_frags = skb_shinfo(skb)->nr_frags;
-	for(f = 0; f < nr_frags; f++)
+	for (f = 0; f < nr_frags; f++)
 		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size,
 				       max_txd_pwr);
-	if(adapter->pcix_82544)
+	if (adapter->pcix_82544)
 		count += nr_frags;
 
- 	local_irq_save(flags); 
- 	if (!spin_trylock(&adapter->tx_lock)) { 
- 		/* Collision - tell upper layer to requeue */ 
- 		local_irq_restore(flags); 
- 		return NETDEV_TX_LOCKED; 
- 	} 
+#endif
+
+	if (adapter->hw.mac.tx_pkt_filtering &&
+	    (adapter->hw.mac.type == e1000_82573))
+		e1000_transfer_dhcp_info(adapter, skb);
+
+#ifdef NETIF_F_LLTX
+	if (!spin_trylock_irqsave(&tx_ring->tx_lock, irq_flags)) {
+		/* Collision - tell upper layer to requeue */
+		return NETDEV_TX_LOCKED;
+	}
+#else
+	spin_lock_irqsave(&tx_ring->tx_lock, irq_flags);
+#endif
 
 	/* need: count + 2 desc gap to keep tail from touching
 	 * head, otherwise try next time */
-	if(E1000_DESC_UNUSED(&adapter->tx_ring) < count + 2) {
-		netif_stop_queue(netdev);
-		spin_unlock_irqrestore(&adapter->tx_lock, flags);
+	if (unlikely(e1000_maybe_stop_tx(netdev, tx_ring, count + 2))) {
+		spin_unlock_irqrestore(&tx_ring->tx_lock, irq_flags);
 		return NETDEV_TX_BUSY;
 	}
 
-	if(unlikely(adapter->hw.mac_type == e1000_82547)) {
-		if(unlikely(e1000_82547_fifo_workaround(adapter, skb))) {
+	if (unlikely(adapter->hw.mac.type == e1000_82547)) {
+		if (unlikely(e1000_82547_fifo_workaround(adapter, skb))) {
 			netif_stop_queue(netdev);
-			mod_timer(&adapter->tx_fifo_stall_timer, jiffies);
-			spin_unlock_irqrestore(&adapter->tx_lock, flags);
+			if (!test_bit(__E1000_DOWN, &adapter->state))
+				mod_timer(&adapter->tx_fifo_stall_timer,
+				          jiffies + 1);
+			spin_unlock_irqrestore(&tx_ring->tx_lock, irq_flags);
 			return NETDEV_TX_BUSY;
 		}
 	}
 
-	if(unlikely(adapter->vlgrp && vlan_tx_tag_present(skb))) {
+#ifndef NETIF_F_LLTX
+	spin_unlock_irqrestore(&tx_ring->tx_lock, irq_flags);
+
+#endif
+#ifdef NETIF_F_HW_VLAN_TX
+	if (unlikely(adapter->vlgrp && vlan_tx_tag_present(skb))) {
 		tx_flags |= E1000_TX_FLAGS_VLAN;
 		tx_flags |= (vlan_tx_tag_get(skb) << E1000_TX_FLAGS_VLAN_SHIFT);
 	}
+#endif
 
-	first = adapter->tx_ring.next_to_use;
-	
-	if(likely(e1000_tso(adapter, skb)))
+	first = tx_ring->next_to_use;
+
+	tso = e1000_tso(adapter, tx_ring, skb);
+	if (tso < 0) {
+		dev_kfree_skb_any(skb);
+#ifdef NETIF_F_LLTX
+		spin_unlock_irqrestore(&tx_ring->tx_lock, irq_flags);
+#endif
+		return NETDEV_TX_OK;
+	}
+
+	if (likely(tso)) {
+		tx_ring->last_tx_tso = 1;
 		tx_flags |= E1000_TX_FLAGS_TSO;
-	else if(likely(e1000_tx_csum(adapter, skb)))
+	} else if (likely(e1000_tx_csum(adapter, tx_ring, skb)))
 		tx_flags |= E1000_TX_FLAGS_CSUM;
 
-	e1000_tx_queue(adapter,
-		e1000_tx_map(adapter, skb, first, max_per_txd, nr_frags, mss),
-		tx_flags);
+	/* Old method was to assume IPv4 packet by default if TSO was enabled.
+	 * 82571 hardware supports TSO capabilities for IPv6 as well...
+	 * no longer assume, we must. */
+	if (likely(skb->protocol == htons(ETH_P_IP)))
+		tx_flags |= E1000_TX_FLAGS_IPV4;
+
+	e1000_tx_queue(adapter, tx_ring, tx_flags,
+	               e1000_tx_map(adapter, tx_ring, skb, first,
+	                            max_per_txd, nr_frags, mss));
 
 	netdev->trans_start = jiffies;
 
-	spin_unlock_irqrestore(&adapter->tx_lock, flags);
+	/* Make sure there is space in the ring for the next send. */
+	e1000_maybe_stop_tx(netdev, tx_ring, MAX_SKB_FRAGS + 2);
+
+#ifdef NETIF_F_LLTX
+	spin_unlock_irqrestore(&tx_ring->tx_lock, irq_flags);
+#endif
 	return NETDEV_TX_OK;
 }
 
+static int e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	struct e1000_tx_ring *tx_ring = adapter->tx_ring;
+
+	/* This goes back to the question of how to logically map a tx queue
+	 * to a flow.  Right now, performance is impacted slightly negatively
+	 * if using multiple tx queues.  If the stack breaks away from a
+	 * single qdisc implementation, we can look at this again. */
+	return (e1000_xmit_frame_ring(skb, netdev, tx_ring));
+}
+
+#ifdef CONFIG_E1000_MQ
+static int e1000_subqueue_xmit_frame(struct sk_buff *skb,
+                                     struct net_device *netdev, int queue)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	struct e1000_tx_ring *tx_ring = &adapter->tx_ring[queue];
+
+	return (e1000_xmit_frame_ring(skb, netdev, tx_ring));
+}
+#endif
+
+
 /**
  * e1000_tx_timeout - Respond to a Tx Hang
  * @netdev: network interface device structure
  **/
-
-static void
-e1000_tx_timeout(struct net_device *netdev)
+static void e1000_tx_timeout(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
 	/* Do the reset outside of interrupt context */
-	schedule_work(&adapter->tx_timeout_task);
+	adapter->tx_timeout_count++;
+	schedule_work(&adapter->reset_task);
 }
 
-static void
-e1000_tx_timeout_task(struct net_device *netdev)
+static void e1000_reset_task(struct work_struct *work)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter;
+	adapter = container_of(work, struct e1000_adapter, reset_task);
 
-	e1000_down(adapter);
-	e1000_up(adapter);
+	e1000_reinit_locked(adapter);
 }
 
 /**
@@ -1889,13 +4011,11 @@
  * Returns the address of the device statistics structure.
  * The statistics are actually updated from the timer callback.
  **/
-
-static struct net_device_stats *
-e1000_get_stats(struct net_device *netdev)
+static struct net_device_stats * e1000_get_stats(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	e1000_update_stats(adapter);
+	/* only return the current stats */
 	return &adapter->net_stats;
 }
 
@@ -1906,44 +4026,116 @@
  *
  * Returns 0 on success, negative on failure
  **/
-
-static int
-e1000_change_mtu(struct net_device *netdev, int new_mtu)
+static int e1000_change_mtu(struct net_device *netdev, int new_mtu)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	int old_mtu = adapter->rx_buffer_len;
-	int max_frame = new_mtu + ENET_HEADER_SIZE + ETHERNET_FCS_SIZE;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	int max_frame = new_mtu + ETH_HLEN + ETHERNET_FCS_SIZE;
+	u16 eeprom_data = 0;
 
-	if((max_frame < MINIMUM_ETHERNET_FRAME_SIZE) ||
-	   (max_frame > MAX_JUMBO_FRAME_SIZE)) {
+	if ((max_frame < ETH_ZLEN + ETHERNET_FCS_SIZE) ||
+	    (max_frame > MAX_JUMBO_FRAME_SIZE)) {
 		DPRINTK(PROBE, ERR, "Invalid MTU setting\n");
 		return -EINVAL;
 	}
 
-	if(max_frame <= MAXIMUM_ETHERNET_FRAME_SIZE) {
-		adapter->rx_buffer_len = E1000_RXBUFFER_2048;
+	/* Adapter-specific max frame size limits. */
+	switch (adapter->hw.mac.type) {
+	case e1000_undefined:
+	case e1000_82542:
+	case e1000_ich8lan:
+		if (max_frame > ETH_FRAME_LEN + ETHERNET_FCS_SIZE) {
+			DPRINTK(PROBE, ERR, "Jumbo Frames not supported.\n");
+			return -EINVAL;
+		}
+		break;
+	case e1000_82573:
+		/* Jumbo Frames not supported if:
+		 * - this is not an 82573L device
+		 * - ASPM is enabled in any way (0x1A bits 3:2) */
+		e1000_read_nvm(&adapter->hw, NVM_INIT_3GIO_3, 1, &eeprom_data);
+		if ((adapter->hw.device_id != E1000_DEV_ID_82573L) ||
+		    (eeprom_data & NVM_WORD1A_ASPM_MASK)) {
+			if (max_frame > ETH_FRAME_LEN + ETHERNET_FCS_SIZE) {
+				DPRINTK(PROBE, ERR,
+			            	"Jumbo Frames not supported.\n");
+				return -EINVAL;
+			}
+			break;
+		}
+		/* ERT will be enabled later to enable wire speed receives */
 
-	} else if(adapter->hw.mac_type < e1000_82543) {
-		DPRINTK(PROBE, ERR, "Jumbo Frames not supported on 82542\n");
-		return -EINVAL;
+		/* fall through to get support */
+	case e1000_ich9lan:
+		if ((adapter->hw.phy.type == e1000_phy_ife) &&
+		    (max_frame > ETH_FRAME_LEN + ETHERNET_FCS_SIZE)) {
+			DPRINTK(PROBE, ERR, "Jumbo Frames not supported.\n");
+			return -EINVAL;
+		}
+		/* fall through to get support */
+	case e1000_82571:
+	case e1000_82572:
+	case e1000_80003es2lan:
+#define MAX_STD_JUMBO_FRAME_SIZE 9234
+		if (max_frame > MAX_STD_JUMBO_FRAME_SIZE) {
+			DPRINTK(PROBE, ERR, "MTU > 9216 not supported.\n");
+			return -EINVAL;
+		}
+		break;
+	default:
+		/* Capable of supporting up to MAX_JUMBO_FRAME_SIZE limit. */
+		break;
+	}
 
-	} else if(max_frame <= E1000_RXBUFFER_4096) {
-		adapter->rx_buffer_len = E1000_RXBUFFER_4096;
+	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
+		msleep(1);
+	/* e1000_down has a dependency on max_frame_size */
+	adapter->hw.mac.max_frame_size = max_frame;
+	if (netif_running(netdev))
+		e1000_down(adapter);
 
-	} else if(max_frame <= E1000_RXBUFFER_8192) {
+	/* NOTE: netdev_alloc_skb reserves 16 bytes, and typically NET_IP_ALIGN
+	 * means we reserve 2 more, this pushes us to allocate from the next
+	 * larger slab size.
+	 * i.e. RXBUFFER_2048 --> size-4096 slab
+	 *  however with the new *_jumbo_rx* routines, jumbo receives will use
+	 *  fragmented skbs */
+
+	if (max_frame <= E1000_RXBUFFER_256)
+		adapter->rx_buffer_len = E1000_RXBUFFER_256;
+	else if (max_frame <= E1000_RXBUFFER_512)
+		adapter->rx_buffer_len = E1000_RXBUFFER_512;
+	else if (max_frame <= E1000_RXBUFFER_1024)
+		adapter->rx_buffer_len = E1000_RXBUFFER_1024;
+	else if (max_frame <= E1000_RXBUFFER_2048)
+		adapter->rx_buffer_len = E1000_RXBUFFER_2048;
+#ifdef CONFIG_E1000_NAPI
+	else
+		adapter->rx_buffer_len = E1000_RXBUFFER_4096;
+#else
+	else if (max_frame <= E1000_RXBUFFER_4096)
+		adapter->rx_buffer_len = E1000_RXBUFFER_4096;
+	else if (max_frame <= E1000_RXBUFFER_8192)
 		adapter->rx_buffer_len = E1000_RXBUFFER_8192;
-
-	} else {
+	else if (max_frame <= E1000_RXBUFFER_16384)
 		adapter->rx_buffer_len = E1000_RXBUFFER_16384;
-	}
+#endif
 
-	if(old_mtu != adapter->rx_buffer_len && netif_running(netdev)) {
-		e1000_down(adapter);
-		e1000_up(adapter);
-	}
+	/* adjust allocation if LPE protects us, and we aren't using SBP */
+	if (!e1000_tbi_sbp_enabled_82543(&adapter->hw) &&
+	    ((max_frame == ETH_FRAME_LEN + ETHERNET_FCS_SIZE) ||
+	     (max_frame == MAXIMUM_ETHERNET_VLAN_SIZE)))
+		adapter->rx_buffer_len = MAXIMUM_ETHERNET_VLAN_SIZE;
 
+	DPRINTK(PROBE, INFO, "changing MTU from %d to %d\n",
+	        netdev->mtu, new_mtu);
 	netdev->mtu = new_mtu;
-	adapter->hw.max_frame_size = max_frame;
+
+	if (netif_running(netdev))
+		e1000_up(adapter);
+	else
+		e1000_reset(adapter);
+
+	clear_bit(__E1000_RESETTING, &adapter->state);
 
 	return 0;
 }
@@ -1952,90 +4144,125 @@
  * e1000_update_stats - Update the board statistics counters
  * @adapter: board private structure
  **/
-
-void
-e1000_update_stats(struct e1000_adapter *adapter)
+void e1000_update_stats(struct e1000_adapter *adapter)
 {
 	struct e1000_hw *hw = &adapter->hw;
-	unsigned long flags;
-	uint16_t phy_tmp;
+#ifdef CONFIG_E1000_PCI_ERS
+	struct pci_dev *pdev = adapter->pdev;
+#endif
+	unsigned long irq_flags;
+	u16 phy_tmp;
 
 #define PHY_IDLE_ERROR_COUNT_MASK 0x00FF
 
-	spin_lock_irqsave(&adapter->stats_lock, flags);
+	/*
+	 * Prevent stats update while adapter is being reset, or if the pci
+	 * connection is down.
+	 */
+	if (adapter->link_speed == 0)
+		return;
+#ifdef CONFIG_E1000_PCI_ERS
+	if (pci_channel_offline(pdev))
+		return;
+#endif
+
+	spin_lock_irqsave(&adapter->stats_lock, irq_flags);
 
 	/* these counters are modified from e1000_adjust_tbi_stats,
 	 * called from the interrupt context, so they must only
 	 * be written while holding adapter->stats_lock
 	 */
 
-	adapter->stats.crcerrs += E1000_READ_REG(hw, CRCERRS);
-	adapter->stats.gprc += E1000_READ_REG(hw, GPRC);
-	adapter->stats.gorcl += E1000_READ_REG(hw, GORCL);
-	adapter->stats.gorch += E1000_READ_REG(hw, GORCH);
-	adapter->stats.bprc += E1000_READ_REG(hw, BPRC);
-	adapter->stats.mprc += E1000_READ_REG(hw, MPRC);
-	adapter->stats.roc += E1000_READ_REG(hw, ROC);
-	adapter->stats.prc64 += E1000_READ_REG(hw, PRC64);
-	adapter->stats.prc127 += E1000_READ_REG(hw, PRC127);
-	adapter->stats.prc255 += E1000_READ_REG(hw, PRC255);
-	adapter->stats.prc511 += E1000_READ_REG(hw, PRC511);
-	adapter->stats.prc1023 += E1000_READ_REG(hw, PRC1023);
-	adapter->stats.prc1522 += E1000_READ_REG(hw, PRC1522);
-
-	adapter->stats.symerrs += E1000_READ_REG(hw, SYMERRS);
-	adapter->stats.mpc += E1000_READ_REG(hw, MPC);
-	adapter->stats.scc += E1000_READ_REG(hw, SCC);
-	adapter->stats.ecol += E1000_READ_REG(hw, ECOL);
-	adapter->stats.mcc += E1000_READ_REG(hw, MCC);
-	adapter->stats.latecol += E1000_READ_REG(hw, LATECOL);
-	adapter->stats.dc += E1000_READ_REG(hw, DC);
-	adapter->stats.sec += E1000_READ_REG(hw, SEC);
-	adapter->stats.rlec += E1000_READ_REG(hw, RLEC);
-	adapter->stats.xonrxc += E1000_READ_REG(hw, XONRXC);
-	adapter->stats.xontxc += E1000_READ_REG(hw, XONTXC);
-	adapter->stats.xoffrxc += E1000_READ_REG(hw, XOFFRXC);
-	adapter->stats.xofftxc += E1000_READ_REG(hw, XOFFTXC);
-	adapter->stats.fcruc += E1000_READ_REG(hw, FCRUC);
-	adapter->stats.gptc += E1000_READ_REG(hw, GPTC);
-	adapter->stats.gotcl += E1000_READ_REG(hw, GOTCL);
-	adapter->stats.gotch += E1000_READ_REG(hw, GOTCH);
-	adapter->stats.rnbc += E1000_READ_REG(hw, RNBC);
-	adapter->stats.ruc += E1000_READ_REG(hw, RUC);
-	adapter->stats.rfc += E1000_READ_REG(hw, RFC);
-	adapter->stats.rjc += E1000_READ_REG(hw, RJC);
-	adapter->stats.torl += E1000_READ_REG(hw, TORL);
-	adapter->stats.torh += E1000_READ_REG(hw, TORH);
-	adapter->stats.totl += E1000_READ_REG(hw, TOTL);
-	adapter->stats.toth += E1000_READ_REG(hw, TOTH);
-	adapter->stats.tpr += E1000_READ_REG(hw, TPR);
-	adapter->stats.ptc64 += E1000_READ_REG(hw, PTC64);
-	adapter->stats.ptc127 += E1000_READ_REG(hw, PTC127);
-	adapter->stats.ptc255 += E1000_READ_REG(hw, PTC255);
-	adapter->stats.ptc511 += E1000_READ_REG(hw, PTC511);
-	adapter->stats.ptc1023 += E1000_READ_REG(hw, PTC1023);
-	adapter->stats.ptc1522 += E1000_READ_REG(hw, PTC1522);
-	adapter->stats.mptc += E1000_READ_REG(hw, MPTC);
-	adapter->stats.bptc += E1000_READ_REG(hw, BPTC);
+	adapter->stats.crcerrs += E1000_READ_REG(hw, E1000_CRCERRS);
+	adapter->stats.gprc += E1000_READ_REG(hw, E1000_GPRC);
+	adapter->stats.gorcl += E1000_READ_REG(hw, E1000_GORCL);
+	adapter->stats.gorch += E1000_READ_REG(hw, E1000_GORCH);
+	adapter->stats.bprc += E1000_READ_REG(hw, E1000_BPRC);
+	adapter->stats.mprc += E1000_READ_REG(hw, E1000_MPRC);
+	adapter->stats.roc += E1000_READ_REG(hw, E1000_ROC);
+
+	if ((adapter->hw.mac.type != e1000_ich8lan) &&
+	    (adapter->hw.mac.type != e1000_ich9lan)) {
+		adapter->stats.prc64 += E1000_READ_REG(hw, E1000_PRC64);
+		adapter->stats.prc127 += E1000_READ_REG(hw, E1000_PRC127);
+		adapter->stats.prc255 += E1000_READ_REG(hw, E1000_PRC255);
+		adapter->stats.prc511 += E1000_READ_REG(hw, E1000_PRC511);
+		adapter->stats.prc1023 += E1000_READ_REG(hw, E1000_PRC1023);
+		adapter->stats.prc1522 += E1000_READ_REG(hw, E1000_PRC1522);
+		adapter->stats.symerrs += E1000_READ_REG(hw, E1000_SYMERRS);
+		adapter->stats.sec += E1000_READ_REG(hw, E1000_SEC);
+	}
+
+	adapter->stats.mpc += E1000_READ_REG(hw, E1000_MPC);
+	adapter->stats.scc += E1000_READ_REG(hw, E1000_SCC);
+	adapter->stats.ecol += E1000_READ_REG(hw, E1000_ECOL);
+	adapter->stats.mcc += E1000_READ_REG(hw, E1000_MCC);
+	adapter->stats.latecol += E1000_READ_REG(hw, E1000_LATECOL);
+	adapter->stats.dc += E1000_READ_REG(hw, E1000_DC);
+	adapter->stats.rlec += E1000_READ_REG(hw, E1000_RLEC);
+	adapter->stats.xonrxc += E1000_READ_REG(hw, E1000_XONRXC);
+	adapter->stats.xontxc += E1000_READ_REG(hw, E1000_XONTXC);
+	adapter->stats.xoffrxc += E1000_READ_REG(hw, E1000_XOFFRXC);
+	adapter->stats.xofftxc += E1000_READ_REG(hw, E1000_XOFFTXC);
+	adapter->stats.fcruc += E1000_READ_REG(hw, E1000_FCRUC);
+	adapter->stats.gptc += E1000_READ_REG(hw, E1000_GPTC);
+	adapter->stats.gotcl += E1000_READ_REG(hw, E1000_GOTCL);
+	adapter->stats.gotch += E1000_READ_REG(hw, E1000_GOTCH);
+	adapter->stats.rnbc += E1000_READ_REG(hw, E1000_RNBC);
+	adapter->stats.ruc += E1000_READ_REG(hw, E1000_RUC);
+	adapter->stats.rfc += E1000_READ_REG(hw, E1000_RFC);
+	adapter->stats.rjc += E1000_READ_REG(hw, E1000_RJC);
+	adapter->stats.torl += E1000_READ_REG(hw, E1000_TORL);
+	adapter->stats.torh += E1000_READ_REG(hw, E1000_TORH);
+	adapter->stats.totl += E1000_READ_REG(hw, E1000_TOTL);
+	adapter->stats.toth += E1000_READ_REG(hw, E1000_TOTH);
+	adapter->stats.tpr += E1000_READ_REG(hw, E1000_TPR);
+
+	if ((adapter->hw.mac.type != e1000_ich8lan) &&
+	    (adapter->hw.mac.type != e1000_ich9lan)) {
+		adapter->stats.ptc64 += E1000_READ_REG(hw, E1000_PTC64);
+		adapter->stats.ptc127 += E1000_READ_REG(hw, E1000_PTC127);
+		adapter->stats.ptc255 += E1000_READ_REG(hw, E1000_PTC255);
+		adapter->stats.ptc511 += E1000_READ_REG(hw, E1000_PTC511);
+		adapter->stats.ptc1023 += E1000_READ_REG(hw, E1000_PTC1023);
+		adapter->stats.ptc1522 += E1000_READ_REG(hw, E1000_PTC1522);
+	}
+
+	adapter->stats.mptc += E1000_READ_REG(hw, E1000_MPTC);
+	adapter->stats.bptc += E1000_READ_REG(hw, E1000_BPTC);
 
 	/* used for adaptive IFS */
 
-	hw->tx_packet_delta = E1000_READ_REG(hw, TPT);
-	adapter->stats.tpt += hw->tx_packet_delta;
-	hw->collision_delta = E1000_READ_REG(hw, COLC);
-	adapter->stats.colc += hw->collision_delta;
-
-	if(hw->mac_type >= e1000_82543) {
-		adapter->stats.algnerrc += E1000_READ_REG(hw, ALGNERRC);
-		adapter->stats.rxerrc += E1000_READ_REG(hw, RXERRC);
-		adapter->stats.tncrs += E1000_READ_REG(hw, TNCRS);
-		adapter->stats.cexterr += E1000_READ_REG(hw, CEXTERR);
-		adapter->stats.tsctc += E1000_READ_REG(hw, TSCTC);
-		adapter->stats.tsctfc += E1000_READ_REG(hw, TSCTFC);
+	hw->mac.tx_packet_delta = E1000_READ_REG(hw, E1000_TPT);
+	adapter->stats.tpt += hw->mac.tx_packet_delta;
+	hw->mac.collision_delta = E1000_READ_REG(hw, E1000_COLC);
+	adapter->stats.colc += hw->mac.collision_delta;
+
+	if (hw->mac.type >= e1000_82543) {
+		adapter->stats.algnerrc += E1000_READ_REG(hw, E1000_ALGNERRC);
+		adapter->stats.rxerrc += E1000_READ_REG(hw, E1000_RXERRC);
+		adapter->stats.tncrs += E1000_READ_REG(hw, E1000_TNCRS);
+		adapter->stats.cexterr += E1000_READ_REG(hw, E1000_CEXTERR);
+		adapter->stats.tsctc += E1000_READ_REG(hw, E1000_TSCTC);
+		adapter->stats.tsctfc += E1000_READ_REG(hw, E1000_TSCTFC);
+	}
+	if (hw->mac.type > e1000_82547_rev_2) {
+		adapter->stats.iac += E1000_READ_REG(hw, E1000_IAC);
+
+		if ((adapter->hw.mac.type != e1000_ich8lan) &&
+		    (adapter->hw.mac.type != e1000_ich9lan)) {
+			adapter->stats.icrxoc += E1000_READ_REG(hw, E1000_ICRXOC);
+			adapter->stats.icrxptc += E1000_READ_REG(hw, E1000_ICRXPTC);
+			adapter->stats.icrxatc += E1000_READ_REG(hw, E1000_ICRXATC);
+			adapter->stats.ictxptc += E1000_READ_REG(hw, E1000_ICTXPTC);
+			adapter->stats.ictxatc += E1000_READ_REG(hw, E1000_ICTXATC);
+			adapter->stats.ictxqec += E1000_READ_REG(hw, E1000_ICTXQEC);
+			adapter->stats.ictxqmtc += E1000_READ_REG(hw, E1000_ICTXQMTC);
+			adapter->stats.icrxdmtc += E1000_READ_REG(hw, E1000_ICRXDMTC);
+		}
 	}
 
 	/* Fill out the OS statistics structure */
-
 	adapter->net_stats.rx_packets = adapter->stats.gprc;
 	adapter->net_stats.tx_packets = adapter->stats.gptc;
 	adapter->net_stats.rx_bytes = adapter->stats.gorcl;
@@ -2045,117 +4272,258 @@
 
 	/* Rx Errors */
 
+	/* RLEC on some newer hardware can be incorrect so build
+	* our own version based on RUC and ROC */
 	adapter->net_stats.rx_errors = adapter->stats.rxerrc +
 		adapter->stats.crcerrs + adapter->stats.algnerrc +
-		adapter->stats.rlec + adapter->stats.rnbc +
-		adapter->stats.mpc + adapter->stats.cexterr;
-	adapter->net_stats.rx_dropped = adapter->stats.rnbc;
-	adapter->net_stats.rx_length_errors = adapter->stats.rlec;
+		adapter->stats.ruc + adapter->stats.roc +
+		adapter->stats.cexterr;
+	adapter->net_stats.rx_length_errors = adapter->stats.ruc +
+	                                      adapter->stats.roc;
 	adapter->net_stats.rx_crc_errors = adapter->stats.crcerrs;
 	adapter->net_stats.rx_frame_errors = adapter->stats.algnerrc;
-	adapter->net_stats.rx_fifo_errors = adapter->stats.mpc;
 	adapter->net_stats.rx_missed_errors = adapter->stats.mpc;
 
 	/* Tx Errors */
-
 	adapter->net_stats.tx_errors = adapter->stats.ecol +
 	                               adapter->stats.latecol;
 	adapter->net_stats.tx_aborted_errors = adapter->stats.ecol;
 	adapter->net_stats.tx_window_errors = adapter->stats.latecol;
-	adapter->net_stats.tx_carrier_errors = adapter->stats.tncrs;
+	if (adapter->flags.bad_tx_carrier_stats_fd &&
+	    adapter->link_duplex == FULL_DUPLEX) {
+		adapter->net_stats.tx_carrier_errors = 0;
+		adapter->stats.tncrs = 0;
+	} else {
+		adapter->net_stats.tx_carrier_errors = adapter->stats.tncrs;
+	}
 
 	/* Tx Dropped needs to be maintained elsewhere */
 
 	/* Phy Stats */
-
-	if(hw->media_type == e1000_media_type_copper) {
-		if((adapter->link_speed == SPEED_1000) &&
+	if (hw->media_type == e1000_media_type_copper) {
+		if ((adapter->link_speed == SPEED_1000) &&
 		   (!e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_tmp))) {
 			phy_tmp &= PHY_IDLE_ERROR_COUNT_MASK;
 			adapter->phy_stats.idle_errors += phy_tmp;
 		}
 
-		if((hw->mac_type <= e1000_82546) &&
-		   (hw->phy_type == e1000_phy_m88) &&
+		if ((hw->mac.type <= e1000_82546) &&
+		   (hw->phy.type == e1000_phy_m88) &&
 		   !e1000_read_phy_reg(hw, M88E1000_RX_ERR_CNTR, &phy_tmp))
 			adapter->phy_stats.receive_errors += phy_tmp;
 	}
 
-	spin_unlock_irqrestore(&adapter->stats_lock, flags);
+	/* Management Stats */
+	if (adapter->flags.has_smbus) {
+		adapter->stats.mgptc += E1000_READ_REG(hw, E1000_MGTPTC);
+		adapter->stats.mgprc += E1000_READ_REG(hw, E1000_MGTPRC);
+		adapter->stats.mgpdc += E1000_READ_REG(hw, E1000_MGTPDC);
+	}
+
+	spin_unlock_irqrestore(&adapter->stats_lock, irq_flags);
 }
+#ifdef CONFIG_PCI_MSI
 
 /**
- * e1000_irq_disable - Mask off interrupt generation on the NIC
- * @adapter: board private structure
+ * e1000_intr_msi - Interrupt Handler
+ * @irq: interrupt number
+ * @data: pointer to a network interface device structure
  **/
-
-static void
-e1000_irq_disable(struct e1000_adapter *adapter)
+static irqreturn_t e1000_intr_msi(int irq, void *data)
 {
+	struct net_device *netdev = data;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+#ifndef CONFIG_E1000_NAPI
+	int i, j;
+	int rx_cleaned, tx_cleaned;
+#endif
+	u32 icr = E1000_READ_REG(hw, E1000_ICR);
+
+#ifdef CONFIG_E1000_NAPI
+	/* read ICR disables interrupts using IAM, so keep up with our
+	 * enable/disable accounting */
 	atomic_inc(&adapter->irq_sem);
-	E1000_WRITE_REG(&adapter->hw, IMC, ~0);
-	E1000_WRITE_FLUSH(&adapter->hw);
-	synchronize_irq(adapter->pdev->irq);
-}
+#endif
+	if (icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC)) {
+		hw->mac.get_link_status = 1;
+		/* ICH8 workaround-- Call gig speed drop workaround on cable
+		 * disconnect (LSC) before accessing any PHY registers */
+		if ((hw->mac.type == e1000_ich8lan) &&
+		    (hw->phy.type == e1000_phy_igp_3) &&
+		    (!(E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_LU)))
+			e1000_gig_downshift_workaround_ich8lan(hw);
+
+		/* 80003ES2LAN workaround-- For packet buffer work-around on
+		 * link down event; disable receives here in the ISR and reset
+		 * adapter in watchdog */
+		if (netif_carrier_ok(netdev) &&
+		    adapter->flags.rx_needs_restart) {
+			/* disable receives */
+			u32 rctl = E1000_READ_REG(hw, E1000_RCTL);
+			E1000_WRITE_REG(hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+		}
+		/* guard against interrupt when we're going down */
+		if (!test_bit(__E1000_DOWN, &adapter->state))
+			mod_timer(&adapter->watchdog_timer, jiffies + 1);
+	}
 
-/**
- * e1000_irq_enable - Enable default interrupt generation settings
- * @adapter: board private structure
- **/
+#ifdef CONFIG_E1000_NAPI
+	if (likely(netif_rx_schedule_prep(netdev))) {
+		adapter->total_tx_bytes = 0;
+		adapter->total_tx_packets = 0;
+		adapter->total_rx_bytes = 0;
+		adapter->total_rx_packets = 0;
+		__netif_rx_schedule(netdev);
+	} else {
+		atomic_dec(&adapter->irq_sem);
+	}
+#else
+	adapter->total_tx_bytes = 0;
+	adapter->total_rx_bytes = 0;
+	adapter->total_tx_packets = 0;
+	adapter->total_rx_packets = 0;
+
+	for (i = 0; i < E1000_MAX_INTR; i++) {
+		rx_cleaned = 0;
+		for (j = 0; j < adapter->num_rx_queues; j++)
+			rx_cleaned |= adapter->clean_rx(adapter,
+			                                &adapter->rx_ring[j]);
+
+		tx_cleaned = 0;
+		for (j = 0 ; j < adapter->num_tx_queues ; j++)
+			tx_cleaned |= e1000_clean_tx_irq(adapter,
+			                                 &adapter->tx_ring[j]);
 
-static void
-e1000_irq_enable(struct e1000_adapter *adapter)
-{
-	if(likely(atomic_dec_and_test(&adapter->irq_sem))) {
-		E1000_WRITE_REG(&adapter->hw, IMS, IMS_ENABLE_MASK);
-		E1000_WRITE_FLUSH(&adapter->hw);
+		if (!rx_cleaned && !tx_cleaned)
+			break;
 	}
+
+	if (likely(adapter->itr_setting & 3))
+		e1000_set_itr(adapter);
+#endif
+
+	return IRQ_HANDLED;
 }
+#endif
 
 /**
  * e1000_intr - Interrupt Handler
  * @irq: interrupt number
  * @data: pointer to a network interface device structure
- * @pt_regs: CPU registers structure
  **/
-
-static irqreturn_t
-e1000_intr(int irq, void *data, struct pt_regs *regs)
+static irqreturn_t e1000_intr(int irq, void *data)
 {
 	struct net_device *netdev = data;
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
-	uint32_t icr = E1000_READ_REG(hw, ICR);
+	u32 rctl, icr = E1000_READ_REG(hw, E1000_ICR);
 #ifndef CONFIG_E1000_NAPI
-	unsigned int i;
+	int i, j;
+	int rx_cleaned, tx_cleaned;
 #endif
-
-	if(unlikely(!icr))
+	if (unlikely(!icr))
 		return IRQ_NONE;  /* Not our interrupt */
 
-	if(unlikely(icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC))) {
-		hw->get_link_status = 1;
-		mod_timer(&adapter->watchdog_timer, jiffies);
-	}
-
 #ifdef CONFIG_E1000_NAPI
-	if(likely(netif_rx_schedule_prep(netdev))) {
+	/* IMS will not auto-mask if INT_ASSERTED is not set, and if it is
+	 * not set, then the adapter didn't send an interrupt */
+	if (adapter->flags.int_assert_auto_mask &&
+	    !(icr & E1000_ICR_INT_ASSERTED))
+		return IRQ_NONE;
+
+	/* Interrupt Auto-Mask...upon reading ICR,
+	 * interrupts are masked.  No need for the
+	 * IMC write, but it does mean we should
+	 * account for it ASAP. */
+	if (likely(hw->mac.type >= e1000_82571))
+		atomic_inc(&adapter->irq_sem);
+#endif
 
-		/* Disable interrupts and register for poll. The flush 
-		  of the posted write is intentionally left out.
-		*/
+	if (unlikely(icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC))) {
+		hw->mac.get_link_status = 1;
+		/* ICH8 workaround-- Call gig speed drop workaround on cable
+		 * disconnect (LSC) before accessing any PHY registers */
+		if ((hw->mac.type == e1000_ich8lan) &&
+		    (hw->phy.type == e1000_phy_igp_3) &&
+		    (!(E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_LU)))
+			e1000_gig_downshift_workaround_ich8lan(hw);
+
+		/* 80003ES2LAN workaround--
+		 * For packet buffer work-around on link down event;
+		 * disable receives here in the ISR and
+		 * reset adapter in watchdog
+		 */
+		if (netif_carrier_ok(netdev) &&
+		    (hw->mac.type == e1000_80003es2lan)) {
+			/* disable receives */
+			rctl = E1000_READ_REG(hw, E1000_RCTL);
+			E1000_WRITE_REG(hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+		}
+		/* guard against interrupt when we're going down */
+		if (!test_bit(__E1000_DOWN, &adapter->state))
+			mod_timer(&adapter->watchdog_timer, jiffies + 1);
+	}
 
+#ifdef CONFIG_E1000_NAPI
+	if (hw->mac.type < e1000_82571) {
+		/* disable interrupts, without the synchronize_irq bit */
 		atomic_inc(&adapter->irq_sem);
-		E1000_WRITE_REG(hw, IMC, ~0);
+		E1000_WRITE_REG(hw, E1000_IMC, ~0);
+		E1000_WRITE_FLUSH(hw);
+	}
+	if (likely(netif_rx_schedule_prep(netdev))) {
+		adapter->total_tx_bytes = 0;
+		adapter->total_tx_packets = 0;
+		adapter->total_rx_bytes = 0;
+		adapter->total_rx_packets = 0;
 		__netif_rx_schedule(netdev);
+	} else {
+		atomic_dec(&adapter->irq_sem);
 	}
 #else
-	for(i = 0; i < E1000_MAX_INTR; i++)
-		if(unlikely(!e1000_clean_rx_irq(adapter) &
-		   !e1000_clean_tx_irq(adapter)))
-			break;
-#endif
+	/* Writing IMC and IMS is needed for 82547.
+	 * Due to Hub Link bus being occupied, an interrupt
+	 * de-assertion message is not able to be sent.
+	 * When an interrupt assertion message is generated later,
+	 * two messages are re-ordered and sent out.
+	 * That causes APIC to think 82547 is in de-assertion
+	 * state, while 82547 is in assertion state, resulting
+	 * in dead lock. Writing IMC forces 82547 into
+	 * de-assertion state.
+	 */
+	if (hw->mac.type == e1000_82547 || hw->mac.type == e1000_82547_rev_2) {
+		atomic_inc(&adapter->irq_sem);
+		E1000_WRITE_REG(hw, E1000_IMC, ~0);
+	}
+
+	adapter->total_tx_bytes = 0;
+	adapter->total_rx_bytes = 0;
+	adapter->total_tx_packets = 0;
+	adapter->total_rx_packets = 0;
+
+	for (i = 0; i < E1000_MAX_INTR; i++) {
+		rx_cleaned = 0;
+		for (j = 0; j < adapter->num_rx_queues; j++)
+			rx_cleaned |= adapter->clean_rx(adapter,
+			                                &adapter->rx_ring[j]);
+
+		tx_cleaned = 0;
+		for (j = 0 ; j < adapter->num_tx_queues ; j++)
+			tx_cleaned |= e1000_clean_tx_irq(adapter,
+			                                 &adapter->tx_ring[j]);
+
+		if (!rx_cleaned && !tx_cleaned)
+			break;
+	}
+
+	if (likely(adapter->itr_setting & 3))
+		e1000_set_itr(adapter);
+
+	if (hw->mac.type == e1000_82547 || hw->mac.type == e1000_82547_rev_2)
+		e1000_irq_enable(adapter);
 
+#endif
 	return IRQ_HANDLED;
 }
 
@@ -2164,323 +4532,1202 @@
  * e1000_clean - NAPI Rx polling callback
  * @adapter: board private structure
  **/
-
-static int
-e1000_clean(struct net_device *netdev, int *budget)
+static int e1000_clean(struct net_device *poll_dev, int *budget)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	int work_to_do = min(*budget, netdev->quota);
-	int tx_cleaned;
-	int work_done = 0;
-	
-	tx_cleaned = e1000_clean_tx_irq(adapter);
-	e1000_clean_rx_irq(adapter, &work_done, work_to_do);
+	struct e1000_adapter *adapter;
+	int work_to_do = min(*budget, poll_dev->quota);
+	int tx_clean_complete = 1, work_done = 0;
+	int i;
+
+	/* Must NOT use netdev_priv macro here. */
+	adapter = poll_dev->priv;
 
+	/* Keep link state information with original netdev */
+	if (!netif_carrier_ok(poll_dev))
+		goto quit_polling;
+
+	/* e1000_clean is called per-cpu.  This lock protects
+	 * tx_ring[i] from being cleaned by multiple cpus
+	 * simultaneously.  A failure obtaining the lock means
+	 * tx_ring[i] is currently being cleaned anyway. */
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+#ifdef CONFIG_E1000_MQ
+		if (spin_trylock(&adapter->tx_ring[i].tx_queue_lock)) {
+			tx_clean_complete &= e1000_clean_tx_irq(adapter,
+			                                &adapter->tx_ring[i]);
+			spin_unlock(&adapter->tx_ring[i].tx_queue_lock);
+		}
+#else
+		if (spin_trylock(&adapter->tx_queue_lock)) {
+			tx_clean_complete &= e1000_clean_tx_irq(adapter,
+			                                &adapter->tx_ring[i]);
+			spin_unlock(&adapter->tx_queue_lock);
+		}
+#endif
+	}
+
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		/* XXX if the number of queues was limited to a power of two
+		 * this would not need a div */
+		adapter->clean_rx(adapter, &adapter->rx_ring[i],
+		                  &work_done,
+		                  work_to_do / adapter->num_rx_queues);
+	}
 	*budget -= work_done;
-	netdev->quota -= work_done;
-	
-	/* if no Rx and Tx cleanup work was done, exit the polling mode */
-	if(!tx_cleaned || (work_done < work_to_do) || 
-				!netif_running(netdev)) {
-		netif_rx_complete(netdev);
-		e1000_irq_enable(adapter);
+	poll_dev->quota -= work_done;
+
+
+	/* If no Tx and not enough Rx work done, exit the polling mode */
+	if ((tx_clean_complete && (work_done == 0)) ||
+	   !netif_running(poll_dev)) {
+quit_polling:
+		if (likely(adapter->itr_setting & 3))
+			e1000_set_itr(adapter);
+		netif_rx_complete(poll_dev);
+		if (test_bit(__E1000_DOWN, &adapter->state))
+			atomic_dec(&adapter->irq_sem);
+		else
+			e1000_irq_enable(adapter);
 		return 0;
 	}
 
-	return (work_done >= work_to_do);
+	return 1;
 }
 
 #endif
 /**
  * e1000_clean_tx_irq - Reclaim resources after transmit completes
  * @adapter: board private structure
+ *
+ * the return value indicates whether actual cleaning was done, there
+ * is no guarantee that everything was cleaned
  **/
-
-static boolean_t
-e1000_clean_tx_irq(struct e1000_adapter *adapter)
+static boolean_t e1000_clean_tx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_tx_ring *tx_ring)
 {
-	struct e1000_desc_ring *tx_ring = &adapter->tx_ring;
 	struct net_device *netdev = adapter->netdev;
-	struct pci_dev *pdev = adapter->pdev;
 	struct e1000_tx_desc *tx_desc, *eop_desc;
 	struct e1000_buffer *buffer_info;
 	unsigned int i, eop;
+#ifdef CONFIG_E1000_NAPI
+	unsigned int count = 0;
+#endif
 	boolean_t cleaned = FALSE;
+	boolean_t retval = TRUE;
+	unsigned int total_tx_bytes=0, total_tx_packets=0;
 
 	i = tx_ring->next_to_clean;
 	eop = tx_ring->buffer_info[i].next_to_watch;
 	eop_desc = E1000_TX_DESC(*tx_ring, eop);
 
-	while(eop_desc->upper.data & cpu_to_le32(E1000_TXD_STAT_DD)) {
-		for(cleaned = FALSE; !cleaned; ) {
+	while (eop_desc->upper.data & cpu_to_le32(E1000_TXD_STAT_DD)) {
+		for (cleaned = FALSE; !cleaned; ) {
 			tx_desc = E1000_TX_DESC(*tx_ring, i);
 			buffer_info = &tx_ring->buffer_info[i];
+			cleaned = (i == eop);
 
-			if(likely(buffer_info->dma)) {
-				pci_unmap_page(pdev,
-					       buffer_info->dma,
-					       buffer_info->length,
-					       PCI_DMA_TODEVICE);
-				buffer_info->dma = 0;
-			}
-
-			if(buffer_info->skb) {
-				dev_kfree_skb_any(buffer_info->skb);
-				buffer_info->skb = NULL;
+#ifdef CONFIG_E1000_MQ
+			tx_ring->tx_stats.bytes += buffer_info->length;
+#endif
+			if (cleaned) {
+				struct sk_buff *skb = buffer_info->skb;
+#ifdef NETIF_F_TSO
+				unsigned int segs, bytecount;
+				segs = skb_shinfo(skb)->gso_segs ?: 1;
+				/* multiply data chunks by size of headers */
+				bytecount = ((segs - 1) * skb_headlen(skb)) +
+				            skb->len;
+				total_tx_packets += segs;
+				total_tx_bytes += bytecount;
+#else
+				total_tx_packets++;
+				total_tx_bytes += skb->len;
+#endif
 			}
-
-			tx_desc->buffer_addr = 0;
-			tx_desc->lower.data = 0;
+			e1000_unmap_and_free_tx_resource(adapter, buffer_info);
 			tx_desc->upper.data = 0;
 
-			cleaned = (i == eop);
-			if(unlikely(++i == tx_ring->count)) i = 0;
+			if (unlikely(++i == tx_ring->count)) i = 0;
 		}
-		
+
+#ifdef CONFIG_E1000_MQ
+		tx_ring->tx_stats.packets++;
+#endif
 		eop = tx_ring->buffer_info[i].next_to_watch;
 		eop_desc = E1000_TX_DESC(*tx_ring, eop);
+#ifdef CONFIG_E1000_NAPI
+#define E1000_TX_WEIGHT 64
+		/* weight of a sort for tx, to avoid endless transmit cleanup */
+		if (count++ == E1000_TX_WEIGHT) {
+			retval = FALSE;
+			break;
+		}
+#endif
 	}
 
 	tx_ring->next_to_clean = i;
 
-	spin_lock(&adapter->tx_lock);
+#define TX_WAKE_THRESHOLD 32
+	if (unlikely(cleaned && netif_carrier_ok(netdev) &&
+		     E1000_DESC_UNUSED(tx_ring) >= TX_WAKE_THRESHOLD)) {
+		/* Make sure that anybody stopping the queue after this
+		 * sees the new next_to_clean.
+		 */
+		smp_mb();
+	
+		if (netif_queue_stopped(netdev) &&
+		    !(test_bit(__E1000_DOWN, &adapter->state))) {
+			netif_wake_queue(netdev);
+			++adapter->restart_queue;
+		}
+	}
 
-	if(unlikely(cleaned && netif_queue_stopped(netdev) &&
-		    netif_carrier_ok(netdev)))
-		netif_wake_queue(netdev);
+	if (adapter->detect_tx_hung) {
+		/* Detect a transmit hang in hardware, this serializes the
+		 * check with the clearing of time_stamp and movement of i */
+		adapter->detect_tx_hung = FALSE;
+		if (tx_ring->buffer_info[eop].dma &&
+		    time_after(jiffies, tx_ring->buffer_info[eop].time_stamp +
+		               (adapter->tx_timeout_factor * HZ))
+		    && !(E1000_READ_REG(&adapter->hw, E1000_STATUS) &
+		         E1000_STATUS_TXOFF)) {
+
+			/* detected Tx unit hang */
+			DPRINTK(DRV, ERR, "Detected Tx Unit Hang\n"
+					"  Tx Queue             <%lu>\n"
+					"  TDH                  <%x>\n"
+					"  TDT                  <%x>\n"
+					"  next_to_use          <%x>\n"
+					"  next_to_clean        <%x>\n"
+					"buffer_info[next_to_clean]\n"
+					"  time_stamp           <%lx>\n"
+					"  next_to_watch        <%x>\n"
+					"  jiffies              <%lx>\n"
+					"  next_to_watch.status <%x>\n",
+				(unsigned long)((tx_ring - adapter->tx_ring) /
+					sizeof(struct e1000_tx_ring)),
+				readl(adapter->hw.hw_addr + tx_ring->tdh),
+				readl(adapter->hw.hw_addr + tx_ring->tdt),
+				tx_ring->next_to_use,
+				tx_ring->next_to_clean,
+				tx_ring->buffer_info[eop].time_stamp,
+				eop,
+				jiffies,
+				eop_desc->upper.fields.status);
+			netif_stop_queue(netdev);
+		}
+	}
+	adapter->total_tx_bytes += total_tx_bytes;
+	adapter->total_tx_packets += total_tx_packets;
+	return retval;
+}
 
-	spin_unlock(&adapter->tx_lock);
+/**
+ * e1000_rx_checksum - Receive Checksum Offload for 82543
+ * @adapter:     board private structure
+ * @status_err:  receive descriptor status and error fields
+ * @csum:        receive descriptor csum field
+ * @sk_buff:     socket buffer with received data
+ **/
+static void e1000_rx_checksum(struct e1000_adapter *adapter, u32 status_err,
+                              u32 csum, struct sk_buff *skb)
+{
+	u16 status = (u16)status_err;
+	u8 errors = (u8)(status_err >> 24);
+	skb->ip_summed = CHECKSUM_NONE;
 
-	return cleaned;
+	/* 82543 or newer only */
+	if (unlikely(adapter->hw.mac.type < e1000_82543)) return;
+	/* Ignore Checksum bit is set */
+	if (unlikely(status & E1000_RXD_STAT_IXSM)) return;
+	/* TCP/UDP checksum error bit is set */
+	if (unlikely(errors & E1000_RXD_ERR_TCPE)) {
+		/* let the stack verify checksum errors */
+		adapter->hw_csum_err++;
+		return;
+	}
+	/* TCP/UDP Checksum has not been calculated */
+	if (adapter->hw.mac.type <= e1000_82547_rev_2) {
+		if (!(status & E1000_RXD_STAT_TCPCS))
+			return;
+	} else {
+		if (!(status & (E1000_RXD_STAT_TCPCS | E1000_RXD_STAT_UDPCS)))
+			return;
+	}
+	/* It must be a TCP or UDP packet with a valid checksum */
+	if (likely(status & E1000_RXD_STAT_TCPCS)) {
+		/* TCP checksum is good */
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else if (adapter->hw.mac.type > e1000_82547_rev_2) {
+		/* IP fragment with UDP payload */
+		/* Hardware complements the payload checksum, so we undo it
+		 * and then put the value in host order for further stack use.
+		 */
+		csum = ntohl(csum ^ 0xFFFF);
+		skb->csum = csum;
+		skb->ip_summed = CHECKSUM_COMPLETE;
+	}
+	adapter->hw_csum_good++;
 }
 
 /**
- * e1000_clean_rx_irq - Send received data up the network stack
+ * e1000_receive_skb - helper function to handle rx indications
  * @adapter: board private structure
+ * @status: descriptor status field as written by hardware
+ * @vlan: descriptor vlan field as written by hardware (no le/be conversion)
+ * @skb: pointer to sk_buff to be indicated to stack
  **/
+static void e1000_receive_skb(struct e1000_adapter *adapter, u8 status,
+                              u16 vlan, struct sk_buff *skb)
+{
+#ifdef CONFIG_E1000_NAPI
+#ifdef NETIF_F_HW_VLAN_TX
+	if (unlikely(adapter->vlgrp && (status & E1000_RXD_STAT_VP))) {
+		vlan_hwaccel_receive_skb(skb, adapter->vlgrp,
+		                         le16_to_cpu(vlan) &
+		                         E1000_RXD_SPC_VLAN_MASK);
+	} else {
+		netif_receive_skb(skb);
+	}
+#else
+	netif_receive_skb(skb);
+#endif
+#else /* CONFIG_E1000_NAPI */
+#ifdef NETIF_F_HW_VLAN_TX
+	if (unlikely(adapter->vlgrp && (status & E1000_RXD_STAT_VP))) {
+		vlan_hwaccel_rx(skb, adapter->vlgrp,
+		                le16_to_cpu(vlan) & E1000_RXD_SPC_VLAN_MASK);
+	} else {
+		netif_rx(skb);
+	}
+#else
+	netif_rx(skb);
+#endif
+#endif /* CONFIG_E1000_NAPI */
+}
+
+#ifdef CONFIG_E1000_NAPI
+/* NOTE: these new jumbo frame routines rely on NAPI because of the
+ * pskb_may_pull call, which eventually must call kmap_atomic which you cannot
+ * call from hard irq context */
+
+/**
+ * e1000_consume_page - helper function
+ **/
+static void e1000_consume_page(struct e1000_rx_buffer *bi, struct sk_buff *skb,
+                               u16 length)
+{
+	bi->page = NULL;
+	skb->len += length;
+	skb->data_len += length;
+	skb->truesize += length;
+}
+
+/**
+ * e1000_clean_jumbo_rx_irq - Send received data up the network stack; legacy
+ * @adapter: board private structure
+ *
+ * the return value indicates whether actual cleaning was done, there
+ * is no guarantee that everything was cleaned
+ **/
+static boolean_t e1000_clean_jumbo_rx_irq(struct e1000_adapter *adapter,
+                                          struct e1000_rx_ring *rx_ring,
+                                          int *work_done, int work_to_do)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_rx_desc *rx_desc, *next_rxd;
+	struct e1000_rx_buffer *buffer_info, *next_buffer;
+	unsigned long irq_flags;
+	u32 length;
+	unsigned int i;
+	int cleaned_count = 0;
+	boolean_t cleaned = FALSE;
+	unsigned int total_rx_bytes=0, total_rx_packets=0;
+
+	i = rx_ring->next_to_clean;
+	rx_desc = E1000_RX_DESC(*rx_ring, i);
+	buffer_info = &rx_ring->buffer_info[i];
+
+	while (rx_desc->status & E1000_RXD_STAT_DD) {
+		struct sk_buff *skb;
+		u8 status;
+
+		if (*work_done >= work_to_do)
+			break;
+		(*work_done)++;
+
+		status = rx_desc->status;
+		skb = buffer_info->skb;
+		buffer_info->skb = NULL;
+
+		if (++i == rx_ring->count) i = 0;
+		next_rxd = E1000_RX_DESC(*rx_ring, i);
+		prefetch(next_rxd);
+
+		next_buffer = &rx_ring->buffer_info[i];
+
+		cleaned = TRUE;
+		cleaned_count++;
+		pci_unmap_page(pdev,
+		               buffer_info->dma,
+		               PAGE_SIZE,
+		               PCI_DMA_FROMDEVICE);
+		buffer_info->dma = 0;
+
+		length = le16_to_cpu(rx_desc->length);
+
+		/* errors is only valid for DD + EOP descriptors */
+		if (unlikely((status & E1000_RXD_STAT_EOP) &&
+		    (rx_desc->errors & E1000_RXD_ERR_FRAME_ERR_MASK))) {
+			u8 last_byte = *(skb->data + length - 1);
+			if (TBI_ACCEPT(&adapter->hw, status,
+			              rx_desc->errors, length, last_byte)) {
+				spin_lock_irqsave(&adapter->stats_lock,
+				                  irq_flags);
+				e1000_tbi_adjust_stats_82543(&adapter->hw,
+				                             &adapter->stats,
+				                             length, skb->data);
+				spin_unlock_irqrestore(&adapter->stats_lock,
+				                       irq_flags);
+				length--;
+			} else {
+				/* recycle both page and skb */
+				buffer_info->skb = skb;
+				/* an error means any chain goes out the window
+				 * too */
+				if (rx_ring->rx_skb_top)
+					dev_kfree_skb(rx_ring->rx_skb_top);
+				rx_ring->rx_skb_top = NULL;
+				goto next_desc;
+			}
+		}
+
+#define rxtop rx_ring->rx_skb_top
+		if (!(status & E1000_RXD_STAT_EOP)) {
+			/* this descriptor is only the beginning (or middle) */
+			if (!rxtop) {
+				/* this is the beginning of a chain */
+				rxtop = skb;
+				skb_fill_page_desc(rxtop, 0, buffer_info->page,
+				                   0, length);
+			} else {
+				/* this is the middle of a chain */
+				skb_fill_page_desc(rxtop,
+				    skb_shinfo(rxtop)->nr_frags,
+				    buffer_info->page, 0, length);
+				/* re-use the skb, only consumed the page */
+				buffer_info->skb = skb;
+			}
+			e1000_consume_page(buffer_info, rxtop, length);
+			goto next_desc;
+		} else {
+			if (rxtop) {
+				/* end of the chain */
+				skb_fill_page_desc(rxtop,
+				    skb_shinfo(rxtop)->nr_frags,
+				    buffer_info->page, 0, length);
+				/* re-use the current skb, we only consumed the
+				 * page */
+				buffer_info->skb = skb;
+				skb = rxtop;
+				rxtop = NULL;
+				e1000_consume_page(buffer_info, skb, length);
+			} else {
+				/* no chain, got EOP, this buf is the packet
+				 * copybreak to save the put_page/alloc_page */
+				if (length <= copybreak &&
+				    skb_tailroom(skb) >= length) {
+					u8 *vaddr;
+					vaddr = kmap_atomic(buffer_info->page,
+					                   KM_SKB_DATA_SOFTIRQ);
+					memcpy(skb_tail_pointer(skb), vaddr, length);
+					kunmap_atomic(vaddr,
+					              KM_SKB_DATA_SOFTIRQ);
+					/* re-use the page, so don't erase
+					 * buffer_info->page */
+					skb_put(skb, length);
+				} else {
+					skb_fill_page_desc(skb, 0,
+					                   buffer_info->page, 0,
+				                           length);
+					e1000_consume_page(buffer_info, skb,
+					                   length);
+				}
+			}
+		}
+
+		/* Receive Checksum Offload XXX recompute due to CRC strip? */
+		e1000_rx_checksum(adapter,
+		                  (u32)(status) |
+		                  ((u32)(rx_desc->errors) << 24),
+		                  le16_to_cpu(rx_desc->csum), skb);
+
+		pskb_trim(skb, skb->len - 4);
+
+		/* probably a little skewed due to removing CRC */
+		total_rx_bytes += skb->len;
+		total_rx_packets++;
+
+		/* eth type trans needs skb->data to point to something */
+		if (!pskb_may_pull(skb, ETH_HLEN)) {
+			DPRINTK(DRV, ERR, "__pskb_pull_tail failed.\n");
+			dev_kfree_skb(skb);
+			goto next_desc;
+		}
+
+		skb->protocol = eth_type_trans(skb, netdev);
+
+		e1000_receive_skb(adapter, status, rx_desc->special, skb);
+
+		netdev->last_rx = jiffies;
+#ifdef CONFIG_E1000_MQ
+		rx_ring->rx_stats.packets++;
+		rx_ring->rx_stats.bytes += length;
+#endif
+
+next_desc:
+		rx_desc->status = 0;
+
+		/* return some buffers to hardware, one at a time is too slow */
+		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE)) {
+			adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
+			cleaned_count = 0;
+		}
+
+		/* use prefetched values */
+		rx_desc = next_rxd;
+		buffer_info = next_buffer;
+	}
+	rx_ring->next_to_clean = i;
+
+	cleaned_count = E1000_DESC_UNUSED(rx_ring);
+	if (cleaned_count)
+		adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
 
-static boolean_t
+	adapter->total_rx_packets += total_rx_packets;
+	adapter->total_rx_bytes += total_rx_bytes;
+	return cleaned;
+}
+#endif /* NAPI */
+
+/**
+ * e1000_clean_rx_irq - Send received data up the network stack; legacy
+ * @adapter: board private structure
+ *
+ * the return value indicates whether actual cleaning was done, there
+ * is no guarantee that everything was cleaned
+ **/
 #ifdef CONFIG_E1000_NAPI
-e1000_clean_rx_irq(struct e1000_adapter *adapter, int *work_done,
-                   int work_to_do)
+static boolean_t e1000_clean_rx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring,
+                                    int *work_done, int work_to_do)
 #else
-e1000_clean_rx_irq(struct e1000_adapter *adapter)
+static boolean_t e1000_clean_rx_irq(struct e1000_adapter *adapter,
+                                    struct e1000_rx_ring *rx_ring)
 #endif
 {
-	struct e1000_desc_ring *rx_ring = &adapter->rx_ring;
 	struct net_device *netdev = adapter->netdev;
 	struct pci_dev *pdev = adapter->pdev;
-	struct e1000_rx_desc *rx_desc;
-	struct e1000_buffer *buffer_info;
-	struct sk_buff *skb;
-	unsigned long flags;
-	uint32_t length;
-	uint8_t last_byte;
+	struct e1000_rx_desc *rx_desc, *next_rxd;
+	struct e1000_rx_buffer *buffer_info, *next_buffer;
+	unsigned long irq_flags;
+	u32 length;
 	unsigned int i;
+	int cleaned_count = 0;
 	boolean_t cleaned = FALSE;
+	unsigned int total_rx_bytes=0, total_rx_packets=0;
 
 	i = rx_ring->next_to_clean;
 	rx_desc = E1000_RX_DESC(*rx_ring, i);
+	buffer_info = &rx_ring->buffer_info[i];
+
+	while (rx_desc->status & E1000_RXD_STAT_DD) {
+		struct sk_buff *skb;
+		u8 status;
 
-	while(rx_desc->status & E1000_RXD_STAT_DD) {
-		buffer_info = &rx_ring->buffer_info[i];
 #ifdef CONFIG_E1000_NAPI
-		if(*work_done >= work_to_do)
+		if (*work_done >= work_to_do)
 			break;
 		(*work_done)++;
 #endif
-		cleaned = TRUE;
+		status = rx_desc->status;
+		skb = buffer_info->skb;
+		buffer_info->skb = NULL;
+
+		prefetch(skb->data - NET_IP_ALIGN);
+
+		if (++i == rx_ring->count) i = 0;
+		next_rxd = E1000_RX_DESC(*rx_ring, i);
+		prefetch(next_rxd);
 
+		next_buffer = &rx_ring->buffer_info[i];
+
+		cleaned = TRUE;
+		cleaned_count++;
 		pci_unmap_single(pdev,
 		                 buffer_info->dma,
-		                 buffer_info->length,
+		                 adapter->rx_buffer_len,
 		                 PCI_DMA_FROMDEVICE);
+		buffer_info->dma = 0;
+
+		length = le16_to_cpu(rx_desc->length);
+
+		/* !EOP means multiple descriptors were used to store a single
+		 * packet, also make sure the frame isn't just CRC only */
+		if (unlikely(!(status & E1000_RXD_STAT_EOP) || (length <= 4))) {
+			/* All receives must fit into a single buffer */
+			E1000_DBG("%s: Receive packet consumed multiple"
+				  " buffers\n", netdev->name);
+			/* recycle */
+			buffer_info->skb = skb;
+			goto next_desc;
+		}
+
+		if (unlikely(rx_desc->errors & E1000_RXD_ERR_FRAME_ERR_MASK)) {
+			u8 last_byte = *(skb->data + length - 1);
+			if (TBI_ACCEPT(&adapter->hw, status,
+			              rx_desc->errors, length, last_byte)) {
+				spin_lock_irqsave(&adapter->stats_lock,
+				                  irq_flags);
+				e1000_tbi_adjust_stats_82543(&adapter->hw,
+				                             &adapter->stats,
+				                             length, skb->data);
+				spin_unlock_irqrestore(&adapter->stats_lock,
+				                       irq_flags);
+				length--;
+			} else {
+				/* recycle */
+				buffer_info->skb = skb;
+				goto next_desc;
+			}
+		}
+
+		/* adjust length to remove Ethernet CRC, this must be
+		 * done after the TBI_ACCEPT workaround above */
+		length -= 4;
+
+		/* probably a little skewed due to removing CRC */
+		total_rx_bytes += length;
+		total_rx_packets++;
+
+		/* code added for copybreak, this should improve
+		 * performance for small packets with large amounts
+		 * of reassembly being done in the stack */
+		if (length < copybreak) {
+			struct sk_buff *new_skb =
+			    netdev_alloc_skb(netdev, length + NET_IP_ALIGN);
+			if (new_skb) {
+				skb_reserve(new_skb, NET_IP_ALIGN);
+				skb_copy_to_linear_data_offset(new_skb,
+				                               -NET_IP_ALIGN,
+				                               (skb->data -
+				                                NET_IP_ALIGN),
+				                               (length +
+				                                NET_IP_ALIGN));
+				/* save the skb in buffer_info as good */
+				buffer_info->skb = skb;
+				skb = new_skb;
+			}
+			/* else just continue with the old one */
+		}
+		/* end copybreak code */
+		skb_put(skb, length);
+
+		/* Receive Checksum Offload */
+		e1000_rx_checksum(adapter,
+				  (u32)(status) |
+				  ((u32)(rx_desc->errors) << 24),
+				  le16_to_cpu(rx_desc->csum), skb);
+
+		skb->protocol = eth_type_trans(skb, netdev);
+
+		e1000_receive_skb(adapter, status, rx_desc->special, skb);
+
+		netdev->last_rx = jiffies;
+#ifdef CONFIG_E1000_MQ
+		rx_ring->rx_stats.packets++;
+		rx_ring->rx_stats.bytes += length;
+#endif
+
+next_desc:
+		rx_desc->status = 0;
+
+		/* return some buffers to hardware, one at a time is too slow */
+		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE)) {
+			adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
+			cleaned_count = 0;
+		}
+
+		/* use prefetched values */
+		rx_desc = next_rxd;
+		buffer_info = next_buffer;
+	}
+	rx_ring->next_to_clean = i;
+
+	cleaned_count = E1000_DESC_UNUSED(rx_ring);
+	if (cleaned_count)
+		adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
+
+	adapter->total_rx_packets += total_rx_packets;
+	adapter->total_rx_bytes += total_rx_bytes;
+	return cleaned;
+}
+
+/**
+ * e1000_clean_rx_irq_ps - Send received data up the network stack; packet split
+ * @adapter: board private structure
+ *
+ * the return value indicates whether actual cleaning was done, there
+ * is no guarantee that everything was cleaned
+ **/
+#ifdef CONFIG_E1000_NAPI
+static boolean_t e1000_clean_rx_irq_ps(struct e1000_adapter *adapter,
+                                       struct e1000_rx_ring *rx_ring,
+                                       int *work_done, int work_to_do)
+#else
+static boolean_t e1000_clean_rx_irq_ps(struct e1000_adapter *adapter,
+                                       struct e1000_rx_ring *rx_ring)
+#endif
+{
+	union e1000_rx_desc_packet_split *rx_desc, *next_rxd;
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_rx_buffer *buffer_info, *next_buffer;
+	struct e1000_ps_page *ps_page;
+	struct e1000_ps_page_dma *ps_page_dma;
+	struct sk_buff *skb;
+	unsigned int i, j;
+	u32 length, staterr;
+	int cleaned_count = 0;
+	boolean_t cleaned = FALSE;
+	unsigned int total_rx_bytes=0, total_rx_packets=0;
+
+	i = rx_ring->next_to_clean;
+	rx_desc = E1000_RX_DESC_PS(*rx_ring, i);
+	staterr = le32_to_cpu(rx_desc->wb.middle.status_error);
+	buffer_info = &rx_ring->buffer_info[i];
+
+	while (staterr & E1000_RXD_STAT_DD) {
+		ps_page = &rx_ring->ps_page[i];
+		ps_page_dma = &rx_ring->ps_page_dma[i];
+#ifdef CONFIG_E1000_NAPI
+		if (unlikely(*work_done >= work_to_do))
+			break;
+		(*work_done)++;
+#endif
+		skb = buffer_info->skb;
+
+		/* in the packet split case this is header only */
+		prefetch(skb->data - NET_IP_ALIGN);
+
+		if (++i == rx_ring->count) i = 0;
+		next_rxd = E1000_RX_DESC_PS(*rx_ring, i);
+		prefetch(next_rxd);
+
+		next_buffer = &rx_ring->buffer_info[i];
+
+		cleaned = TRUE;
+		cleaned_count++;
+		pci_unmap_single(pdev, buffer_info->dma,
+		                 adapter->rx_ps_bsize0,
+		                 PCI_DMA_FROMDEVICE);
+		buffer_info->dma = 0;
+
+		if (unlikely(!(staterr & E1000_RXD_STAT_EOP))) {
+			E1000_DBG("%s: Packet Split buffers didn't pick up"
+				  " the full packet\n", netdev->name);
+			dev_kfree_skb_irq(skb);
+			goto next_desc;
+		}
+
+		if (unlikely(staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK)) {
+			dev_kfree_skb_irq(skb);
+			goto next_desc;
+		}
+
+		length = le16_to_cpu(rx_desc->wb.middle.length0);
+
+		if (unlikely(!length)) {
+			E1000_DBG("%s: Last part of the packet spanning"
+				  " multiple descriptors\n", netdev->name);
+			dev_kfree_skb_irq(skb);
+			goto next_desc;
+		}
+
+		/* Good Receive */
+		skb_put(skb, length);
+#ifdef CONFIG_E1000_MQ
+		rx_ring->rx_stats.packets++;
+		rx_ring->rx_stats.bytes += skb->len;
+#endif
+
+#ifdef CONFIG_E1000_NAPI
+		{
+		/* this looks ugly, but it seems compiler issues make it
+		   more efficient than reusing j */
+		int l1 = le16_to_cpu(rx_desc->wb.upper.length[0]);
+
+		/* page alloc/put takes too long and effects small packet
+		 * throughput, so unsplit small packets and save the alloc/put
+		 * only valid in softirq (napi) context to call kmap_* */
+		if (l1 && (l1 <= copybreak) &&
+		    ((length + l1) <= adapter->rx_ps_bsize0)) {
+			u8 *vaddr;
+			/* there is no documentation about how to call
+			 * kmap_atomic, so we can't hold the mapping
+			 * very long */
+			pci_dma_sync_single_for_cpu(pdev,
+				ps_page_dma->ps_page_dma[0],
+				PAGE_SIZE,
+				PCI_DMA_FROMDEVICE);
+			vaddr = kmap_atomic(ps_page->ps_page[0],
+			                    KM_SKB_DATA_SOFTIRQ);
+			memcpy(skb->tail, vaddr, l1);
+			kunmap_atomic(vaddr, KM_SKB_DATA_SOFTIRQ);
+			pci_dma_sync_single_for_device(pdev,
+				ps_page_dma->ps_page_dma[0],
+				PAGE_SIZE, PCI_DMA_FROMDEVICE);
+			/* remove the CRC */
+			l1 -= 4;
+			skb_put(skb, l1);
+			goto copydone;
+		} /* if */
+		}
+#endif
+
+		for (j = 0; j < adapter->rx_ps_pages; j++) {
+			if (!(length= le16_to_cpu(rx_desc->wb.upper.length[j])))
+				break;
+			pci_unmap_page(pdev, ps_page_dma->ps_page_dma[j],
+					PAGE_SIZE, PCI_DMA_FROMDEVICE);
+			ps_page_dma->ps_page_dma[j] = 0;
+			skb_fill_page_desc(skb, j, ps_page->ps_page[j], 0,
+			                   length);
+			ps_page->ps_page[j] = NULL;
+			skb->len += length;
+			skb->data_len += length;
+			skb->truesize += length;
+		}
+
+		/* strip the ethernet crc, problem is we're using pages now so
+		 * this whole operation can get a little cpu intensive */
+		pskb_trim(skb, skb->len - 4);
+
+#ifdef CONFIG_E1000_NAPI
+copydone:
+#endif
+		total_rx_bytes += skb->len;
+		total_rx_packets++;
+
+		e1000_rx_checksum(adapter, staterr,
+				  le16_to_cpu(rx_desc->wb.lower.hi_dword.csum_ip.csum), skb);
+		skb->protocol = eth_type_trans(skb, netdev);
+
+		if (likely(rx_desc->wb.upper.header_status &
+			   cpu_to_le16(E1000_RXDPS_HDRSTAT_HDRSP)))
+			adapter->rx_hdr_split++;
+
+		e1000_receive_skb(adapter, staterr, rx_desc->wb.middle.vlan,
+		                  skb);
+		netdev->last_rx = jiffies;
+
+next_desc:
+		rx_desc->wb.middle.status_error &= cpu_to_le32(~0xFF);
+		buffer_info->skb = NULL;
+
+		/* return some buffers to hardware, one at a time is too slow */
+		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE)) {
+			adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
+			cleaned_count = 0;
+		}
+
+		/* use prefetched values */
+		rx_desc = next_rxd;
+		buffer_info = next_buffer;
+
+		staterr = le32_to_cpu(rx_desc->wb.middle.status_error);
+	}
+	rx_ring->next_to_clean = i;
+
+	cleaned_count = E1000_DESC_UNUSED(rx_ring);
+	if (cleaned_count)
+		adapter->alloc_rx_buf(adapter, rx_ring, cleaned_count);
+
+	adapter->total_rx_packets += total_rx_packets;
+	adapter->total_rx_bytes += total_rx_bytes;
+	return cleaned;
+}
+
+#ifdef CONFIG_E1000_NAPI
+/**
+ * e1000_alloc_jumbo_rx_buffers - Replace used jumbo receive buffers
+ * @adapter: address of board private structure
+ * @rx_ring: pointer to receive ring structure
+ * @cleaned_count: number of buffers to allocate this pass
+ **/
+static void e1000_alloc_jumbo_rx_buffers(struct e1000_adapter *adapter,
+                                         struct e1000_rx_ring *rx_ring,
+                                         int cleaned_count)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_rx_desc *rx_desc;
+	struct e1000_rx_buffer *buffer_info;
+	struct sk_buff *skb;
+	unsigned int i;
+	unsigned int bufsz = 256 -
+	                     16 /*for skb_reserve */ -
+	                     NET_IP_ALIGN;
+
+	i = rx_ring->next_to_use;
+	buffer_info = &rx_ring->buffer_info[i];
 
+	while (cleaned_count--) {
 		skb = buffer_info->skb;
-		length = le16_to_cpu(rx_desc->length);
-
-		if(unlikely(!(rx_desc->status & E1000_RXD_STAT_EOP))) {
-			/* All receives must fit into a single buffer */
-			E1000_DBG("%s: Receive packet consumed multiple"
-				  " buffers\n", netdev->name);
-			dev_kfree_skb_irq(skb);
-			goto next_desc;
+		if (skb) {
+			skb_trim(skb, 0);
+			goto check_page;
 		}
 
-		if(unlikely(rx_desc->errors & E1000_RXD_ERR_FRAME_ERR_MASK)) {
-			last_byte = *(skb->data + length - 1);
-			if(TBI_ACCEPT(&adapter->hw, rx_desc->status,
-			              rx_desc->errors, length, last_byte)) {
-				spin_lock_irqsave(&adapter->stats_lock, flags);
-				e1000_tbi_adjust_stats(&adapter->hw,
-				                       &adapter->stats,
-				                       length, skb->data);
-				spin_unlock_irqrestore(&adapter->stats_lock,
-				                       flags);
-				length--;
-			} else {
-				dev_kfree_skb_irq(skb);
-				goto next_desc;
-			}
+		skb = netdev_alloc_skb(netdev, bufsz);
+		if (unlikely(!skb)) {
+			/* Better luck next round */
+			adapter->alloc_rx_buff_failed++;
+			break;
 		}
 
-		/* Good Receive */
-		skb_put(skb, length - ETHERNET_FCS_SIZE);
+		/* Fix for errata 23, can't cross 64kB boundary */
+		if (!e1000_check_64k_bound(adapter, skb->data, bufsz)) {
+			struct sk_buff *oldskb = skb;
+			DPRINTK(PROBE, ERR, "skb align check failed: %u bytes "
+					     "at %p\n", bufsz, skb->data);
+			/* Try again, without freeing the previous */
+			skb = netdev_alloc_skb(netdev, bufsz);
+			/* Failed allocation, critical failure */
+			if (!skb) {
+				dev_kfree_skb(oldskb);
+				adapter->alloc_rx_buff_failed++;
+				break;
+			}
 
-		/* Receive Checksum Offload */
-		e1000_rx_checksum(adapter, rx_desc, skb);
+			if (!e1000_check_64k_bound(adapter, skb->data, bufsz)) {
+				/* give up */
+				dev_kfree_skb(skb);
+				dev_kfree_skb(oldskb);
+				adapter->alloc_rx_buff_failed++;
+				break; /* while !buffer_info->skb */
+			}
 
-		skb->protocol = eth_type_trans(skb, netdev);
-#ifdef CONFIG_E1000_NAPI
-		if(unlikely(adapter->vlgrp &&
-			    (rx_desc->status & E1000_RXD_STAT_VP))) {
-			vlan_hwaccel_receive_skb(skb, adapter->vlgrp,
-						 le16_to_cpu(rx_desc->special &
-						 E1000_RXD_SPC_VLAN_MASK));
-		} else {
-			netif_receive_skb(skb);
+			/* Use new allocation */
+			dev_kfree_skb(oldskb);
 		}
-#else /* CONFIG_E1000_NAPI */
-		if(unlikely(adapter->vlgrp &&
-			    (rx_desc->status & E1000_RXD_STAT_VP))) {
-			vlan_hwaccel_rx(skb, adapter->vlgrp,
-					le16_to_cpu(rx_desc->special &
-					E1000_RXD_SPC_VLAN_MASK));
-		} else {
-			netif_rx(skb);
+		/* Make buffer alignment 2 beyond a 16 byte boundary
+		 * this will result in a 16 byte aligned IP header after
+		 * the 14 byte MAC header is removed
+		 */
+		skb_reserve(skb, NET_IP_ALIGN);
+
+		buffer_info->skb = skb;
+check_page:
+		/* allocate a new page if necessary */
+		if (!buffer_info->page) {
+			buffer_info->page = alloc_page(GFP_ATOMIC);
+			if (unlikely(!buffer_info->page)) {
+				adapter->alloc_rx_buff_failed++;
+				break;
+			}
 		}
-#endif /* CONFIG_E1000_NAPI */
-		netdev->last_rx = jiffies;
 
-next_desc:
-		rx_desc->status = 0;
-		buffer_info->skb = NULL;
-		if(unlikely(++i == rx_ring->count)) i = 0;
+		if (!buffer_info->dma)
+			buffer_info->dma = pci_map_page(pdev,
+			                                buffer_info->page, 0,
+			                                PAGE_SIZE,
+			                                PCI_DMA_FROMDEVICE);
 
 		rx_desc = E1000_RX_DESC(*rx_ring, i);
-	}
-
-	rx_ring->next_to_clean = i;
+		rx_desc->buffer_addr = cpu_to_le64(buffer_info->dma);
 
-	e1000_alloc_rx_buffers(adapter);
+		if (unlikely(++i == rx_ring->count))
+			i = 0;
+		buffer_info = &rx_ring->buffer_info[i];
+	}
 
-	return cleaned;
+	if (likely(rx_ring->next_to_use != i)) {
+		rx_ring->next_to_use = i;
+		if (unlikely(i-- == 0))
+			i = (rx_ring->count - 1);
+
+		/* Force memory writes to complete before letting h/w
+		 * know there are new descriptors to fetch.  (Only
+		 * applicable for weak-ordered memory model archs,
+		 * such as IA-64). */
+		wmb();
+		writel(i, adapter->hw.hw_addr + rx_ring->rdt);
+	}
 }
+#endif /* NAPI */
 
 /**
- * e1000_alloc_rx_buffers - Replace used receive buffers
+ * e1000_alloc_rx_buffers - Replace used receive buffers; legacy & extended
  * @adapter: address of board private structure
  **/
-
-static void
-e1000_alloc_rx_buffers(struct e1000_adapter *adapter)
+static void e1000_alloc_rx_buffers(struct e1000_adapter *adapter,
+                                   struct e1000_rx_ring *rx_ring,
+                                   int cleaned_count)
 {
-	struct e1000_desc_ring *rx_ring = &adapter->rx_ring;
 	struct net_device *netdev = adapter->netdev;
 	struct pci_dev *pdev = adapter->pdev;
 	struct e1000_rx_desc *rx_desc;
-	struct e1000_buffer *buffer_info;
+	struct e1000_rx_buffer *buffer_info;
 	struct sk_buff *skb;
 	unsigned int i;
+	unsigned int bufsz = adapter->rx_buffer_len + NET_IP_ALIGN;
 
 	i = rx_ring->next_to_use;
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while(!buffer_info->skb) {
+	while (cleaned_count--) {
+		skb = buffer_info->skb;
+		if (skb) {
+			skb_trim(skb, 0);
+			goto map_skb;
+		}
 
-		skb = dev_alloc_skb(adapter->rx_buffer_len + NET_IP_ALIGN);
-		if(unlikely(!skb)) {
+		skb = netdev_alloc_skb(netdev, bufsz);
+		if (unlikely(!skb)) {
 			/* Better luck next round */
+			adapter->alloc_rx_buff_failed++;
 			break;
 		}
 
+		/* Fix for errata 23, can't cross 64kB boundary */
+		if (!e1000_check_64k_bound(adapter, skb->data, bufsz)) {
+			struct sk_buff *oldskb = skb;
+			DPRINTK(RX_ERR, ERR, "skb align check failed: %u bytes "
+					     "at %p\n", bufsz, skb->data);
+			/* Try again, without freeing the previous */
+			skb = netdev_alloc_skb(netdev, bufsz);
+			/* Failed allocation, critical failure */
+			if (!skb) {
+				dev_kfree_skb(oldskb);
+				adapter->alloc_rx_buff_failed++;
+				break;
+			}
+
+			if (!e1000_check_64k_bound(adapter, skb->data, bufsz)) {
+				/* give up */
+				dev_kfree_skb(skb);
+				dev_kfree_skb(oldskb);
+				adapter->alloc_rx_buff_failed++;
+				break; /* while !buffer_info->skb */
+			}
+
+			/* Use new allocation */
+			dev_kfree_skb(oldskb);
+		}
 		/* Make buffer alignment 2 beyond a 16 byte boundary
 		 * this will result in a 16 byte aligned IP header after
 		 * the 14 byte MAC header is removed
 		 */
 		skb_reserve(skb, NET_IP_ALIGN);
 
-		skb->dev = netdev;
-
 		buffer_info->skb = skb;
-		buffer_info->length = adapter->rx_buffer_len;
+map_skb:
 		buffer_info->dma = pci_map_single(pdev,
 						  skb->data,
 						  adapter->rx_buffer_len,
 						  PCI_DMA_FROMDEVICE);
 
+		/* Fix for errata 23, can't cross 64kB boundary */
+		if (!e1000_check_64k_bound(adapter,
+					(void *)(unsigned long)buffer_info->dma,
+					adapter->rx_buffer_len)) {
+			DPRINTK(RX_ERR, ERR,
+				"dma align check failed: %u bytes at %p\n",
+				adapter->rx_buffer_len,
+				(void *)(unsigned long)buffer_info->dma);
+			dev_kfree_skb(skb);
+			buffer_info->skb = NULL;
+
+			pci_unmap_single(pdev, buffer_info->dma,
+					 adapter->rx_buffer_len,
+					 PCI_DMA_FROMDEVICE);
+			buffer_info->dma = 0;
+
+			adapter->alloc_rx_buff_failed++;
+			break; /* while !buffer_info->skb */
+		}
 		rx_desc = E1000_RX_DESC(*rx_ring, i);
 		rx_desc->buffer_addr = cpu_to_le64(buffer_info->dma);
 
-		if(unlikely((i & ~(E1000_RX_BUFFER_WRITE - 1)) == i)) {
-			/* Force memory writes to complete before letting h/w
-			 * know there are new descriptors to fetch.  (Only
-			 * applicable for weak-ordered memory model archs,
-			 * such as IA-64). */
-			wmb();
+		if (unlikely(++i == rx_ring->count))
+			i = 0;
+		buffer_info = &rx_ring->buffer_info[i];
+	}
+
+	if (likely(rx_ring->next_to_use != i)) {
+		rx_ring->next_to_use = i;
+		if (unlikely(i-- == 0))
+			i = (rx_ring->count - 1);
+
+		/* Force memory writes to complete before letting h/w
+		 * know there are new descriptors to fetch.  (Only
+		 * applicable for weak-ordered memory model archs,
+		 * such as IA-64). */
+		wmb();
+		writel(i, adapter->hw.hw_addr + rx_ring->rdt);
+	}
+}
+
+/**
+ * e1000_alloc_rx_buffers_ps - Replace used receive buffers; packet split
+ * @adapter: address of board private structure
+ **/
+static void e1000_alloc_rx_buffers_ps(struct e1000_adapter *adapter,
+                                      struct e1000_rx_ring *rx_ring,
+                                      int cleaned_count)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+	union e1000_rx_desc_packet_split *rx_desc;
+	struct e1000_rx_buffer *buffer_info;
+	struct e1000_ps_page *ps_page;
+	struct e1000_ps_page_dma *ps_page_dma;
+	struct sk_buff *skb;
+	unsigned int i, j;
+
+	i = rx_ring->next_to_use;
+	buffer_info = &rx_ring->buffer_info[i];
+	ps_page = &rx_ring->ps_page[i];
+	ps_page_dma = &rx_ring->ps_page_dma[i];
+
+	while (cleaned_count--) {
+		rx_desc = E1000_RX_DESC_PS(*rx_ring, i);
 
-			E1000_WRITE_REG(&adapter->hw, RDT, i);
+		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
+			if (j < adapter->rx_ps_pages) {
+				if (likely(!ps_page->ps_page[j])) {
+					ps_page->ps_page[j] =
+						alloc_page(GFP_ATOMIC);
+					if (unlikely(!ps_page->ps_page[j])) {
+						adapter->alloc_rx_buff_failed++;
+						goto no_buffers;
+					}
+					ps_page_dma->ps_page_dma[j] =
+						pci_map_page(pdev,
+							    ps_page->ps_page[j],
+							    0, PAGE_SIZE,
+							    PCI_DMA_FROMDEVICE);
+				}
+				/* Refresh the desc even if buffer_addrs didn't
+				 * change because each write-back erases
+				 * this info.
+				 */
+				rx_desc->read.buffer_addr[j+1] =
+				     cpu_to_le64(ps_page_dma->ps_page_dma[j]);
+			} else {
+				rx_desc->read.buffer_addr[j+1] = ~0;
+			}
+		}
+
+		skb = netdev_alloc_skb(netdev,
+		                       adapter->rx_ps_bsize0 + NET_IP_ALIGN);
+
+		if (unlikely(!skb)) {
+			adapter->alloc_rx_buff_failed++;
+			break;
 		}
 
-		if(unlikely(++i == rx_ring->count)) i = 0;
+		/* Make buffer alignment 2 beyond a 16 byte boundary
+		 * this will result in a 16 byte aligned IP header after
+		 * the 14 byte MAC header is removed
+		 */
+		skb_reserve(skb, NET_IP_ALIGN);
+
+		buffer_info->skb = skb;
+		buffer_info->dma = pci_map_single(pdev, skb->data,
+		                                  adapter->rx_ps_bsize0,
+		                                  PCI_DMA_FROMDEVICE);
+
+		rx_desc->read.buffer_addr[0] = cpu_to_le64(buffer_info->dma);
+
+		if (unlikely(++i == rx_ring->count)) i = 0;
 		buffer_info = &rx_ring->buffer_info[i];
+		ps_page = &rx_ring->ps_page[i];
+		ps_page_dma = &rx_ring->ps_page_dma[i];
 	}
 
-	rx_ring->next_to_use = i;
+no_buffers:
+	if (likely(rx_ring->next_to_use != i)) {
+		rx_ring->next_to_use = i;
+		if (unlikely(i-- == 0)) i = (rx_ring->count - 1);
+
+		/* Force memory writes to complete before letting h/w
+		 * know there are new descriptors to fetch.  (Only
+		 * applicable for weak-ordered memory model archs,
+		 * such as IA-64). */
+		wmb();
+		/* Hardware increments by 16 bytes, but packet split
+		 * descriptors are 32 bytes...so we increment tail
+		 * twice as much.
+		 */
+		writel(i<<1, adapter->hw.hw_addr + rx_ring->rdt);
+	}
 }
 
 /**
  * e1000_smartspeed - Workaround for SmartSpeed on 82541 and 82547 controllers.
  * @adapter:
  **/
-
-static void
-e1000_smartspeed(struct e1000_adapter *adapter)
+static void e1000_smartspeed(struct e1000_adapter *adapter)
 {
-	uint16_t phy_status;
-	uint16_t phy_ctrl;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+	struct e1000_phy_info *phy = &adapter->hw.phy;
+	u16 phy_status;
+	u16 phy_ctrl;
 
-	if((adapter->hw.phy_type != e1000_phy_igp) || !adapter->hw.autoneg ||
-	   !(adapter->hw.autoneg_advertised & ADVERTISE_1000_FULL))
+	if ((phy->type != e1000_phy_igp) || !mac->autoneg ||
+	    !(phy->autoneg_advertised & ADVERTISE_1000_FULL))
 		return;
 
-	if(adapter->smartspeed == 0) {
+	if (adapter->smartspeed == 0) {
 		/* If Master/Slave config fault is asserted twice,
 		 * we assume back-to-back */
 		e1000_read_phy_reg(&adapter->hw, PHY_1000T_STATUS, &phy_status);
-		if(!(phy_status & SR_1000T_MS_CONFIG_FAULT)) return;
+		if (!(phy_status & SR_1000T_MS_CONFIG_FAULT)) return;
 		e1000_read_phy_reg(&adapter->hw, PHY_1000T_STATUS, &phy_status);
-		if(!(phy_status & SR_1000T_MS_CONFIG_FAULT)) return;
+		if (!(phy_status & SR_1000T_MS_CONFIG_FAULT)) return;
 		e1000_read_phy_reg(&adapter->hw, PHY_1000T_CTRL, &phy_ctrl);
-		if(phy_ctrl & CR_1000T_MS_ENABLE) {
+		if (phy_ctrl & CR_1000T_MS_ENABLE) {
 			phy_ctrl &= ~CR_1000T_MS_ENABLE;
 			e1000_write_phy_reg(&adapter->hw, PHY_1000T_CTRL,
 					    phy_ctrl);
 			adapter->smartspeed++;
-			if(!e1000_phy_setup_autoneg(&adapter->hw) &&
-			   !e1000_read_phy_reg(&adapter->hw, PHY_CTRL,
+			if (!e1000_phy_setup_autoneg(&adapter->hw) &&
+			   !e1000_read_phy_reg(&adapter->hw, PHY_CONTROL,
 				   	       &phy_ctrl)) {
 				phy_ctrl |= (MII_CR_AUTO_NEG_EN |
 					     MII_CR_RESTART_AUTO_NEG);
-				e1000_write_phy_reg(&adapter->hw, PHY_CTRL,
+				e1000_write_phy_reg(&adapter->hw, PHY_CONTROL,
 						    phy_ctrl);
 			}
 		}
 		return;
-	} else if(adapter->smartspeed == E1000_SMARTSPEED_DOWNSHIFT) {
+	} else if (adapter->smartspeed == E1000_SMARTSPEED_DOWNSHIFT) {
 		/* If still no link, perhaps using 2/3 pair cable */
 		e1000_read_phy_reg(&adapter->hw, PHY_1000T_CTRL, &phy_ctrl);
 		phy_ctrl |= CR_1000T_MS_ENABLE;
 		e1000_write_phy_reg(&adapter->hw, PHY_1000T_CTRL, phy_ctrl);
-		if(!e1000_phy_setup_autoneg(&adapter->hw) &&
-		   !e1000_read_phy_reg(&adapter->hw, PHY_CTRL, &phy_ctrl)) {
+		if (!e1000_phy_setup_autoneg(&adapter->hw) &&
+		   !e1000_read_phy_reg(&adapter->hw, PHY_CONTROL, &phy_ctrl)) {
 			phy_ctrl |= (MII_CR_AUTO_NEG_EN |
 				     MII_CR_RESTART_AUTO_NEG);
-			e1000_write_phy_reg(&adapter->hw, PHY_CTRL, phy_ctrl);
+			e1000_write_phy_reg(&adapter->hw, PHY_CONTROL, phy_ctrl);
 		}
 	}
 	/* Restart process after E1000_SMARTSPEED_MAX iterations */
-	if(adapter->smartspeed++ == E1000_SMARTSPEED_MAX)
+	if (adapter->smartspeed++ == E1000_SMARTSPEED_MAX)
 		adapter->smartspeed = 0;
 }
 
@@ -2490,452 +5737,548 @@
  * @ifreq:
  * @cmd:
  **/
-
-static int
-e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
+static int e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
 {
 	switch (cmd) {
+#ifdef SIOCGMIIPHY
 	case SIOCGMIIPHY:
 	case SIOCGMIIREG:
 	case SIOCSMIIREG:
 		return e1000_mii_ioctl(netdev, ifr, cmd);
+#endif
+#ifdef ETHTOOL_OPS_COMPAT
+	case SIOCETHTOOL:
+		return ethtool_ioctl(ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#ifdef SIOCGMIIPHY
 /**
  * e1000_mii_ioctl -
  * @netdev:
  * @ifreq:
  * @cmd:
  **/
-
-static int
-e1000_mii_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
+static int e1000_mii_ioctl(struct net_device *netdev, struct ifreq *ifr,
+                           int cmd)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct mii_ioctl_data *data = if_mii(ifr);
-	int retval;
-	uint16_t mii_reg;
-	uint16_t spddplx;
+	unsigned long irq_flags;
 
-	if(adapter->hw.media_type != e1000_media_type_copper)
+	if (adapter->hw.media_type != e1000_media_type_copper)
 		return -EOPNOTSUPP;
 
 	switch (cmd) {
 	case SIOCGMIIPHY:
-		data->phy_id = adapter->hw.phy_addr;
+		data->phy_id = adapter->hw.phy.addr;
 		break;
 	case SIOCGMIIREG:
 		if (!capable(CAP_NET_ADMIN))
 			return -EPERM;
+		spin_lock_irqsave(&adapter->stats_lock, irq_flags);
 		if (e1000_read_phy_reg(&adapter->hw, data->reg_num & 0x1F,
-				   &data->val_out))
-			return -EIO;
-		break;
-	case SIOCSMIIREG:
-		if (!capable(CAP_NET_ADMIN))
-			return -EPERM;
-		if (data->reg_num & ~(0x1F))
-			return -EFAULT;
-		mii_reg = data->val_in;
-		if (e1000_write_phy_reg(&adapter->hw, data->reg_num,
-					mii_reg))
+				   &data->val_out)) {
+			spin_unlock_irqrestore(&adapter->stats_lock, irq_flags);
 			return -EIO;
-		if (adapter->hw.phy_type == e1000_phy_m88) {
-			switch (data->reg_num) {
-			case PHY_CTRL:
-				if(mii_reg & MII_CR_POWER_DOWN)
-					break;
-				if(mii_reg & MII_CR_AUTO_NEG_EN) {
-					adapter->hw.autoneg = 1;
-					adapter->hw.autoneg_advertised = 0x2F;
-				} else {
-					if (mii_reg & 0x40)
-						spddplx = SPEED_1000;
-					else if (mii_reg & 0x2000)
-						spddplx = SPEED_100;
-					else
-						spddplx = SPEED_10;
-					spddplx += (mii_reg & 0x100)
-						   ? FULL_DUPLEX :
-						   HALF_DUPLEX;
-					retval = e1000_set_spd_dplx(adapter,
-								    spddplx);
-					if(retval)
-						return retval;
-				}
-				if(netif_running(adapter->netdev)) {
-					e1000_down(adapter);
-					e1000_up(adapter);
-				} else
-					e1000_reset(adapter);
-				break;
-			case M88E1000_PHY_SPEC_CTRL:
-			case M88E1000_EXT_PHY_SPEC_CTRL:
-				if (e1000_phy_reset(&adapter->hw))
-					return -EIO;
-				break;
-			}
-		} else {
-			switch (data->reg_num) {
-			case PHY_CTRL:
-				if(mii_reg & MII_CR_POWER_DOWN)
-					break;
-				if(netif_running(adapter->netdev)) {
-					e1000_down(adapter);
-					e1000_up(adapter);
-				} else
-					e1000_reset(adapter);
-				break;
-			}
 		}
+		spin_unlock_irqrestore(&adapter->stats_lock, irq_flags);
 		break;
+	case SIOCSMIIREG:
 	default:
 		return -EOPNOTSUPP;
 	}
 	return E1000_SUCCESS;
 }
+#endif
 
-/**
- * e1000_rx_checksum - Receive Checksum Offload for 82543
- * @adapter: board private structure
- * @rx_desc: receive descriptor
- * @sk_buff: socket buffer with received data
- **/
-
-static void
-e1000_rx_checksum(struct e1000_adapter *adapter,
-                  struct e1000_rx_desc *rx_desc,
-                  struct sk_buff *skb)
-{
-	/* 82543 or newer only */
-	if(unlikely((adapter->hw.mac_type < e1000_82543) ||
-	/* Ignore Checksum bit is set */
-	(rx_desc->status & E1000_RXD_STAT_IXSM) ||
-	/* TCP Checksum has not been calculated */
-	(!(rx_desc->status & E1000_RXD_STAT_TCPCS)))) {
-		skb->ip_summed = CHECKSUM_NONE;
-		return;
-	}
-
-	/* At this point we know the hardware did the TCP checksum */
-	/* now look at the TCP checksum error bit */
-	if(rx_desc->errors & E1000_RXD_ERR_TCPE) {
-		/* let the stack verify checksum errors */
-		skb->ip_summed = CHECKSUM_NONE;
-		adapter->hw_csum_err++;
-	} else {
-		/* TCP checksum is good */
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
-		adapter->hw_csum_good++;
-	}
-}
-
-void
-e1000_pci_set_mwi(struct e1000_hw *hw)
+void e1000_pci_set_mwi(struct e1000_hw *hw)
 {
 	struct e1000_adapter *adapter = hw->back;
+	int ret_val = pci_set_mwi(adapter->pdev);
 
-	int ret;
-	ret = pci_set_mwi(adapter->pdev);
+	if (ret_val)
+		DPRINTK(PROBE, ERR, "Error in setting MWI\n");
 }
 
-void
-e1000_pci_clear_mwi(struct e1000_hw *hw)
+void e1000_pci_clear_mwi(struct e1000_hw *hw)
 {
 	struct e1000_adapter *adapter = hw->back;
 
 	pci_clear_mwi(adapter->pdev);
 }
 
-void
-e1000_read_pci_cfg(struct e1000_hw *hw, uint32_t reg, uint16_t *value)
+void e1000_read_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value)
 {
 	struct e1000_adapter *adapter = hw->back;
 
 	pci_read_config_word(adapter->pdev, reg, value);
 }
 
-void
-e1000_write_pci_cfg(struct e1000_hw *hw, uint32_t reg, uint16_t *value)
+void e1000_write_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value)
 {
 	struct e1000_adapter *adapter = hw->back;
 
 	pci_write_config_word(adapter->pdev, reg, *value);
 }
 
-uint32_t
-e1000_io_read(struct e1000_hw *hw, unsigned long port)
+s32 e1000_read_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value)
 {
-	return inl(port);
-}
+	struct e1000_adapter *adapter = hw->back;
+	u16 cap_offset;
 
-void
-e1000_io_write(struct e1000_hw *hw, unsigned long port, uint32_t value)
-{
-	outl(value, port);
+	cap_offset = pci_find_capability(adapter->pdev, PCI_CAP_ID_EXP);
+	if (!cap_offset)
+		return -E1000_ERR_CONFIG;
+
+	pci_read_config_word(adapter->pdev, cap_offset + reg, value);
+
+	return E1000_SUCCESS;
 }
 
-static void
-e1000_vlan_rx_register(struct net_device *netdev, struct vlan_group *grp)
+#ifdef NETIF_F_HW_VLAN_TX
+static void e1000_vlan_rx_register(struct net_device *netdev,
+                                   struct vlan_group *grp)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t ctrl, rctl;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u32 ctrl, rctl;
 
 	e1000_irq_disable(adapter);
 	adapter->vlgrp = grp;
 
-	if(grp) {
+	if (grp) {
 		/* enable VLAN tag insert/strip */
-		ctrl = E1000_READ_REG(&adapter->hw, CTRL);
+		ctrl = E1000_READ_REG(&adapter->hw, E1000_CTRL);
 		ctrl |= E1000_CTRL_VME;
-		E1000_WRITE_REG(&adapter->hw, CTRL, ctrl);
+		E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl);
 
-		/* enable VLAN receive filtering */
-		rctl = E1000_READ_REG(&adapter->hw, RCTL);
-		rctl |= E1000_RCTL_VFE;
-		rctl &= ~E1000_RCTL_CFIEN;
-		E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+		if ((adapter->hw.mac.type != e1000_ich8lan) &&
+		    (adapter->hw.mac.type != e1000_ich9lan)) {
+			/* enable VLAN receive filtering */
+			rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
+			rctl |= E1000_RCTL_VFE;
+			rctl &= ~E1000_RCTL_CFIEN;
+			E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
+			e1000_update_mng_vlan(adapter);
+		}
 	} else {
 		/* disable VLAN tag insert/strip */
-		ctrl = E1000_READ_REG(&adapter->hw, CTRL);
+		ctrl = E1000_READ_REG(&adapter->hw, E1000_CTRL);
 		ctrl &= ~E1000_CTRL_VME;
-		E1000_WRITE_REG(&adapter->hw, CTRL, ctrl);
+		E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl);
 
-		/* disable VLAN filtering */
-		rctl = E1000_READ_REG(&adapter->hw, RCTL);
-		rctl &= ~E1000_RCTL_VFE;
-		E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+		if ((adapter->hw.mac.type != e1000_ich8lan) &&
+		    (adapter->hw.mac.type != e1000_ich9lan)) {
+			/* disable VLAN filtering */
+			rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
+			rctl &= ~E1000_RCTL_VFE;
+			E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
+			if (adapter->mng_vlan_id !=
+			    (u16)E1000_MNG_VLAN_NONE) {
+				e1000_vlan_rx_kill_vid(netdev,
+				                       adapter->mng_vlan_id);
+				adapter->mng_vlan_id = E1000_MNG_VLAN_NONE;
+			}
+		}
 	}
 
 	e1000_irq_enable(adapter);
 }
 
-static void
-e1000_vlan_rx_add_vid(struct net_device *netdev, uint16_t vid)
+static void e1000_vlan_rx_add_vid(struct net_device *netdev, u16 vid)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t vfta, index;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u32 vfta, index;
 
+	if ((adapter->hw.mng_cookie.status &
+	     E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
+	    (vid == adapter->mng_vlan_id))
+		return;
 	/* add VID to filter table */
 	index = (vid >> 5) & 0x7F;
-	vfta = E1000_READ_REG_ARRAY(&adapter->hw, VFTA, index);
+	vfta = E1000_READ_REG_ARRAY(&adapter->hw, E1000_VFTA, index);
 	vfta |= (1 << (vid & 0x1F));
 	e1000_write_vfta(&adapter->hw, index, vfta);
 }
 
-static void
-e1000_vlan_rx_kill_vid(struct net_device *netdev, uint16_t vid)
+static void e1000_vlan_rx_kill_vid(struct net_device *netdev, u16 vid)
 {
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t vfta, index;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u32 vfta, index;
 
 	e1000_irq_disable(adapter);
-
-	if(adapter->vlgrp)
-		adapter->vlgrp->vlan_devices[vid] = NULL;
-
+	vlan_group_set_device(adapter->vlgrp, vid, NULL);
 	e1000_irq_enable(adapter);
 
+	if ((adapter->hw.mng_cookie.status &
+	     E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
+	    (vid == adapter->mng_vlan_id)) {
+		/* release control to f/w */
+		e1000_release_hw_control(adapter);
+		return;
+	}
+
 	/* remove VID from filter table */
 	index = (vid >> 5) & 0x7F;
-	vfta = E1000_READ_REG_ARRAY(&adapter->hw, VFTA, index);
+	vfta = E1000_READ_REG_ARRAY(&adapter->hw, E1000_VFTA, index);
 	vfta &= ~(1 << (vid & 0x1F));
 	e1000_write_vfta(&adapter->hw, index, vfta);
 }
 
-static void
-e1000_restore_vlan(struct e1000_adapter *adapter)
+static void e1000_restore_vlan(struct e1000_adapter *adapter)
 {
 	e1000_vlan_rx_register(adapter->netdev, adapter->vlgrp);
 
-	if(adapter->vlgrp) {
-		uint16_t vid;
-		for(vid = 0; vid < VLAN_GROUP_ARRAY_LEN; vid++) {
-			if(!adapter->vlgrp->vlan_devices[vid])
+	if (adapter->vlgrp) {
+		u16 vid;
+		for (vid = 0; vid < VLAN_GROUP_ARRAY_LEN; vid++) {
+			if (!vlan_group_get_device(adapter->vlgrp, vid))
 				continue;
 			e1000_vlan_rx_add_vid(adapter->netdev, vid);
 		}
 	}
 }
+#endif
 
-int
-e1000_set_spd_dplx(struct e1000_adapter *adapter, uint16_t spddplx)
+int e1000_set_spd_dplx(struct e1000_adapter *adapter, u16 spddplx)
 {
-	adapter->hw.autoneg = 0;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+
+	mac->autoneg = 0;
+
+	/* Fiber NICs only allow 1000 gbps Full duplex */
+	if ((adapter->hw.media_type == e1000_media_type_fiber) &&
+		spddplx != (SPEED_1000 + DUPLEX_FULL)) {
+		DPRINTK(PROBE, ERR, "Unsupported Speed/Duplex configuration\n");
+		return -EINVAL;
+	}
 
-	switch(spddplx) {
+	switch (spddplx) {
 	case SPEED_10 + DUPLEX_HALF:
-		adapter->hw.forced_speed_duplex = e1000_10_half;
+		mac->forced_speed_duplex = ADVERTISE_10_HALF;
 		break;
 	case SPEED_10 + DUPLEX_FULL:
-		adapter->hw.forced_speed_duplex = e1000_10_full;
+		mac->forced_speed_duplex = ADVERTISE_10_FULL;
 		break;
 	case SPEED_100 + DUPLEX_HALF:
-		adapter->hw.forced_speed_duplex = e1000_100_half;
+		mac->forced_speed_duplex = ADVERTISE_100_HALF;
 		break;
 	case SPEED_100 + DUPLEX_FULL:
-		adapter->hw.forced_speed_duplex = e1000_100_full;
+		mac->forced_speed_duplex = ADVERTISE_100_FULL;
 		break;
 	case SPEED_1000 + DUPLEX_FULL:
-		adapter->hw.autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_1000_FULL;
+		mac->autoneg = 1;
+		adapter->hw.phy.autoneg_advertised = ADVERTISE_1000_FULL;
 		break;
 	case SPEED_1000 + DUPLEX_HALF: /* not supported */
 	default:
-		DPRINTK(PROBE, ERR, 
-			"Unsupported Speed/Duplexity configuration\n");
+		DPRINTK(PROBE, ERR, "Unsupported Speed/Duplex configuration\n");
 		return -EINVAL;
 	}
 	return 0;
 }
 
-static int
-e1000_notify_reboot(struct notifier_block *nb, unsigned long event, void *p)
+#ifdef USE_REBOOT_NOTIFIER
+/* only want to do this for 2.4 kernels? */
+static int e1000_notify_reboot(struct notifier_block *nb,
+                               unsigned long event, void *p)
 {
 	struct pci_dev *pdev = NULL;
 
-	switch(event) {
+	switch (event) {
 	case SYS_DOWN:
 	case SYS_HALT:
 	case SYS_POWER_OFF:
-		while((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev))) {
-			if(pci_dev_driver(pdev) == &e1000_driver)
-				e1000_suspend(pdev, 3);
+		while ((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev))) {
+			if (pci_dev_driver(pdev) == &e1000_driver)
+				e1000_suspend(pdev, PMSG_SUSPEND);
 		}
 	}
 	return NOTIFY_DONE;
 }
+#endif
 
-static int
-e1000_suspend(struct pci_dev *pdev, uint32_t state)
+static int e1000_suspend(struct pci_dev *pdev, pm_message_t state)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t ctrl, ctrl_ext, rctl, manc, status;
-	uint32_t wufc = adapter->wol;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u32 ctrl, ctrl_ext, rctl, status;
+	u32 wufc = adapter->wol;
+#ifdef CONFIG_PM
+	int retval = 0;
+#endif
 
 	netif_device_detach(netdev);
 
-	if(netif_running(netdev))
+	if (netif_running(netdev)) {
+		WARN_ON(test_bit(__E1000_RESETTING, &adapter->state));
 		e1000_down(adapter);
+		e1000_free_irq(adapter);
+	}
+
+#ifdef CONFIG_PM
+	retval = pci_save_state(pdev);
+	if (retval)
+		return retval;
+#endif
 
-	status = E1000_READ_REG(&adapter->hw, STATUS);
-	if(status & E1000_STATUS_LU)
+	status = E1000_READ_REG(&adapter->hw, E1000_STATUS);
+	if (status & E1000_STATUS_LU)
 		wufc &= ~E1000_WUFC_LNKC;
 
-	if(wufc) {
+	if (wufc) {
 		e1000_setup_rctl(adapter);
 		e1000_set_multi(netdev);
 
 		/* turn on all-multi mode if wake on multicast is enabled */
-		if(adapter->wol & E1000_WUFC_MC) {
-			rctl = E1000_READ_REG(&adapter->hw, RCTL);
+		if (wufc & E1000_WUFC_MC) {
+			rctl = E1000_READ_REG(&adapter->hw, E1000_RCTL);
 			rctl |= E1000_RCTL_MPE;
-			E1000_WRITE_REG(&adapter->hw, RCTL, rctl);
+			E1000_WRITE_REG(&adapter->hw, E1000_RCTL, rctl);
 		}
 
-		if(adapter->hw.mac_type >= e1000_82540) {
-			ctrl = E1000_READ_REG(&adapter->hw, CTRL);
+		if (adapter->hw.mac.type >= e1000_82540) {
+			ctrl = E1000_READ_REG(&adapter->hw, E1000_CTRL);
 			/* advertise wake from D3Cold */
 			#define E1000_CTRL_ADVD3WUC 0x00100000
 			/* phy power management enable */
 			#define E1000_CTRL_EN_PHY_PWR_MGMT 0x00200000
 			ctrl |= E1000_CTRL_ADVD3WUC |
 				E1000_CTRL_EN_PHY_PWR_MGMT;
-			E1000_WRITE_REG(&adapter->hw, CTRL, ctrl);
+			E1000_WRITE_REG(&adapter->hw, E1000_CTRL, ctrl);
 		}
 
-		if(adapter->hw.media_type == e1000_media_type_fiber ||
+		if (adapter->hw.media_type == e1000_media_type_fiber ||
 		   adapter->hw.media_type == e1000_media_type_internal_serdes) {
 			/* keep the laser running in D3 */
-			ctrl_ext = E1000_READ_REG(&adapter->hw, CTRL_EXT);
+			ctrl_ext = E1000_READ_REG(&adapter->hw, E1000_CTRL_EXT);
 			ctrl_ext |= E1000_CTRL_EXT_SDP7_DATA;
-			E1000_WRITE_REG(&adapter->hw, CTRL_EXT, ctrl_ext);
+			E1000_WRITE_REG(&adapter->hw, E1000_CTRL_EXT, ctrl_ext);
 		}
 
-		E1000_WRITE_REG(&adapter->hw, WUC, E1000_WUC_PME_EN);
-		E1000_WRITE_REG(&adapter->hw, WUFC, wufc);
-		pci_enable_wake(pdev, 3, 1);
-		pci_enable_wake(pdev, 4, 1); /* 4 == D3 cold */
+		/* Allow time for pending master requests to run */
+		e1000_disable_pcie_master(&adapter->hw);
+
+		E1000_WRITE_REG(&adapter->hw, E1000_WUC, E1000_WUC_PME_EN);
+		E1000_WRITE_REG(&adapter->hw, E1000_WUFC, wufc);
+		pci_enable_wake(pdev, PCI_D3hot, 1);
+		pci_enable_wake(pdev, PCI_D3cold, 1);
 	} else {
-		E1000_WRITE_REG(&adapter->hw, WUC, 0);
-		E1000_WRITE_REG(&adapter->hw, WUFC, 0);
-		pci_enable_wake(pdev, 3, 0);
-		pci_enable_wake(pdev, 4, 0); /* 4 == D3 cold */
+		E1000_WRITE_REG(&adapter->hw, E1000_WUC, 0);
+		E1000_WRITE_REG(&adapter->hw, E1000_WUFC, 0);
+		pci_enable_wake(pdev, PCI_D3hot, 0);
+		pci_enable_wake(pdev, PCI_D3cold, 0);
 	}
 
-	pci_save_state(pdev, adapter->pci_state);
+	e1000_release_manageability(adapter);
 
-	if(adapter->hw.mac_type >= e1000_82540 &&
-	   adapter->hw.media_type == e1000_media_type_copper) {
-		manc = E1000_READ_REG(&adapter->hw, MANC);
-		if(manc & E1000_MANC_SMBUS_EN) {
-			manc |= E1000_MANC_ARP_EN;
-			E1000_WRITE_REG(&adapter->hw, MANC, manc);
-			pci_enable_wake(pdev, 3, 1);
-			pci_enable_wake(pdev, 4, 1); /* 4 == D3 cold */
-		}
+	/* make sure adapter isn't asleep if manageability is enabled */
+	if (adapter->en_mng_pt) {
+		pci_enable_wake(pdev, PCI_D3hot, 1);
+		pci_enable_wake(pdev, PCI_D3cold, 1);
 	}
 
+	if (adapter->hw.phy.type == e1000_phy_igp_3)
+		e1000_igp3_phy_powerdown_workaround_ich8lan(&adapter->hw);
+
+	/* Release control of h/w to f/w.  If f/w is AMT enabled, this
+	 * would have already happened in close and is redundant. */
+	e1000_release_hw_control(adapter);
+
 	pci_disable_device(pdev);
 
-	state = (state > 0) ? 3 : 0;
-	pci_set_power_state(pdev, state);
+	pci_set_power_state(pdev, pci_choose_state(pdev, state));
 
 	return 0;
 }
 
 #ifdef CONFIG_PM
-static int
-e1000_resume(struct pci_dev *pdev)
+static int e1000_resume(struct pci_dev *pdev)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
-	struct e1000_adapter *adapter = netdev->priv;
-	uint32_t manc;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	u32 err;
+
+	pci_set_power_state(pdev, PCI_D0);
+	pci_restore_state(pdev);
+	if ((err = pci_enable_device(pdev))) {
+		printk(KERN_ERR "e1000: Cannot enable PCI device from suspend\n");
+		return err;
+	}
+	pci_set_master(pdev);
 
-	pci_enable_device(pdev);
-	pci_set_power_state(pdev, 0);
-	pci_restore_state(pdev, adapter->pci_state);
+	pci_enable_wake(pdev, PCI_D3hot, 0);
+	pci_enable_wake(pdev, PCI_D3cold, 0);
 
-	pci_enable_wake(pdev, 3, 0);
-	pci_enable_wake(pdev, 4, 0); /* 4 == D3 cold */
+	if (netif_running(netdev) && (err = e1000_request_irq(adapter)))
+		return err;
 
+	e1000_power_up_phy(adapter);
 	e1000_reset(adapter);
-	E1000_WRITE_REG(&adapter->hw, WUS, ~0);
+	E1000_WRITE_REG(&adapter->hw, E1000_WUS, ~0);
 
-	if(netif_running(netdev))
+	e1000_init_manageability(adapter);
+
+	if (netif_running(netdev))
 		e1000_up(adapter);
 
 	netif_device_attach(netdev);
 
-	if(adapter->hw.mac_type >= e1000_82540 &&
-	   adapter->hw.media_type == e1000_media_type_copper) {
-		manc = E1000_READ_REG(&adapter->hw, MANC);
-		manc &= ~(E1000_MANC_ARP_EN);
-		E1000_WRITE_REG(&adapter->hw, MANC, manc);
-	}
+	/* If the controller is 82573 or ICHx and f/w is AMT, do not set
+	 * DRV_LOAD until the interface is up.  For all other cases,
+	 * let the f/w know that the h/w is now under the control
+	 * of the driver. */
+	if (((adapter->hw.mac.type != e1000_82573) &&
+	     (adapter->hw.mac.type != e1000_ich8lan) &&
+	     (adapter->hw.mac.type != e1000_ich9lan)) ||
+	    !e1000_check_mng_mode(&adapter->hw))
+		e1000_get_hw_control(adapter);
 
 	return 0;
 }
 #endif
 
+#ifndef USE_REBOOT_NOTIFIER
+static void e1000_shutdown(struct pci_dev *pdev)
+{
+	e1000_suspend(pdev, PMSG_SUSPEND);
+}
+#endif
+
 #ifdef CONFIG_NET_POLL_CONTROLLER
 /*
  * Polling 'interrupt' - used by things like netconsole to send skbs
  * without having to re-enable interrupts. It's not called while
  * the interrupt routine is executing.
  */
-static void
-e1000_netpoll (struct net_device *netdev)
+static void e1000_netpoll(struct net_device *netdev)
 {
-	struct e1000_adapter *adapter = netdev->priv;
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	int i;
+
 	disable_irq(adapter->pdev->irq);
-	e1000_intr(adapter->pdev->irq, netdev, NULL);
+	e1000_intr(adapter->pdev->irq, netdev);
+
+	for (i = 0; i < adapter->num_tx_queues ; i++ )
+		e1000_clean_tx_irq(adapter, &adapter->tx_ring[i]);
+#ifndef CONFIG_E1000_NAPI
+	for (i = 0; i < adapter->num_rx_queues ; i++ )
+		adapter->clean_rx(adapter, &adapter->rx_ring[i]);
+#endif
 	enable_irq(adapter->pdev->irq);
 }
 #endif
 
+#ifdef CONFIG_E1000_PCI_ERS
+/**
+ * e1000_io_error_detected - called when PCI error is detected
+ * @pdev: Pointer to PCI device
+ * @state: The current pci connection state
+ *
+ * This function is called after a PCI bus error affecting
+ * this device has been detected.
+ */
+static pci_ers_result_t e1000_io_error_detected(struct pci_dev *pdev,
+                                                pci_channel_state_t state)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct e1000_adapter *adapter = netdev->priv;
+
+	netif_device_detach(netdev);
+
+	if (netif_running(netdev))
+		e1000_down(adapter);
+	pci_disable_device(pdev);
+
+	/* Request a slot slot reset. */
+	return PCI_ERS_RESULT_NEED_RESET;
+}
+
+/**
+ * e1000_io_slot_reset - called after the pci bus has been reset.
+ * @pdev: Pointer to PCI device
+ *
+ * Restart the card from scratch, as if from a cold-boot. Implementation
+ * resembles the first-half of the e1000_resume routine.
+ */
+static pci_ers_result_t e1000_io_slot_reset(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct e1000_adapter *adapter = netdev->priv;
+
+	if (pci_enable_device(pdev)) {
+		printk(KERN_ERR "e1000: Cannot re-enable PCI device after reset.\n");
+		return PCI_ERS_RESULT_DISCONNECT;
+	}
+	pci_set_master(pdev);
+
+	pci_enable_wake(pdev, PCI_D3hot, 0);
+	pci_enable_wake(pdev, PCI_D3cold, 0);
+
+	e1000_reset(adapter);
+	E1000_WRITE_REG(&adapter->hw, E1000_WUS, ~0);
+
+	return PCI_ERS_RESULT_RECOVERED;
+}
+
+/**
+ * e1000_io_resume - called when traffic can start flowing again.
+ * @pdev: Pointer to PCI device
+ *
+ * This callback is called when the error recovery driver tells us that
+ * its OK to resume normal operation. Implementation resembles the
+ * second-half of the e1000_resume routine.
+ */
+static void e1000_io_resume(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct e1000_adapter *adapter = netdev->priv;
+
+	e1000_init_manageability(adapter);
+
+	if (netif_running(netdev)) {
+		if (e1000_up(adapter)) {
+			printk("e1000: can't bring device back up after reset\n");
+			return;
+		}
+	}
+
+	netif_device_attach(netdev);
+
+	/* If the controller is 82573 or ICHx and f/w is AMT, do not set
+	 * DRV_LOAD until the interface is up.  For all other cases,
+	 * let the f/w know that the h/w is now under the control
+	 * of the driver. */
+	if (((adapter->hw.mac.type != e1000_82573) &&
+	     (adapter->hw.mac.type != e1000_ich8lan) &&
+	     (adapter->hw.mac.type != e1000_ich9lan)) ||
+	    !e1000_check_mng_mode(&adapter->hw))
+		e1000_get_hw_control(adapter);
+
+}
+#endif /* CONFIG_E1000_PCI_ERS */
+
+s32 e1000_alloc_zeroed_dev_spec_struct(struct e1000_hw *hw, u32 size)
+{
+	hw->dev_spec = kmalloc(size, GFP_KERNEL);
+
+	if (!hw->dev_spec)
+		return -ENOMEM;
+
+	memset(hw->dev_spec, 0, size);
+
+	return E1000_SUCCESS;
+}
+
+void e1000_free_dev_spec_struct(struct e1000_hw *hw)
+{
+	if (!hw->dev_spec)
+		return;
+
+	kfree(hw->dev_spec);
+}
+
 /* e1000_main.c */
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_manage.c linux-2.6.9/drivers/net/e1000/e1000_manage.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_manage.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_manage.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,380 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "e1000_api.h"
+#include "e1000_manage.h"
+
+static u8 e1000_calculate_checksum(u8 *buffer, u32 length);
+
+/**
+ *  e1000_calculate_checksum - Calculate checksum for buffer
+ *  @buffer: pointer to EEPROM
+ *  @length: size of EEPROM to calculate a checksum for
+ *
+ *  Calculates the checksum for some buffer on a specified length.  The
+ *  checksum calculated is returned.
+ **/
+static u8 e1000_calculate_checksum(u8 *buffer, u32 length)
+{
+	u32 i;
+	u8  sum = 0;
+
+	DEBUGFUNC("e1000_calculate_checksum");
+
+	if (!buffer)
+		return 0;
+
+	for (i = 0; i < length; i++)
+		sum += buffer[i];
+
+	return (u8) (0 - sum);
+}
+
+/**
+ *  e1000_mng_enable_host_if_generic - Checks host interface is enabled
+ *  @hw: pointer to the HW structure
+ *
+ *  Returns E1000_success upon success, else E1000_ERR_HOST_INTERFACE_COMMAND
+ *
+ *  This function checks whether the HOST IF is enabled for command operaton
+ *  and also checks whether the previous command is completed.  It busy waits
+ *  in case of previous command is not completed.
+ **/
+s32 e1000_mng_enable_host_if_generic(struct e1000_hw * hw)
+{
+	u32 hicr;
+	s32 ret_val = E1000_SUCCESS;
+	u8  i;
+
+	DEBUGFUNC("e1000_mng_enable_host_if_generic");
+
+	/* Check that the host interface is enabled. */
+	hicr = E1000_READ_REG(hw, E1000_HICR);
+	if ((hicr & E1000_HICR_EN) == 0) {
+		DEBUGOUT("E1000_HOST_EN bit disabled.\n");
+		ret_val = -E1000_ERR_HOST_INTERFACE_COMMAND;
+		goto out;
+	}
+	/* check the previous command is completed */
+	for (i = 0; i < E1000_MNG_DHCP_COMMAND_TIMEOUT; i++) {
+		hicr = E1000_READ_REG(hw, E1000_HICR);
+		if (!(hicr & E1000_HICR_C))
+			break;
+		msec_delay_irq(1);
+	}
+
+	if (i == E1000_MNG_DHCP_COMMAND_TIMEOUT) {
+		DEBUGOUT("Previous command timeout failed .\n");
+		ret_val = -E1000_ERR_HOST_INTERFACE_COMMAND;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_mng_mode_generic - Generic check managament mode
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the firmware semaphore register and returns true (>0) if
+ *  manageability is enabled, else false (0).
+ **/
+boolean_t e1000_check_mng_mode_generic(struct e1000_hw *hw)
+{
+	u32 fwsm;
+
+	DEBUGFUNC("e1000_check_mng_mode_generic");
+
+	fwsm = E1000_READ_REG(hw, E1000_FWSM);
+
+	return ((fwsm & E1000_FWSM_MODE_MASK) ==
+	        (E1000_MNG_IAMT_MODE << E1000_FWSM_MODE_SHIFT));
+}
+
+/**
+ *  e1000_enable_tx_pkt_filtering_generic - Enable packet filtering on TX
+ *  @hw: pointer to the HW structure
+ *
+ *  Enables packet filtering on transmit packets if manageability is enabled
+ *  and host interface is enabled.
+ **/
+boolean_t e1000_enable_tx_pkt_filtering_generic(struct e1000_hw *hw)
+{
+	struct e1000_host_mng_dhcp_cookie *hdr = &hw->mng_cookie;
+	u32 *buffer = (u32 *)&hw->mng_cookie;
+	u32 offset;
+	s32 ret_val, hdr_csum, csum;
+	u8 i, len;
+	boolean_t tx_filter = TRUE;
+
+	DEBUGFUNC("e1000_enable_tx_pkt_filtering_generic");
+
+	/* No manageability, no filtering */
+	if (!e1000_check_mng_mode(hw)) {
+		tx_filter = FALSE;
+		goto out;
+	}
+
+	/* If we can't read from the host interface for whatever
+	 * reason, disable filtering.
+	 */
+	ret_val = e1000_mng_enable_host_if(hw);
+	if (ret_val != E1000_SUCCESS) {
+		tx_filter = FALSE;
+		goto out;
+	}
+
+	/* Read in the header.  Length and offset are in dwords. */
+	len    = E1000_MNG_DHCP_COOKIE_LENGTH >> 2;
+	offset = E1000_MNG_DHCP_COOKIE_OFFSET >> 2;
+	for (i = 0; i < len; i++) {
+		*(buffer + i) = E1000_READ_REG_ARRAY_DWORD(hw,
+							   E1000_HOST_IF,
+							   offset + i);
+	}
+	hdr_csum = hdr->checksum;
+	hdr->checksum = 0;
+	csum = e1000_calculate_checksum((u8 *)hdr,
+	                                E1000_MNG_DHCP_COOKIE_LENGTH);
+	/* If either the checksums or signature don't match, then
+	 * the cookie area isn't considered valid, in which case we
+	 * take the safe route of assuming Tx filtering is enabled.
+	 */
+	if (hdr_csum != csum)
+		goto out;
+	if (hdr->signature != E1000_IAMT_SIGNATURE)
+		goto out;
+
+	/* Cookie area is valid, make the final check for filtering. */
+	if (!(hdr->status & E1000_MNG_DHCP_COOKIE_STATUS_PARSING))
+		tx_filter = FALSE;
+
+out:
+	hw->mac.tx_pkt_filtering = tx_filter;
+	return tx_filter;
+}
+
+/**
+ *  e1000_mng_write_dhcp_info_generic - Writes DHCP info to host interface
+ *  @hw: pointer to the HW structure
+ *  @buffer: pointer to the host interface
+ *  @length: size of the buffer
+ *
+ *  Writes the DHCP information to the host interface.
+ **/
+s32 e1000_mng_write_dhcp_info_generic(struct e1000_hw * hw, u8 *buffer,
+                                      u16 length)
+{
+	struct e1000_host_mng_command_header hdr;
+	s32 ret_val;
+	u32 hicr;
+
+	DEBUGFUNC("e1000_mng_write_dhcp_info_generic");
+
+	hdr.command_id = E1000_MNG_DHCP_TX_PAYLOAD_CMD;
+	hdr.command_length = length;
+	hdr.reserved1 = 0;
+	hdr.reserved2 = 0;
+	hdr.checksum = 0;
+
+	/* Enable the host interface */
+	ret_val = e1000_mng_enable_host_if(hw);
+	if (ret_val)
+		goto out;
+
+	/* Populate the host interface with the contents of "buffer". */
+	ret_val = e1000_mng_host_if_write(hw, buffer, length,
+	                                  sizeof(hdr), &(hdr.checksum));
+	if (ret_val)
+		goto out;
+
+	/* Write the manageability command header */
+	ret_val = e1000_mng_write_cmd_header(hw, &hdr);
+	if (ret_val)
+		goto out;
+
+	/* Tell the ARC a new command is pending. */
+	hicr = E1000_READ_REG(hw, E1000_HICR);
+	E1000_WRITE_REG(hw, E1000_HICR, hicr | E1000_HICR_C);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_mng_write_cmd_header_generic - Writes manageability command header
+ *  @hw: pointer to the HW structure
+ *  @hdr: pointer to the host interface command header
+ *
+ *  Writes the command header after does the checksum calculation.
+ **/
+s32 e1000_mng_write_cmd_header_generic(struct e1000_hw * hw,
+                                       struct e1000_host_mng_command_header * hdr)
+{
+	u16 i, length = sizeof(struct e1000_host_mng_command_header);
+
+	DEBUGFUNC("e1000_mng_write_cmd_header_generic");
+
+	/* Write the whole command header structure with new checksum. */
+
+	hdr->checksum = e1000_calculate_checksum((u8 *)hdr, length);
+
+	length >>= 2;
+	/* Write the relevant command block into the ram area. */
+	for (i = 0; i < length; i++) {
+		E1000_WRITE_REG_ARRAY_DWORD(hw, E1000_HOST_IF, i,
+		                            *((u32 *) hdr + i));
+		E1000_WRITE_FLUSH(hw);
+	}
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_mng_host_if_write_generic - Writes to the manageability host interface
+ *  @hw: pointer to the HW structure
+ *  @buffer: pointer to the host interface buffer
+ *  @length: size of the buffer
+ *  @offset: location in the buffer to write to
+ *  @sum: sum of the data (not checksum)
+ *
+ *  This function writes the buffer content at the offset given on the host if.
+ *  It also does alignment considerations to do the writes in most efficient
+ *  way.  Also fills up the sum of the buffer in *buffer parameter.
+ **/
+s32 e1000_mng_host_if_write_generic(struct e1000_hw * hw, u8 *buffer,
+                                    u16 length, u16 offset, u8 *sum)
+{
+	u8 *tmp;
+	u8 *bufptr = buffer;
+	u32 data = 0;
+	s32 ret_val = E1000_SUCCESS;
+	u16 remaining, i, j, prev_bytes;
+
+	DEBUGFUNC("e1000_mng_host_if_write_generic");
+
+	/* sum = only sum of the data and it is not checksum */
+
+	if (length == 0 || offset + length > E1000_HI_MAX_MNG_DATA_LENGTH) {
+		ret_val = -E1000_ERR_PARAM;
+		goto out;
+	}
+
+	tmp = (u8 *)&data;
+	prev_bytes = offset & 0x3;
+	offset >>= 2;
+
+	if (prev_bytes) {
+		data = E1000_READ_REG_ARRAY_DWORD(hw, E1000_HOST_IF, offset);
+		for (j = prev_bytes; j < sizeof(u32); j++) {
+			*(tmp + j) = *bufptr++;
+			*sum += *(tmp + j);
+		}
+		E1000_WRITE_REG_ARRAY_DWORD(hw, E1000_HOST_IF, offset, data);
+		length -= j - prev_bytes;
+		offset++;
+	}
+
+	remaining = length & 0x3;
+	length -= remaining;
+
+	/* Calculate length in DWORDs */
+	length >>= 2;
+
+	/* The device driver writes the relevant command block into the
+	 * ram area. */
+	for (i = 0; i < length; i++) {
+		for (j = 0; j < sizeof(u32); j++) {
+			*(tmp + j) = *bufptr++;
+			*sum += *(tmp + j);
+		}
+
+		E1000_WRITE_REG_ARRAY_DWORD(hw, E1000_HOST_IF, offset + i, data);
+	}
+	if (remaining) {
+		for (j = 0; j < sizeof(u32); j++) {
+			if (j < remaining)
+				*(tmp + j) = *bufptr++;
+			else
+				*(tmp + j) = 0;
+
+			*sum += *(tmp + j);
+		}
+		E1000_WRITE_REG_ARRAY_DWORD(hw, E1000_HOST_IF, offset + i, data);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_enable_mng_pass_thru - Enable processing of ARP's
+ *  @hw: pointer to the HW structure
+ *
+ *  Verifies the hardware needs to allow ARPs to be processed by the host.
+ **/
+boolean_t e1000_enable_mng_pass_thru(struct e1000_hw *hw)
+{
+	u32 manc;
+	u32 fwsm, factps;
+	boolean_t ret_val = FALSE;
+
+	DEBUGFUNC("e1000_enable_mng_pass_thru");
+
+	if (!hw->mac.asf_firmware_present)
+		goto out;
+
+	manc = E1000_READ_REG(hw, E1000_MANC);
+
+	if (!(manc & E1000_MANC_RCV_TCO_EN) ||
+	    !(manc & E1000_MANC_EN_MAC_ADDR_FILTER))
+		goto out;
+
+	if (hw->mac.arc_subsystem_valid == TRUE) {
+		fwsm = E1000_READ_REG(hw, E1000_FWSM);
+		factps = E1000_READ_REG(hw, E1000_FACTPS);
+
+		if (!(factps & E1000_FACTPS_MNGCG) &&
+		    ((fwsm & E1000_FWSM_MODE_MASK) ==
+		     (e1000_mng_mode_pt << E1000_FWSM_MODE_SHIFT))) {
+			ret_val = TRUE;
+			goto out;
+		}
+	} else {
+		if ((manc & E1000_MANC_SMBUS_EN) &&
+		    !(manc & E1000_MANC_ASF_EN)) {
+			ret_val = TRUE;
+			goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_manage.h linux-2.6.9/drivers/net/e1000/e1000_manage.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_manage.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_manage.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,81 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_MANAGE_H_
+#define _E1000_MANAGE_H_
+
+boolean_t e1000_check_mng_mode_generic(struct e1000_hw *hw);
+boolean_t e1000_enable_tx_pkt_filtering_generic(struct e1000_hw *hw);
+s32       e1000_mng_enable_host_if_generic(struct e1000_hw *hw);
+s32       e1000_mng_host_if_write_generic(struct e1000_hw *hw, u8 *buffer,
+                                          u16 length, u16 offset, u8 *sum);
+s32       e1000_mng_write_cmd_header_generic(struct e1000_hw *hw,
+                                     struct e1000_host_mng_command_header *hdr);
+s32       e1000_mng_write_dhcp_info_generic(struct e1000_hw *hw,
+                                            u8 *buffer, u16 length);
+
+typedef enum {
+	e1000_mng_mode_none = 0,
+	e1000_mng_mode_asf,
+	e1000_mng_mode_pt,
+	e1000_mng_mode_ipmi,
+	e1000_mng_mode_host_if_only
+} e1000_mng_mode;
+
+#define E1000_FACTPS_MNGCG    0x20000000
+
+#define E1000_FWSM_MODE_MASK  0xE
+#define E1000_FWSM_MODE_SHIFT 1
+
+#define E1000_MNG_IAMT_MODE                  0x3
+#define E1000_MNG_DHCP_COOKIE_LENGTH         0x10
+#define E1000_MNG_DHCP_COOKIE_OFFSET         0x6F0
+#define E1000_MNG_DHCP_COMMAND_TIMEOUT       10
+#define E1000_MNG_DHCP_TX_PAYLOAD_CMD        64
+#define E1000_MNG_DHCP_COOKIE_STATUS_PARSING 0x1
+#define E1000_MNG_DHCP_COOKIE_STATUS_VLAN    0x2
+
+#define E1000_VFTA_ENTRY_SHIFT               5
+#define E1000_VFTA_ENTRY_MASK                0x7F
+#define E1000_VFTA_ENTRY_BIT_SHIFT_MASK      0x1F
+
+#define E1000_HI_MAX_BLOCK_BYTE_LENGTH       1792 /* Number of bytes in range */
+#define E1000_HI_MAX_BLOCK_DWORD_LENGTH      448 /* Number of dwords in range */
+#define E1000_HI_COMMAND_TIMEOUT             500 /* Process HI command limit */
+
+#define E1000_HICR_EN              0x01  /* Enable bit - RO */
+#define E1000_HICR_C               0x02  /* Driver sets this bit when done
+                                          * to put command in RAM */
+#define E1000_HICR_SV              0x04  /* Status Validity */
+#define E1000_HICR_FW_RESET_ENABLE 0x40
+#define E1000_HICR_FW_RESET        0x80
+
+#define E1000_IAMT_SIGNATURE  0x544D4149 /* Intel(R) Active Management
+                                          * Technology signature */
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_nvm.c linux-2.6.9/drivers/net/e1000/e1000_nvm.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_nvm.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_nvm.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,875 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "e1000_api.h"
+#include "e1000_nvm.h"
+
+/**
+ *  e1000_raise_eec_clk - Raise EEPROM clock
+ *  @hw: pointer to the HW structure
+ *  @eecd: pointer to the EEPROM
+ *
+ *  Enable/Raise the EEPROM clock bit.
+ **/
+static void e1000_raise_eec_clk(struct e1000_hw *hw, u32 *eecd)
+{
+	*eecd = *eecd | E1000_EECD_SK;
+	E1000_WRITE_REG(hw, E1000_EECD, *eecd);
+	E1000_WRITE_FLUSH(hw);
+	usec_delay(hw->nvm.delay_usec);
+}
+
+/**
+ *  e1000_lower_eec_clk - Lower EEPROM clock
+ *  @hw: pointer to the HW structure
+ *  @eecd: pointer to the EEPROM
+ *
+ *  Clear/Lower the EEPROM clock bit.
+ **/
+static void e1000_lower_eec_clk(struct e1000_hw *hw, u32 *eecd)
+{
+	*eecd = *eecd & ~E1000_EECD_SK;
+	E1000_WRITE_REG(hw, E1000_EECD, *eecd);
+	E1000_WRITE_FLUSH(hw);
+	usec_delay(hw->nvm.delay_usec);
+}
+
+/**
+ *  e1000_shift_out_eec_bits - Shift data bits our to the EEPROM
+ *  @hw: pointer to the HW structure
+ *  @data: data to send to the EEPROM
+ *  @count: number of bits to shift out
+ *
+ *  We need to shift 'count' bits out to the EEPROM.  So, the value in the
+ *  "data" parameter will be shifted out to the EEPROM one bit at a time.
+ *  In order to do this, "data" must be broken down into bits.
+ **/
+static void e1000_shift_out_eec_bits(struct e1000_hw *hw, u16 data, u16 count)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	u32 mask;
+
+	DEBUGFUNC("e1000_shift_out_eec_bits");
+
+	mask = 0x01 << (count - 1);
+	if (nvm->type == e1000_nvm_eeprom_microwire)
+		eecd &= ~E1000_EECD_DO;
+	else if (nvm->type == e1000_nvm_eeprom_spi)
+		eecd |= E1000_EECD_DO;
+
+	do {
+		eecd &= ~E1000_EECD_DI;
+
+		if (data & mask)
+			eecd |= E1000_EECD_DI;
+
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		E1000_WRITE_FLUSH(hw);
+
+		usec_delay(nvm->delay_usec);
+
+		e1000_raise_eec_clk(hw, &eecd);
+		e1000_lower_eec_clk(hw, &eecd);
+
+		mask >>= 1;
+	} while (mask);
+
+	eecd &= ~E1000_EECD_DI;
+	E1000_WRITE_REG(hw, E1000_EECD, eecd);
+}
+
+/**
+ *  e1000_shift_in_eec_bits - Shift data bits in from the EEPROM
+ *  @hw: pointer to the HW structure
+ *  @count: number of bits to shift in
+ *
+ *  In order to read a register from the EEPROM, we need to shift 'count' bits
+ *  in from the EEPROM.  Bits are "shifted in" by raising the clock input to
+ *  the EEPROM (setting the SK bit), and then reading the value of the data out
+ *  "DO" bit.  During this "shifting in" process the data in "DI" bit should
+ *  always be clear.
+ **/
+static u16 e1000_shift_in_eec_bits(struct e1000_hw *hw, u16 count)
+{
+	u32 eecd;
+	u32 i;
+	u16 data;
+
+	DEBUGFUNC("e1000_shift_in_eec_bits");
+
+	eecd = E1000_READ_REG(hw, E1000_EECD);
+
+	eecd &= ~(E1000_EECD_DO | E1000_EECD_DI);
+	data = 0;
+
+	for (i = 0; i < count; i++) {
+		data <<= 1;
+		e1000_raise_eec_clk(hw, &eecd);
+
+		eecd = E1000_READ_REG(hw, E1000_EECD);
+
+		eecd &= ~E1000_EECD_DI;
+		if (eecd & E1000_EECD_DO)
+			data |= 1;
+
+		e1000_lower_eec_clk(hw, &eecd);
+	}
+
+	return data;
+}
+
+/**
+ *  e1000_poll_eerd_eewr_done - Poll for EEPROM read/write completion
+ *  @hw: pointer to the HW structure
+ *  @ee_reg: EEPROM flag for polling
+ *
+ *  Polls the EEPROM status bit for either read or write completion based
+ *  upon the value of 'ee_reg'.
+ **/
+s32 e1000_poll_eerd_eewr_done(struct e1000_hw *hw, int ee_reg)
+{
+	u32 attempts = 100000;
+	u32 i, reg = 0;
+	s32 ret_val = -E1000_ERR_NVM;
+
+	DEBUGFUNC("e1000_poll_eerd_eewr_done");
+
+	for (i = 0; i < attempts; i++) {
+		if (ee_reg == E1000_NVM_POLL_READ)
+			reg = E1000_READ_REG(hw, E1000_EERD);
+		else
+			reg = E1000_READ_REG(hw, E1000_EEWR);
+
+		if (reg & E1000_NVM_RW_REG_DONE) {
+			ret_val = E1000_SUCCESS;
+			break;
+		}
+
+		usec_delay(5);
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_acquire_nvm_generic - Generic request for access to EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Set the EEPROM access request bit and wait for EEPROM access grant bit.
+ *  Return successful if access grant bit set, else clear the request for
+ *  EEPROM access and return -E1000_ERR_NVM (-1).
+ **/
+s32 e1000_acquire_nvm_generic(struct e1000_hw *hw)
+{
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	s32 timeout = E1000_NVM_GRANT_ATTEMPTS;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_acquire_nvm_generic");
+
+	E1000_WRITE_REG(hw, E1000_EECD, eecd | E1000_EECD_REQ);
+	eecd = E1000_READ_REG(hw, E1000_EECD);
+
+	while (timeout) {
+		if (eecd & E1000_EECD_GNT)
+			break;
+		usec_delay(5);
+		eecd = E1000_READ_REG(hw, E1000_EECD);
+		timeout--;
+	}
+
+	if (!timeout) {
+		eecd &= ~E1000_EECD_REQ;
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		DEBUGOUT("Could not acquire NVM grant\n");
+		ret_val = -E1000_ERR_NVM;
+	}
+
+	return ret_val;
+}
+
+/**
+ *  e1000_standby_nvm - Return EEPROM to standby state
+ *  @hw: pointer to the HW structure
+ *
+ *  Return the EEPROM to a standby state.
+ **/
+static void e1000_standby_nvm(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+
+	DEBUGFUNC("e1000_standby_nvm");
+
+	if (nvm->type == e1000_nvm_eeprom_microwire) {
+		eecd &= ~(E1000_EECD_CS | E1000_EECD_SK);
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		E1000_WRITE_FLUSH(hw);
+		usec_delay(nvm->delay_usec);
+
+		e1000_raise_eec_clk(hw, &eecd);
+
+		/* Select EEPROM */
+		eecd |= E1000_EECD_CS;
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		E1000_WRITE_FLUSH(hw);
+		usec_delay(nvm->delay_usec);
+
+		e1000_lower_eec_clk(hw, &eecd);
+	} else if (nvm->type == e1000_nvm_eeprom_spi) {
+		/* Toggle CS to flush commands */
+		eecd |= E1000_EECD_CS;
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		E1000_WRITE_FLUSH(hw);
+		usec_delay(nvm->delay_usec);
+		eecd &= ~E1000_EECD_CS;
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		E1000_WRITE_FLUSH(hw);
+		usec_delay(nvm->delay_usec);
+	}
+}
+
+/**
+ *  e1000_stop_nvm - Terminate EEPROM command
+ *  @hw: pointer to the HW structure
+ *
+ *  Terminates the current command by inverting the EEPROM's chip select pin.
+ **/
+void e1000_stop_nvm(struct e1000_hw *hw)
+{
+	u32 eecd;
+
+	DEBUGFUNC("e1000_stop_nvm");
+
+	eecd = E1000_READ_REG(hw, E1000_EECD);
+	if (hw->nvm.type == e1000_nvm_eeprom_spi) {
+		/* Pull CS high */
+		eecd |= E1000_EECD_CS;
+		e1000_lower_eec_clk(hw, &eecd);
+	} else if (hw->nvm.type == e1000_nvm_eeprom_microwire) {
+		/* CS on Microcwire is active-high */
+		eecd &= ~(E1000_EECD_CS | E1000_EECD_DI);
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		e1000_raise_eec_clk(hw, &eecd);
+		e1000_lower_eec_clk(hw, &eecd);
+	}
+}
+
+/**
+ *  e1000_release_nvm_generic - Release exclusive access to EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Stop any current commands to the EEPROM and clear the EEPROM request bit.
+ **/
+void e1000_release_nvm_generic(struct e1000_hw *hw)
+{
+	u32 eecd;
+
+	DEBUGFUNC("e1000_release_nvm_generic");
+
+	e1000_stop_nvm(hw);
+
+	eecd = E1000_READ_REG(hw, E1000_EECD);
+	eecd &= ~E1000_EECD_REQ;
+	E1000_WRITE_REG(hw, E1000_EECD, eecd);
+}
+
+/**
+ *  e1000_ready_nvm_eeprom - Prepares EEPROM for read/write
+ *  @hw: pointer to the HW structure
+ *
+ *  Setups the EEPROM for reading and writing.
+ **/
+static s32 e1000_ready_nvm_eeprom(struct e1000_hw *hw)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 eecd = E1000_READ_REG(hw, E1000_EECD);
+	s32 ret_val = E1000_SUCCESS;
+	u16 timeout = 0;
+	u8 spi_stat_reg;
+
+	DEBUGFUNC("e1000_ready_nvm_eeprom");
+
+	if (nvm->type == e1000_nvm_eeprom_microwire) {
+		/* Clear SK and DI */
+		eecd &= ~(E1000_EECD_DI | E1000_EECD_SK);
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		/* Set CS */
+		eecd |= E1000_EECD_CS;
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+	} else if (nvm->type == e1000_nvm_eeprom_spi) {
+		/* Clear SK and CS */
+		eecd &= ~(E1000_EECD_CS | E1000_EECD_SK);
+		E1000_WRITE_REG(hw, E1000_EECD, eecd);
+		usec_delay(1);
+		timeout = NVM_MAX_RETRY_SPI;
+
+		/* Read "Status Register" repeatedly until the LSB is cleared.
+		 * The EEPROM will signal that the command has been completed
+		 * by clearing bit 0 of the internal status register.  If it's
+		 * not cleared within 'timeout', then error out. */
+		while (timeout) {
+			e1000_shift_out_eec_bits(hw, NVM_RDSR_OPCODE_SPI,
+			                         hw->nvm.opcode_bits);
+			spi_stat_reg = (u8)e1000_shift_in_eec_bits(hw, 8);
+			if (!(spi_stat_reg & NVM_STATUS_RDY_SPI))
+				break;
+
+			usec_delay(5);
+			e1000_standby_nvm(hw);
+			timeout--;
+		}
+
+		if (!timeout) {
+			DEBUGOUT("SPI NVM Status error\n");
+			ret_val = -E1000_ERR_NVM;
+			goto out;
+		}
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_nvm_spi - Read EEPROM's using SPI
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of word in the EEPROM to read
+ *  @words: number of words to read
+ *  @data: word read from the EEPROM
+ *
+ *  Reads a 16 bit word from the EEPROM.
+ **/
+s32 e1000_read_nvm_spi(struct e1000_hw *hw, u16 offset, u16 words, u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 i = 0;
+	s32 ret_val;
+	u16 word_in;
+	u8 read_opcode = NVM_READ_OPCODE_SPI;
+
+	DEBUGFUNC("e1000_read_nvm_spi");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_ready_nvm_eeprom(hw);
+	if (ret_val)
+		goto release;
+
+	e1000_standby_nvm(hw);
+
+	if ((nvm->address_bits == 8) && (offset >= 128))
+		read_opcode |= NVM_A8_OPCODE_SPI;
+
+	/* Send the READ command (opcode + addr) */
+	e1000_shift_out_eec_bits(hw, read_opcode, nvm->opcode_bits);
+	e1000_shift_out_eec_bits(hw, (u16)(offset*2), nvm->address_bits);
+
+	/* Read the data.  SPI NVMs increment the address with each byte
+	 * read and will roll over if reading beyond the end.  This allows
+	 * us to read the whole NVM from any offset */
+	for (i = 0; i < words; i++) {
+		word_in = e1000_shift_in_eec_bits(hw, 16);
+		data[i] = (word_in >> 8) | (word_in << 8);
+	}
+
+release:
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_nvm_microwire - Reads EEPROM's using microwire
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of word in the EEPROM to read
+ *  @words: number of words to read
+ *  @data: word read from the EEPROM
+ *
+ *  Reads a 16 bit word from the EEPROM.
+ **/
+s32 e1000_read_nvm_microwire(struct e1000_hw *hw, u16 offset, u16 words,
+                             u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 i = 0;
+	s32 ret_val;
+	u8 read_opcode = NVM_READ_OPCODE_MICROWIRE;
+
+	DEBUGFUNC("e1000_read_nvm_microwire");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_ready_nvm_eeprom(hw);
+	if (ret_val)
+		goto release;
+
+	for (i = 0; i < words; i++) {
+		/* Send the READ command (opcode + addr) */
+		e1000_shift_out_eec_bits(hw, read_opcode, nvm->opcode_bits);
+		e1000_shift_out_eec_bits(hw, (u16)(offset + i),
+					nvm->address_bits);
+
+		/* Read the data.  For microwire, each word requires the
+		 * overhead of setup and tear-down. */
+		data[i] = e1000_shift_in_eec_bits(hw, 16);
+		e1000_standby_nvm(hw);
+	}
+
+release:
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_nvm_eerd - Reads EEPROM using EERD register
+ *  @hw: pointer to the HW structure
+ *  @offset: offset of word in the EEPROM to read
+ *  @words: number of words to read
+ *  @data: word read from the EEPROM
+ *
+ *  Reads a 16 bit word from the EEPROM using the EERD register.
+ **/
+s32 e1000_read_nvm_eerd(struct e1000_hw *hw, u16 offset, u16 words, u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	u32 i, eerd = 0;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_read_nvm_eerd");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	for (i = 0; i < words; i++) {
+		eerd = ((offset+i) << E1000_NVM_RW_ADDR_SHIFT) +
+		       E1000_NVM_RW_REG_START;
+
+		E1000_WRITE_REG(hw, E1000_EERD, eerd);
+		ret_val = e1000_poll_eerd_eewr_done(hw, E1000_NVM_POLL_READ);
+		if (ret_val)
+			break;
+
+		data[i] = (E1000_READ_REG(hw, E1000_EERD) >>
+		           E1000_NVM_RW_REG_DATA);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_nvm_spi - Write to EEPROM using SPI
+ *  @hw: pointer to the HW structure
+ *  @offset: offset within the EEPROM to be written to
+ *  @words: number of words to write
+ *  @data: 16 bit word(s) to be written to the EEPROM
+ *
+ *  Writes data to EEPROM at offset using SPI interface.
+ *
+ *  If e1000_update_nvm_checksum is not called after this function , the
+ *  EEPROM will most likley contain an invalid checksum.
+ **/
+s32 e1000_write_nvm_spi(struct e1000_hw *hw, u16 offset, u16 words, u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	s32 ret_val;
+	u16 widx = 0;
+
+	DEBUGFUNC("e1000_write_nvm_spi");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	msec_delay(10);
+
+	while (widx < words) {
+		u8 write_opcode = NVM_WRITE_OPCODE_SPI;
+
+		ret_val = e1000_ready_nvm_eeprom(hw);
+		if (ret_val)
+			goto release;
+
+		e1000_standby_nvm(hw);
+
+		/* Send the WRITE ENABLE command (8 bit opcode) */
+		e1000_shift_out_eec_bits(hw, NVM_WREN_OPCODE_SPI,
+		                         nvm->opcode_bits);
+
+		e1000_standby_nvm(hw);
+
+		/* Some SPI eeproms use the 8th address bit embedded in the
+		 * opcode */
+		if ((nvm->address_bits == 8) && (offset >= 128))
+			write_opcode |= NVM_A8_OPCODE_SPI;
+
+		/* Send the Write command (8-bit opcode + addr) */
+		e1000_shift_out_eec_bits(hw, write_opcode, nvm->opcode_bits);
+		e1000_shift_out_eec_bits(hw, (u16)((offset + widx) * 2),
+		                         nvm->address_bits);
+
+		/* Loop to allow for up to whole page write of eeprom */
+		while (widx < words) {
+			u16 word_out = data[widx];
+			word_out = (word_out >> 8) | (word_out << 8);
+			e1000_shift_out_eec_bits(hw, word_out, 16);
+			widx++;
+
+			if ((((offset + widx) * 2) % nvm->page_size) == 0) {
+				e1000_standby_nvm(hw);
+				break;
+			}
+		}
+	}
+
+	msec_delay(10);
+release:
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_nvm_microwire - Writes EEPROM using microwire
+ *  @hw: pointer to the HW structure
+ *  @offset: offset within the EEPROM to be written to
+ *  @words: number of words to write
+ *  @data: 16 bit word(s) to be written to the EEPROM
+ *
+ *  Writes data to EEPROM at offset using microwire interface.
+ *
+ *  If e1000_update_nvm_checksum is not called after this function , the
+ *  EEPROM will most likley contain an invalid checksum.
+ **/
+s32 e1000_write_nvm_microwire(struct e1000_hw *hw, u16 offset, u16 words,
+                              u16 *data)
+{
+	struct e1000_nvm_info *nvm = &hw->nvm;
+	s32  ret_val;
+	u32 eecd;
+	u16 words_written = 0;
+	u16 widx = 0;
+
+	DEBUGFUNC("e1000_write_nvm_microwire");
+
+	/* A check for invalid values:  offset too large, too many words,
+	 * and not enough words. */
+	if ((offset >= nvm->word_size) || (words > (nvm->word_size - offset)) ||
+	    (words == 0)) {
+		DEBUGOUT("nvm parameter(s) out of bounds\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_nvm(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_ready_nvm_eeprom(hw);
+	if (ret_val)
+		goto release;
+
+	e1000_shift_out_eec_bits(hw, NVM_EWEN_OPCODE_MICROWIRE,
+	                         (u16)(nvm->opcode_bits + 2));
+
+	e1000_shift_out_eec_bits(hw, 0, (u16)(nvm->address_bits - 2));
+
+	e1000_standby_nvm(hw);
+
+	while (words_written < words) {
+		e1000_shift_out_eec_bits(hw, NVM_WRITE_OPCODE_MICROWIRE,
+		                         nvm->opcode_bits);
+
+		e1000_shift_out_eec_bits(hw, (u16)(offset + words_written),
+		                         nvm->address_bits);
+
+		e1000_shift_out_eec_bits(hw, data[words_written], 16);
+
+		e1000_standby_nvm(hw);
+
+		for (widx = 0; widx < 200; widx++) {
+			eecd = E1000_READ_REG(hw, E1000_EECD);
+			if (eecd & E1000_EECD_DO)
+				break;
+			usec_delay(50);
+		}
+
+		if (widx == 200) {
+			DEBUGOUT("NVM Write did not complete\n");
+			ret_val = -E1000_ERR_NVM;
+			goto release;
+		}
+
+		e1000_standby_nvm(hw);
+
+		words_written++;
+	}
+
+	e1000_shift_out_eec_bits(hw, NVM_EWDS_OPCODE_MICROWIRE,
+	                         (u16)(nvm->opcode_bits + 2));
+
+	e1000_shift_out_eec_bits(hw, 0, (u16)(nvm->address_bits - 2));
+
+release:
+	e1000_release_nvm(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_part_num_generic - Read device part number
+ *  @hw: pointer to the HW structure
+ *  @part_num: pointer to device part number
+ *
+ *  Reads the product board assembly (PBA) number from the EEPROM and stores
+ *  the value in part_num.
+ **/
+s32 e1000_read_part_num_generic(struct e1000_hw *hw, u32 *part_num)
+{
+	s32  ret_val;
+	u16 nvm_data;
+
+	DEBUGFUNC("e1000_read_part_num_generic");
+
+	ret_val = e1000_read_nvm(hw, NVM_PBA_OFFSET_0, 1, &nvm_data);
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+	*part_num = (u32)(nvm_data << 16);
+
+	ret_val = e1000_read_nvm(hw, NVM_PBA_OFFSET_1, 1, &nvm_data);
+	if (ret_val) {
+		DEBUGOUT("NVM Read Error\n");
+		goto out;
+	}
+	*part_num |= nvm_data;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_mac_addr_generic - Read device MAC address
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the device MAC address from the EEPROM and stores the value.
+ *  Since devices with two ports use the same EEPROM, we increment the
+ *  last bit in the MAC address for the second port.
+ **/
+s32 e1000_read_mac_addr_generic(struct e1000_hw *hw)
+{
+	s32  ret_val = E1000_SUCCESS;
+	u16 offset, nvm_data, i;
+
+	DEBUGFUNC("e1000_read_mac_addr");
+
+	for (i = 0; i < ETH_ADDR_LEN; i += 2) {
+		offset = i >> 1;
+		ret_val = e1000_read_nvm(hw, offset, 1, &nvm_data);
+		if (ret_val) {
+			DEBUGOUT("NVM Read Error\n");
+			goto out;
+		}
+		hw->mac.perm_addr[i] = (u8)(nvm_data & 0xFF);
+		hw->mac.perm_addr[i+1] = (u8)(nvm_data >> 8);
+	}
+
+	/* Flip last bit of mac address if we're on second port */
+	if (hw->bus.func == E1000_FUNC_1)
+		hw->mac.perm_addr[5] ^= 1;
+
+	for (i = 0; i < ETH_ADDR_LEN; i++)
+		hw->mac.addr[i] = hw->mac.perm_addr[i];
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_validate_nvm_checksum_generic - Validate EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Calculates the EEPROM checksum by reading/adding each word of the EEPROM
+ *  and then verifies that the sum of the EEPROM is equal to 0xBABA.
+ **/
+s32 e1000_validate_nvm_checksum_generic(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 checksum = 0;
+	u16 i, nvm_data;
+
+	DEBUGFUNC("e1000_validate_nvm_checksum_generic");
+
+	for (i = 0; i < (NVM_CHECKSUM_REG + 1); i++) {
+		ret_val = e1000_read_nvm(hw, i, 1, &nvm_data);
+		if (ret_val) {
+			DEBUGOUT("NVM Read Error\n");
+			goto out;
+		}
+		checksum += nvm_data;
+	}
+
+	if (checksum != (u16) NVM_SUM) {
+		DEBUGOUT("NVM Checksum Invalid\n");
+		ret_val = -E1000_ERR_NVM;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_update_nvm_checksum_generic - Update EEPROM checksum
+ *  @hw: pointer to the HW structure
+ *
+ *  Updates the EEPROM checksum by reading/adding each word of the EEPROM
+ *  up to the checksum.  Then calculates the EEPROM checksum and writes the
+ *  value to the EEPROM.
+ **/
+s32 e1000_update_nvm_checksum_generic(struct e1000_hw *hw)
+{
+	s32  ret_val;
+	u16 checksum = 0;
+	u16 i, nvm_data;
+
+	DEBUGFUNC("e1000_update_nvm_checksum");
+
+	for (i = 0; i < NVM_CHECKSUM_REG; i++) {
+		ret_val = e1000_read_nvm(hw, i, 1, &nvm_data);
+		if (ret_val) {
+			DEBUGOUT("NVM Read Error while updating checksum.\n");
+			goto out;
+		}
+		checksum += nvm_data;
+	}
+	checksum = (u16) NVM_SUM - checksum;
+	ret_val = e1000_write_nvm(hw, NVM_CHECKSUM_REG, 1, &checksum);
+	if (ret_val) {
+		DEBUGOUT("NVM Write Error while updating checksum.\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_reload_nvm_generic - Reloads EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  Reloads the EEPROM by setting the "Reinitialize from EEPROM" bit in the
+ *  extended control register.
+ **/
+void e1000_reload_nvm_generic(struct e1000_hw *hw)
+{
+	u32 ctrl_ext;
+
+	DEBUGFUNC("e1000_reload_nvm_generic");
+
+	usec_delay(10);
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	ctrl_ext |= E1000_CTRL_EXT_EE_RST;
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	E1000_WRITE_FLUSH(hw);
+}
+
+/* Function pointers local to this file and not intended for public use */
+
+/**
+ *  e1000_acquire_nvm - Acquire exclusive access to EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  For those silicon families which have implemented a NVM acquire function,
+ *  run the defined function else return success.
+ **/
+s32 e1000_acquire_nvm(struct e1000_hw *hw)
+{
+	if (hw->func.acquire_nvm != NULL)
+		return hw->func.acquire_nvm(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_release_nvm - Release exclusive access to EEPROM
+ *  @hw: pointer to the HW structure
+ *
+ *  For those silicon families which have implemented a NVM release function,
+ *  run the defined fucntion else return success.
+ **/
+void e1000_release_nvm(struct e1000_hw *hw)
+{
+	if (hw->func.release_nvm != NULL)
+		hw->func.release_nvm(hw);
+}
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_nvm.h linux-2.6.9/drivers/net/e1000/e1000_nvm.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_nvm.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_nvm.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,59 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_NVM_H_
+#define _E1000_NVM_H_
+
+s32  e1000_acquire_nvm_generic(struct e1000_hw *hw);
+
+s32  e1000_poll_eerd_eewr_done(struct e1000_hw *hw, int ee_reg);
+s32  e1000_read_mac_addr_generic(struct e1000_hw *hw);
+s32  e1000_read_part_num_generic(struct e1000_hw *hw, u32 *part_num);
+s32  e1000_read_nvm_spi(struct e1000_hw *hw, u16 offset, u16 words, u16 *data);
+s32  e1000_read_nvm_microwire(struct e1000_hw *hw, u16 offset,
+                              u16 words, u16 *data);
+s32  e1000_read_nvm_eerd(struct e1000_hw *hw, u16 offset, u16 words, u16 *data);
+s32  e1000_valid_led_default_generic(struct e1000_hw *hw, u16 *data);
+s32  e1000_validate_nvm_checksum_generic(struct e1000_hw *hw);
+s32  e1000_write_nvm_eewr(struct e1000_hw *hw, u16 offset,
+                          u16 words, u16 *data);
+s32  e1000_write_nvm_microwire(struct e1000_hw *hw, u16 offset,
+                               u16 words, u16 *data);
+s32  e1000_write_nvm_spi(struct e1000_hw *hw, u16 offset, u16 words, u16 *data);
+s32  e1000_update_nvm_checksum_generic(struct e1000_hw *hw);
+void e1000_stop_nvm(struct e1000_hw *hw);
+void e1000_release_nvm_generic(struct e1000_hw *hw);
+void e1000_reload_nvm_generic(struct e1000_hw *hw);
+
+/* Function pointers */
+s32  e1000_acquire_nvm(struct e1000_hw *hw);
+void e1000_release_nvm(struct e1000_hw *hw);
+
+#define E1000_STM_OPCODE  0xDB00
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_osdep.h linux-2.6.9/drivers/net/e1000/e1000_osdep.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_osdep.h	2004-10-18 23:53:43.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_osdep.h	2007-07-16 13:33:15.000000000 +0200
@@ -1,58 +1,71 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
 
 
-/* glue for the OS independent part of e1000
+/* glue for the OS-dependent part of e1000
  * includes register access macros
  */
 
 #ifndef _E1000_OSDEP_H_
 #define _E1000_OSDEP_H_
 
-#include <linux/types.h>
 #include <linux/pci.h>
 #include <linux/delay.h>
-#include <asm/io.h>
 #include <linux/interrupt.h>
-#include <linux/sched.h>
+#include <linux/if_ether.h>
+
+#include "kcompat.h"
 
+#define usec_delay(x) udelay(x)
 #ifndef msec_delay
 #define msec_delay(x)	do { if(in_interrupt()) { \
-				/* Don't mdelay in interrupt context! */ \
+				/* Don't sleep in interrupt context! */ \
 	                	BUG(); \
 			} else { \
-				set_current_state(TASK_UNINTERRUPTIBLE); \
-				schedule_timeout((x * HZ)/1000 + 2); \
-			} } while(0)
+				msleep(x); \
+			} } while (0)
+
+/* Some workarounds require millisecond delays and are run during interrupt
+ * context.  Most notably, when establishing link, the phy may need tweaking
+ * but cannot process phy register reads/writes faster than millisecond
+ * intervals...and we establish link due to a "link status change" interrupt.
+ */
+#define msec_delay_irq(x) mdelay(x)
 #endif
 
 #define PCI_COMMAND_REGISTER   PCI_COMMAND
 #define CMD_MEM_WRT_INVALIDATE PCI_COMMAND_INVALIDATE
+#define ETH_ADDR_LEN           ETH_ALEN
+
+#ifdef __BIG_ENDIAN
+#define E1000_BIG_ENDIAN __BIG_ENDIAN
+#endif
+
 
 typedef enum {
 #undef FALSE
@@ -61,40 +74,58 @@
     TRUE = 1
 } boolean_t;
 
-#define MSGOUT(S, A, B)	printk(KERN_DEBUG S "\n", A, B)
-
-#ifdef DBG
-#define DEBUGOUT(S)		printk(KERN_DEBUG S "\n")
-#define DEBUGOUT1(S, A...)	printk(KERN_DEBUG S "\n", A)
-#else
 #define DEBUGOUT(S)
 #define DEBUGOUT1(S, A...)
-#endif
 
-#define DEBUGFUNC(F) DEBUGOUT(F)
+#define DEBUGFUNC(F) DEBUGOUT(F "\n")
 #define DEBUGOUT2 DEBUGOUT1
 #define DEBUGOUT3 DEBUGOUT2
 #define DEBUGOUT7 DEBUGOUT3
 
+#define E1000_REGISTER(a, reg) (((a)->mac.type >= e1000_82543) \
+                               ? reg                           \
+                               : e1000_translate_register_82542(reg))
 
 #define E1000_WRITE_REG(a, reg, value) ( \
-    writel((value), ((a)->hw_addr + \
-        (((a)->mac_type >= e1000_82543) ? E1000_##reg : E1000_82542_##reg))))
+    writel((value), ((a)->hw_addr + E1000_REGISTER(a, reg))))
 
-#define E1000_READ_REG(a, reg) ( \
-    readl((a)->hw_addr + \
-        (((a)->mac_type >= e1000_82543) ? E1000_##reg : E1000_82542_##reg)))
+#define E1000_READ_REG(a, reg) (readl((a)->hw_addr + E1000_REGISTER(a, reg)))
 
 #define E1000_WRITE_REG_ARRAY(a, reg, offset, value) ( \
-    writel((value), ((a)->hw_addr + \
-        (((a)->mac_type >= e1000_82543) ? E1000_##reg : E1000_82542_##reg) + \
-        ((offset) << 2))))
+    writel((value), ((a)->hw_addr + E1000_REGISTER(a, reg) + ((offset) << 2))))
 
 #define E1000_READ_REG_ARRAY(a, reg, offset) ( \
-    readl((a)->hw_addr + \
-        (((a)->mac_type >= e1000_82543) ? E1000_##reg : E1000_82542_##reg) + \
-        ((offset) << 2)))
+    readl((a)->hw_addr + E1000_REGISTER(a, reg) + ((offset) << 2)))
+
+#define E1000_READ_REG_ARRAY_DWORD E1000_READ_REG_ARRAY
+#define E1000_WRITE_REG_ARRAY_DWORD E1000_WRITE_REG_ARRAY
+
+#define E1000_WRITE_REG_ARRAY_WORD(a, reg, offset, value) ( \
+    writew((value), ((a)->hw_addr + E1000_REGISTER(a, reg) + ((offset) << 1))))
+
+#define E1000_READ_REG_ARRAY_WORD(a, reg, offset) ( \
+    readw((a)->hw_addr + E1000_REGISTER(a, reg) + ((offset) << 1)))
+
+#define E1000_WRITE_REG_ARRAY_BYTE(a, reg, offset, value) ( \
+    writeb((value), ((a)->hw_addr + E1000_REGISTER(a, reg) + (offset))))
+
+#define E1000_READ_REG_ARRAY_BYTE(a, reg, offset) ( \
+    readb((a)->hw_addr + E1000_REGISTER(a, reg) + (offset)))
+
+#define E1000_WRITE_REG_IO(a, reg, offset) do { \
+    outl(reg, ((a)->io_base));                  \
+    outl(offset, ((a)->io_base + 4));      } while(0)
+
+#define E1000_WRITE_FLUSH(a) E1000_READ_REG(a, E1000_STATUS)
+
+#define E1000_WRITE_FLASH_REG(a, reg, value) ( \
+    writel((value), ((a)->flash_address + reg)))
+
+#define E1000_WRITE_FLASH_REG16(a, reg, value) ( \
+    writew((value), ((a)->flash_address + reg)))
+
+#define E1000_READ_FLASH_REG(a, reg) (readl((a)->flash_address + reg))
 
-#define E1000_WRITE_FLUSH(a) E1000_READ_REG(a, STATUS)
+#define E1000_READ_FLASH_REG16(a, reg) (readw((a)->flash_address + reg))
 
 #endif /* _E1000_OSDEP_H_ */
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_param.c linux-2.6.9/drivers/net/e1000/e1000_param.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_param.c	2004-10-18 23:54:39.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/e1000_param.c	2007-07-16 13:33:15.000000000 +0200
@@ -1,31 +1,34 @@
 /*******************************************************************************
 
-  
-  Copyright(c) 1999 - 2004 Intel Corporation. All rights reserved.
-  
-  This program is free software; you can redistribute it and/or modify it 
-  under the terms of the GNU General Public License as published by the Free 
-  Software Foundation; either version 2 of the License, or (at your option) 
-  any later version.
-  
-  This program is distributed in the hope that it will be useful, but WITHOUT 
-  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
   more details.
-  
+
   You should have received a copy of the GNU General Public License along with
-  this program; if not, write to the Free Software Foundation, Inc., 59 
-  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-  
-  The full GNU General Public License is included in this distribution in the
-  file called LICENSE.
-  
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
   Contact Information:
   Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
   Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 *******************************************************************************/
 
+
+#include <linux/netdevice.h>
+
 #include "e1000.h"
 
 /* This is the only thing that needs to be changed to adjust the
@@ -34,10 +37,17 @@
 
 #define E1000_MAX_NIC 32
 
-#define OPTION_UNSET    -1
+#define OPTION_UNSET   -1
 #define OPTION_DISABLED 0
 #define OPTION_ENABLED  1
 
+/* All parameters are treated the same, as an integer array of values.
+ * This macro just reduces the need to repeat the same declaration code
+ * over and over (plus this helps to avoid typo bugs).
+ */
+
+#define E1000_PARAM_INIT { [0 ... E1000_MAX_NIC] = OPTION_UNSET }
+#ifndef module_param_array
 /* Module Parameters are always initialized to -1, so that the driver
  * can tell the difference between no user specified value or the
  * user asking for the default value.
@@ -48,17 +58,17 @@
  * "Extensions to the C Language Family" of the GCC documentation.
  */
 
-#define E1000_PARAM_INIT { [0 ... E1000_MAX_NIC] = OPTION_UNSET }
-
-/* All parameters are treated the same, as an integer array of values.
- * This macro just reduces the need to repeat the same declaration code
- * over and over (plus this helps to avoid typo bugs).
- */
-
-#define E1000_PARAM(X, S) \
-static const int __devinitdata X[E1000_MAX_NIC + 1] = E1000_PARAM_INIT; \
-MODULE_PARM(X, "1-" __MODULE_STRING(E1000_MAX_NIC) "i"); \
-MODULE_PARM_DESC(X, S);
+#define E1000_PARAM(X, desc) \
+	static const int __devinitdata X[E1000_MAX_NIC+1] = E1000_PARAM_INIT; \
+	MODULE_PARM(X, "1-" __MODULE_STRING(E1000_MAX_NIC) "i"); \
+	MODULE_PARM_DESC(X, desc);
+#else
+#define E1000_PARAM(X, desc) \
+	static int __devinitdata X[E1000_MAX_NIC+1] = E1000_PARAM_INIT; \
+	static int num_##X = 0; \
+	module_param_array_named(X, X, int, &num_##X, 0); \
+	MODULE_PARM_DESC(X, desc);
+#endif
 
 /* Transmit Descriptor Count
  *
@@ -67,7 +77,6 @@
  *
  * Default Value: 256
  */
-
 E1000_PARAM(TxDescriptors, "Number of transmit descriptors");
 
 /* Receive Descriptor Count
@@ -77,7 +86,6 @@
  *
  * Default Value: 256
  */
-
 E1000_PARAM(RxDescriptors, "Number of receive descriptors");
 
 /* User Specified Speed Override
@@ -90,7 +98,6 @@
  *
  * Default Value: 0
  */
-
 E1000_PARAM(Speed, "Speed setting");
 
 /* User Specified Duplex Override
@@ -102,7 +109,6 @@
  *
  * Default Value: 0
  */
-
 E1000_PARAM(Duplex, "Duplex setting");
 
 /* Auto-negotiation Advertisement Override
@@ -119,8 +125,9 @@
  *
  * Default Value: 0x2F (copper); 0x20 (fiber)
  */
-
 E1000_PARAM(AutoNeg, "Advertised auto-negotiation setting");
+#define AUTONEG_ADV_DEFAULT  0x2F
+#define AUTONEG_ADV_MASK     0x2F
 
 /* User Specified Flow Control Override
  *
@@ -132,8 +139,8 @@
  *
  * Default Value: Read flow control settings from the EEPROM
  */
-
 E1000_PARAM(FlowControl, "Flow Control setting");
+#define FLOW_CONTROL_DEFAULT FLOW_CONTROL_FULL
 
 /* XsumRX - Receive Checksum Offload Enable/Disable
  *
@@ -144,78 +151,71 @@
  *
  * Default Value: 1
  */
-
 E1000_PARAM(XsumRX, "Disable or enable Receive Checksum offload");
 
 /* Transmit Interrupt Delay in units of 1.024 microseconds
+ *  Tx interrupt delay needs to typically be set to something non zero
  *
  * Valid Range: 0-65535
- *
- * Default Value: 64
  */
-
 E1000_PARAM(TxIntDelay, "Transmit Interrupt Delay");
+#define DEFAULT_TIDV                   8
+#define MAX_TXDELAY               0xFFFF
+#define MIN_TXDELAY                    0
 
 /* Transmit Absolute Interrupt Delay in units of 1.024 microseconds
  *
  * Valid Range: 0-65535
- *
- * Default Value: 0
  */
-
 E1000_PARAM(TxAbsIntDelay, "Transmit Absolute Interrupt Delay");
+#define DEFAULT_TADV                  32
+#define MAX_TXABSDELAY            0xFFFF
+#define MIN_TXABSDELAY                 0
 
 /* Receive Interrupt Delay in units of 1.024 microseconds
+ *   hardware will likely hang if you set this to anything but zero.
  *
  * Valid Range: 0-65535
- *
- * Default Value: 0
  */
-
 E1000_PARAM(RxIntDelay, "Receive Interrupt Delay");
+#define DEFAULT_RDTR                   0
+#define MAX_RXDELAY               0xFFFF
+#define MIN_RXDELAY                    0
 
 /* Receive Absolute Interrupt Delay in units of 1.024 microseconds
  *
  * Valid Range: 0-65535
- *
- * Default Value: 128
  */
-
 E1000_PARAM(RxAbsIntDelay, "Receive Absolute Interrupt Delay");
+#define DEFAULT_RADV                   8
+#define MAX_RXABSDELAY            0xFFFF
+#define MIN_RXABSDELAY                 0
 
 /* Interrupt Throttle Rate (interrupts/sec)
  *
- * Valid Range: 100-100000 (0=off, 1=dynamic)
- *
- * Default Value: 1
+ * Valid Range: 100-100000 (0=off, 1=dynamic, 3=dynamic conservative)
  */
-
 E1000_PARAM(InterruptThrottleRate, "Interrupt Throttling Rate");
-
-#define AUTONEG_ADV_DEFAULT  0x2F
-#define AUTONEG_ADV_MASK     0x2F
-#define FLOW_CONTROL_DEFAULT FLOW_CONTROL_FULL
-
-#define DEFAULT_RDTR                   0
-#define MAX_RXDELAY               0xFFFF
-#define MIN_RXDELAY                    0
-
-#define DEFAULT_RADV                 128
-#define MAX_RXABSDELAY            0xFFFF
-#define MIN_RXABSDELAY                 0
-
-#define DEFAULT_TIDV                  64
-#define MAX_TXDELAY               0xFFFF
-#define MIN_TXDELAY                    0
-
-#define DEFAULT_TADV                  64
-#define MAX_TXABSDELAY            0xFFFF
-#define MIN_TXABSDELAY                 0
-
-#define DEFAULT_ITR                 8000
+#define DEFAULT_ITR                    3
 #define MAX_ITR                   100000
 #define MIN_ITR                      100
 
+/* Enable Smart Power Down of the PHY
+ *
+ * Valid Range: 0, 1
+ *
+ * Default Value: 0 (disabled)
+ */
+E1000_PARAM(SmartPowerDownEnable, "Enable PHY smart power down");
+
+/* Enable Kumeran Lock Loss workaround
+ *
+ * Valid Range: 0, 1
+ *
+ * Default Value: 1 (enabled)
+ */
+E1000_PARAM(KumeranLockLoss, "Enable Kumeran lock loss workaround");
+
 struct e1000_option {
 	enum { enable_option, range_option, list_option } type;
 	char *name;
@@ -233,11 +233,10 @@
 	} arg;
 };
 
-static int __devinit
-e1000_validate_option(int *value, struct e1000_option *opt,
-		struct e1000_adapter *adapter)
+static int __devinit e1000_validate_option(int *value, struct e1000_option *opt,
+                                           struct e1000_adapter *adapter)
 {
-	if(*value == OPTION_UNSET) {
+	if (*value == OPTION_UNSET) {
 		*value = opt->def;
 		return 0;
 	}
@@ -254,7 +253,7 @@
 		}
 		break;
 	case range_option:
-		if(*value >= opt->arg.r.min && *value <= opt->arg.r.max) {
+		if (*value >= opt->arg.r.min && *value <= opt->arg.r.max) {
 			DPRINTK(PROBE, INFO,
 					"%s set to %i\n", opt->name, *value);
 			return 0;
@@ -264,10 +263,10 @@
 		int i;
 		struct e1000_opt_list *ent;
 
-		for(i = 0; i < opt->arg.l.nr; i++) {
+		for (i = 0; i < opt->arg.l.nr; i++) {
 			ent = &opt->arg.l.p[i];
-			if(*value == ent->i) {
-				if(ent->str[0] != '\0')
+			if (*value == ent->i) {
+				if (ent->str[0] != '\0')
 					DPRINTK(PROBE, INFO, "%s\n", ent->str);
 				return 0;
 			}
@@ -278,7 +277,7 @@
 		BUG();
 	}
 
-	DPRINTK(PROBE, INFO, "Invalid %s specified (%i) %s\n",
+	DPRINTK(PROBE, INFO, "Invalid %s value specified (%i) %s\n",
 	       opt->name, *value, opt->err);
 	*value = opt->def;
 	return -1;
@@ -296,16 +295,17 @@
  * value exists, a default value is used.  The final value is stored
  * in a variable in the adapter structure.
  **/
-
-void __devinit
-e1000_check_options(struct e1000_adapter *adapter)
+void __devinit e1000_check_options(struct e1000_adapter *adapter)
 {
+	struct e1000_hw *hw = &adapter->hw;
 	int bd = adapter->bd_number;
-	if(bd >= E1000_MAX_NIC) {
+	if (bd >= E1000_MAX_NIC) {
 		DPRINTK(PROBE, NOTICE,
 		       "Warning: no configuration for board #%i\n", bd);
 		DPRINTK(PROBE, NOTICE, "Using defaults for all values\n");
+#ifndef module_param_array
 		bd = E1000_MAX_NIC;
+#endif
 	}
 
 	{ /* Transmit Descriptor Count */
@@ -317,14 +317,25 @@
 			.def  = E1000_DEFAULT_TXD,
 			.arg  = { .r = { .min = E1000_MIN_TXD }}
 		};
-		struct e1000_desc_ring *tx_ring = &adapter->tx_ring;
-		e1000_mac_type mac_type = adapter->hw.mac_type;
-		opt.arg.r.max = mac_type < e1000_82544 ?
+		struct e1000_tx_ring *tx_ring = adapter->tx_ring;
+		int i;
+		opt.arg.r.max = hw->mac.type < e1000_82544 ?
 			E1000_MAX_TXD : E1000_MAX_82544_TXD;
 
-		tx_ring->count = TxDescriptors[bd];
-		e1000_validate_option(&tx_ring->count, &opt, adapter);
-		E1000_ROUNDUP(tx_ring->count, REQ_TX_DESCRIPTOR_MULTIPLE);
+#ifdef module_param_array
+		if (num_TxDescriptors > bd) {
+#endif
+			tx_ring->count = TxDescriptors[bd];
+			e1000_validate_option(&tx_ring->count, &opt, adapter);
+			tx_ring->count = ALIGN(tx_ring->count,
+			                       REQ_TX_DESCRIPTOR_MULTIPLE);
+#ifdef module_param_array
+		} else {
+			tx_ring->count = opt.def;
+		}
+#endif
+		for (i = 0; i < adapter->num_tx_queues; i++)
+			tx_ring[i].count = tx_ring->count;
 	}
 	{ /* Receive Descriptor Count */
 		struct e1000_option opt = {
@@ -335,14 +346,25 @@
 			.def  = E1000_DEFAULT_RXD,
 			.arg  = { .r = { .min = E1000_MIN_RXD }}
 		};
-		struct e1000_desc_ring *rx_ring = &adapter->rx_ring;
-		e1000_mac_type mac_type = adapter->hw.mac_type;
-		opt.arg.r.max = mac_type < e1000_82544 ? E1000_MAX_RXD :
+		struct e1000_rx_ring *rx_ring = adapter->rx_ring;
+		int i;
+		opt.arg.r.max = hw->mac.type < e1000_82544 ? E1000_MAX_RXD :
 			E1000_MAX_82544_RXD;
 
-		rx_ring->count = RxDescriptors[bd];
-		e1000_validate_option(&rx_ring->count, &opt, adapter);
-		E1000_ROUNDUP(rx_ring->count, REQ_RX_DESCRIPTOR_MULTIPLE);
+#ifdef module_param_array
+		if (num_RxDescriptors > bd) {
+#endif
+			rx_ring->count = RxDescriptors[bd];
+			e1000_validate_option(&rx_ring->count, &opt, adapter);
+			rx_ring->count = ALIGN(rx_ring->count,
+			                       REQ_RX_DESCRIPTOR_MULTIPLE);
+#ifdef module_param_array
+		} else {
+			rx_ring->count = opt.def;
+		}
+#endif
+		for (i = 0; i < adapter->num_rx_queues; i++)
+			rx_ring[i].count = rx_ring->count;
 	}
 	{ /* Checksum Offload Enable/Disable */
 		struct e1000_option opt = {
@@ -352,9 +374,17 @@
 			.def  = OPTION_ENABLED
 		};
 
-		int rx_csum = XsumRX[bd];
-		e1000_validate_option(&rx_csum, &opt, adapter);
-		adapter->rx_csum = rx_csum;
+#ifdef module_param_array
+		if (num_XsumRX > bd) {
+#endif
+			int rx_csum = XsumRX[bd];
+			e1000_validate_option(&rx_csum, &opt, adapter);
+			adapter->rx_csum = rx_csum;
+#ifdef module_param_array
+		} else {
+			adapter->rx_csum = opt.def;
+		}
+#endif
 	}
 	{ /* Flow Control */
 
@@ -374,9 +404,19 @@
 					 .p = fc_list }}
 		};
 
-		int fc = FlowControl[bd];
-		e1000_validate_option(&fc, &opt, adapter);
-		adapter->hw.fc = adapter->hw.original_fc = fc;
+#ifdef module_param_array
+		if (num_FlowControl > bd) {
+#endif
+			int fc = FlowControl[bd];
+			e1000_validate_option(&fc, &opt, adapter);
+			hw->mac.original_fc = fc;
+			hw->mac.fc = fc;
+#ifdef module_param_array
+		} else {
+			hw->mac.original_fc = opt.def;
+			hw->mac.fc = opt.def;
+		}
+#endif
 	}
 	{ /* Transmit Interrupt Delay */
 		struct e1000_option opt = {
@@ -388,8 +428,17 @@
 					 .max = MAX_TXDELAY }}
 		};
 
-		adapter->tx_int_delay = TxIntDelay[bd];
-		e1000_validate_option(&adapter->tx_int_delay, &opt, adapter);
+#ifdef module_param_array
+		if (num_TxIntDelay > bd) {
+#endif
+			adapter->tx_int_delay = TxIntDelay[bd];
+			e1000_validate_option(&adapter->tx_int_delay, &opt,
+			                      adapter);
+#ifdef module_param_array
+		} else {
+			adapter->tx_int_delay = opt.def;
+		}
+#endif
 	}
 	{ /* Transmit Absolute Interrupt Delay */
 		struct e1000_option opt = {
@@ -401,8 +450,17 @@
 					 .max = MAX_TXABSDELAY }}
 		};
 
-		adapter->tx_abs_int_delay = TxAbsIntDelay[bd];
-		e1000_validate_option(&adapter->tx_abs_int_delay, &opt, adapter);
+#ifdef module_param_array
+		if (num_TxAbsIntDelay > bd) {
+#endif
+			adapter->tx_abs_int_delay = TxAbsIntDelay[bd];
+			e1000_validate_option(&adapter->tx_abs_int_delay, &opt,
+			                      adapter);
+#ifdef module_param_array
+		} else {
+			adapter->tx_abs_int_delay = opt.def;
+		}
+#endif
 	}
 	{ /* Receive Interrupt Delay */
 		struct e1000_option opt = {
@@ -414,8 +472,24 @@
 					 .max = MAX_RXDELAY }}
 		};
 
-		adapter->rx_int_delay = RxIntDelay[bd];
-		e1000_validate_option(&adapter->rx_int_delay, &opt, adapter);
+		/* modify min and default if 82573 for slow ping w/a,
+		 * a value greater than 8 needs to be set for RDTR */
+		if (adapter->hw.mac.type == e1000_82573) {
+			opt.def = 32;
+			opt.arg.r.min = 8;
+		}
+
+#ifdef module_param_array
+		if (num_RxIntDelay > bd) {
+#endif
+			adapter->rx_int_delay = RxIntDelay[bd];
+			e1000_validate_option(&adapter->rx_int_delay, &opt,
+			                      adapter);
+#ifdef module_param_array
+		} else {
+			adapter->rx_int_delay = opt.def;
+		}
+#endif
 	}
 	{ /* Receive Absolute Interrupt Delay */
 		struct e1000_option opt = {
@@ -427,8 +501,17 @@
 					 .max = MAX_RXABSDELAY }}
 		};
 
-		adapter->rx_abs_int_delay = RxAbsIntDelay[bd];
-		e1000_validate_option(&adapter->rx_abs_int_delay, &opt, adapter);
+#ifdef module_param_array
+		if (num_RxAbsIntDelay > bd) {
+#endif
+			adapter->rx_abs_int_delay = RxAbsIntDelay[bd];
+			e1000_validate_option(&adapter->rx_abs_int_delay, &opt,
+			                      adapter);
+#ifdef module_param_array
+		} else {
+			adapter->rx_abs_int_delay = opt.def;
+		}
+#endif
 	}
 	{ /* Interrupt Throttling Rate */
 		struct e1000_option opt = {
@@ -440,24 +523,90 @@
 					 .max = MAX_ITR }}
 		};
 
-		adapter->itr = InterruptThrottleRate[bd];
-		switch(adapter->itr) {
-		case -1:
-			adapter->itr = 1;
-			break;
-		case 0:
-			DPRINTK(PROBE, INFO, "%s turned off\n", opt.name);
-			break;
-		case 1:
-			DPRINTK(PROBE, INFO, "%s set to dynamic mode\n", opt.name);
-			break;
-		default:
-			e1000_validate_option(&adapter->itr, &opt, adapter);
-			break;
+#ifdef module_param_array
+		if (num_InterruptThrottleRate > bd) {
+#endif
+			adapter->itr = InterruptThrottleRate[bd];
+			switch (adapter->itr) {
+			case 0:
+				DPRINTK(PROBE, INFO, "%s turned off\n",
+				        opt.name);
+				break;
+			case 1:
+				DPRINTK(PROBE, INFO, "%s set to dynamic mode\n",
+					opt.name);
+				adapter->itr_setting = adapter->itr;
+				adapter->itr = 20000;
+				break;
+			case 3:
+				DPRINTK(PROBE, INFO,
+				        "%s set to dynamic conservative mode\n",
+					opt.name);
+				adapter->itr_setting = adapter->itr;
+				adapter->itr = 20000;
+				break;
+			default:
+				e1000_validate_option(&adapter->itr, &opt,
+				        adapter);
+				/* save the setting, because the dynamic bits change itr */
+				/* clear the lower two bits because they are
+				 * used as control */
+				adapter->itr_setting = adapter->itr & ~3;
+				break;
+			}
+#ifdef module_param_array
+		} else {
+			adapter->itr_setting = opt.def;
+			adapter->itr = 20000;
 		}
+#endif
 	}
+	{ /* Smart Power Down */
+		struct e1000_option opt = {
+			.type = enable_option,
+			.name = "PHY Smart Power Down",
+			.err  = "defaulting to Disabled",
+			.def  = OPTION_DISABLED
+		};
 
-	switch(adapter->hw.media_type) {
+#ifdef module_param_array
+		if (num_SmartPowerDownEnable > bd) {
+#endif
+			int spd = SmartPowerDownEnable[bd];
+			e1000_validate_option(&spd, &opt, adapter);
+			adapter->flags.smart_power_down = spd;
+#ifdef module_param_array
+		} else {
+			adapter->flags.smart_power_down = opt.def;
+		}
+#endif
+	}
+	{ /* Kumeran Lock Loss Workaround */
+		struct e1000_option opt = {
+			.type = enable_option,
+			.name = "Kumeran Lock Loss Workaround",
+			.err  = "defaulting to Enabled",
+			.def  = OPTION_ENABLED
+		};
+
+#ifdef module_param_array
+		if (num_KumeranLockLoss > bd) {
+#endif
+			int kmrn_lock_loss = KumeranLockLoss[bd];
+			e1000_validate_option(&kmrn_lock_loss, &opt, adapter);
+			if (hw->mac.type == e1000_ich8lan)
+				e1000_set_kmrn_lock_loss_workaround_ich8lan(hw,
+				                                kmrn_lock_loss);
+#ifdef module_param_array
+		} else {
+			if (hw->mac.type == e1000_ich8lan)
+				e1000_set_kmrn_lock_loss_workaround_ich8lan(hw,
+				                                       opt.def);
+		}
+#endif
+	}
+
+	switch (hw->media_type) {
 	case e1000_media_type_fiber:
 	case e1000_media_type_internal_serdes:
 		e1000_check_fiber_options(adapter);
@@ -476,22 +625,33 @@
  *
  * Handles speed and duplex options on fiber adapters
  **/
-
-static void __devinit
-e1000_check_fiber_options(struct e1000_adapter *adapter)
+static void __devinit e1000_check_fiber_options(struct e1000_adapter *adapter)
 {
 	int bd = adapter->bd_number;
+#ifndef module_param_array
 	bd = bd > E1000_MAX_NIC ? E1000_MAX_NIC : bd;
-
-	if((Speed[bd] != OPTION_UNSET)) {
+	if ((Speed[bd] != OPTION_UNSET)) {
+#else
+	if (num_Speed > bd) {
+#endif
 		DPRINTK(PROBE, INFO, "Speed not valid for fiber adapters, "
 		       "parameter ignored\n");
 	}
-	if((Duplex[bd] != OPTION_UNSET)) {
+
+#ifndef module_param_array
+	if ((Duplex[bd] != OPTION_UNSET)) {
+#else
+	if (num_Duplex > bd) {
+#endif
 		DPRINTK(PROBE, INFO, "Duplex not valid for fiber adapters, "
 		       "parameter ignored\n");
 	}
-	if((AutoNeg[bd] != OPTION_UNSET) && (AutoNeg[bd] != 0x20)) {
+
+#ifndef module_param_array
+	if ((AutoNeg[bd] != OPTION_UNSET) && (AutoNeg[bd] != 0x20)) {
+#else
+	if ((num_AutoNeg > bd) && (AutoNeg[bd] != 0x20)) {
+#endif
 		DPRINTK(PROBE, INFO, "AutoNeg other than 1000/Full is "
 				 "not valid for fiber adapters, "
 				 "parameter ignored\n");
@@ -504,13 +664,14 @@
  *
  * Handles speed and duplex options on copper adapters
  **/
-
-static void __devinit
-e1000_check_copper_options(struct e1000_adapter *adapter)
+static void __devinit e1000_check_copper_options(struct e1000_adapter *adapter)
 {
-	int speed, dplx;
+	struct e1000_hw *hw = &adapter->hw;
+	int speed, dplx, an;
 	int bd = adapter->bd_number;
+#ifndef module_param_array
 	bd = bd > E1000_MAX_NIC ? E1000_MAX_NIC : bd;
+#endif
 
 	{ /* Speed */
 		struct e1000_opt_list speed_list[] = {{          0, "" },
@@ -527,8 +688,16 @@
 					 .p = speed_list }}
 		};
 
-		speed = Speed[bd];
-		e1000_validate_option(&speed, &opt, adapter);
+#ifdef module_param_array
+		if (num_Speed > bd) {
+#endif
+			speed = Speed[bd];
+			e1000_validate_option(&speed, &opt, adapter);
+#ifdef module_param_array
+		} else {
+			speed = opt.def;
+		}
+#endif
 	}
 	{ /* Duplex */
 		struct e1000_opt_list dplx_list[] = {{           0, "" },
@@ -544,15 +713,33 @@
 					 .p = dplx_list }}
 		};
 
-		dplx = Duplex[bd];
-		e1000_validate_option(&dplx, &opt, adapter);
+		if (e1000_check_reset_block(hw)) {
+			DPRINTK(PROBE, INFO,
+				"Link active due to SoL/IDER Session. "
+			        "Speed/Duplex/AutoNeg parameter ignored.\n");
+			return;
+		}
+#ifdef module_param_array
+		if (num_Duplex > bd) {
+#endif
+			dplx = Duplex[bd];
+			e1000_validate_option(&dplx, &opt, adapter);
+#ifdef module_param_array
+		} else {
+			dplx = opt.def;
+		}
+#endif
 	}
 
-	if(AutoNeg[bd] != OPTION_UNSET && (speed != 0 || dplx != 0)) {
+#ifdef module_param_array
+	if ((num_AutoNeg > bd) && (speed != 0 || dplx != 0)) {
+#else
+	if (AutoNeg[bd] != OPTION_UNSET && (speed != 0 || dplx != 0)) {
+#endif
 		DPRINTK(PROBE, INFO,
 		       "AutoNeg specified along with Speed or Duplex, "
 		       "parameter ignored\n");
-		adapter->hw.autoneg_advertised = AUTONEG_ADV_DEFAULT;
+		hw->phy.autoneg_advertised = AUTONEG_ADV_DEFAULT;
 	} else { /* Autoneg */
 		struct e1000_opt_list an_list[] =
 			#define AA "AutoNeg advertising "
@@ -597,15 +784,27 @@
 					 .p = an_list }}
 		};
 
-		int an = AutoNeg[bd];
-		e1000_validate_option(&an, &opt, adapter);
-		adapter->hw.autoneg_advertised = an;
+#ifdef module_param_array
+		if (num_AutoNeg > bd) {
+#endif
+			an = AutoNeg[bd];
+			e1000_validate_option(&an, &opt, adapter);
+#ifdef module_param_array
+		} else {
+			an = opt.def;
+		}
+#endif
+		hw->phy.autoneg_advertised = an;
 	}
 
 	switch (speed + dplx) {
 	case 0:
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		if(Speed[bd] != OPTION_UNSET || Duplex[bd] != OPTION_UNSET)
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+#ifdef module_param_array
+		if ((num_Speed > bd) && (speed != 0 || dplx != 0))
+#else
+		if (Speed[bd] != OPTION_UNSET || Duplex[bd] != OPTION_UNSET)
+#endif
 			DPRINTK(PROBE, INFO,
 			       "Speed and duplex autonegotiation enabled\n");
 		break;
@@ -613,83 +812,74 @@
 		DPRINTK(PROBE, INFO, "Half Duplex specified without Speed\n");
 		DPRINTK(PROBE, INFO, "Using Autonegotiation at "
 			"Half Duplex only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_10_HALF |
-		                                 ADVERTISE_100_HALF;
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+		hw->phy.autoneg_advertised = ADVERTISE_10_HALF |
+		                             ADVERTISE_100_HALF;
 		break;
 	case FULL_DUPLEX:
 		DPRINTK(PROBE, INFO, "Full Duplex specified without Speed\n");
 		DPRINTK(PROBE, INFO, "Using Autonegotiation at "
 			"Full Duplex only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_10_FULL |
-		                                 ADVERTISE_100_FULL |
-		                                 ADVERTISE_1000_FULL;
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+		hw->phy.autoneg_advertised = ADVERTISE_10_FULL |
+		                             ADVERTISE_100_FULL |
+		                             ADVERTISE_1000_FULL;
 		break;
 	case SPEED_10:
 		DPRINTK(PROBE, INFO, "10 Mbps Speed specified "
 			"without Duplex\n");
 		DPRINTK(PROBE, INFO, "Using Autonegotiation at 10 Mbps only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_10_HALF |
-		                                 ADVERTISE_10_FULL;
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+		hw->phy.autoneg_advertised = ADVERTISE_10_HALF |
+		                             ADVERTISE_10_FULL;
 		break;
 	case SPEED_10 + HALF_DUPLEX:
 		DPRINTK(PROBE, INFO, "Forcing to 10 Mbps Half Duplex\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 0;
-		adapter->hw.forced_speed_duplex = e1000_10_half;
-		adapter->hw.autoneg_advertised = 0;
+		hw->mac.autoneg = adapter->fc_autoneg = 0;
+		hw->mac.forced_speed_duplex = ADVERTISE_10_HALF;
+		hw->phy.autoneg_advertised = 0;
 		break;
 	case SPEED_10 + FULL_DUPLEX:
 		DPRINTK(PROBE, INFO, "Forcing to 10 Mbps Full Duplex\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 0;
-		adapter->hw.forced_speed_duplex = e1000_10_full;
-		adapter->hw.autoneg_advertised = 0;
+		hw->mac.autoneg = adapter->fc_autoneg = 0;
+		hw->mac.forced_speed_duplex = ADVERTISE_10_FULL;
+		hw->phy.autoneg_advertised = 0;
 		break;
 	case SPEED_100:
 		DPRINTK(PROBE, INFO, "100 Mbps Speed specified "
 			"without Duplex\n");
 		DPRINTK(PROBE, INFO, "Using Autonegotiation at "
 			"100 Mbps only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_100_HALF |
-		                                 ADVERTISE_100_FULL;
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+		hw->phy.autoneg_advertised = ADVERTISE_100_HALF |
+		                             ADVERTISE_100_FULL;
 		break;
 	case SPEED_100 + HALF_DUPLEX:
 		DPRINTK(PROBE, INFO, "Forcing to 100 Mbps Half Duplex\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 0;
-		adapter->hw.forced_speed_duplex = e1000_100_half;
-		adapter->hw.autoneg_advertised = 0;
+		hw->mac.autoneg = adapter->fc_autoneg = 0;
+		hw->mac.forced_speed_duplex = ADVERTISE_100_HALF;
+		hw->phy.autoneg_advertised = 0;
 		break;
 	case SPEED_100 + FULL_DUPLEX:
 		DPRINTK(PROBE, INFO, "Forcing to 100 Mbps Full Duplex\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 0;
-		adapter->hw.forced_speed_duplex = e1000_100_full;
-		adapter->hw.autoneg_advertised = 0;
+		hw->mac.autoneg = adapter->fc_autoneg = 0;
+		hw->mac.forced_speed_duplex = ADVERTISE_100_FULL;
+		hw->phy.autoneg_advertised = 0;
 		break;
 	case SPEED_1000:
 		DPRINTK(PROBE, INFO, "1000 Mbps Speed specified without "
 			"Duplex\n");
-		DPRINTK(PROBE, INFO,
-			"Using Autonegotiation at 1000 Mbps "
-			"Full Duplex only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_1000_FULL;
-		break;
+		goto full_duplex_only;
 	case SPEED_1000 + HALF_DUPLEX:
 		DPRINTK(PROBE, INFO,
 			"Half Duplex is not supported at 1000 Mbps\n");
-		DPRINTK(PROBE, INFO,
-			"Using Autonegotiation at 1000 Mbps "
-			"Full Duplex only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_1000_FULL;
-		break;
+		/* fall through */
 	case SPEED_1000 + FULL_DUPLEX:
+full_duplex_only:
 		DPRINTK(PROBE, INFO,
 		       "Using Autonegotiation at 1000 Mbps Full Duplex only\n");
-		adapter->hw.autoneg = adapter->fc_autoneg = 1;
-		adapter->hw.autoneg_advertised = ADVERTISE_1000_FULL;
+		hw->mac.autoneg = adapter->fc_autoneg = 1;
+		hw->phy.autoneg_advertised = ADVERTISE_1000_FULL;
 		break;
 	default:
 		BUG();
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_phy.c linux-2.6.9/drivers/net/e1000/e1000_phy.c
--- linux-2.6.9.src/drivers/net/e1000/e1000_phy.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_phy.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,2017 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "e1000_api.h"
+#include "e1000_phy.h"
+
+static s32  e1000_get_phy_cfg_done(struct e1000_hw *hw);
+static void e1000_release_phy(struct e1000_hw *hw);
+static s32  e1000_acquire_phy(struct e1000_hw *hw);
+
+/* Cable length tables */
+static const u16 e1000_m88_cable_length_table[] =
+	{ 0, 50, 80, 110, 140, 140, E1000_CABLE_LENGTH_UNDEFINED };
+#define M88E1000_CABLE_LENGTH_TABLE_SIZE \
+                (sizeof(e1000_m88_cable_length_table) / \
+                 sizeof(e1000_m88_cable_length_table[0]))
+
+static const u16 e1000_igp_2_cable_length_table[] =
+    { 0, 0, 0, 0, 0, 0, 0, 0, 3, 5, 8, 11, 13, 16, 18, 21,
+      0, 0, 0, 3, 6, 10, 13, 16, 19, 23, 26, 29, 32, 35, 38, 41,
+      6, 10, 14, 18, 22, 26, 30, 33, 37, 41, 44, 48, 51, 54, 58, 61,
+      21, 26, 31, 35, 40, 44, 49, 53, 57, 61, 65, 68, 72, 75, 79, 82,
+      40, 45, 51, 56, 61, 66, 70, 75, 79, 83, 87, 91, 94, 98, 101, 104,
+      60, 66, 72, 77, 82, 87, 92, 96, 100, 104, 108, 111, 114, 117, 119, 121,
+      83, 89, 95, 100, 105, 109, 113, 116, 119, 122, 124,
+      104, 109, 114, 118, 121, 124};
+#define IGP02E1000_CABLE_LENGTH_TABLE_SIZE \
+                (sizeof(e1000_igp_2_cable_length_table) / \
+                 sizeof(e1000_igp_2_cable_length_table[0]))
+
+/**
+ *  e1000_check_reset_block_generic - Check if PHY reset is blocked
+ *  @hw: pointer to the HW structure
+ *
+ *  Read the PHY management control register and check whether a PHY reset
+ *  is blocked.  If a reset is not blocked return E1000_SUCCESS, otherwise
+ *  return E1000_BLK_PHY_RESET (12).
+ **/
+s32 e1000_check_reset_block_generic(struct e1000_hw *hw)
+{
+	u32 manc;
+
+	DEBUGFUNC("e1000_check_reset_block");
+
+	manc = E1000_READ_REG(hw, E1000_MANC);
+
+	return (manc & E1000_MANC_BLK_PHY_RST_ON_IDE) ?
+	       E1000_BLK_PHY_RESET : E1000_SUCCESS;
+}
+
+/**
+ *  e1000_get_phy_id - Retrieve the PHY ID and revision
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the PHY registers and stores the PHY ID and possibly the PHY
+ *  revision in the hardware structure.
+ **/
+s32 e1000_get_phy_id(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val = E1000_SUCCESS;
+	u16 phy_id;
+
+	DEBUGFUNC("e1000_get_phy_id");
+
+	ret_val = e1000_read_phy_reg(hw, PHY_ID1, &phy_id);
+	if (ret_val)
+		goto out;
+
+	phy->id = (u32)(phy_id << 16);
+	usec_delay(20);
+	ret_val = e1000_read_phy_reg(hw, PHY_ID2, &phy_id);
+	if (ret_val)
+		goto out;
+
+	phy->id |= (u32)(phy_id & PHY_REVISION_MASK);
+	phy->revision = (u32)(phy_id & ~PHY_REVISION_MASK);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_reset_dsp_generic - Reset PHY DSP
+ *  @hw: pointer to the HW structure
+ *
+ *  Reset the digital signal processor.
+ **/
+s32 e1000_phy_reset_dsp_generic(struct e1000_hw *hw)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_phy_reset_dsp_generic");
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0xC1);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_GEN_CONTROL, 0);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_phy_reg_mdic - Read MDI control register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be read
+ *  @data: pointer to the read data
+ *
+ *  Reads the MDI control regsiter in the PHY at offset and stores the
+ *  information read to data.
+ **/
+static s32 e1000_read_phy_reg_mdic(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	u32 i, mdic = 0;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_read_phy_reg_mdic");
+
+	if (offset > MAX_PHY_REG_ADDRESS) {
+		DEBUGOUT1("PHY Address %d is out of range\n", offset);
+		ret_val = -E1000_ERR_PARAM;
+		goto out;
+	}
+
+	/* Set up Op-code, Phy Address, and register offset in the MDI
+	 * Control register.  The MAC will take care of interfacing with the
+	 * PHY to retrieve the desired data.
+	 */
+	mdic = ((offset << E1000_MDIC_REG_SHIFT) |
+	        (phy->addr << E1000_MDIC_PHY_SHIFT) |
+	        (E1000_MDIC_OP_READ));
+
+	E1000_WRITE_REG(hw, E1000_MDIC, mdic);
+
+	/* Poll the ready bit to see if the MDI read completed */
+	for (i = 0; i < 64; i++) {
+		usec_delay(50);
+		mdic = E1000_READ_REG(hw, E1000_MDIC);
+		if (mdic & E1000_MDIC_READY)
+			break;
+	}
+	if (!(mdic & E1000_MDIC_READY)) {
+		DEBUGOUT("MDI Read did not complete\n");
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+	if (mdic & E1000_MDIC_ERROR) {
+		DEBUGOUT("MDI Error\n");
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+	*data = (u16) mdic;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_phy_reg_mdic - Write MDI control register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to write to
+ *  @data: data to write to register at offset
+ *
+ *  Writes data to MDI control register in the PHY at offset.
+ **/
+static s32 e1000_write_phy_reg_mdic(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	u32 i, mdic = 0;
+	s32 ret_val = E1000_SUCCESS;
+
+	DEBUGFUNC("e1000_write_phy_reg_mdic");
+
+	if (offset > MAX_PHY_REG_ADDRESS) {
+		DEBUGOUT1("PHY Address %d is out of range\n", offset);
+		ret_val = -E1000_ERR_PARAM;
+		goto out;
+	}
+
+	/* Set up Op-code, Phy Address, and register offset in the MDI
+	 * Control register.  The MAC will take care of interfacing with the
+	 * PHY to retrieve the desired data.
+	 */
+	mdic = (((u32)data) |
+	        (offset << E1000_MDIC_REG_SHIFT) |
+	        (phy->addr << E1000_MDIC_PHY_SHIFT) |
+	        (E1000_MDIC_OP_WRITE));
+
+	E1000_WRITE_REG(hw, E1000_MDIC, mdic);
+
+	/* Poll the ready bit to see if the MDI read completed */
+	for (i = 0; i < E1000_GEN_POLL_TIMEOUT; i++) {
+		usec_delay(5);
+		mdic = E1000_READ_REG(hw, E1000_MDIC);
+		if (mdic & E1000_MDIC_READY)
+			break;
+	}
+	if (!(mdic & E1000_MDIC_READY)) {
+		DEBUGOUT("MDI Write did not complete\n");
+		ret_val = -E1000_ERR_PHY;
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_phy_reg_m88 - Read m88 PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be read
+ *  @data: pointer to the read data
+ *
+ *  Acquires semaphore, if necessary, then reads the PHY register at offset
+ *  and storing the retrieved information in data.  Release any acquired
+ *  semaphores before exiting.
+ **/
+s32 e1000_read_phy_reg_m88(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_read_phy_reg_m88");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg_mdic(hw,
+	                                  MAX_PHY_REG_ADDRESS & offset,
+	                                  data);
+
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_phy_reg_m88 - Write m88 PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to write to
+ *  @data: data to write at register offset
+ *
+ *  Acquires semaphore, if necessary, then writes the data to PHY register
+ *  at the offset.  Release any acquired semaphores before exiting.
+ **/
+s32 e1000_write_phy_reg_m88(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_write_phy_reg_m88");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_write_phy_reg_mdic(hw,
+	                                   MAX_PHY_REG_ADDRESS & offset,
+	                                   data);
+
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_phy_reg_igp - Read igp PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be read
+ *  @data: pointer to the read data
+ *
+ *  Acquires semaphore, if necessary, then reads the PHY register at offset
+ *  and storing the retrieved information in data.  Release any acquired
+ *  semaphores before exiting.
+ **/
+s32 e1000_read_phy_reg_igp(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_read_phy_reg_igp");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	if (offset > MAX_PHY_MULTI_PAGE_REG) {
+		ret_val = e1000_write_phy_reg_mdic(hw,
+						   IGP01E1000_PHY_PAGE_SELECT,
+						   (u16)offset);
+		if (ret_val) {
+			e1000_release_phy(hw);
+			goto out;
+		}
+	}
+
+	ret_val = e1000_read_phy_reg_mdic(hw,
+					  MAX_PHY_REG_ADDRESS & offset,
+					  data);
+
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_phy_reg_igp - Write igp PHY register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to write to
+ *  @data: data to write at register offset
+ *
+ *  Acquires semaphore, if necessary, then writes the data to PHY register
+ *  at the offset.  Release any acquired semaphores before exiting.
+ **/
+s32 e1000_write_phy_reg_igp(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_write_phy_reg_igp");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	if (offset > MAX_PHY_MULTI_PAGE_REG) {
+		ret_val = e1000_write_phy_reg_mdic(hw,
+						   IGP01E1000_PHY_PAGE_SELECT,
+						   (u16)offset);
+		if (ret_val) {
+			e1000_release_phy(hw);
+			goto out;
+		}
+	}
+
+	ret_val = e1000_write_phy_reg_mdic(hw,
+					   MAX_PHY_REG_ADDRESS & offset,
+					   data);
+
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_read_kmrn_reg_generic - Read kumeran register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to be read
+ *  @data: pointer to the read data
+ *
+ *  Acquires semaphore, if necessary.  Then reads the PHY register at offset
+ *  using the kumeran interface.  The information retrieved is stored in data.
+ *  Release any acquired semaphores before exiting.
+ **/
+s32 e1000_read_kmrn_reg_generic(struct e1000_hw *hw, u32 offset, u16 *data)
+{
+	u32 kmrnctrlsta;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_read_kmrn_reg_generic");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	kmrnctrlsta = ((offset << E1000_KMRNCTRLSTA_OFFSET_SHIFT) &
+	               E1000_KMRNCTRLSTA_OFFSET) | E1000_KMRNCTRLSTA_REN;
+	E1000_WRITE_REG(hw, E1000_KMRNCTRLSTA, kmrnctrlsta);
+
+	usec_delay(2);
+
+	kmrnctrlsta = E1000_READ_REG(hw, E1000_KMRNCTRLSTA);
+	*data = (u16)kmrnctrlsta;
+
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_write_kmrn_reg_generic - Write kumeran register
+ *  @hw: pointer to the HW structure
+ *  @offset: register offset to write to
+ *  @data: data to write at register offset
+ *
+ *  Acquires semaphore, if necessary.  Then write the data to PHY register
+ *  at the offset using the kumeran interface.  Release any acquired semaphores
+ *  before exiting.
+ **/
+s32 e1000_write_kmrn_reg_generic(struct e1000_hw *hw, u32 offset, u16 data)
+{
+	u32 kmrnctrlsta;
+	s32 ret_val;
+
+	DEBUGFUNC("e1000_write_kmrn_reg_generic");
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	kmrnctrlsta = ((offset << E1000_KMRNCTRLSTA_OFFSET_SHIFT) &
+	               E1000_KMRNCTRLSTA_OFFSET) | data;
+	E1000_WRITE_REG(hw, E1000_KMRNCTRLSTA, kmrnctrlsta);
+
+	usec_delay(2);
+	e1000_release_phy(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_copper_link_setup_m88 - Setup m88 PHY's for copper link
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets up MDI/MDI-X and polarity for m88 PHY's.  If necessary, transmit clock
+ *  and downshift values are set also.
+ **/
+s32 e1000_copper_link_setup_m88(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data;
+
+	DEBUGFUNC("e1000_copper_link_setup_m88");
+
+	if (phy->reset_disable) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	/* Enable CRS on TX. This must be set for half-duplex operation. */
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data |= M88E1000_PSCR_ASSERT_CRS_ON_TX;
+
+	/* Options:
+	 *   MDI/MDI-X = 0 (default)
+	 *   0 - Auto for all speeds
+	 *   1 - MDI mode
+	 *   2 - MDI-X mode
+	 *   3 - Auto for 1000Base-T only (MDI-X for 10/100Base-T modes)
+	 */
+	phy_data &= ~M88E1000_PSCR_AUTO_X_MODE;
+
+	switch (phy->mdix) {
+		case 1:
+			phy_data |= M88E1000_PSCR_MDI_MANUAL_MODE;
+			break;
+		case 2:
+			phy_data |= M88E1000_PSCR_MDIX_MANUAL_MODE;
+			break;
+		case 3:
+			phy_data |= M88E1000_PSCR_AUTO_X_1000T;
+			break;
+		case 0:
+		default:
+			phy_data |= M88E1000_PSCR_AUTO_X_MODE;
+			break;
+	}
+
+	/* Options:
+	 *   disable_polarity_correction = 0 (default)
+	 *       Automatic Correction for Reversed Cable Polarity
+	 *   0 - Disabled
+	 *   1 - Enabled
+	 */
+	phy_data &= ~M88E1000_PSCR_POLARITY_REVERSAL;
+	if (phy->disable_polarity_correction == 1)
+		phy_data |= M88E1000_PSCR_POLARITY_REVERSAL;
+
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
+	if (ret_val)
+		goto out;
+
+	if (phy->revision < E1000_REVISION_4) {
+		/* Force TX_CLK in the Extended PHY Specific Control Register
+		 * to 25MHz clock.
+		 */
+		ret_val = e1000_read_phy_reg(hw,
+		                             M88E1000_EXT_PHY_SPEC_CTRL,
+		                             &phy_data);
+		if (ret_val)
+			goto out;
+
+		phy_data |= M88E1000_EPSCR_TX_CLK_25;
+
+		if ((phy->revision == E1000_REVISION_2) &&
+		    (phy->id == M88E1111_I_PHY_ID)) {
+			/* 82573L PHY - set the downshift counter to 5x. */
+			phy_data &= ~M88EC018_EPSCR_DOWNSHIFT_COUNTER_MASK;
+			phy_data |= M88EC018_EPSCR_DOWNSHIFT_COUNTER_5X;
+		} else {
+			/* Configure Master and Slave downshift values */
+			phy_data &= ~(M88E1000_EPSCR_MASTER_DOWNSHIFT_MASK |
+				      M88E1000_EPSCR_SLAVE_DOWNSHIFT_MASK);
+			phy_data |= (M88E1000_EPSCR_MASTER_DOWNSHIFT_1X |
+				     M88E1000_EPSCR_SLAVE_DOWNSHIFT_1X);
+		}
+		ret_val = e1000_write_phy_reg(hw,
+		                             M88E1000_EXT_PHY_SPEC_CTRL,
+		                             phy_data);
+		if (ret_val)
+			goto out;
+	}
+
+	/* Commit the changes. */
+	ret_val = e1000_phy_commit(hw);
+	if (ret_val) {
+		DEBUGOUT("Error committing the PHY changes\n");
+		goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_copper_link_setup_igp - Setup igp PHY's for copper link
+ *  @hw: pointer to the HW structure
+ *
+ *  Sets up LPLU, MDI/MDI-X, polarity, Smartspeed and Master/Slave config for
+ *  igp PHY's.
+ **/
+s32 e1000_copper_link_setup_igp(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_copper_link_setup_igp");
+
+	if (phy->reset_disable) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	ret_val = e1000_phy_hw_reset(hw);
+	if (ret_val) {
+		DEBUGOUT("Error resetting the PHY.\n");
+		goto out;
+	}
+
+	/* Wait 15ms for MAC to configure PHY from NVM settings. */
+	msec_delay(15);
+
+	/* The NVM settings will configure LPLU in D3 for
+	 * non-IGP1 PHYs. */
+	if (phy->type == e1000_phy_igp) {
+		/* disable lplu d3 during driver init */
+		ret_val = e1000_set_d3_lplu_state(hw, FALSE);
+		if (ret_val) {
+			DEBUGOUT("Error Disabling LPLU D3\n");
+			goto out;
+		}
+	}
+
+	/* disable lplu d0 during driver init */
+	ret_val = e1000_set_d0_lplu_state(hw, FALSE);
+	if (ret_val) {
+		DEBUGOUT("Error Disabling LPLU D0\n");
+		goto out;
+	}
+	/* Configure mdi-mdix settings */
+	ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, &data);
+	if (ret_val)
+		goto out;
+
+	data &= ~IGP01E1000_PSCR_AUTO_MDIX;
+
+	switch (phy->mdix) {
+	case 1:
+		data &= ~IGP01E1000_PSCR_FORCE_MDI_MDIX;
+		break;
+	case 2:
+		data |= IGP01E1000_PSCR_FORCE_MDI_MDIX;
+		break;
+	case 0:
+	default:
+		data |= IGP01E1000_PSCR_AUTO_MDIX;
+		break;
+	}
+	ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, data);
+	if (ret_val)
+		goto out;
+
+	/* set auto-master slave resolution settings */
+	if (hw->mac.autoneg) {
+		/* when autonegotiation advertisement is only 1000Mbps then we
+		 * should disable SmartSpeed and enable Auto MasterSlave
+		 * resolution as hardware default. */
+		if (phy->autoneg_advertised == ADVERTISE_1000_FULL) {
+			/* Disable SmartSpeed */
+			ret_val = e1000_read_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+
+			/* Set auto Master/Slave resolution process */
+			ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~CR_1000T_MS_ENABLE;
+			ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, data);
+			if (ret_val)
+				goto out;
+		}
+
+		ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &data);
+		if (ret_val)
+			goto out;
+
+		/* load defaults for future use */
+		phy->original_ms_type = (data & CR_1000T_MS_ENABLE) ?
+			((data & CR_1000T_MS_VALUE) ?
+			e1000_ms_force_master :
+			e1000_ms_force_slave) :
+			e1000_ms_auto;
+
+		switch (phy->ms_type) {
+		case e1000_ms_force_master:
+			data |= (CR_1000T_MS_ENABLE | CR_1000T_MS_VALUE);
+			break;
+		case e1000_ms_force_slave:
+			data |= CR_1000T_MS_ENABLE;
+			data &= ~(CR_1000T_MS_VALUE);
+			break;
+		case e1000_ms_auto:
+			data &= ~CR_1000T_MS_ENABLE;
+		default:
+			break;
+		}
+		ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL, data);
+		if (ret_val)
+			goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_copper_link_autoneg - Setup/Enable autoneg for copper link
+ *  @hw: pointer to the HW structure
+ *
+ *  Performs initial bounds checking on autoneg advertisement parameter, then
+ *  configure to advertise the full capability.  Setup the PHY to autoneg
+ *  and restart the negotiation process between the link partner.  If
+ *  wait_for_link, then wait for autoneg to complete before exiting.
+ **/
+s32 e1000_copper_link_autoneg(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_ctrl;
+
+	DEBUGFUNC("e1000_copper_link_autoneg");
+
+	/* Perform some bounds checking on the autoneg advertisement
+	 * parameter.
+	 */
+	phy->autoneg_advertised &= phy->autoneg_mask;
+
+	/* If autoneg_advertised is zero, we assume it was not defaulted
+	 * by the calling code so we set to advertise full capability.
+	 */
+	if (phy->autoneg_advertised == 0)
+		phy->autoneg_advertised = phy->autoneg_mask;
+
+	DEBUGOUT("Reconfiguring auto-neg advertisement params\n");
+	ret_val = e1000_phy_setup_autoneg(hw);
+	if (ret_val) {
+		DEBUGOUT("Error Setting up Auto-Negotiation\n");
+		goto out;
+	}
+	DEBUGOUT("Restarting Auto-Neg\n");
+
+	/* Restart auto-negotiation by setting the Auto Neg Enable bit and
+	 * the Auto Neg Restart bit in the PHY control register.
+	 */
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &phy_ctrl);
+	if (ret_val)
+		goto out;
+
+	phy_ctrl |= (MII_CR_AUTO_NEG_EN | MII_CR_RESTART_AUTO_NEG);
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, phy_ctrl);
+	if (ret_val)
+		goto out;
+
+	/* Does the user want to wait for Auto-Neg to complete here, or
+	 * check at a later time (for example, callback routine).
+	 */
+	if (phy->wait_for_link) {
+		ret_val = e1000_wait_autoneg(hw);
+		if (ret_val) {
+			DEBUGOUT("Error while waiting for "
+			         "autoneg to complete\n");
+			goto out;
+		}
+	}
+
+	hw->mac.get_link_status = TRUE;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_setup_autoneg - Configure PHY for auto-negotiation
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the MII auto-neg advertisement register and/or the 1000T control
+ *  register and if the PHY is already setup for auto-negotiation, then
+ *  return successful.  Otherwise, setup advertisement and flow control to
+ *  the appropriate values for the wanted auto-negotiation.
+ **/
+s32 e1000_phy_setup_autoneg(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 mii_autoneg_adv_reg;
+	u16 mii_1000t_ctrl_reg = 0;
+
+	DEBUGFUNC("e1000_phy_setup_autoneg");
+
+	phy->autoneg_advertised &= phy->autoneg_mask;
+
+	/* Read the MII Auto-Neg Advertisement Register (Address 4). */
+	ret_val = e1000_read_phy_reg(hw, PHY_AUTONEG_ADV, &mii_autoneg_adv_reg);
+	if (ret_val)
+		goto out;
+
+	if (phy->autoneg_mask & ADVERTISE_1000_FULL) {
+		/* Read the MII 1000Base-T Control Register (Address 9). */
+		ret_val = e1000_read_phy_reg(hw,
+		                            PHY_1000T_CTRL,
+		                            &mii_1000t_ctrl_reg);
+		if (ret_val)
+			goto out;
+	}
+
+	/* Need to parse both autoneg_advertised and fc and set up
+	 * the appropriate PHY registers.  First we will parse for
+	 * autoneg_advertised software override.  Since we can advertise
+	 * a plethora of combinations, we need to check each bit
+	 * individually.
+	 */
+
+	/* First we clear all the 10/100 mb speed bits in the Auto-Neg
+	 * Advertisement Register (Address 4) and the 1000 mb speed bits in
+	 * the  1000Base-T Control Register (Address 9).
+	 */
+	mii_autoneg_adv_reg &= ~(NWAY_AR_100TX_FD_CAPS |
+	                         NWAY_AR_100TX_HD_CAPS |
+	                         NWAY_AR_10T_FD_CAPS   |
+	                         NWAY_AR_10T_HD_CAPS);
+	mii_1000t_ctrl_reg &= ~(CR_1000T_HD_CAPS | CR_1000T_FD_CAPS);
+
+	DEBUGOUT1("autoneg_advertised %x\n", phy->autoneg_advertised);
+
+	/* Do we want to advertise 10 Mb Half Duplex? */
+	if (phy->autoneg_advertised & ADVERTISE_10_HALF) {
+		DEBUGOUT("Advertise 10mb Half duplex\n");
+		mii_autoneg_adv_reg |= NWAY_AR_10T_HD_CAPS;
+	}
+
+	/* Do we want to advertise 10 Mb Full Duplex? */
+	if (phy->autoneg_advertised & ADVERTISE_10_FULL) {
+		DEBUGOUT("Advertise 10mb Full duplex\n");
+		mii_autoneg_adv_reg |= NWAY_AR_10T_FD_CAPS;
+	}
+
+	/* Do we want to advertise 100 Mb Half Duplex? */
+	if (phy->autoneg_advertised & ADVERTISE_100_HALF) {
+		DEBUGOUT("Advertise 100mb Half duplex\n");
+		mii_autoneg_adv_reg |= NWAY_AR_100TX_HD_CAPS;
+	}
+
+	/* Do we want to advertise 100 Mb Full Duplex? */
+	if (phy->autoneg_advertised & ADVERTISE_100_FULL) {
+		DEBUGOUT("Advertise 100mb Full duplex\n");
+		mii_autoneg_adv_reg |= NWAY_AR_100TX_FD_CAPS;
+	}
+
+	/* We do not allow the Phy to advertise 1000 Mb Half Duplex */
+	if (phy->autoneg_advertised & ADVERTISE_1000_HALF) {
+		DEBUGOUT("Advertise 1000mb Half duplex request denied!\n");
+	}
+
+	/* Do we want to advertise 1000 Mb Full Duplex? */
+	if (phy->autoneg_advertised & ADVERTISE_1000_FULL) {
+		DEBUGOUT("Advertise 1000mb Full duplex\n");
+		mii_1000t_ctrl_reg |= CR_1000T_FD_CAPS;
+	}
+
+	/* Check for a software override of the flow control settings, and
+	 * setup the PHY advertisement registers accordingly.  If
+	 * auto-negotiation is enabled, then software will have to set the
+	 * "PAUSE" bits to the correct value in the Auto-Negotiation
+	 * Advertisement Register (PHY_AUTONEG_ADV) and re-start auto-
+	 * negotiation.
+	 *
+	 * The possible values of the "fc" parameter are:
+	 *      0:  Flow control is completely disabled
+	 *      1:  Rx flow control is enabled (we can receive pause frames
+	 *          but not send pause frames).
+	 *      2:  Tx flow control is enabled (we can send pause frames
+	 *          but we do not support receiving pause frames).
+	 *      3:  Both Rx and TX flow control (symmetric) are enabled.
+	 *  other:  No software override.  The flow control configuration
+	 *          in the EEPROM is used.
+	 */
+	switch (hw->mac.fc) {
+	case e1000_fc_none:
+		/* Flow control (RX & TX) is completely disabled by a
+		 * software over-ride.
+		 */
+		mii_autoneg_adv_reg &= ~(NWAY_AR_ASM_DIR | NWAY_AR_PAUSE);
+		break;
+	case e1000_fc_rx_pause:
+		/* RX Flow control is enabled, and TX Flow control is
+		 * disabled, by a software over-ride.
+		 */
+		/* Since there really isn't a way to advertise that we are
+		 * capable of RX Pause ONLY, we will advertise that we
+		 * support both symmetric and asymmetric RX PAUSE.  Later
+		 * (in e1000_config_fc_after_link_up) we will disable the
+		 * hw's ability to send PAUSE frames.
+		 */
+		mii_autoneg_adv_reg |= (NWAY_AR_ASM_DIR | NWAY_AR_PAUSE);
+		break;
+	case e1000_fc_tx_pause:
+		/* TX Flow control is enabled, and RX Flow control is
+		 * disabled, by a software over-ride.
+		 */
+		mii_autoneg_adv_reg |= NWAY_AR_ASM_DIR;
+		mii_autoneg_adv_reg &= ~NWAY_AR_PAUSE;
+		break;
+	case e1000_fc_full:
+		/* Flow control (both RX and TX) is enabled by a software
+		 * over-ride.
+		 */
+		mii_autoneg_adv_reg |= (NWAY_AR_ASM_DIR | NWAY_AR_PAUSE);
+		break;
+	default:
+		DEBUGOUT("Flow control param set incorrectly\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	ret_val = e1000_write_phy_reg(hw, PHY_AUTONEG_ADV, mii_autoneg_adv_reg);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT1("Auto-Neg Advertising %x\n", mii_autoneg_adv_reg);
+
+	if (phy->autoneg_mask & ADVERTISE_1000_FULL) {
+		ret_val = e1000_write_phy_reg(hw,
+		                              PHY_1000T_CTRL,
+		                              mii_1000t_ctrl_reg);
+		if (ret_val)
+			goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_setup_copper_link_generic - Configure copper link settings
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the appropriate function to configure the link for auto-neg or forced
+ *  speed and duplex.  Then we check for link, once link is established calls
+ *  to configure collision distance and flow control are called.  If link is
+ *  not established, we return -E1000_ERR_PHY (-2).
+ **/
+s32 e1000_setup_copper_link_generic(struct e1000_hw *hw)
+{
+	s32 ret_val;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_setup_copper_link_generic");
+
+	if (hw->mac.autoneg) {
+		/* Setup autoneg and flow control advertisement and perform
+		 * autonegotiation. */
+		ret_val = e1000_copper_link_autoneg(hw);
+		if (ret_val)
+			goto out;
+	} else {
+		/* PHY will be set to 10H, 10F, 100H or 100F
+		 * depending on user settings. */
+		DEBUGOUT("Forcing Speed and Duplex\n");
+		ret_val = e1000_phy_force_speed_duplex(hw);
+		if (ret_val) {
+			DEBUGOUT("Error Forcing Speed and Duplex\n");
+			goto out;
+		}
+	}
+
+	/* Check link status. Wait up to 100 microseconds for link to become
+	 * valid.
+	 */
+	ret_val = e1000_phy_has_link_generic(hw,
+	                                     COPPER_LINK_UP_LIMIT,
+	                                     10,
+	                                     &link);
+	if (ret_val)
+		goto out;
+
+	if (link) {
+		DEBUGOUT("Valid link established!!!\n");
+		e1000_config_collision_dist_generic(hw);
+		ret_val = e1000_config_fc_after_link_up_generic(hw);
+	} else {
+		DEBUGOUT("Unable to establish link!!!\n");
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_igp - Force speed/duplex for igp PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the PHY setup function to force speed and duplex.  Clears the
+ *  auto-crossover to force MDI manually.  Waits for link and returns
+ *  successful if link up is successful, else -E1000_ERR_PHY (-2).
+ **/
+s32 e1000_phy_force_speed_duplex_igp(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_igp");
+
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	e1000_phy_force_speed_duplex_setup(hw, &phy_data);
+
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, phy_data);
+	if (ret_val)
+		goto out;
+
+	/* Clear Auto-Crossover to force MDI manually.  IGP requires MDI
+	 * forced whenever speed and duplex are forced.
+	 */
+	ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data &= ~IGP01E1000_PSCR_AUTO_MDIX;
+	phy_data &= ~IGP01E1000_PSCR_FORCE_MDI_MDIX;
+
+	ret_val = e1000_write_phy_reg(hw, IGP01E1000_PHY_PORT_CTRL, phy_data);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT1("IGP PSCR: %X\n", phy_data);
+
+	usec_delay(1);
+
+	if (phy->wait_for_link) {
+		DEBUGOUT("Waiting for forced speed/duplex link on IGP phy.\n");
+
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+
+		if (!link) {
+			DEBUGOUT("Link taking longer than expected.\n");
+		}
+
+		/* Try once more */
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_m88 - Force speed/duplex for m88 PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Calls the PHY setup function to force speed and duplex.  Clears the
+ *  auto-crossover to force MDI manually.  Resets the PHY to commit the
+ *  changes.  If time expires while waiting for link up, we reset the DSP.
+ *  After reset, TX_CLK and CRS on TX must be set.  Return successful upon
+ *  successful completion, else return corresponding error code.
+ **/
+s32 e1000_phy_force_speed_duplex_m88(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_m88");
+
+	/* Clear Auto-Crossover to force MDI manually.  M88E1000 requires MDI
+	 * forced whenever speed and duplex are forced.
+	 */
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data &= ~M88E1000_PSCR_AUTO_X_MODE;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
+	if (ret_val)
+		goto out;
+
+	DEBUGOUT1("M88E1000 PSCR: %X\n", phy_data);
+
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	e1000_phy_force_speed_duplex_setup(hw, &phy_data);
+
+	/* Reset the phy to commit changes. */
+	phy_data |= MII_CR_RESET;
+
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, phy_data);
+	if (ret_val)
+		goto out;
+
+	usec_delay(1);
+
+	if (phy->wait_for_link) {
+		DEBUGOUT("Waiting for forced speed/duplex link on M88 phy.\n");
+
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+
+		if (!link) {
+			/* We didn't get link.
+			 * Reset the DSP and cross our fingers.
+			 */
+			ret_val = e1000_write_phy_reg(hw,
+			                              M88E1000_PHY_PAGE_SELECT,
+			                              0x001d);
+			if (ret_val)
+				goto out;
+			ret_val = e1000_phy_reset_dsp_generic(hw);
+			if (ret_val)
+				goto out;
+		}
+
+		/* Try once more */
+		ret_val = e1000_phy_has_link_generic(hw,
+		                                     PHY_FORCE_LIMIT,
+		                                     100000,
+		                                     &link);
+		if (ret_val)
+			goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	/* Resetting the phy means we need to re-force TX_CLK in the
+	 * Extended PHY Specific Control Register to 25MHz clock from
+	 * the reset value of 2.5MHz.
+	 */
+	phy_data |= M88E1000_EPSCR_TX_CLK_25;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_EXT_PHY_SPEC_CTRL, phy_data);
+	if (ret_val)
+		goto out;
+
+	/* In addition, we must re-enable CRS on Tx for both half and full
+	 * duplex.
+	 */
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy_data |= M88E1000_PSCR_ASSERT_CRS_ON_TX;
+	ret_val = e1000_write_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, phy_data);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex_setup - Configure forced PHY speed/duplex
+ *  @hw: pointer to the HW structure
+ *  @phy_ctrl: pointer to current value of PHY_CONTROL
+ *
+ *  Forces speed and duplex on the PHY by doing the following: disable flow
+ *  control, force speed/duplex on the MAC, disable auto speed detection,
+ *  disable auto-negotiation, configure duplex, configure speed, configure
+ *  the collision distance, write configuration to CTRL register.  The
+ *  caller must write to the PHY_CONTROL register for these settings to
+ *  take affect.
+ **/
+void e1000_phy_force_speed_duplex_setup(struct e1000_hw *hw, u16 *phy_ctrl)
+{
+	struct e1000_mac_info *mac = &hw->mac;
+	u32 ctrl;
+
+	DEBUGFUNC("e1000_phy_force_speed_duplex_setup");
+
+	/* Turn off flow control when forcing speed/duplex */
+	mac->fc = e1000_fc_none;
+
+	/* Force speed/duplex on the mac */
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	ctrl |= (E1000_CTRL_FRCSPD | E1000_CTRL_FRCDPX);
+	ctrl &= ~E1000_CTRL_SPD_SEL;
+
+	/* Disable Auto Speed Detection */
+	ctrl &= ~E1000_CTRL_ASDE;
+
+	/* Disable autoneg on the phy */
+	*phy_ctrl &= ~MII_CR_AUTO_NEG_EN;
+
+	/* Forcing Full or Half Duplex? */
+	if (mac->forced_speed_duplex & E1000_ALL_HALF_DUPLEX) {
+		ctrl &= ~E1000_CTRL_FD;
+		*phy_ctrl &= ~MII_CR_FULL_DUPLEX;
+		DEBUGOUT("Half Duplex\n");
+	} else {
+		ctrl |= E1000_CTRL_FD;
+		*phy_ctrl |= MII_CR_FULL_DUPLEX;
+		DEBUGOUT("Full Duplex\n");
+	}
+
+	/* Forcing 10mb or 100mb? */
+	if (mac->forced_speed_duplex & E1000_ALL_100_SPEED) {
+		ctrl |= E1000_CTRL_SPD_100;
+		*phy_ctrl |= MII_CR_SPEED_100;
+		*phy_ctrl &= ~(MII_CR_SPEED_1000 | MII_CR_SPEED_10);
+		DEBUGOUT("Forcing 100mb\n");
+	} else {
+		ctrl &= ~(E1000_CTRL_SPD_1000 | E1000_CTRL_SPD_100);
+		*phy_ctrl |= MII_CR_SPEED_10;
+		*phy_ctrl &= ~(MII_CR_SPEED_1000 | MII_CR_SPEED_100);
+		DEBUGOUT("Forcing 10mb\n");
+	}
+
+	e1000_config_collision_dist_generic(hw);
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+}
+
+/**
+ *  e1000_set_d3_lplu_state_generic - Sets low power link up state for D3
+ *  @hw: pointer to the HW structure
+ *  @active: boolean used to enable/disable lplu
+ *
+ *  Success returns 0, Failure returns 1
+ *
+ *  The low power link up (lplu) state is set to the power management level D3
+ *  and SmartSpeed is disabled when active is true, else clear lplu for D3
+ *  and enable Smartspeed.  LPLU and Smartspeed are mutually exclusive.  LPLU
+ *  is used during Dx states where the power conservation is most important.
+ *  During driver activity, SmartSpeed should be enabled so performance is
+ *  maintained.
+ **/
+s32 e1000_set_d3_lplu_state_generic(struct e1000_hw *hw, boolean_t active)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_set_d3_lplu_state_generic");
+
+	ret_val = e1000_read_phy_reg(hw, IGP02E1000_PHY_POWER_MGMT, &data);
+	if (ret_val)
+		goto out;
+
+	if (!active) {
+		data &= ~IGP02E1000_PM_D3_LPLU;
+		ret_val = e1000_write_phy_reg(hw,
+		                             IGP02E1000_PHY_POWER_MGMT,
+		                             data);
+		if (ret_val)
+			goto out;
+		/* LPLU and SmartSpeed are mutually exclusive.  LPLU is used
+		 * during Dx states where the power conservation is most
+		 * important.  During driver activity we should enable
+		 * SmartSpeed, so performance is maintained. */
+		if (phy->smart_speed == e1000_smart_speed_on) {
+			ret_val = e1000_read_phy_reg(hw,
+			                            IGP01E1000_PHY_PORT_CONFIG,
+			                            &data);
+			if (ret_val)
+				goto out;
+
+			data |= IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		} else if (phy->smart_speed == e1000_smart_speed_off) {
+			ret_val = e1000_read_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             &data);
+			if (ret_val)
+				goto out;
+
+			data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+			ret_val = e1000_write_phy_reg(hw,
+			                             IGP01E1000_PHY_PORT_CONFIG,
+			                             data);
+			if (ret_val)
+				goto out;
+		}
+	} else if ((phy->autoneg_advertised == E1000_ALL_SPEED_DUPLEX) ||
+	           (phy->autoneg_advertised == E1000_ALL_NOT_GIG) ||
+	           (phy->autoneg_advertised == E1000_ALL_10_SPEED)) {
+		data |= IGP02E1000_PM_D3_LPLU;
+		ret_val = e1000_write_phy_reg(hw,
+		                              IGP02E1000_PHY_POWER_MGMT,
+		                              data);
+		if (ret_val)
+			goto out;
+
+		/* When LPLU is enabled, we should disable SmartSpeed */
+		ret_val = e1000_read_phy_reg(hw,
+		                             IGP01E1000_PHY_PORT_CONFIG,
+		                             &data);
+		if (ret_val)
+			goto out;
+
+		data &= ~IGP01E1000_PSCFR_SMART_SPEED;
+		ret_val = e1000_write_phy_reg(hw,
+		                              IGP01E1000_PHY_PORT_CONFIG,
+		                              data);
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_downshift_generic - Checks whether a downshift in speed occured
+ *  @hw: pointer to the HW structure
+ *
+ *  Success returns 0, Failure returns 1
+ *
+ *  A downshift is detected by querying the PHY link health.
+ **/
+s32 e1000_check_downshift_generic(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data, offset, mask;
+
+	DEBUGFUNC("e1000_check_downshift_generic");
+
+	switch (phy->type) {
+	case e1000_phy_m88:
+	case e1000_phy_gg82563:
+		offset	= M88E1000_PHY_SPEC_STATUS;
+		mask	= M88E1000_PSSR_DOWNSHIFT;
+		break;
+	case e1000_phy_igp_2:
+	case e1000_phy_igp:
+	case e1000_phy_igp_3:
+		offset	= IGP01E1000_PHY_LINK_HEALTH;
+		mask	= IGP01E1000_PLHR_SS_DOWNGRADE;
+		break;
+	default:
+		/* speed downshift not supported */
+		phy->speed_downgraded = FALSE;
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, offset, &phy_data);
+
+	if (!ret_val)
+		phy->speed_downgraded = (phy_data & mask) ? TRUE : FALSE;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_check_polarity_m88 - Checks the polarity.
+ *  @hw: pointer to the HW structure
+ *
+ *  Success returns 0, Failure returns -E1000_ERR_PHY (-2)
+ *
+ *  Polarity is determined based on the PHY specific status register.
+ **/
+s32 e1000_check_polarity_m88(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+
+	DEBUGFUNC("e1000_check_polarity_m88");
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &data);
+
+	if (!ret_val)
+		phy->cable_polarity = (data & M88E1000_PSSR_REV_POLARITY)
+		                      ? e1000_rev_polarity_reversed
+		                      : e1000_rev_polarity_normal;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_check_polarity_igp - Checks the polarity.
+ *  @hw: pointer to the HW structure
+ *
+ *  Success returns 0, Failure returns -E1000_ERR_PHY (-2)
+ *
+ *  Polarity is determined based on the PHY port status register, and the
+ *  current speed (since there is no polarity at 100Mbps).
+ **/
+s32 e1000_check_polarity_igp(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data, offset, mask;
+
+	DEBUGFUNC("e1000_check_polarity_igp");
+
+	/* Polarity is determined based on the speed of
+	 * our connection. */
+	ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS, &data);
+	if (ret_val)
+		goto out;
+
+	if ((data & IGP01E1000_PSSR_SPEED_MASK) ==
+	    IGP01E1000_PSSR_SPEED_1000MBPS) {
+		offset	= IGP01E1000_PHY_PCS_INIT_REG;
+		mask	= IGP01E1000_PHY_POLARITY_MASK;
+	} else {
+		/* This really only applies to 10Mbps since
+		 * there is no polarity for 100Mbps (always 0).
+		 */
+		offset	= IGP01E1000_PHY_PORT_STATUS;
+		mask	= IGP01E1000_PSSR_POLARITY_REVERSED;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, offset, &data);
+
+	if (!ret_val)
+		phy->cable_polarity = (data & mask)
+		                      ? e1000_rev_polarity_reversed
+		                      : e1000_rev_polarity_normal;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_wait_autoneg_generic - Wait for auto-neg compeletion
+ *  @hw: pointer to the HW structure
+ *
+ *  Waits for auto-negotiation to complete or for the auto-negotiation time
+ *  limit to expire, which ever happens first.
+ **/
+s32 e1000_wait_autoneg_generic(struct e1000_hw *hw)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 i, phy_status;
+
+	DEBUGFUNC("e1000_wait_autoneg_generic");
+
+	/* Break after autoneg completes or PHY_AUTO_NEG_LIMIT expires. */
+	for (i = PHY_AUTO_NEG_LIMIT; i > 0; i--) {
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_status);
+		if (ret_val)
+			break;
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_status);
+		if (ret_val)
+			break;
+		if (phy_status & MII_SR_AUTONEG_COMPLETE)
+			break;
+		msec_delay(100);
+	}
+
+	/* PHY_AUTO_NEG_TIME expiration doesn't guarantee auto-negotiation
+	 * has completed.
+	 */
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_has_link_generic - Polls PHY for link
+ *  @hw: pointer to the HW structure
+ *  @iterations: number of times to poll for link
+ *  @usec_interval: delay between polling attempts
+ *  @success: pointer to whether polling was successful or not
+ *
+ *  Polls the PHY status register for link, 'iterations' number of times.
+ **/
+s32 e1000_phy_has_link_generic(struct e1000_hw *hw, u32 iterations,
+                               u32 usec_interval, boolean_t *success)
+{
+	s32 ret_val = E1000_SUCCESS;
+	u16 i, phy_status;
+
+	DEBUGFUNC("e1000_phy_has_link_generic");
+
+	for (i = 0; i < iterations; i++) {
+		/* Some PHYs require the PHY_STATUS register to be read
+		 * twice due to the link bit being sticky.  No harm doing
+		 * it across the board.
+		 */
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_status);
+		if (ret_val)
+			break;
+		ret_val = e1000_read_phy_reg(hw, PHY_STATUS, &phy_status);
+		if (ret_val)
+			break;
+		if (phy_status & MII_SR_LINK_STATUS)
+			break;
+		if (usec_interval >= 1000)
+			msec_delay_irq(usec_interval/1000);
+		else
+			usec_delay(usec_interval);
+	}
+
+	*success = (i < iterations) ? TRUE : FALSE;
+
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cable_length_m88 - Determine cable length for m88 PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Reads the PHY specific status register to retrieve the cable length
+ *  information.  The cable length is determined by averaging the minimum and
+ *  maximum values to get the "average" cable length.  The m88 PHY has four
+ *  possible cable length values, which are:
+ *	Register Value		Cable Length
+ *	0			< 50 meters
+ *	1			50 - 80 meters
+ *	2			80 - 110 meters
+ *	3			110 - 140 meters
+ *	4			> 140 meters
+ **/
+s32 e1000_get_cable_length_m88(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data, index;
+
+	DEBUGFUNC("e1000_get_cable_length_m88");
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
+	if (ret_val)
+		goto out;
+
+	index = (phy_data & M88E1000_PSSR_CABLE_LENGTH) >>
+	        M88E1000_PSSR_CABLE_LENGTH_SHIFT;
+	phy->min_cable_length = e1000_m88_cable_length_table[index];
+	phy->max_cable_length = e1000_m88_cable_length_table[index+1];
+
+	phy->cable_length = (phy->min_cable_length + phy->max_cable_length) / 2;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cable_length_igp_2 - Determine cable length for igp2 PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  The automatic gain control (agc) normalizes the amplitude of the
+ *  received signal, adjusting for the attenuation produced by the
+ *  cable.  By reading the AGC registers, which reperesent the
+ *  cobination of course and fine gain value, the value can be put
+ *  into a lookup table to obtain the approximate cable length
+ *  for each channel.
+ **/
+s32 e1000_get_cable_length_igp_2(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 phy_data, i, agc_value = 0;
+	u16 cur_agc_index, max_agc_index = 0;
+	u16 min_agc_index = IGP02E1000_CABLE_LENGTH_TABLE_SIZE - 1;
+	u16 agc_reg_array[IGP02E1000_PHY_CHANNEL_NUM] =
+	                                                 {IGP02E1000_PHY_AGC_A,
+	                                                  IGP02E1000_PHY_AGC_B,
+	                                                  IGP02E1000_PHY_AGC_C,
+	                                                  IGP02E1000_PHY_AGC_D};
+
+	DEBUGFUNC("e1000_get_cable_length_igp_2");
+
+	/* Read the AGC registers for all channels */
+	for (i = 0; i < IGP02E1000_PHY_CHANNEL_NUM; i++) {
+		ret_val = e1000_read_phy_reg(hw, agc_reg_array[i], &phy_data);
+		if (ret_val)
+			goto out;
+
+		/* Getting bits 15:9, which represent the combination of
+		 * course and fine gain values.  The result is a number
+		 * that can be put into the lookup table to obtain the
+		 * approximate cable length. */
+		cur_agc_index = (phy_data >> IGP02E1000_AGC_LENGTH_SHIFT) &
+		                IGP02E1000_AGC_LENGTH_MASK;
+
+		/* Array index bound check. */
+		if ((cur_agc_index >= IGP02E1000_CABLE_LENGTH_TABLE_SIZE) ||
+		    (cur_agc_index == 0)) {
+			ret_val = -E1000_ERR_PHY;
+			goto out;
+		}
+
+		/* Remove min & max AGC values from calculation. */
+		if (e1000_igp_2_cable_length_table[min_agc_index] >
+		    e1000_igp_2_cable_length_table[cur_agc_index])
+			min_agc_index = cur_agc_index;
+		if (e1000_igp_2_cable_length_table[max_agc_index] <
+		    e1000_igp_2_cable_length_table[cur_agc_index])
+			max_agc_index = cur_agc_index;
+
+		agc_value += e1000_igp_2_cable_length_table[cur_agc_index];
+	}
+
+	agc_value -= (e1000_igp_2_cable_length_table[min_agc_index] +
+	              e1000_igp_2_cable_length_table[max_agc_index]);
+	agc_value /= (IGP02E1000_PHY_CHANNEL_NUM - 2);
+
+	/* Calculate cable length with the error range of +/- 10 meters. */
+	phy->min_cable_length = ((agc_value - IGP02E1000_AGC_RANGE) > 0) ?
+	                         (agc_value - IGP02E1000_AGC_RANGE) : 0;
+	phy->max_cable_length = agc_value + IGP02E1000_AGC_RANGE;
+
+	phy->cable_length = (phy->min_cable_length + phy->max_cable_length) / 2;
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_phy_info_m88 - Retrieve PHY information
+ *  @hw: pointer to the HW structure
+ *
+ *  Valid for only copper links.  Read the PHY status register (sticky read)
+ *  to verify that link is up.  Read the PHY special control register to
+ *  determine the polarity and 10base-T extended distance.  Read the PHY
+ *  special status register to determine MDI/MDIx and current speed.  If
+ *  speed is 1000, then determine cable length, local and remote receiver.
+ **/
+s32 e1000_get_phy_info_m88(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32  ret_val;
+	u16 phy_data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_get_phy_info_m88");
+
+	if (hw->media_type != e1000_media_type_copper) {
+		DEBUGOUT("Phy info is only valid for copper media\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link) {
+		DEBUGOUT("Phy info is only valid if link is up\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_CTRL, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy->polarity_correction = (phy_data & M88E1000_PSCR_POLARITY_REVERSAL)
+	                           ? TRUE
+	                           : FALSE;
+
+	ret_val = e1000_check_polarity_m88(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg(hw, M88E1000_PHY_SPEC_STATUS, &phy_data);
+	if (ret_val)
+		goto out;
+
+	phy->is_mdix = (phy_data & M88E1000_PSSR_MDIX) ? TRUE : FALSE;
+
+	if ((phy_data & M88E1000_PSSR_SPEED) == M88E1000_PSSR_1000MBS) {
+		ret_val = e1000_get_cable_length(hw);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_data);
+		if (ret_val)
+			goto out;
+
+		phy->local_rx = (phy_data & SR_1000T_LOCAL_RX_STATUS)
+		                ? e1000_1000t_rx_status_ok
+		                : e1000_1000t_rx_status_not_ok;
+
+		phy->remote_rx = (phy_data & SR_1000T_REMOTE_RX_STATUS)
+		                 ? e1000_1000t_rx_status_ok
+		                 : e1000_1000t_rx_status_not_ok;
+	} else {
+		/* Set values to "undefined" */
+		phy->cable_length = E1000_CABLE_LENGTH_UNDEFINED;
+		phy->local_rx = e1000_1000t_rx_status_undefined;
+		phy->remote_rx = e1000_1000t_rx_status_undefined;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_phy_info_igp - Retrieve igp PHY information
+ *  @hw: pointer to the HW structure
+ *
+ *  Read PHY status to determine if link is up.  If link is up, then
+ *  set/determine 10base-T extended distance and polarity correction.  Read
+ *  PHY port status to determine MDI/MDIx and speed.  Based on the speed,
+ *  determine on the cable length, local and remote receiver.
+ **/
+s32 e1000_get_phy_info_igp(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32 ret_val;
+	u16 data;
+	boolean_t link;
+
+	DEBUGFUNC("e1000_get_phy_info_igp");
+
+	ret_val = e1000_phy_has_link_generic(hw, 1, 0, &link);
+	if (ret_val)
+		goto out;
+
+	if (!link) {
+		DEBUGOUT("Phy info is only valid if link is up\n");
+		ret_val = -E1000_ERR_CONFIG;
+		goto out;
+	}
+
+	phy->polarity_correction = TRUE;
+
+	ret_val = e1000_check_polarity_igp(hw);
+	if (ret_val)
+		goto out;
+
+	ret_val = e1000_read_phy_reg(hw, IGP01E1000_PHY_PORT_STATUS, &data);
+	if (ret_val)
+		goto out;
+
+	phy->is_mdix = (data & IGP01E1000_PSSR_MDIX) ? TRUE : FALSE;
+
+	if ((data & IGP01E1000_PSSR_SPEED_MASK) ==
+	    IGP01E1000_PSSR_SPEED_1000MBPS) {
+		ret_val = e1000_get_cable_length(hw);
+		if (ret_val)
+			goto out;
+
+		ret_val = e1000_read_phy_reg(hw, PHY_1000T_STATUS, &data);
+		if (ret_val)
+			goto out;
+
+		phy->local_rx = (data & SR_1000T_LOCAL_RX_STATUS)
+		                ? e1000_1000t_rx_status_ok
+		                : e1000_1000t_rx_status_not_ok;
+
+		phy->remote_rx = (data & SR_1000T_REMOTE_RX_STATUS)
+		                 ? e1000_1000t_rx_status_ok
+		                 : e1000_1000t_rx_status_not_ok;
+	} else {
+		phy->cable_length = E1000_CABLE_LENGTH_UNDEFINED;
+		phy->local_rx = e1000_1000t_rx_status_undefined;
+		phy->remote_rx = e1000_1000t_rx_status_undefined;
+	}
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_sw_reset_generic - PHY software reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Does a software reset of the PHY by reading the PHY control register and
+ *  setting/write the control register reset bit to the PHY.
+ **/
+s32 e1000_phy_sw_reset_generic(struct e1000_hw *hw)
+{
+	s32 ret_val;
+	u16 phy_ctrl;
+
+	DEBUGFUNC("e1000_phy_sw_reset_generic");
+
+	ret_val = e1000_read_phy_reg(hw, PHY_CONTROL, &phy_ctrl);
+	if (ret_val)
+		goto out;
+
+	phy_ctrl |= MII_CR_RESET;
+	ret_val = e1000_write_phy_reg(hw, PHY_CONTROL, phy_ctrl);
+	if (ret_val)
+		goto out;
+
+	usec_delay(1);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_phy_hw_reset_generic - PHY hardware reset
+ *  @hw: pointer to the HW structure
+ *
+ *  Verify the reset block is not blocking us from resetting.  Acquire
+ *  semaphore (if necessary) and read/set/write the device control reset
+ *  bit in the PHY.  Wait the appropriate delay time for the device to
+ *  reset and relase the semaphore (if necessary).
+ **/
+s32 e1000_phy_hw_reset_generic(struct e1000_hw *hw)
+{
+	struct e1000_phy_info *phy = &hw->phy;
+	s32  ret_val;
+	u32 ctrl;
+
+	DEBUGFUNC("e1000_phy_hw_reset_generic");
+
+	ret_val = e1000_check_reset_block(hw);
+	if (ret_val) {
+		ret_val = E1000_SUCCESS;
+		goto out;
+	}
+
+	ret_val = e1000_acquire_phy(hw);
+	if (ret_val)
+		goto out;
+
+	ctrl = E1000_READ_REG(hw, E1000_CTRL);
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl | E1000_CTRL_PHY_RST);
+	E1000_WRITE_FLUSH(hw);
+
+	usec_delay(phy->reset_delay_us);
+
+	E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	E1000_WRITE_FLUSH(hw);
+
+	usec_delay(150);
+
+	e1000_release_phy(hw);
+
+	ret_val = e1000_get_phy_cfg_done(hw);
+
+out:
+	return ret_val;
+}
+
+/**
+ *  e1000_get_cfg_done_generic - Generic configuration done
+ *  @hw: pointer to the HW structure
+ *
+ *  Generic function to wait 10 milli-seconds for configuration to complete
+ *  and return success.
+ **/
+s32 e1000_get_cfg_done_generic(struct e1000_hw *hw)
+{
+	DEBUGFUNC("e1000_get_cfg_done_generic");
+
+	msec_delay_irq(10);
+
+	return E1000_SUCCESS;
+}
+
+/* Internal function pointers */
+
+/**
+ *  e1000_get_phy_cfg_done - Generic PHY configuration done
+ *  @hw: pointer to the HW structure
+ *
+ *  Return success if silicon family did not implement a family specific
+ *  get_cfg_done function.
+ **/
+s32 e1000_get_phy_cfg_done(struct e1000_hw *hw)
+{
+	if (hw->func.get_cfg_done != NULL)
+		return hw->func.get_cfg_done(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_release_phy - Generic release PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Return if silicon family does not require a semaphore when accessing the
+ *  PHY.
+ **/
+void e1000_release_phy(struct e1000_hw *hw)
+{
+	if (hw->func.release_phy != NULL)
+		hw->func.release_phy(hw);
+}
+
+/**
+ *  e1000_acquire_phy - Generic acquire PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Return success if silicon family does not require a semaphore when
+ *  accessing the PHY.
+ **/
+s32 e1000_acquire_phy(struct e1000_hw *hw)
+{
+	if (hw->func.acquire_phy != NULL)
+		return hw->func.acquire_phy(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_phy_force_speed_duplex - Generic force PHY speed/duplex
+ *  @hw: pointer to the HW structure
+ *
+ *  When the silicon family has not implemented a forced speed/duplex
+ *  function for the PHY, simply return E1000_SUCCESS.
+ **/
+s32 e1000_phy_force_speed_duplex(struct e1000_hw *hw)
+{
+	if (hw->func.force_speed_duplex != NULL)
+		return hw->func.force_speed_duplex(hw);
+	else
+		return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_phy_init_script_igp3 - Inits the IGP3 PHY
+ *  @hw: pointer to the HW structure
+ *
+ *  Initializes a Intel Gigabit PHY3 when an EEPROM is not present.
+ **/
+s32 e1000_phy_init_script_igp3(struct e1000_hw *hw)
+{
+	DEBUGOUT("Running IGP 3 PHY init script\n");
+
+	/* PHY init IGP 3 */
+	/* Enable rise/fall, 10-mode work in class-A */
+	e1000_write_phy_reg(hw, 0x2F5B, 0x9018);
+	/* Remove all caps from Replica path filter */
+	e1000_write_phy_reg(hw, 0x2F52, 0x0000);
+	/* Bias trimming for ADC, AFE and Driver (Default) */
+	e1000_write_phy_reg(hw, 0x2FB1, 0x8B24);
+	/* Increase Hybrid poly bias */
+	e1000_write_phy_reg(hw, 0x2FB2, 0xF8F0);
+	/* Add 4% to TX amplitude in Giga mode */
+	e1000_write_phy_reg(hw, 0x2010, 0x10B0);
+	/* Disable trimming (TTT) */
+	e1000_write_phy_reg(hw, 0x2011, 0x0000);
+	/* Poly DC correction to 94.6% + 2% for all channels */
+	e1000_write_phy_reg(hw, 0x20DD, 0x249A);
+	/* ABS DC correction to 95.9% */
+	e1000_write_phy_reg(hw, 0x20DE, 0x00D3);
+	/* BG temp curve trim */
+	e1000_write_phy_reg(hw, 0x28B4, 0x04CE);
+	/* Increasing ADC OPAMP stage 1 currents to max */
+	e1000_write_phy_reg(hw, 0x2F70, 0x29E4);
+	/* Force 1000 ( required for enabling PHY regs configuration) */
+	e1000_write_phy_reg(hw, 0x0000, 0x0140);
+	/* Set upd_freq to 6 */
+	e1000_write_phy_reg(hw, 0x1F30, 0x1606);
+	/* Disable NPDFE */
+	e1000_write_phy_reg(hw, 0x1F31, 0xB814);
+	/* Disable adaptive fixed FFE (Default) */
+	e1000_write_phy_reg(hw, 0x1F35, 0x002A);
+	/* Enable FFE hysteresis */
+	e1000_write_phy_reg(hw, 0x1F3E, 0x0067);
+	/* Fixed FFE for short cable lengths */
+	e1000_write_phy_reg(hw, 0x1F54, 0x0065);
+	/* Fixed FFE for medium cable lengths */
+	e1000_write_phy_reg(hw, 0x1F55, 0x002A);
+	/* Fixed FFE for long cable lengths */
+	e1000_write_phy_reg(hw, 0x1F56, 0x002A);
+	/* Enable Adaptive Clip Threshold */
+	e1000_write_phy_reg(hw, 0x1F72, 0x3FB0);
+	/* AHT reset limit to 1 */
+	e1000_write_phy_reg(hw, 0x1F76, 0xC0FF);
+	/* Set AHT master delay to 127 msec */
+	e1000_write_phy_reg(hw, 0x1F77, 0x1DEC);
+	/* Set scan bits for AHT */
+	e1000_write_phy_reg(hw, 0x1F78, 0xF9EF);
+	/* Set AHT Preset bits */
+	e1000_write_phy_reg(hw, 0x1F79, 0x0210);
+	/* Change integ_factor of channel A to 3 */
+	e1000_write_phy_reg(hw, 0x1895, 0x0003);
+	/* Change prop_factor of channels BCD to 8 */
+	e1000_write_phy_reg(hw, 0x1796, 0x0008);
+	/* Change cg_icount + enable integbp for channels BCD */
+	e1000_write_phy_reg(hw, 0x1798, 0xD008);
+	/* Change cg_icount + enable integbp + change prop_factor_master
+	 * to 8 for channel A
+	 */
+	e1000_write_phy_reg(hw, 0x1898, 0xD918);
+	/* Disable AHT in Slave mode on channel A */
+	e1000_write_phy_reg(hw, 0x187A, 0x0800);
+	/* Enable LPLU and disable AN to 1000 in non-D0a states,
+	 * Enable SPD+B2B
+	 */
+	e1000_write_phy_reg(hw, 0x0019, 0x008D);
+	/* Enable restart AN on an1000_dis change */
+	e1000_write_phy_reg(hw, 0x001B, 0x2080);
+	/* Enable wh_fifo read clock in 10/100 modes */
+	e1000_write_phy_reg(hw, 0x0014, 0x0045);
+	/* Restart AN, Speed selection is 1000 */
+	e1000_write_phy_reg(hw, 0x0000, 0x1340);
+
+	return E1000_SUCCESS;
+}
+
+/**
+ *  e1000_get_phy_type_from_id - Get PHY type from id
+ *  @phy_id: phy_id read from the phy
+ *
+ *  Returns the phy type from the id.
+ **/
+e1000_phy_type e1000_get_phy_type_from_id(u32 phy_id)
+{
+	e1000_phy_type phy_type = e1000_phy_unknown;
+
+	switch (phy_id)	{
+	case M88E1000_I_PHY_ID:
+	case M88E1000_E_PHY_ID:
+	case M88E1111_I_PHY_ID:
+	case M88E1011_I_PHY_ID:
+		phy_type = e1000_phy_m88;
+		break;
+	case IGP01E1000_I_PHY_ID: /* IGP 1 & 2 share this */
+		phy_type = e1000_phy_igp_2;
+		break;
+	case GG82563_E_PHY_ID:
+		phy_type = e1000_phy_gg82563;
+		break;
+	case IGP03E1000_E_PHY_ID:
+		phy_type = e1000_phy_igp_3;
+		break;
+	case IFE_E_PHY_ID:
+	case IFE_PLUS_E_PHY_ID:
+	case IFE_C_E_PHY_ID:
+		phy_type = e1000_phy_ife;
+		break;
+	default:
+		phy_type = e1000_phy_unknown;
+		break;
+	}
+	return phy_type;
+}
+
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_phy.h linux-2.6.9/drivers/net/e1000/e1000_phy.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_phy.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_phy.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,170 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_PHY_H_
+#define _E1000_PHY_H_
+
+typedef enum {
+	e1000_ms_hw_default = 0,
+	e1000_ms_force_master,
+	e1000_ms_force_slave,
+	e1000_ms_auto
+} e1000_ms_type;
+
+typedef enum {
+	e1000_smart_speed_default = 0,
+	e1000_smart_speed_on,
+	e1000_smart_speed_off
+} e1000_smart_speed;
+
+s32  e1000_check_downshift_generic(struct e1000_hw *hw);
+s32  e1000_check_polarity_m88(struct e1000_hw *hw);
+s32  e1000_check_polarity_igp(struct e1000_hw *hw);
+s32  e1000_check_reset_block_generic(struct e1000_hw *hw);
+s32  e1000_copper_link_autoneg(struct e1000_hw *hw);
+s32  e1000_phy_force_speed_duplex(struct e1000_hw *hw);
+s32  e1000_copper_link_setup_igp(struct e1000_hw *hw);
+s32  e1000_copper_link_setup_m88(struct e1000_hw *hw);
+s32  e1000_phy_force_speed_duplex_igp(struct e1000_hw *hw);
+s32  e1000_phy_force_speed_duplex_m88(struct e1000_hw *hw);
+s32  e1000_get_cable_length_m88(struct e1000_hw *hw);
+s32  e1000_get_cable_length_igp_2(struct e1000_hw *hw);
+s32  e1000_get_cfg_done_generic(struct e1000_hw *hw);
+s32  e1000_get_phy_id(struct e1000_hw *hw);
+s32  e1000_get_phy_info_igp(struct e1000_hw *hw);
+s32  e1000_get_phy_info_m88(struct e1000_hw *hw);
+s32  e1000_phy_sw_reset_generic(struct e1000_hw *hw);
+void e1000_phy_force_speed_duplex_setup(struct e1000_hw *hw, u16 *phy_ctrl);
+s32  e1000_phy_hw_reset_generic(struct e1000_hw *hw);
+s32  e1000_phy_reset_dsp_generic(struct e1000_hw *hw);
+s32  e1000_phy_setup_autoneg(struct e1000_hw *hw);
+s32  e1000_read_kmrn_reg_generic(struct e1000_hw *hw, u32 offset, u16 *data);
+s32  e1000_read_phy_reg_igp(struct e1000_hw *hw, u32 offset, u16 *data);
+s32  e1000_read_phy_reg_m88(struct e1000_hw *hw, u32 offset, u16 *data);
+s32  e1000_set_d3_lplu_state_generic(struct e1000_hw *hw, boolean_t active);
+s32  e1000_setup_copper_link_generic(struct e1000_hw *hw);
+s32  e1000_wait_autoneg_generic(struct e1000_hw *hw);
+s32  e1000_write_kmrn_reg_generic(struct e1000_hw *hw, u32 offset, u16 data);
+s32  e1000_write_phy_reg_igp(struct e1000_hw *hw, u32 offset, u16 data);
+s32  e1000_write_phy_reg_m88(struct e1000_hw *hw, u32 offset, u16 data);
+s32  e1000_phy_reset_dsp(struct e1000_hw *hw);
+s32  e1000_phy_has_link_generic(struct e1000_hw *hw, u32 iterations,
+                                u32 usec_interval, boolean_t *success);
+s32  e1000_phy_init_script_igp3(struct e1000_hw *hw);
+e1000_phy_type e1000_get_phy_type_from_id(u32 phy_id);
+#define E1000_MAX_PHY_ADDR                4
+
+/* IGP01E1000 Specific Registers */
+#define IGP01E1000_PHY_PORT_CONFIG        0x10 /* Port Config */
+#define IGP01E1000_PHY_PORT_STATUS        0x11 /* Status */
+#define IGP01E1000_PHY_PORT_CTRL          0x12 /* Control */
+#define IGP01E1000_PHY_LINK_HEALTH        0x13 /* PHY Link Health */
+#define IGP01E1000_GMII_FIFO              0x14 /* GMII FIFO */
+#define IGP01E1000_PHY_CHANNEL_QUALITY    0x15 /* PHY Channel Quality */
+#define IGP02E1000_PHY_POWER_MGMT         0x19 /* Power Management */
+#define IGP01E1000_PHY_PAGE_SELECT        0x1F /* Page Select */
+#define IGP4_PHY_PAGE_SELECT              22   /* Page Select for IGP 4 */
+#define IGP_PAGE_SHIFT                    5
+#define PHY_REG_MASK                      0x1F
+
+#define IGP4_WUC_PAGE                     800
+#define IGP4_WUC_ADDRESS_OPCODE           0x11
+#define IGP4_WUC_DATA_OPCODE              0x12
+#define IGP4_WUC_ENABLE_PAGE              769
+#define IGP4_WUC_ENABLE_REG               17
+#define IGP4_WUC_ENABLE_BIT               (1 << 2)
+#define IGP4_WUC_HOST_WU_BIT              (1 << 4)
+
+#define IGP01E1000_PHY_PCS_INIT_REG       0x00B4
+#define IGP01E1000_PHY_POLARITY_MASK      0x0078
+
+#define IGP01E1000_PSCR_AUTO_MDIX         0x1000
+#define IGP01E1000_PSCR_FORCE_MDI_MDIX    0x2000 /* 0=MDI, 1=MDIX */
+
+#define IGP01E1000_PSCFR_SMART_SPEED      0x0080
+
+#define IGP01E1000_GMII_FLEX_SPD          0x0010 /* Enable flexible speed
+                                                  * on link-up */
+#define IGP01E1000_GMII_SPD               0x0020 /* Enable SPD */
+
+#define IGP02E1000_PM_SPD                 0x0001 /* Smart Power Down */
+#define IGP02E1000_PM_D0_LPLU             0x0002 /* For D0a states */
+#define IGP02E1000_PM_D3_LPLU             0x0004 /* For all other states */
+
+#define IGP01E1000_PLHR_SS_DOWNGRADE      0x8000
+
+#define IGP01E1000_PSSR_POLARITY_REVERSED 0x0002
+#define IGP01E1000_PSSR_MDIX              0x0008
+#define IGP01E1000_PSSR_SPEED_MASK        0xC000
+#define IGP01E1000_PSSR_SPEED_1000MBPS    0xC000
+
+#define IGP02E1000_PHY_CHANNEL_NUM        4
+#define IGP02E1000_PHY_AGC_A              0x11B1
+#define IGP02E1000_PHY_AGC_B              0x12B1
+#define IGP02E1000_PHY_AGC_C              0x14B1
+#define IGP02E1000_PHY_AGC_D              0x18B1
+
+#define IGP02E1000_AGC_LENGTH_SHIFT       9   /* Course - 15:13, Fine - 12:9 */
+#define IGP02E1000_AGC_LENGTH_MASK        0x7F
+#define IGP02E1000_AGC_RANGE              15
+
+#define IGP03E1000_PHY_MISC_CTRL          0x1B
+#define IGP03E1000_PHY_MISC_DUPLEX_MANUAL_SET  0x1000 /* Manually Set Duplex */
+
+#define E1000_CABLE_LENGTH_UNDEFINED      0xFF
+
+#define E1000_KMRNCTRLSTA_OFFSET          0x001F0000
+#define E1000_KMRNCTRLSTA_OFFSET_SHIFT    16
+#define E1000_KMRNCTRLSTA_REN             0x00200000
+#define E1000_KMRNCTRLSTA_DIAG_OFFSET     0x3    /* Kumeran Diagnostic */
+#define E1000_KMRNCTRLSTA_DIAG_NELPBK     0x1000 /* Nearend Loopback mode */
+
+#define IFE_PHY_EXTENDED_STATUS_CONTROL 0x10
+#define IFE_PHY_SPECIAL_CONTROL     0x11 /* 100BaseTx PHY Special Control */
+#define IFE_PHY_SPECIAL_CONTROL_LED 0x1B /* PHY Special and LED Control */
+#define IFE_PHY_MDIX_CONTROL        0x1C /* MDI/MDI-X Control */
+
+/* IFE PHY Extended Status Control */
+#define IFE_PESC_POLARITY_REVERSED    0x0100
+
+/* IFE PHY Special Control */
+#define IFE_PSC_AUTO_POLARITY_DISABLE      0x0010
+#define IFE_PSC_FORCE_POLARITY             0x0020
+#define IFE_PSC_DISABLE_DYNAMIC_POWER_DOWN 0x0100
+
+/* IFE PHY Special Control and LED Control */
+#define IFE_PSCL_PROBE_MODE            0x0020
+#define IFE_PSCL_PROBE_LEDS_OFF        0x0006 /* Force LEDs 0 and 2 off */
+#define IFE_PSCL_PROBE_LEDS_ON         0x0007 /* Force LEDs 0 and 2 on */
+
+/* IFE PHY MDIX Control */
+#define IFE_PMC_MDIX_STATUS      0x0020 /* 1=MDI-X, 0=MDI */
+#define IFE_PMC_FORCE_MDIX       0x0040 /* 1=force MDI-X, 0=force MDI */
+#define IFE_PMC_AUTO_MDIX        0x0080 /* 1=enable auto MDI/MDI-X, 0=disable */
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/e1000_regs.h linux-2.6.9/drivers/net/e1000/e1000_regs.h
--- linux-2.6.9.src/drivers/net/e1000/e1000_regs.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/e1000_regs.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,261 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _E1000_REGS_H_
+#define _E1000_REGS_H_
+
+#define E1000_CTRL     0x00000  /* Device Control - RW */
+#define E1000_CTRL_DUP 0x00004  /* Device Control Duplicate (Shadow) - RW */
+#define E1000_STATUS   0x00008  /* Device Status - RO */
+#define E1000_EECD     0x00010  /* EEPROM/Flash Control - RW */
+#define E1000_EERD     0x00014  /* EEPROM Read - RW */
+#define E1000_CTRL_EXT 0x00018  /* Extended Device Control - RW */
+#define E1000_FLA      0x0001C  /* Flash Access - RW */
+#define E1000_MDIC     0x00020  /* MDI Control - RW */
+#define E1000_SCTL     0x00024  /* SerDes Control - RW */
+#define E1000_FCAL     0x00028  /* Flow Control Address Low - RW */
+#define E1000_FCAH     0x0002C  /* Flow Control Address High -RW */
+#define E1000_FEXTNVM  0x00028  /* Future Extended NVM - RW */
+#define E1000_FCT      0x00030  /* Flow Control Type - RW */
+#define E1000_VET      0x00038  /* VLAN Ether Type - RW */
+#define E1000_ICR      0x000C0  /* Interrupt Cause Read - R/clr */
+#define E1000_ITR      0x000C4  /* Interrupt Throttling Rate - RW */
+#define E1000_ICS      0x000C8  /* Interrupt Cause Set - WO */
+#define E1000_IMS      0x000D0  /* Interrupt Mask Set - RW */
+#define E1000_IMC      0x000D8  /* Interrupt Mask Clear - WO */
+#define E1000_IAM      0x000E0  /* Interrupt Acknowledge Auto Mask */
+#define E1000_RCTL     0x00100  /* RX Control - RW */
+#define E1000_RDTR1    0x02820  /* RX Delay Timer (1) - RW */
+#define E1000_RDBAL1   0x02900  /* RX Descriptor Base Address Low (1) - RW */
+#define E1000_RDBAH1   0x02904  /* RX Descriptor Base Address High (1) - RW */
+#define E1000_RDLEN1   0x02908  /* RX Descriptor Length (1) - RW */
+#define E1000_RDH1     0x02910  /* RX Descriptor Head (1) - RW */
+#define E1000_RDT1     0x02918  /* RX Descriptor Tail (1) - RW */
+#define E1000_FCTTV    0x00170  /* Flow Control Transmit Timer Value - RW */
+#define E1000_TXCW     0x00178  /* TX Configuration Word - RW */
+#define E1000_RXCW     0x00180  /* RX Configuration Word - RO */
+#define E1000_TCTL     0x00400  /* TX Control - RW */
+#define E1000_TCTL_EXT 0x00404  /* Extended TX Control - RW */
+#define E1000_TIPG     0x00410  /* TX Inter-packet gap -RW */
+#define E1000_TBT      0x00448  /* TX Burst Timer - RW */
+#define E1000_AIT      0x00458  /* Adaptive Interframe Spacing Throttle - RW */
+#define E1000_LEDCTL   0x00E00  /* LED Control - RW */
+#define E1000_EXTCNF_CTRL  0x00F00  /* Extended Configuration Control */
+#define E1000_EXTCNF_SIZE  0x00F08  /* Extended Configuration Size */
+#define E1000_PHY_CTRL     0x00F10  /* PHY Control Register in CSR */
+#define E1000_PBA      0x01000  /* Packet Buffer Allocation - RW */
+#define E1000_PBS      0x01008  /* Packet Buffer Size */
+#define E1000_EEMNGCTL 0x01010  /* MNG EEprom Control */
+#define E1000_EEARBC   0x01024  /* EEPROM Auto Read Bus Control */
+#define E1000_FLASHT   0x01028  /* FLASH Timer Register */
+#define E1000_EEWR     0x0102C  /* EEPROM Write Register - RW */
+#define E1000_FLSWCTL  0x01030  /* FLASH control register */
+#define E1000_FLSWDATA 0x01034  /* FLASH data register */
+#define E1000_FLSWCNT  0x01038  /* FLASH Access Counter */
+#define E1000_FLOP     0x0103C  /* FLASH Opcode Register */
+#define E1000_ERT      0x02008  /* Early Rx Threshold - RW */
+#define E1000_FCRTL    0x02160  /* Flow Control Receive Threshold Low - RW */
+#define E1000_FCRTH    0x02168  /* Flow Control Receive Threshold High - RW */
+#define E1000_PSRCTL   0x02170  /* Packet Split Receive Control - RW */
+#define E1000_RDBAL    0x02800  /* RX Descriptor Base Address Low - RW */
+#define E1000_RDBAH    0x02804  /* RX Descriptor Base Address High - RW */
+#define E1000_RDLEN    0x02808  /* RX Descriptor Length - RW */
+#define E1000_RDH      0x02810  /* RX Descriptor Head - RW */
+#define E1000_RDT      0x02818  /* RX Descriptor Tail - RW */
+#define E1000_RDTR     0x02820  /* RX Delay Timer - RW */
+#define E1000_RDBAL0   E1000_RDBAL /* RX Desc Base Address Low (0) - RW */
+#define E1000_RDBAH0   E1000_RDBAH /* RX Desc Base Address High (0) - RW */
+#define E1000_RDLEN0   E1000_RDLEN /* RX Desc Length (0) - RW */
+#define E1000_RDH0     E1000_RDH   /* RX Desc Head (0) - RW */
+#define E1000_RDT0     E1000_RDT   /* RX Desc Tail (0) - RW */
+#define E1000_RDTR0    E1000_RDTR  /* RX Delay Timer (0) - RW */
+#define E1000_RXDCTL   0x02828  /* RX Descriptor Control queue 0 - RW */
+#define E1000_RXDCTL1  0x02928  /* RX Descriptor Control queue 1 - RW */
+#define E1000_RADV     0x0282C  /* RX Interrupt Absolute Delay Timer - RW */
+/* Convenience macros
+ *
+ * Note: "_n" is the queue number of the register to be written to.
+ *
+ * Example usage:
+ * E1000_RDBAL_REG(current_rx_queue)
+ *
+ */
+#define E1000_RDBAL_REG(_n)   (E1000_RDBAL + (_n << 8))
+#define E1000_RDBAH_REG(_n)   (E1000_RDBAH + (_n << 8))
+#define E1000_RDLEN_REG(_n)   (E1000_RDLEN + (_n << 8))
+#define E1000_RDH_REG(_n)     (E1000_RDH + (_n << 8))
+#define E1000_RDT_REG(_n)     (E1000_RDT + (_n << 8))
+#define E1000_RXDCTL_REG(_n)  (E1000_RXDCTL + (_n << 8))
+#define E1000_TDBAL_REG(_n)   (E1000_TDBAL + (_n << 8))
+#define E1000_TDBAH_REG(_n)   (E1000_TDBAH + (_n << 8))
+#define E1000_TDLEN_REG(_n)   (E1000_TDLEN + (_n << 8))
+#define E1000_TDH_REG(_n)     (E1000_TDH + (_n << 8))
+#define E1000_TDT_REG(_n)     (E1000_TDT + (_n << 8))
+#define E1000_TXDCTL_REG(_n)  (E1000_TXDCTL + (_n << 8))
+#define E1000_TARC_REG(_n)    (E1000_TARC0 + (_n << 8))
+#define E1000_RSRPD    0x02C00  /* RX Small Packet Detect - RW */
+#define E1000_RAID     0x02C08  /* Receive Ack Interrupt Delay - RW */
+#define E1000_TXDMAC   0x03000  /* TX DMA Control - RW */
+#define E1000_KABGTXD  0x03004  /* AFE Band Gap Transmit Ref Data */
+#define E1000_TDFH     0x03410  /* TX Data FIFO Head - RW */
+#define E1000_TDFT     0x03418  /* TX Data FIFO Tail - RW */
+#define E1000_TDFHS    0x03420  /* TX Data FIFO Head Saved - RW */
+#define E1000_TDFTS    0x03428  /* TX Data FIFO Tail Saved - RW */
+#define E1000_TDFPC    0x03430  /* TX Data FIFO Packet Count - RW */
+#define E1000_TDBAL    0x03800  /* TX Descriptor Base Address Low - RW */
+#define E1000_TDBAH    0x03804  /* TX Descriptor Base Address High - RW */
+#define E1000_TDLEN    0x03808  /* TX Descriptor Length - RW */
+#define E1000_TDH      0x03810  /* TX Descriptor Head - RW */
+#define E1000_TDT      0x03818  /* TX Descriptor Tail - RW */
+#define E1000_TDBAL0   E1000_TDBAL /* TX Descriptor Base Address Low - RW */
+#define E1000_TDBAH0   E1000_TDBAH /* TX Descriptor Base Address High - RW */
+#define E1000_TDLEN0   E1000_TDLEN /* TX Descriptor Length - RW */
+#define E1000_TDH0     E1000_TDH   /* TX Descriptor Head - RW */
+#define E1000_TDT0     E1000_TDT   /* TX Descriptor Tail - RW */
+#define E1000_TIDV     0x03820  /* TX Interrupt Delay Value - RW */
+#define E1000_TXDCTL   0x03828  /* TX Descriptor Control - RW */
+#define E1000_TADV     0x0382C  /* TX Interrupt Absolute Delay Val - RW */
+#define E1000_TSPMT    0x03830  /* TCP Segmentation PAD & Min Threshold - RW */
+#define E1000_TARC0    0x03840  /* TX Arbitration Count (0) */
+#define E1000_TDBAL1   0x03900  /* TX Desc Base Address Low (1) - RW */
+#define E1000_TDBAH1   0x03904  /* TX Desc Base Address High (1) - RW */
+#define E1000_TDLEN1   0x03908  /* TX Desc Length (1) - RW */
+#define E1000_TDH1     0x03910  /* TX Desc Head (1) - RW */
+#define E1000_TDT1     0x03918  /* TX Desc Tail (1) - RW */
+#define E1000_TXDCTL1  0x03928  /* TX Descriptor Control (1) - RW */
+#define E1000_TARC1    0x03940  /* TX Arbitration Count (1) */
+#define E1000_CRCERRS  0x04000  /* CRC Error Count - R/clr */
+#define E1000_ALGNERRC 0x04004  /* Alignment Error Count - R/clr */
+#define E1000_SYMERRS  0x04008  /* Symbol Error Count - R/clr */
+#define E1000_RXERRC   0x0400C  /* Receive Error Count - R/clr */
+#define E1000_MPC      0x04010  /* Missed Packet Count - R/clr */
+#define E1000_SCC      0x04014  /* Single Collision Count - R/clr */
+#define E1000_ECOL     0x04018  /* Excessive Collision Count - R/clr */
+#define E1000_MCC      0x0401C  /* Multiple Collision Count - R/clr */
+#define E1000_LATECOL  0x04020  /* Late Collision Count - R/clr */
+#define E1000_COLC     0x04028  /* Collision Count - R/clr */
+#define E1000_DC       0x04030  /* Defer Count - R/clr */
+#define E1000_TNCRS    0x04034  /* TX-No CRS - R/clr */
+#define E1000_SEC      0x04038  /* Sequence Error Count - R/clr */
+#define E1000_CEXTERR  0x0403C  /* Carrier Extension Error Count - R/clr */
+#define E1000_RLEC     0x04040  /* Receive Length Error Count - R/clr */
+#define E1000_XONRXC   0x04048  /* XON RX Count - R/clr */
+#define E1000_XONTXC   0x0404C  /* XON TX Count - R/clr */
+#define E1000_XOFFRXC  0x04050  /* XOFF RX Count - R/clr */
+#define E1000_XOFFTXC  0x04054  /* XOFF TX Count - R/clr */
+#define E1000_FCRUC    0x04058  /* Flow Control RX Unsupported Count- R/clr */
+#define E1000_PRC64    0x0405C  /* Packets RX (64 bytes) - R/clr */
+#define E1000_PRC127   0x04060  /* Packets RX (65-127 bytes) - R/clr */
+#define E1000_PRC255   0x04064  /* Packets RX (128-255 bytes) - R/clr */
+#define E1000_PRC511   0x04068  /* Packets RX (255-511 bytes) - R/clr */
+#define E1000_PRC1023  0x0406C  /* Packets RX (512-1023 bytes) - R/clr */
+#define E1000_PRC1522  0x04070  /* Packets RX (1024-1522 bytes) - R/clr */
+#define E1000_GPRC     0x04074  /* Good Packets RX Count - R/clr */
+#define E1000_BPRC     0x04078  /* Broadcast Packets RX Count - R/clr */
+#define E1000_MPRC     0x0407C  /* Multicast Packets RX Count - R/clr */
+#define E1000_GPTC     0x04080  /* Good Packets TX Count - R/clr */
+#define E1000_GORCL    0x04088  /* Good Octets RX Count Low - R/clr */
+#define E1000_GORCH    0x0408C  /* Good Octets RX Count High - R/clr */
+#define E1000_GOTCL    0x04090  /* Good Octets TX Count Low - R/clr */
+#define E1000_GOTCH    0x04094  /* Good Octets TX Count High - R/clr */
+#define E1000_RNBC     0x040A0  /* RX No Buffers Count - R/clr */
+#define E1000_RUC      0x040A4  /* RX Undersize Count - R/clr */
+#define E1000_RFC      0x040A8  /* RX Fragment Count - R/clr */
+#define E1000_ROC      0x040AC  /* RX Oversize Count - R/clr */
+#define E1000_RJC      0x040B0  /* RX Jabber Count - R/clr */
+#define E1000_MGTPRC   0x040B4  /* Management Packets RX Count - R/clr */
+#define E1000_MGTPDC   0x040B8  /* Management Packets Dropped Count - R/clr */
+#define E1000_MGTPTC   0x040BC  /* Management Packets TX Count - R/clr */
+#define E1000_TORL     0x040C0  /* Total Octets RX Low - R/clr */
+#define E1000_TORH     0x040C4  /* Total Octets RX High - R/clr */
+#define E1000_TOTL     0x040C8  /* Total Octets TX Low - R/clr */
+#define E1000_TOTH     0x040CC  /* Total Octets TX High - R/clr */
+#define E1000_TPR      0x040D0  /* Total Packets RX - R/clr */
+#define E1000_TPT      0x040D4  /* Total Packets TX - R/clr */
+#define E1000_PTC64    0x040D8  /* Packets TX (64 bytes) - R/clr */
+#define E1000_PTC127   0x040DC  /* Packets TX (65-127 bytes) - R/clr */
+#define E1000_PTC255   0x040E0  /* Packets TX (128-255 bytes) - R/clr */
+#define E1000_PTC511   0x040E4  /* Packets TX (256-511 bytes) - R/clr */
+#define E1000_PTC1023  0x040E8  /* Packets TX (512-1023 bytes) - R/clr */
+#define E1000_PTC1522  0x040EC  /* Packets TX (1024-1522 Bytes) - R/clr */
+#define E1000_MPTC     0x040F0  /* Multicast Packets TX Count - R/clr */
+#define E1000_BPTC     0x040F4  /* Broadcast Packets TX Count - R/clr */
+#define E1000_TSCTC    0x040F8  /* TCP Segmentation Context TX - R/clr */
+#define E1000_TSCTFC   0x040FC  /* TCP Segmentation Context TX Fail - R/clr */
+#define E1000_IAC      0x04100  /* Interrupt Assertion Count */
+#define E1000_ICRXPTC  0x04104  /* Interrupt Cause Rx Packet Timer Expire Count */
+#define E1000_ICRXATC  0x04108  /* Interrupt Cause Rx Absolute Timer Expire Count */
+#define E1000_ICTXPTC  0x0410C  /* Interrupt Cause Tx Packet Timer Expire Count */
+#define E1000_ICTXATC  0x04110  /* Interrupt Cause Tx Absolute Timer Expire Count */
+#define E1000_ICTXQEC  0x04118  /* Interrupt Cause Tx Queue Empty Count */
+#define E1000_ICTXQMTC 0x0411C  /* Interrupt Cause Tx Queue Minimum Threshold Count */
+#define E1000_ICRXDMTC 0x04120  /* Interrupt Cause Rx Descriptor Minimum Threshold Count */
+#define E1000_ICRXOC   0x04124  /* Interrupt Cause Receiver Overrun Count */
+#define E1000_RXCSUM   0x05000  /* RX Checksum Control - RW */
+#define E1000_RFCTL    0x05008  /* Receive Filter Control*/
+#define E1000_MTA      0x05200  /* Multicast Table Array - RW Array */
+#define E1000_RA       0x05400  /* Receive Address - RW Array */
+#define E1000_VFTA     0x05600  /* VLAN Filter Table Array - RW Array */
+#define E1000_WUC      0x05800  /* Wakeup Control - RW */
+#define E1000_WUFC     0x05808  /* Wakeup Filter Control - RW */
+#define E1000_WUS      0x05810  /* Wakeup Status - RO */
+#define E1000_MANC     0x05820  /* Management Control - RW */
+#define E1000_IPAV     0x05838  /* IP Address Valid - RW */
+#define E1000_IP4AT    0x05840  /* IPv4 Address Table - RW Array */
+#define E1000_IP6AT    0x05880  /* IPv6 Address Table - RW Array */
+#define E1000_WUPL     0x05900  /* Wakeup Packet Length - RW */
+#define E1000_WUPM     0x05A00  /* Wakeup Packet Memory - RO A */
+#define E1000_FFLT     0x05F00  /* Flexible Filter Length Table - RW Array */
+#define E1000_HOST_IF  0x08800  /* Host Interface */
+#define E1000_FFMT     0x09000  /* Flexible Filter Mask Table - RW Array */
+#define E1000_FFVT     0x09800  /* Flexible Filter Value Table - RW Array */
+
+#define E1000_KMRNCTRLSTA 0x00034 /* MAC-PHY interface - RW */
+#define E1000_MDPHYA      0x0003C /* PHY address - RW */
+#define E1000_MANC2H      0x05860 /* Management Control To Host - RW */
+#define E1000_SW_FW_SYNC  0x05B5C /* Software-Firmware Synchronization - RW */
+#define E1000_GCR         0x05B00 /* PCI-Ex Control */
+#define E1000_GSCL_1    0x05B10 /* PCI-Ex Statistic Control #1 */
+#define E1000_GSCL_2    0x05B14 /* PCI-Ex Statistic Control #2 */
+#define E1000_GSCL_3    0x05B18 /* PCI-Ex Statistic Control #3 */
+#define E1000_GSCL_4    0x05B1C /* PCI-Ex Statistic Control #4 */
+#define E1000_FACTPS    0x05B30 /* Function Active and Power State to MNG */
+#define E1000_SWSM      0x05B50 /* SW Semaphore */
+#define E1000_FWSM      0x05B54 /* FW Semaphore */
+#define E1000_FFLT_DBG  0x05F04 /* Debug Register */
+#define E1000_HICR      0x08F00 /* Host Inteface Control */
+
+/* RSS registers */
+#define E1000_CPUVEC    0x02C10 /* CPU Vector Register - RW */
+#define E1000_MRQC      0x05818 /* Multiple Receive Control - RW */
+#define E1000_RETA      0x05C00 /* Redirection Table - RW Array */
+#define E1000_RSSRK     0x05C80 /* RSS Random Key - RW Array */
+#define E1000_RSSIM     0x05864 /* RSS Interrupt Mask */
+#define E1000_RSSIR     0x05868 /* RSS Interrupt Request */
+
+#endif
diff -Naur linux-2.6.9.src/drivers/net/e1000/kcompat.c linux-2.6.9/drivers/net/e1000/kcompat.c
--- linux-2.6.9.src/drivers/net/e1000/kcompat.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/kcompat.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,229 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include "kcompat.h"
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,13) )
+
+/**************************************/
+/* PCI DMA MAPPING */
+
+#if defined(CONFIG_HIGHMEM)
+
+#ifndef PCI_DRAM_OFFSET
+#define PCI_DRAM_OFFSET 0
+#endif
+
+u64
+_kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset,
+                 size_t size, int direction)
+{
+	return (((u64) (page - mem_map) << PAGE_SHIFT) + offset +
+		PCI_DRAM_OFFSET);
+}
+
+#else /* CONFIG_HIGHMEM */
+
+u64
+_kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset,
+                 size_t size, int direction)
+{
+	return pci_map_single(dev, (void *)page_address(page) + offset, size,
+			      direction);
+}
+
+#endif /* CONFIG_HIGHMEM */
+
+void
+_kc_pci_unmap_page(struct pci_dev *dev, u64 dma_addr, size_t size,
+                   int direction)
+{
+	return pci_unmap_single(dev, dma_addr, size, direction);
+}
+
+#endif /* 2.4.13 => 2.4.3 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,3) )
+
+/**************************************/
+/* PCI DRIVER API */
+
+int
+_kc_pci_set_dma_mask(struct pci_dev *dev, dma_addr_t mask)
+{
+	if (!pci_dma_supported(dev, mask))
+		return -EIO;
+	dev->dma_mask = mask;
+	return 0;
+}
+
+int
+_kc_pci_request_regions(struct pci_dev *dev, char *res_name)
+{
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		if (pci_resource_len(dev, i) == 0)
+			continue;
+
+		if (pci_resource_flags(dev, i) & IORESOURCE_IO) {
+			if (!request_region(pci_resource_start(dev, i), pci_resource_len(dev, i), res_name)) {
+				pci_release_regions(dev);
+				return -EBUSY;
+			}
+		} else if (pci_resource_flags(dev, i) & IORESOURCE_MEM) {
+			if (!request_mem_region(pci_resource_start(dev, i), pci_resource_len(dev, i), res_name)) {
+				pci_release_regions(dev);
+				return -EBUSY;
+			}
+		}
+	}
+	return 0;
+}
+
+void
+_kc_pci_release_regions(struct pci_dev *dev)
+{
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		if (pci_resource_len(dev, i) == 0)
+			continue;
+
+		if (pci_resource_flags(dev, i) & IORESOURCE_IO)
+			release_region(pci_resource_start(dev, i), pci_resource_len(dev, i));
+
+		else if (pci_resource_flags(dev, i) & IORESOURCE_MEM)
+			release_mem_region(pci_resource_start(dev, i), pci_resource_len(dev, i));
+	}
+}
+
+/**************************************/
+/* NETWORK DRIVER API */
+
+struct net_device *
+_kc_alloc_etherdev(int sizeof_priv)
+{
+	struct net_device *dev;
+	int alloc_size;
+
+	alloc_size = sizeof(*dev) + sizeof_priv + IFNAMSIZ + 31;
+	dev = kmalloc(alloc_size, GFP_KERNEL);
+	if (!dev)
+		return NULL;
+	memset(dev, 0, alloc_size);
+
+	if (sizeof_priv)
+		dev->priv = (void *) (((unsigned long)(dev + 1) + 31) & ~31);
+	dev->name[0] = '\0';
+	ether_setup(dev);
+
+	return dev;
+}
+
+int
+_kc_is_valid_ether_addr(u8 *addr)
+{
+	const char zaddr[6] = { 0, };
+
+	return !(addr[0] & 1) && memcmp(addr, zaddr, 6);
+}
+
+#endif /* 2.4.3 => 2.4.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,6) )
+
+int
+_kc_pci_set_power_state(struct pci_dev *dev, int state)
+{
+	return 0;
+}
+
+int
+_kc_pci_save_state(struct pci_dev *dev, u32 *buffer)
+{
+	return 0;
+}
+
+int
+_kc_pci_restore_state(struct pci_dev *pdev, u32 *buffer)
+{
+	return 0;
+}
+
+int
+_kc_pci_enable_wake(struct pci_dev *pdev, u32 state, int enable)
+{
+	return 0;
+}
+
+#endif /* 2.4.6 => 2.4.3 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+void _kc_skb_fill_page_desc(struct sk_buff *skb, int i, struct page *page,
+                            int off, int size)
+{
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+	frag->page = page;
+	frag->page_offset = off;
+	frag->size = size;
+	skb_shinfo(skb)->nr_frags = i + 1;
+}
+
+#endif /* 2.6.0 => 2.4.6 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,14) )
+void *_kc_kzalloc(size_t size, int flags)
+{
+	void *ret = kmalloc(size, flags);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+#endif /* <= 2.6.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18) )
+struct sk_buff *_kc_netdev_alloc_skb(struct net_device *dev,
+                                     unsigned int length)
+{
+	/* 16 == NET_PAD_SKB */
+	struct sk_buff *skb;
+	skb = alloc_skb(length + 16, GFP_ATOMIC);
+	if (likely(skb != NULL)) {
+		skb_reserve(skb, 16);
+		skb->dev = dev;
+	}
+	return skb;
+}
+#endif /* <= 2.6.17 */
diff -Naur linux-2.6.9.src/drivers/net/e1000/kcompat_ethtool.c linux-2.6.9/drivers/net/e1000/kcompat_ethtool.c
--- linux-2.6.9.src/drivers/net/e1000/kcompat_ethtool.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/kcompat_ethtool.c	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1163 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+/*
+ * net/core/ethtool.c - Ethtool ioctl handler
+ * Copyright (c) 2003 Matthew Wilcox <matthew@wil.cx>
+ *
+ * This file is where we call all the ethtool_ops commands to get
+ * the information ethtool needs.  We fall back to calling do_ioctl()
+ * for drivers which haven't been converted to ethtool_ops yet.
+ *
+ * It's GPL, stupid.
+ *
+ * Modification by sfeldma@pobox.com to work as backward compat
+ * solution for pre-ethtool_ops kernels.
+ * 	- copied struct ethtool_ops from ethtool.h
+ * 	- defined SET_ETHTOOL_OPS
+ * 	- put in some #ifndef NETIF_F_xxx wrappers
+ * 	- changes refs to dev->ethtool_ops to ethtool_ops
+ * 	- changed dev_ethtool to ethtool_ioctl
+ *      - remove EXPORT_SYMBOL()s
+ *      - added _kc_ prefix in built-in ethtool_op_xxx ops.
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <asm/uaccess.h>
+
+#include "kcompat.h"
+
+#undef SUPPORTED_10000baseT_Full
+#define SUPPORTED_10000baseT_Full	(1 << 12)
+#undef ADVERTISED_10000baseT_Full
+#define ADVERTISED_10000baseT_Full	(1 << 12)
+#undef SPEED_10000
+#define SPEED_10000		10000
+
+#undef ethtool_ops
+#define ethtool_ops _kc_ethtool_ops
+
+struct _kc_ethtool_ops {
+	int  (*get_settings)(struct net_device *, struct ethtool_cmd *);
+	int  (*set_settings)(struct net_device *, struct ethtool_cmd *);
+	void (*get_drvinfo)(struct net_device *, struct ethtool_drvinfo *);
+	int  (*get_regs_len)(struct net_device *);
+	void (*get_regs)(struct net_device *, struct ethtool_regs *, void *);
+	void (*get_wol)(struct net_device *, struct ethtool_wolinfo *);
+	int  (*set_wol)(struct net_device *, struct ethtool_wolinfo *);
+	u32  (*get_msglevel)(struct net_device *);
+	void (*set_msglevel)(struct net_device *, u32);
+	int  (*nway_reset)(struct net_device *);
+	u32  (*get_link)(struct net_device *);
+	int  (*get_eeprom_len)(struct net_device *);
+	int  (*get_eeprom)(struct net_device *, struct ethtool_eeprom *, u8 *);
+	int  (*set_eeprom)(struct net_device *, struct ethtool_eeprom *, u8 *);
+	int  (*get_coalesce)(struct net_device *, struct ethtool_coalesce *);
+	int  (*set_coalesce)(struct net_device *, struct ethtool_coalesce *);
+	void (*get_ringparam)(struct net_device *, struct ethtool_ringparam *);
+	int  (*set_ringparam)(struct net_device *, struct ethtool_ringparam *);
+	void (*get_pauseparam)(struct net_device *,
+	                       struct ethtool_pauseparam*);
+	int  (*set_pauseparam)(struct net_device *,
+	                       struct ethtool_pauseparam*);
+	u32  (*get_rx_csum)(struct net_device *);
+	int  (*set_rx_csum)(struct net_device *, u32);
+	u32  (*get_tx_csum)(struct net_device *);
+	int  (*set_tx_csum)(struct net_device *, u32);
+	u32  (*get_sg)(struct net_device *);
+	int  (*set_sg)(struct net_device *, u32);
+	u32  (*get_tso)(struct net_device *);
+	int  (*set_tso)(struct net_device *, u32);
+	int  (*self_test_count)(struct net_device *);
+	void (*self_test)(struct net_device *, struct ethtool_test *, u64 *);
+	void (*get_strings)(struct net_device *, u32 stringset, u8 *);
+	int  (*phys_id)(struct net_device *, u32);
+	int  (*get_stats_count)(struct net_device *);
+	void (*get_ethtool_stats)(struct net_device *, struct ethtool_stats *,
+	                          u64 *);
+} *ethtool_ops = NULL;
+
+#undef SET_ETHTOOL_OPS
+#define SET_ETHTOOL_OPS(netdev, ops) (ethtool_ops = (ops))
+
+/*
+ * Some useful ethtool_ops methods that are device independent. If we find that
+ * all drivers want to do the same thing here, we can turn these into dev_()
+ * function calls.
+ */
+
+#undef ethtool_op_get_link
+#define ethtool_op_get_link _kc_ethtool_op_get_link
+u32 _kc_ethtool_op_get_link(struct net_device *dev)
+{
+	return netif_carrier_ok(dev) ? 1 : 0;
+}
+
+#undef ethtool_op_get_tx_csum
+#define ethtool_op_get_tx_csum _kc_ethtool_op_get_tx_csum
+u32 _kc_ethtool_op_get_tx_csum(struct net_device *dev)
+{
+#ifdef NETIF_F_IP_CSUM
+	return (dev->features & NETIF_F_IP_CSUM) != 0;
+#else
+	return 0;
+#endif
+}
+
+#undef ethtool_op_set_tx_csum
+#define ethtool_op_set_tx_csum _kc_ethtool_op_set_tx_csum
+int _kc_ethtool_op_set_tx_csum(struct net_device *dev, u32 data)
+{
+#ifdef NETIF_F_IP_CSUM
+	if (data)
+		dev->features |= NETIF_F_IP_CSUM;
+	else
+		dev->features &= ~NETIF_F_IP_CSUM;
+#endif
+
+	return 0;
+}
+
+#undef ethtool_op_get_sg
+#define ethtool_op_get_sg _kc_ethtool_op_get_sg
+u32 _kc_ethtool_op_get_sg(struct net_device *dev)
+{
+#ifdef NETIF_F_SG
+	return (dev->features & NETIF_F_SG) != 0;
+#else
+	return 0;
+#endif
+}
+
+#undef ethtool_op_set_sg
+#define ethtool_op_set_sg _kc_ethtool_op_set_sg
+int _kc_ethtool_op_set_sg(struct net_device *dev, u32 data)
+{
+#ifdef NETIF_F_SG
+	if (data)
+		dev->features |= NETIF_F_SG;
+	else
+		dev->features &= ~NETIF_F_SG;
+#endif
+
+	return 0;
+}
+
+#undef ethtool_op_get_tso
+#define ethtool_op_get_tso _kc_ethtool_op_get_tso
+u32 _kc_ethtool_op_get_tso(struct net_device *dev)
+{
+#ifdef NETIF_F_TSO
+	return (dev->features & NETIF_F_TSO) != 0;
+#else
+	return 0;
+#endif
+}
+
+#undef ethtool_op_set_tso
+#define ethtool_op_set_tso _kc_ethtool_op_set_tso
+int _kc_ethtool_op_set_tso(struct net_device *dev, u32 data)
+{
+#ifdef NETIF_F_TSO
+	if (data)
+		dev->features |= NETIF_F_TSO;
+	else
+		dev->features &= ~NETIF_F_TSO;
+#endif
+
+	return 0;
+}
+
+/* Handlers for each ethtool command */
+
+static int ethtool_get_settings(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_cmd cmd = { ETHTOOL_GSET };
+	int err;
+
+	if (!ethtool_ops->get_settings)
+		return -EOPNOTSUPP;
+
+	err = ethtool_ops->get_settings(dev, &cmd);
+	if (err < 0)
+		return err;
+
+	if (copy_to_user(useraddr, &cmd, sizeof(cmd)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_settings(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_cmd cmd;
+
+	if (!ethtool_ops->set_settings)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&cmd, useraddr, sizeof(cmd)))
+		return -EFAULT;
+
+	return ethtool_ops->set_settings(dev, &cmd);
+}
+
+static int ethtool_get_drvinfo(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_drvinfo info;
+	struct ethtool_ops *ops = ethtool_ops;
+
+	if (!ops->get_drvinfo)
+		return -EOPNOTSUPP;
+
+	memset(&info, 0, sizeof(info));
+	info.cmd = ETHTOOL_GDRVINFO;
+	ops->get_drvinfo(dev, &info);
+
+	if (ops->self_test_count)
+		info.testinfo_len = ops->self_test_count(dev);
+	if (ops->get_stats_count)
+		info.n_stats = ops->get_stats_count(dev);
+	if (ops->get_regs_len)
+		info.regdump_len = ops->get_regs_len(dev);
+	if (ops->get_eeprom_len)
+		info.eedump_len = ops->get_eeprom_len(dev);
+
+	if (copy_to_user(useraddr, &info, sizeof(info)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_get_regs(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_regs regs;
+	struct ethtool_ops *ops = ethtool_ops;
+	void *regbuf;
+	int reglen, ret;
+
+	if (!ops->get_regs || !ops->get_regs_len)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&regs, useraddr, sizeof(regs)))
+		return -EFAULT;
+
+	reglen = ops->get_regs_len(dev);
+	if (regs.len > reglen)
+		regs.len = reglen;
+
+	regbuf = kmalloc(reglen, GFP_USER);
+	if (!regbuf)
+		return -ENOMEM;
+
+	ops->get_regs(dev, &regs, regbuf);
+
+	ret = -EFAULT;
+	if (copy_to_user(useraddr, &regs, sizeof(regs)))
+		goto out;
+	useraddr += offsetof(struct ethtool_regs, data);
+	if (copy_to_user(useraddr, regbuf, reglen))
+		goto out;
+	ret = 0;
+
+out:
+	kfree(regbuf);
+	return ret;
+}
+
+static int ethtool_get_wol(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_wolinfo wol = { ETHTOOL_GWOL };
+
+	if (!ethtool_ops->get_wol)
+		return -EOPNOTSUPP;
+
+	ethtool_ops->get_wol(dev, &wol);
+
+	if (copy_to_user(useraddr, &wol, sizeof(wol)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_wol(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_wolinfo wol;
+
+	if (!ethtool_ops->set_wol)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&wol, useraddr, sizeof(wol)))
+		return -EFAULT;
+
+	return ethtool_ops->set_wol(dev, &wol);
+}
+
+static int ethtool_get_msglevel(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GMSGLVL };
+
+	if (!ethtool_ops->get_msglevel)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_msglevel(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_msglevel(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata;
+
+	if (!ethtool_ops->set_msglevel)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&edata, useraddr, sizeof(edata)))
+		return -EFAULT;
+
+	ethtool_ops->set_msglevel(dev, edata.data);
+	return 0;
+}
+
+static int ethtool_nway_reset(struct net_device *dev)
+{
+	if (!ethtool_ops->nway_reset)
+		return -EOPNOTSUPP;
+
+	return ethtool_ops->nway_reset(dev);
+}
+
+static int ethtool_get_link(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GLINK };
+
+	if (!ethtool_ops->get_link)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_link(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_get_eeprom(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_eeprom eeprom;
+	struct ethtool_ops *ops = ethtool_ops;
+	u8 *data;
+	int ret;
+
+	if (!ops->get_eeprom || !ops->get_eeprom_len)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&eeprom, useraddr, sizeof(eeprom)))
+		return -EFAULT;
+
+	/* Check for wrap and zero */
+	if (eeprom.offset + eeprom.len <= eeprom.offset)
+		return -EINVAL;
+
+	/* Check for exceeding total eeprom len */
+	if (eeprom.offset + eeprom.len > ops->get_eeprom_len(dev))
+		return -EINVAL;
+
+	data = kmalloc(eeprom.len, GFP_USER);
+	if (!data)
+		return -ENOMEM;
+
+	ret = -EFAULT;
+	if (copy_from_user(data, useraddr + sizeof(eeprom), eeprom.len))
+		goto out;
+
+	ret = ops->get_eeprom(dev, &eeprom, data);
+	if (ret)
+		goto out;
+
+	ret = -EFAULT;
+	if (copy_to_user(useraddr, &eeprom, sizeof(eeprom)))
+		goto out;
+	if (copy_to_user(useraddr + sizeof(eeprom), data, eeprom.len))
+		goto out;
+	ret = 0;
+
+out:
+	kfree(data);
+	return ret;
+}
+
+static int ethtool_set_eeprom(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_eeprom eeprom;
+	struct ethtool_ops *ops = ethtool_ops;
+	u8 *data;
+	int ret;
+
+	if (!ops->set_eeprom || !ops->get_eeprom_len)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&eeprom, useraddr, sizeof(eeprom)))
+		return -EFAULT;
+
+	/* Check for wrap and zero */
+	if (eeprom.offset + eeprom.len <= eeprom.offset)
+		return -EINVAL;
+
+	/* Check for exceeding total eeprom len */
+	if (eeprom.offset + eeprom.len > ops->get_eeprom_len(dev))
+		return -EINVAL;
+
+	data = kmalloc(eeprom.len, GFP_USER);
+	if (!data)
+		return -ENOMEM;
+
+	ret = -EFAULT;
+	if (copy_from_user(data, useraddr + sizeof(eeprom), eeprom.len))
+		goto out;
+
+	ret = ops->set_eeprom(dev, &eeprom, data);
+	if (ret)
+		goto out;
+
+	if (copy_to_user(useraddr + sizeof(eeprom), data, eeprom.len))
+		ret = -EFAULT;
+
+out:
+	kfree(data);
+	return ret;
+}
+
+static int ethtool_get_coalesce(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_coalesce coalesce = { ETHTOOL_GCOALESCE };
+
+	if (!ethtool_ops->get_coalesce)
+		return -EOPNOTSUPP;
+
+	ethtool_ops->get_coalesce(dev, &coalesce);
+
+	if (copy_to_user(useraddr, &coalesce, sizeof(coalesce)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_coalesce(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_coalesce coalesce;
+
+	if (!ethtool_ops->get_coalesce)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&coalesce, useraddr, sizeof(coalesce)))
+		return -EFAULT;
+
+	return ethtool_ops->set_coalesce(dev, &coalesce);
+}
+
+static int ethtool_get_ringparam(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_ringparam ringparam = { ETHTOOL_GRINGPARAM };
+
+	if (!ethtool_ops->get_ringparam)
+		return -EOPNOTSUPP;
+
+	ethtool_ops->get_ringparam(dev, &ringparam);
+
+	if (copy_to_user(useraddr, &ringparam, sizeof(ringparam)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_ringparam(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_ringparam ringparam;
+
+	if (!ethtool_ops->get_ringparam)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&ringparam, useraddr, sizeof(ringparam)))
+		return -EFAULT;
+
+	return ethtool_ops->set_ringparam(dev, &ringparam);
+}
+
+static int ethtool_get_pauseparam(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_pauseparam pauseparam = { ETHTOOL_GPAUSEPARAM };
+
+	if (!ethtool_ops->get_pauseparam)
+		return -EOPNOTSUPP;
+
+	ethtool_ops->get_pauseparam(dev, &pauseparam);
+
+	if (copy_to_user(useraddr, &pauseparam, sizeof(pauseparam)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_pauseparam(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_pauseparam pauseparam;
+
+	if (!ethtool_ops->get_pauseparam)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&pauseparam, useraddr, sizeof(pauseparam)))
+		return -EFAULT;
+
+	return ethtool_ops->set_pauseparam(dev, &pauseparam);
+}
+
+static int ethtool_get_rx_csum(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GRXCSUM };
+
+	if (!ethtool_ops->get_rx_csum)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_rx_csum(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_rx_csum(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata;
+
+	if (!ethtool_ops->set_rx_csum)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&edata, useraddr, sizeof(edata)))
+		return -EFAULT;
+
+	ethtool_ops->set_rx_csum(dev, edata.data);
+	return 0;
+}
+
+static int ethtool_get_tx_csum(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GTXCSUM };
+
+	if (!ethtool_ops->get_tx_csum)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_tx_csum(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_tx_csum(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata;
+
+	if (!ethtool_ops->set_tx_csum)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&edata, useraddr, sizeof(edata)))
+		return -EFAULT;
+
+	return ethtool_ops->set_tx_csum(dev, edata.data);
+}
+
+static int ethtool_get_sg(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GSG };
+
+	if (!ethtool_ops->get_sg)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_sg(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_sg(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata;
+
+	if (!ethtool_ops->set_sg)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&edata, useraddr, sizeof(edata)))
+		return -EFAULT;
+
+	return ethtool_ops->set_sg(dev, edata.data);
+}
+
+static int ethtool_get_tso(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GTSO };
+
+	if (!ethtool_ops->get_tso)
+		return -EOPNOTSUPP;
+
+	edata.data = ethtool_ops->get_tso(dev);
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+static int ethtool_set_tso(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata;
+
+	if (!ethtool_ops->set_tso)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&edata, useraddr, sizeof(edata)))
+		return -EFAULT;
+
+	return ethtool_ops->set_tso(dev, edata.data);
+}
+
+static int ethtool_self_test(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_test test;
+	struct ethtool_ops *ops = ethtool_ops;
+	u64 *data;
+	int ret;
+
+	if (!ops->self_test || !ops->self_test_count)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&test, useraddr, sizeof(test)))
+		return -EFAULT;
+
+	test.len = ops->self_test_count(dev);
+	data = kmalloc(test.len * sizeof(u64), GFP_USER);
+	if (!data)
+		return -ENOMEM;
+
+	ops->self_test(dev, &test, data);
+
+	ret = -EFAULT;
+	if (copy_to_user(useraddr, &test, sizeof(test)))
+		goto out;
+	useraddr += sizeof(test);
+	if (copy_to_user(useraddr, data, test.len * sizeof(u64)))
+		goto out;
+	ret = 0;
+
+out:
+	kfree(data);
+	return ret;
+}
+
+static int ethtool_get_strings(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_gstrings gstrings;
+	struct ethtool_ops *ops = ethtool_ops;
+	u8 *data;
+	int ret;
+
+	if (!ops->get_strings)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&gstrings, useraddr, sizeof(gstrings)))
+		return -EFAULT;
+
+	switch (gstrings.string_set) {
+	case ETH_SS_TEST:
+		if (!ops->self_test_count)
+			return -EOPNOTSUPP;
+		gstrings.len = ops->self_test_count(dev);
+		break;
+	case ETH_SS_STATS:
+		if (!ops->get_stats_count)
+			return -EOPNOTSUPP;
+		gstrings.len = ops->get_stats_count(dev);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	data = kmalloc(gstrings.len * ETH_GSTRING_LEN, GFP_USER);
+	if (!data)
+		return -ENOMEM;
+
+	ops->get_strings(dev, gstrings.string_set, data);
+
+	ret = -EFAULT;
+	if (copy_to_user(useraddr, &gstrings, sizeof(gstrings)))
+		goto out;
+	useraddr += sizeof(gstrings);
+	if (copy_to_user(useraddr, data, gstrings.len * ETH_GSTRING_LEN))
+		goto out;
+	ret = 0;
+
+out:
+	kfree(data);
+	return ret;
+}
+
+static int ethtool_phys_id(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_value id;
+
+	if (!ethtool_ops->phys_id)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&id, useraddr, sizeof(id)))
+		return -EFAULT;
+
+	return ethtool_ops->phys_id(dev, id.data);
+}
+
+static int ethtool_get_stats(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_stats stats;
+	struct ethtool_ops *ops = ethtool_ops;
+	u64 *data;
+	int ret;
+
+	if (!ops->get_ethtool_stats || !ops->get_stats_count)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&stats, useraddr, sizeof(stats)))
+		return -EFAULT;
+
+	stats.n_stats = ops->get_stats_count(dev);
+	data = kmalloc(stats.n_stats * sizeof(u64), GFP_USER);
+	if (!data)
+		return -ENOMEM;
+
+	ops->get_ethtool_stats(dev, &stats, data);
+
+	ret = -EFAULT;
+	if (copy_to_user(useraddr, &stats, sizeof(stats)))
+		goto out;
+	useraddr += sizeof(stats);
+	if (copy_to_user(useraddr, data, stats.n_stats * sizeof(u64)))
+		goto out;
+	ret = 0;
+
+out:
+	kfree(data);
+	return ret;
+}
+
+/* The main entry point in this file.  Called from net/core/dev.c */
+
+#define ETHTOOL_OPS_COMPAT
+int ethtool_ioctl(struct ifreq *ifr)
+{
+	struct net_device *dev = __dev_get_by_name(ifr->ifr_name);
+	void *useraddr = (void *) ifr->ifr_data;
+	u32 ethcmd;
+
+	/*
+	 * XXX: This can be pushed down into the ethtool_* handlers that
+	 * need it.  Keep existing behaviour for the moment.
+	 */
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (!dev || !netif_device_present(dev))
+		return -ENODEV;
+
+	if (copy_from_user(&ethcmd, useraddr, sizeof (ethcmd)))
+		return -EFAULT;
+
+	switch (ethcmd) {
+	case ETHTOOL_GSET:
+		return ethtool_get_settings(dev, useraddr);
+	case ETHTOOL_SSET:
+		return ethtool_set_settings(dev, useraddr);
+	case ETHTOOL_GDRVINFO:
+		return ethtool_get_drvinfo(dev, useraddr);
+	case ETHTOOL_GREGS:
+		return ethtool_get_regs(dev, useraddr);
+	case ETHTOOL_GWOL:
+		return ethtool_get_wol(dev, useraddr);
+	case ETHTOOL_SWOL:
+		return ethtool_set_wol(dev, useraddr);
+	case ETHTOOL_GMSGLVL:
+		return ethtool_get_msglevel(dev, useraddr);
+	case ETHTOOL_SMSGLVL:
+		return ethtool_set_msglevel(dev, useraddr);
+	case ETHTOOL_NWAY_RST:
+		return ethtool_nway_reset(dev);
+	case ETHTOOL_GLINK:
+		return ethtool_get_link(dev, useraddr);
+	case ETHTOOL_GEEPROM:
+		return ethtool_get_eeprom(dev, useraddr);
+	case ETHTOOL_SEEPROM:
+		return ethtool_set_eeprom(dev, useraddr);
+	case ETHTOOL_GCOALESCE:
+		return ethtool_get_coalesce(dev, useraddr);
+	case ETHTOOL_SCOALESCE:
+		return ethtool_set_coalesce(dev, useraddr);
+	case ETHTOOL_GRINGPARAM:
+		return ethtool_get_ringparam(dev, useraddr);
+	case ETHTOOL_SRINGPARAM:
+		return ethtool_set_ringparam(dev, useraddr);
+	case ETHTOOL_GPAUSEPARAM:
+		return ethtool_get_pauseparam(dev, useraddr);
+	case ETHTOOL_SPAUSEPARAM:
+		return ethtool_set_pauseparam(dev, useraddr);
+	case ETHTOOL_GRXCSUM:
+		return ethtool_get_rx_csum(dev, useraddr);
+	case ETHTOOL_SRXCSUM:
+		return ethtool_set_rx_csum(dev, useraddr);
+	case ETHTOOL_GTXCSUM:
+		return ethtool_get_tx_csum(dev, useraddr);
+	case ETHTOOL_STXCSUM:
+		return ethtool_set_tx_csum(dev, useraddr);
+	case ETHTOOL_GSG:
+		return ethtool_get_sg(dev, useraddr);
+	case ETHTOOL_SSG:
+		return ethtool_set_sg(dev, useraddr);
+	case ETHTOOL_GTSO:
+		return ethtool_get_tso(dev, useraddr);
+	case ETHTOOL_STSO:
+		return ethtool_set_tso(dev, useraddr);
+	case ETHTOOL_TEST:
+		return ethtool_self_test(dev, useraddr);
+	case ETHTOOL_GSTRINGS:
+		return ethtool_get_strings(dev, useraddr);
+	case ETHTOOL_PHYS_ID:
+		return ethtool_phys_id(dev, useraddr);
+	case ETHTOOL_GSTATS:
+		return ethtool_get_stats(dev, useraddr);
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+#define mii_if_info _kc_mii_if_info
+struct _kc_mii_if_info {
+	int phy_id;
+	int advertising;
+	int phy_id_mask;
+	int reg_num_mask;
+
+	unsigned int full_duplex : 1;	/* is full duplex? */
+	unsigned int force_media : 1;	/* is autoneg. disabled? */
+
+	struct net_device *dev;
+	int (*mdio_read) (struct net_device *dev, int phy_id, int location);
+	void (*mdio_write) (struct net_device *dev, int phy_id, int location, int val);
+};
+
+struct ethtool_cmd;
+struct mii_ioctl_data;
+
+#undef mii_link_ok
+#define mii_link_ok _kc_mii_link_ok
+#undef mii_nway_restart
+#define mii_nway_restart _kc_mii_nway_restart
+#undef mii_ethtool_gset
+#define mii_ethtool_gset _kc_mii_ethtool_gset
+#undef mii_ethtool_sset
+#define mii_ethtool_sset _kc_mii_ethtool_sset
+#undef mii_check_link
+#define mii_check_link _kc_mii_check_link
+#undef generic_mii_ioctl
+#define generic_mii_ioctl _kc_generic_mii_ioctl
+extern int _kc_mii_link_ok (struct mii_if_info *mii);
+extern int _kc_mii_nway_restart (struct mii_if_info *mii);
+extern int _kc_mii_ethtool_gset(struct mii_if_info *mii,
+                                struct ethtool_cmd *ecmd);
+extern int _kc_mii_ethtool_sset(struct mii_if_info *mii,
+                                struct ethtool_cmd *ecmd);
+extern void _kc_mii_check_link (struct mii_if_info *mii);
+extern int _kc_generic_mii_ioctl(struct mii_if_info *mii_if,
+                                 struct mii_ioctl_data *mii_data, int cmd,
+                                 unsigned int *duplex_changed);
+
+
+struct _kc_pci_dev_ext {
+	struct pci_dev *dev;
+	void *pci_drvdata;
+	struct pci_driver *driver;
+};
+
+struct _kc_net_dev_ext {
+	struct net_device *dev;
+	unsigned int carrier;
+};
+
+
+/**************************************/
+/* mii support */
+
+int _kc_mii_ethtool_gset(struct mii_if_info *mii, struct ethtool_cmd *ecmd)
+{
+	struct net_device *dev = mii->dev;
+	u32 advert, bmcr, lpa, nego;
+
+	ecmd->supported =
+	    (SUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full |
+	     SUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full |
+	     SUPPORTED_Autoneg | SUPPORTED_TP | SUPPORTED_MII);
+
+	/* only supports twisted-pair */
+	ecmd->port = PORT_MII;
+
+	/* only supports internal transceiver */
+	ecmd->transceiver = XCVR_INTERNAL;
+
+	/* this isn't fully supported at higher layers */
+	ecmd->phy_address = mii->phy_id;
+
+	ecmd->advertising = ADVERTISED_TP | ADVERTISED_MII;
+	advert = mii->mdio_read(dev, mii->phy_id, MII_ADVERTISE);
+	if (advert & ADVERTISE_10HALF)
+		ecmd->advertising |= ADVERTISED_10baseT_Half;
+	if (advert & ADVERTISE_10FULL)
+		ecmd->advertising |= ADVERTISED_10baseT_Full;
+	if (advert & ADVERTISE_100HALF)
+		ecmd->advertising |= ADVERTISED_100baseT_Half;
+	if (advert & ADVERTISE_100FULL)
+		ecmd->advertising |= ADVERTISED_100baseT_Full;
+
+	bmcr = mii->mdio_read(dev, mii->phy_id, MII_BMCR);
+	lpa = mii->mdio_read(dev, mii->phy_id, MII_LPA);
+	if (bmcr & BMCR_ANENABLE) {
+		ecmd->advertising |= ADVERTISED_Autoneg;
+		ecmd->autoneg = AUTONEG_ENABLE;
+		
+		nego = mii_nway_result(advert & lpa);
+		if (nego == LPA_100FULL || nego == LPA_100HALF)
+			ecmd->speed = SPEED_100;
+		else
+			ecmd->speed = SPEED_10;
+		if (nego == LPA_100FULL || nego == LPA_10FULL) {
+			ecmd->duplex = DUPLEX_FULL;
+			mii->full_duplex = 1;
+		} else {
+			ecmd->duplex = DUPLEX_HALF;
+			mii->full_duplex = 0;
+		}
+	} else {
+		ecmd->autoneg = AUTONEG_DISABLE;
+
+		ecmd->speed = (bmcr & BMCR_SPEED100) ? SPEED_100 : SPEED_10;
+		ecmd->duplex = (bmcr & BMCR_FULLDPLX) ? DUPLEX_FULL : DUPLEX_HALF;
+	}
+
+	/* ignore maxtxpkt, maxrxpkt for now */
+
+	return 0;
+}
+
+int _kc_mii_ethtool_sset(struct mii_if_info *mii, struct ethtool_cmd *ecmd)
+{
+	struct net_device *dev = mii->dev;
+
+	if (ecmd->speed != SPEED_10 && ecmd->speed != SPEED_100)
+		return -EINVAL;
+	if (ecmd->duplex != DUPLEX_HALF && ecmd->duplex != DUPLEX_FULL)
+		return -EINVAL;
+	if (ecmd->port != PORT_MII)
+		return -EINVAL;
+	if (ecmd->transceiver != XCVR_INTERNAL)
+		return -EINVAL;
+	if (ecmd->phy_address != mii->phy_id)
+		return -EINVAL;
+	if (ecmd->autoneg != AUTONEG_DISABLE && ecmd->autoneg != AUTONEG_ENABLE)
+		return -EINVAL;
+				  
+	/* ignore supported, maxtxpkt, maxrxpkt */
+	
+	if (ecmd->autoneg == AUTONEG_ENABLE) {
+		u32 bmcr, advert, tmp;
+
+		if ((ecmd->advertising & (ADVERTISED_10baseT_Half |
+					  ADVERTISED_10baseT_Full |
+					  ADVERTISED_100baseT_Half |
+					  ADVERTISED_100baseT_Full)) == 0)
+			return -EINVAL;
+
+		/* advertise only what has been requested */
+		advert = mii->mdio_read(dev, mii->phy_id, MII_ADVERTISE);
+		tmp = advert & ~(ADVERTISE_ALL | ADVERTISE_100BASE4);
+		if (ADVERTISED_10baseT_Half)
+			tmp |= ADVERTISE_10HALF;
+		if (ADVERTISED_10baseT_Full)
+			tmp |= ADVERTISE_10FULL;
+		if (ADVERTISED_100baseT_Half)
+			tmp |= ADVERTISE_100HALF;
+		if (ADVERTISED_100baseT_Full)
+			tmp |= ADVERTISE_100FULL;
+		if (advert != tmp) {
+			mii->mdio_write(dev, mii->phy_id, MII_ADVERTISE, tmp);
+			mii->advertising = tmp;
+		}
+		
+		/* turn on autonegotiation, and force a renegotiate */
+		bmcr = mii->mdio_read(dev, mii->phy_id, MII_BMCR);
+		bmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);
+		mii->mdio_write(dev, mii->phy_id, MII_BMCR, bmcr);
+
+		mii->force_media = 0;
+	} else {
+		u32 bmcr, tmp;
+
+		/* turn off auto negotiation, set speed and duplexity */
+		bmcr = mii->mdio_read(dev, mii->phy_id, MII_BMCR);
+		tmp = bmcr & ~(BMCR_ANENABLE | BMCR_SPEED100 | BMCR_FULLDPLX);
+		if (ecmd->speed == SPEED_100)
+			tmp |= BMCR_SPEED100;
+		if (ecmd->duplex == DUPLEX_FULL) {
+			tmp |= BMCR_FULLDPLX;
+			mii->full_duplex = 1;
+		} else
+			mii->full_duplex = 0;
+		if (bmcr != tmp)
+			mii->mdio_write(dev, mii->phy_id, MII_BMCR, tmp);
+
+		mii->force_media = 1;
+	}
+	return 0;
+}
+
+int _kc_mii_link_ok (struct mii_if_info *mii)
+{
+	/* first, a dummy read, needed to latch some MII phys */
+	mii->mdio_read(mii->dev, mii->phy_id, MII_BMSR);
+	if (mii->mdio_read(mii->dev, mii->phy_id, MII_BMSR) & BMSR_LSTATUS)
+		return 1;
+	return 0;
+}
+
+int _kc_mii_nway_restart (struct mii_if_info *mii)
+{
+	int bmcr;
+	int r = -EINVAL;
+
+	/* if autoneg is off, it's an error */
+	bmcr = mii->mdio_read(mii->dev, mii->phy_id, MII_BMCR);
+
+	if (bmcr & BMCR_ANENABLE) {
+		bmcr |= BMCR_ANRESTART;
+		mii->mdio_write(mii->dev, mii->phy_id, MII_BMCR, bmcr);
+		r = 0;
+	}
+
+	return r;
+}
+
+void _kc_mii_check_link (struct mii_if_info *mii)
+{
+	int cur_link = mii_link_ok(mii);
+	int prev_link = netif_carrier_ok(mii->dev);
+
+	if (cur_link && !prev_link)
+		netif_carrier_on(mii->dev);
+	else if (prev_link && !cur_link)
+		netif_carrier_off(mii->dev);
+}
+
+int _kc_generic_mii_ioctl(struct mii_if_info *mii_if,
+                          struct mii_ioctl_data *mii_data, int cmd,
+                          unsigned int *duplex_chg_out)
+{
+	int rc = 0;
+	unsigned int duplex_changed = 0;
+
+	if (duplex_chg_out)
+		*duplex_chg_out = 0;
+
+	mii_data->phy_id &= mii_if->phy_id_mask;
+	mii_data->reg_num &= mii_if->reg_num_mask;
+
+	switch(cmd) {
+	case SIOCDEVPRIVATE:	/* binary compat, remove in 2.5 */
+	case SIOCGMIIPHY:
+		mii_data->phy_id = mii_if->phy_id;
+		/* fall through */
+
+	case SIOCDEVPRIVATE + 1:/* binary compat, remove in 2.5 */
+	case SIOCGMIIREG:
+		mii_data->val_out =
+			mii_if->mdio_read(mii_if->dev, mii_data->phy_id,
+					  mii_data->reg_num);
+		break;
+
+	case SIOCDEVPRIVATE + 2:/* binary compat, remove in 2.5 */
+	case SIOCSMIIREG: {
+		u16 val = mii_data->val_in;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+
+		if (mii_data->phy_id == mii_if->phy_id) {
+			switch(mii_data->reg_num) {
+			case MII_BMCR: {
+				unsigned int new_duplex = 0;
+				if (val & (BMCR_RESET|BMCR_ANENABLE))
+					mii_if->force_media = 0;
+				else
+					mii_if->force_media = 1;
+				if (mii_if->force_media &&
+				    (val & BMCR_FULLDPLX))
+					new_duplex = 1;
+				if (mii_if->full_duplex != new_duplex) {
+					duplex_changed = 1;
+					mii_if->full_duplex = new_duplex;
+				}
+				break;
+			}
+			case MII_ADVERTISE:
+				mii_if->advertising = val;
+				break;
+			default:
+				/* do nothing */
+				break;
+			}
+		}
+
+		mii_if->mdio_write(mii_if->dev, mii_data->phy_id,
+				   mii_data->reg_num, val);
+		break;
+	}
+
+	default:
+		rc = -EOPNOTSUPP;
+		break;
+	}
+
+	if ((rc == 0) && (duplex_chg_out) && (duplex_changed))
+		*duplex_chg_out = 1;
+
+	return rc;
+}
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/kcompat.h linux-2.6.9/drivers/net/e1000/kcompat.h
--- linux-2.6.9.src/drivers/net/e1000/kcompat.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.9/drivers/net/e1000/kcompat.h	2007-07-16 13:33:15.000000000 +0200
@@ -0,0 +1,1245 @@
+/*******************************************************************************
+
+  Intel PRO/1000 Linux driver
+  Copyright(c) 1999 - 2007 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#ifndef _KCOMPAT_H_
+#define _KCOMPAT_H_
+
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/ioport.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/udp.h>
+#include <linux/mii.h>
+#include <asm/io.h>
+
+/* NAPI enable/disable flags here */
+
+#ifdef _E1000_H_
+#ifdef CONFIG_E1000_NAPI
+#define NAPI
+#endif
+#ifdef E1000_NAPI
+#undef NAPI
+#define NAPI
+#endif
+#ifdef E1000_NO_NAPI
+#undef NAPI
+#endif
+#endif
+
+#ifdef _IGB_H_
+#define NAPI
+#endif
+
+#ifdef _IXGB_H_
+#ifdef CONFIG_IXGB_NAPI
+#define NAPI
+#endif
+#ifdef IXGB_NAPI
+#undef NAPI
+#define NAPI
+#endif
+#ifdef IXGB_NO_NAPI
+#undef NAPI
+#endif
+#endif
+
+
+/* and finally set defines so that the code sees the changes */
+#ifdef NAPI
+#ifndef CONFIG_E1000_NAPI
+#define CONFIG_E1000_NAPI
+#endif
+#ifndef CONFIG_IXGB_NAPI
+#define CONFIG_IXGB_NAPI
+#endif
+#else
+#undef CONFIG_E1000_NAPI
+#undef CONFIG_IXGB_NAPI
+#endif
+
+/* packet split disable/enable */
+#ifdef DISABLE_PACKET_SPLIT
+#undef CONFIG_E1000_DISABLE_PACKET_SPLIT
+#define CONFIG_E1000_DISABLE_PACKET_SPLIT
+#endif
+
+/* general compatibility flags unclassified per kernel */
+#ifdef DISABLE_PCI_MSI
+#undef CONFIG_PCI_MSI
+#endif
+
+#ifdef DISABLE_PM
+#undef CONFIG_PM
+#endif
+
+#ifdef DISABLE_NET_POLL_CONTROLLER
+#undef CONFIG_NET_POLL_CONTROLLER
+#endif
+
+#ifndef PMSG_SUSPEND
+#define PMSG_SUSPEND 3
+#endif
+
+#ifndef module_param
+#define module_param(v,t,p) MODULE_PARM(v, "i");
+#endif
+
+#ifndef DMA_64BIT_MASK
+#define DMA_64BIT_MASK  0xffffffffffffffffULL
+#endif
+
+#ifndef DMA_32BIT_MASK
+#define DMA_32BIT_MASK  0x00000000ffffffffULL
+#endif
+
+#ifndef PCI_CAP_ID_EXP
+#define PCI_CAP_ID_EXP 0x10
+#endif
+
+#ifndef mmiowb
+#ifdef CONFIG_IA64
+#define mmiowb() asm volatile ("mf.a" ::: "memory")
+#else
+#define mmiowb()
+#endif
+#endif
+
+#ifndef IRQ_HANDLED
+#define irqreturn_t void
+#define IRQ_HANDLED
+#define IRQ_NONE
+#endif
+
+#ifndef SET_NETDEV_DEV
+#define SET_NETDEV_DEV(net, pdev)
+#endif
+
+#ifndef HAVE_FREE_NETDEV
+#define free_netdev(x)	kfree(x)
+#endif
+
+#ifdef HAVE_POLL_CONTROLLER
+#define CONFIG_NET_POLL_CONTROLLER
+#endif
+
+#ifndef NETDEV_TX_OK
+#define NETDEV_TX_OK 0
+#endif
+
+#ifndef NETDEV_TX_BUSY
+#define NETDEV_TX_BUSY 1
+#endif
+
+#ifndef NETDEV_TX_LOCKED
+#define NETDEV_TX_LOCKED -1
+#endif
+
+#ifndef SKB_DATAREF_SHIFT
+/* if we do not have the infrastructure to detect if skb_header is cloned
+   just return false in all cases */
+#define skb_header_cloned(x) 0
+#endif
+
+#ifndef NETIF_F_GSO
+#define gso_size tso_size
+#define gso_segs tso_segs
+#endif
+
+#ifndef CHECKSUM_PARTIAL
+#define CHECKSUM_PARTIAL CHECKSUM_HW
+#define CHECKSUM_COMPLETE CHECKSUM_HW
+#endif
+
+#ifndef __read_mostly
+#define __read_mostly
+#endif
+
+#ifndef HAVE_NETIF_MSG
+#define HAVE_NETIF_MSG 1
+enum {
+	NETIF_MSG_DRV		= 0x0001,
+	NETIF_MSG_PROBE		= 0x0002,
+	NETIF_MSG_LINK		= 0x0004,
+	NETIF_MSG_TIMER		= 0x0008,
+	NETIF_MSG_IFDOWN	= 0x0010,
+	NETIF_MSG_IFUP		= 0x0020,
+	NETIF_MSG_RX_ERR	= 0x0040,
+	NETIF_MSG_TX_ERR	= 0x0080,
+	NETIF_MSG_TX_QUEUED	= 0x0100,
+	NETIF_MSG_INTR		= 0x0200,
+	NETIF_MSG_TX_DONE	= 0x0400,
+	NETIF_MSG_RX_STATUS	= 0x0800,
+	NETIF_MSG_PKTDATA	= 0x1000,
+	NETIF_MSG_HW		= 0x2000,
+	NETIF_MSG_WOL		= 0x4000,
+};
+
+#else
+#define NETIF_MSG_HW	0x2000
+#define NETIF_MSG_WOL	0x4000
+#endif /* HAVE_NETIF_MSG */
+
+#ifndef MII_RESV1
+#define MII_RESV1		0x17		/* Reserved...		*/
+#endif
+
+#ifndef unlikely
+#define unlikely(_x) _x
+#define likely(_x) _x
+#endif
+
+#ifndef WARN_ON
+#define WARN_ON(x)
+#endif
+
+#ifndef PCI_DEVICE
+#define PCI_DEVICE(vend,dev) \
+	.vendor = (vend), .device = (dev), \
+	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID
+#endif
+
+#ifndef num_online_cpus
+#define num_online_cpus() smp_num_cpus
+#endif
+
+#ifndef _LINUX_RANDOM_H
+#include <linux/random.h>
+#endif
+
+/*****************************************************************************/
+/* Installations with ethtool version without eeprom, adapter id, or statistics
+ * support */
+
+#ifndef ETH_GSTRING_LEN
+#define ETH_GSTRING_LEN 32
+#endif
+
+#ifndef ETHTOOL_GSTATS
+#define ETHTOOL_GSTATS 0x1d
+#undef ethtool_drvinfo
+#define ethtool_drvinfo k_ethtool_drvinfo
+struct k_ethtool_drvinfo {
+	u32 cmd;
+	char driver[32];
+	char version[32];
+	char fw_version[32];
+	char bus_info[32];
+	char reserved1[32];
+	char reserved2[16];
+	u32 n_stats;
+	u32 testinfo_len;
+	u32 eedump_len;
+	u32 regdump_len;
+};
+
+struct ethtool_stats {
+	u32 cmd;
+	u32 n_stats;
+	u64 data[0];
+};
+#endif /* ETHTOOL_GSTATS */
+
+#ifndef ETHTOOL_PHYS_ID
+#define ETHTOOL_PHYS_ID 0x1c
+#endif /* ETHTOOL_PHYS_ID */
+
+#ifndef ETHTOOL_GSTRINGS
+#define ETHTOOL_GSTRINGS 0x1b
+enum ethtool_stringset {
+	ETH_SS_TEST             = 0,
+	ETH_SS_STATS,
+};
+struct ethtool_gstrings {
+	u32 cmd;            /* ETHTOOL_GSTRINGS */
+	u32 string_set;     /* string set id e.c. ETH_SS_TEST, etc*/
+	u32 len;            /* number of strings in the string set */
+	u8 data[0];
+};
+#endif /* ETHTOOL_GSTRINGS */
+
+#ifndef ETHTOOL_TEST
+#define ETHTOOL_TEST 0x1a
+enum ethtool_test_flags {
+	ETH_TEST_FL_OFFLINE	= (1 << 0),
+	ETH_TEST_FL_FAILED	= (1 << 1),
+};
+struct ethtool_test {
+	u32 cmd;
+	u32 flags;
+	u32 reserved;
+	u32 len;
+	u64 data[0];
+};
+#endif /* ETHTOOL_TEST */
+
+#ifndef ETHTOOL_GEEPROM
+#define ETHTOOL_GEEPROM 0xb
+#undef ETHTOOL_GREGS
+struct ethtool_eeprom {
+	u32 cmd;
+	u32 magic;
+	u32 offset;
+	u32 len;
+	u8 data[0];
+};
+
+struct ethtool_value {
+	u32 cmd;
+	u32 data;
+};
+#endif /* ETHTOOL_GEEPROM */
+
+#ifndef ETHTOOL_GLINK
+#define ETHTOOL_GLINK 0xa
+#endif /* ETHTOOL_GLINK */
+
+#ifndef ETHTOOL_GREGS
+#define ETHTOOL_GREGS		0x00000004 /* Get NIC registers */
+#define ethtool_regs _kc_ethtool_regs
+/* for passing big chunks of data */
+struct _kc_ethtool_regs {
+	u32 cmd;
+	u32 version; /* driver-specific, indicates different chips/revs */
+	u32 len; /* bytes */
+	u8 data[0];
+};
+#endif /* ETHTOOL_GREGS */
+
+#ifndef ETHTOOL_GMSGLVL
+#define ETHTOOL_GMSGLVL		0x00000007 /* Get driver message level */
+#endif
+#ifndef ETHTOOL_SMSGLVL
+#define ETHTOOL_SMSGLVL		0x00000008 /* Set driver msg level, priv. */
+#endif
+#ifndef ETHTOOL_NWAY_RST
+#define ETHTOOL_NWAY_RST	0x00000009 /* Restart autonegotiation, priv */
+#endif
+#ifndef ETHTOOL_GLINK
+#define ETHTOOL_GLINK		0x0000000a /* Get link status */
+#endif
+#ifndef ETHTOOL_GEEPROM
+#define ETHTOOL_GEEPROM		0x0000000b /* Get EEPROM data */
+#endif
+#ifndef ETHTOOL_SEEPROM
+#define ETHTOOL_SEEPROM		0x0000000c /* Set EEPROM data */
+#endif
+#ifndef ETHTOOL_GCOALESCE
+#define ETHTOOL_GCOALESCE	0x0000000e /* Get coalesce config */
+/* for configuring coalescing parameters of chip */
+#define ethtool_coalesce _kc_ethtool_coalesce
+struct _kc_ethtool_coalesce {
+	u32	cmd;	/* ETHTOOL_{G,S}COALESCE */
+
+	/* How many usecs to delay an RX interrupt after
+	 * a packet arrives.  If 0, only rx_max_coalesced_frames
+	 * is used.
+	 */
+	u32	rx_coalesce_usecs;
+
+	/* How many packets to delay an RX interrupt after
+	 * a packet arrives.  If 0, only rx_coalesce_usecs is
+	 * used.  It is illegal to set both usecs and max frames
+	 * to zero as this would cause RX interrupts to never be
+	 * generated.
+	 */
+	u32	rx_max_coalesced_frames;
+
+	/* Same as above two parameters, except that these values
+	 * apply while an IRQ is being serviced by the host.  Not
+	 * all cards support this feature and the values are ignored
+	 * in that case.
+	 */
+	u32	rx_coalesce_usecs_irq;
+	u32	rx_max_coalesced_frames_irq;
+
+	/* How many usecs to delay a TX interrupt after
+	 * a packet is sent.  If 0, only tx_max_coalesced_frames
+	 * is used.
+	 */
+	u32	tx_coalesce_usecs;
+
+	/* How many packets to delay a TX interrupt after
+	 * a packet is sent.  If 0, only tx_coalesce_usecs is
+	 * used.  It is illegal to set both usecs and max frames
+	 * to zero as this would cause TX interrupts to never be
+	 * generated.
+	 */
+	u32	tx_max_coalesced_frames;
+
+	/* Same as above two parameters, except that these values
+	 * apply while an IRQ is being serviced by the host.  Not
+	 * all cards support this feature and the values are ignored
+	 * in that case.
+	 */
+	u32	tx_coalesce_usecs_irq;
+	u32	tx_max_coalesced_frames_irq;
+
+	/* How many usecs to delay in-memory statistics
+	 * block updates.  Some drivers do not have an in-memory
+	 * statistic block, and in such cases this value is ignored.
+	 * This value must not be zero.
+	 */
+	u32	stats_block_coalesce_usecs;
+
+	/* Adaptive RX/TX coalescing is an algorithm implemented by
+	 * some drivers to improve latency under low packet rates and
+	 * improve throughput under high packet rates.  Some drivers
+	 * only implement one of RX or TX adaptive coalescing.  Anything
+	 * not implemented by the driver causes these values to be
+	 * silently ignored.
+	 */
+	u32	use_adaptive_rx_coalesce;
+	u32	use_adaptive_tx_coalesce;
+
+	/* When the packet rate (measured in packets per second)
+	 * is below pkt_rate_low, the {rx,tx}_*_low parameters are
+	 * used.
+	 */
+	u32	pkt_rate_low;
+	u32	rx_coalesce_usecs_low;
+	u32	rx_max_coalesced_frames_low;
+	u32	tx_coalesce_usecs_low;
+	u32	tx_max_coalesced_frames_low;
+
+	/* When the packet rate is below pkt_rate_high but above
+	 * pkt_rate_low (both measured in packets per second) the
+	 * normal {rx,tx}_* coalescing parameters are used.
+	 */
+
+	/* When the packet rate is (measured in packets per second)
+	 * is above pkt_rate_high, the {rx,tx}_*_high parameters are
+	 * used.
+	 */
+	u32	pkt_rate_high;
+	u32	rx_coalesce_usecs_high;
+	u32	rx_max_coalesced_frames_high;
+	u32	tx_coalesce_usecs_high;
+	u32	tx_max_coalesced_frames_high;
+
+	/* How often to do adaptive coalescing packet rate sampling,
+	 * measured in seconds.  Must not be zero.
+	 */
+	u32	rate_sample_interval;
+};
+#endif /* ETHTOOL_GCOALESCE */
+
+#ifndef ETHTOOL_SCOALESCE
+#define ETHTOOL_SCOALESCE	0x0000000f /* Set coalesce config. */
+#endif
+#ifndef ETHTOOL_GRINGPARAM
+#define ETHTOOL_GRINGPARAM	0x00000010 /* Get ring parameters */
+/* for configuring RX/TX ring parameters */
+#define ethtool_ringparam _kc_ethtool_ringparam
+struct _kc_ethtool_ringparam {
+	u32	cmd;	/* ETHTOOL_{G,S}RINGPARAM */
+
+	/* Read only attributes.  These indicate the maximum number
+	 * of pending RX/TX ring entries the driver will allow the
+	 * user to set.
+	 */
+	u32	rx_max_pending;
+	u32	rx_mini_max_pending;
+	u32	rx_jumbo_max_pending;
+	u32	tx_max_pending;
+
+	/* Values changeable by the user.  The valid values are
+	 * in the range 1 to the "*_max_pending" counterpart above.
+	 */
+	u32	rx_pending;
+	u32	rx_mini_pending;
+	u32	rx_jumbo_pending;
+	u32	tx_pending;
+};
+#endif /* ETHTOOL_GRINGPARAM */
+
+#ifndef ETHTOOL_SRINGPARAM
+#define ETHTOOL_SRINGPARAM	0x00000011 /* Set ring parameters, priv. */
+#endif
+#ifndef ETHTOOL_GPAUSEPARAM
+#define ETHTOOL_GPAUSEPARAM	0x00000012 /* Get pause parameters */
+/* for configuring link flow control parameters */
+#define ethtool_pauseparam _kc_ethtool_pauseparam
+struct _kc_ethtool_pauseparam {
+	u32	cmd;	/* ETHTOOL_{G,S}PAUSEPARAM */
+
+	/* If the link is being auto-negotiated (via ethtool_cmd.autoneg
+	 * being true) the user may set 'autonet' here non-zero to have the
+	 * pause parameters be auto-negotiated too.  In such a case, the
+	 * {rx,tx}_pause values below determine what capabilities are
+	 * advertised.
+	 *
+	 * If 'autoneg' is zero or the link is not being auto-negotiated,
+	 * then {rx,tx}_pause force the driver to use/not-use pause
+	 * flow control.
+	 */
+	u32	autoneg;
+	u32	rx_pause;
+	u32	tx_pause;
+};
+#endif /* ETHTOOL_GPAUSEPARAM */
+
+#ifndef ETHTOOL_SPAUSEPARAM
+#define ETHTOOL_SPAUSEPARAM	0x00000013 /* Set pause parameters. */
+#endif
+#ifndef ETHTOOL_GRXCSUM
+#define ETHTOOL_GRXCSUM		0x00000014 /* Get RX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_SRXCSUM
+#define ETHTOOL_SRXCSUM		0x00000015 /* Set RX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_GTXCSUM
+#define ETHTOOL_GTXCSUM		0x00000016 /* Get TX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_STXCSUM
+#define ETHTOOL_STXCSUM		0x00000017 /* Set TX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_GSG
+#define ETHTOOL_GSG		0x00000018 /* Get scatter-gather enable
+					    * (ethtool_value) */
+#endif
+#ifndef ETHTOOL_SSG
+#define ETHTOOL_SSG		0x00000019 /* Set scatter-gather enable
+					    * (ethtool_value). */
+#endif
+#ifndef ETHTOOL_TEST
+#define ETHTOOL_TEST		0x0000001a /* execute NIC self-test, priv. */
+#endif
+#ifndef ETHTOOL_GSTRINGS
+#define ETHTOOL_GSTRINGS	0x0000001b /* get specified string set */
+#endif
+#ifndef ETHTOOL_PHYS_ID
+#define ETHTOOL_PHYS_ID		0x0000001c /* identify the NIC */
+#endif
+#ifndef ETHTOOL_GSTATS
+#define ETHTOOL_GSTATS		0x0000001d /* get NIC-specific statistics */
+#endif
+#ifndef ETHTOOL_GTSO
+#define ETHTOOL_GTSO		0x0000001e /* Get TSO enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_STSO
+#define ETHTOOL_STSO		0x0000001f /* Set TSO enable (ethtool_value) */
+#endif
+
+#ifndef ETHTOOL_BUSINFO_LEN
+#define ETHTOOL_BUSINFO_LEN	32
+#endif
+
+/*****************************************************************************/
+/* 2.4.3 => 2.4.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,3) )
+
+/**************************************/
+/* PCI DRIVER API */
+
+#ifndef pci_set_dma_mask
+#define pci_set_dma_mask _kc_pci_set_dma_mask
+extern int _kc_pci_set_dma_mask(struct pci_dev *dev, dma_addr_t mask);
+#endif
+
+#ifndef pci_request_regions
+#define pci_request_regions _kc_pci_request_regions
+extern int _kc_pci_request_regions(struct pci_dev *pdev, char *res_name);
+#endif
+
+#ifndef pci_release_regions
+#define pci_release_regions _kc_pci_release_regions
+extern void _kc_pci_release_regions(struct pci_dev *pdev);
+#endif
+
+/**************************************/
+/* NETWORK DRIVER API */
+
+#ifndef alloc_etherdev
+#define alloc_etherdev _kc_alloc_etherdev
+extern struct net_device * _kc_alloc_etherdev(int sizeof_priv);
+#endif
+
+#ifndef is_valid_ether_addr
+#define is_valid_ether_addr _kc_is_valid_ether_addr
+extern int _kc_is_valid_ether_addr(u8 *addr);
+#endif
+
+/**************************************/
+/* MISCELLANEOUS */
+
+#ifndef INIT_TQUEUE
+#define INIT_TQUEUE(_tq, _routine, _data)		\
+	do {						\
+		INIT_LIST_HEAD(&(_tq)->list);		\
+		(_tq)->sync = 0;			\
+		(_tq)->routine = _routine;		\
+		(_tq)->data = _data;			\
+	} while (0)
+#endif
+
+#endif /* 2.4.3 => 2.4.0 */
+
+/*****************************************************************************/
+/* 2.4.6 => 2.4.3 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,6) )
+
+#ifndef pci_set_power_state
+#define pci_set_power_state _kc_pci_set_power_state
+extern int _kc_pci_set_power_state(struct pci_dev *dev, int state);
+#endif
+
+#ifndef pci_save_state
+#define pci_save_state _kc_pci_save_state
+extern int _kc_pci_save_state(struct pci_dev *dev, u32 *buffer);
+#endif
+
+#ifndef pci_restore_state
+#define pci_restore_state _kc_pci_restore_state
+extern int _kc_pci_restore_state(struct pci_dev *pdev, u32 *buffer);
+#endif
+
+#ifndef pci_enable_wake
+#define pci_enable_wake _kc_pci_enable_wake
+extern int _kc_pci_enable_wake(struct pci_dev *pdev, u32 state, int enable);
+#endif
+
+#ifndef pci_disable_device
+#define pci_disable_device _kc_pci_disable_device
+extern void _kc_pci_disable_device(struct pci_dev *pdev);
+#endif
+
+/* PCI PM entry point syntax changed, so don't support suspend/resume */
+#undef CONFIG_PM
+
+#endif /* 2.4.6 => 2.4.3 */
+
+#ifndef HAVE_PCI_SET_MWI
+#define pci_set_mwi(X) pci_write_config_word(X, \
+			       PCI_COMMAND, adapter->hw.bus.pci_cmd_word | \
+			       PCI_COMMAND_INVALIDATE);
+#define pci_clear_mwi(X) pci_write_config_word(X, \
+			       PCI_COMMAND, adapter->hw.bus.pci_cmd_word & \
+			       ~PCI_COMMAND_INVALIDATE);
+#endif
+
+/*****************************************************************************/
+/* 2.4.10 => 2.4.9 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,10) )
+
+/**************************************/
+/* MODULE API */
+
+#ifndef MODULE_LICENSE
+	#define MODULE_LICENSE(X)
+#endif
+
+/**************************************/
+/* OTHER */
+
+#undef min
+#define min(x,y) ({ \
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x < _y ? _x : _y; })
+
+#undef max
+#define max(x,y) ({ \
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x > _y ? _x : _y; })
+
+#ifndef list_for_each_safe
+#define list_for_each_safe(pos, n, head) \
+	for (pos = (head)->next, n = pos->next; pos != (head); \
+		pos = n, n = pos->next)
+#endif
+
+#endif /* 2.4.10 -> 2.4.6 */
+
+
+/*****************************************************************************/
+/* 2.4.13 => 2.4.10 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,13) )
+
+/**************************************/
+/* PCI DMA MAPPING */
+
+#ifndef virt_to_page
+	#define virt_to_page(v) (mem_map + (virt_to_phys(v) >> PAGE_SHIFT))
+#endif
+
+#ifndef pci_map_page
+#define pci_map_page _kc_pci_map_page
+extern u64 _kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset, size_t size, int direction);
+#endif
+
+#ifndef pci_unmap_page
+#define pci_unmap_page _kc_pci_unmap_page
+extern void _kc_pci_unmap_page(struct pci_dev *dev, u64 dma_addr, size_t size, int direction);
+#endif
+
+/* pci_set_dma_mask takes dma_addr_t, which is only 32-bits prior to 2.4.13 */
+
+#undef DMA_32BIT_MASK
+#define DMA_32BIT_MASK	0xffffffff
+#undef DMA_64BIT_MASK
+#define DMA_64BIT_MASK	0xffffffff
+
+/**************************************/
+/* OTHER */
+
+#ifndef cpu_relax
+#define cpu_relax()	rep_nop()
+#endif
+
+#endif /* 2.4.13 => 2.4.10 */
+
+/*****************************************************************************/
+/* 2.4.17 => 2.4.12 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,17) )
+
+#ifndef __devexit_p
+	#define __devexit_p(x) &(x)
+#endif
+
+#ifndef VLAN_HLEN
+#define VLAN_HLEN 4
+#endif
+
+#ifndef VLAN_ETH_HLEN
+#define VLAN_ETH_HLEN 18
+#endif
+
+#ifndef VLAN_ETH_FRAME_LEN
+#define VLAN_ETH_FRAME_LEN 1518
+#endif
+
+#endif /* 2.4.17 => 2.4.13 */
+
+/*****************************************************************************/
+/* 2.4.20 => 2.4.19 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,20) )
+
+/* we won't support NAPI on less than 2.4.20 */
+#ifdef NAPI
+#undef CONFIG_E1000_NAPI
+#undef CONFIG_IXGB_NAPI
+#endif
+
+#endif /* 2.4.20 => 2.4.19 */
+/*****************************************************************************/
+/* 2.4.22 => 2.4.17 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,22) )
+#define pci_name(x)	((x)->slot_name)
+#endif
+
+/*****************************************************************************/
+/* 2.4.23 => 2.4.22 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,23) )
+/*****************************************************************************/
+#ifdef NAPI
+#ifndef netif_poll_disable
+#define netif_poll_disable(x) _kc_netif_poll_disable(x)
+static inline void _kc_netif_poll_disable(struct net_device *netdev)
+{
+	while (test_and_set_bit(__LINK_STATE_RX_SCHED, &netdev->state)) {
+		/* No hurry */
+		current->state = TASK_INTERRUPTIBLE;
+		schedule_timeout(1);
+	}
+}
+#endif
+
+#ifndef netif_poll_enable
+#define netif_poll_enable(x) _kc_netif_poll_enable(x)
+static inline void _kc_netif_poll_enable(struct net_device *netdev)
+{
+	clear_bit(__LINK_STATE_RX_SCHED, &netdev->state);
+}
+#endif
+#endif /* NAPI */
+#ifndef netif_tx_disable
+#define netif_tx_disable(x) _kc_netif_tx_disable(x)
+static inline void _kc_netif_tx_disable(struct net_device *dev)
+{
+	spin_lock_bh(&dev->xmit_lock);
+	netif_stop_queue(dev);
+	spin_unlock_bh(&dev->xmit_lock);
+}
+#endif
+#endif /* 2.4.23 => 2.4.22 */
+
+/*****************************************************************************/
+/* 2.6.4 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,25) || \
+    ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) ) )
+#define ETHTOOL_OPS_COMPAT
+#endif /* 2.6.4 => 2.6.0 */
+
+/*****************************************************************************/
+/* 2.5.71 => 2.4.x */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,71) )
+#include <net/sock.h>
+#define sk_protocol protocol
+#endif /* 2.5.70 => 2.4.x */
+
+/*****************************************************************************/
+/* < 2.4.27 or 2.6.0 <= 2.6.5 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,27) || \
+    ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(2,6,5) ) )
+
+#ifndef netif_msg_init
+#define netif_msg_init _kc_netif_msg_init
+static inline u32 _kc_netif_msg_init(int debug_value, int default_msg_enable_bits)
+{
+	/* use default */
+	if (debug_value < 0 || debug_value >= (sizeof(u32) * 8))
+		return default_msg_enable_bits;
+	if (debug_value == 0) /* no output */
+		return 0;
+	/* set low N bits */
+	return (1 << debug_value) -1;
+}
+#endif
+
+#endif /* < 2.4.27 or 2.6.0 <= 2.6.5 */
+/*****************************************************************************/
+#if (( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,27) ) || \
+     (( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) ) && \
+      ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,3) )))
+#define netdev_priv(x) x->priv
+#endif
+
+/*****************************************************************************/
+/* <= 2.5.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,0) )
+#undef pci_register_driver
+#define pci_register_driver pci_module_init
+#endif /* <= 2.5.0 */
+
+/*****************************************************************************/
+/* 2.5.28 => 2.4.23 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,28) )
+
+static inline void _kc_synchronize_irq(void)
+{
+	synchronize_irq();
+}
+#undef synchronize_irq
+#define synchronize_irq(X) _kc_synchronize_irq()
+
+#include <linux/tqueue.h>
+#define work_struct tq_struct
+#undef INIT_WORK
+#define INIT_WORK(a,b) INIT_TQUEUE(a,(void (*)(void *))b,a)
+#undef container_of
+#define container_of list_entry
+#define schedule_work schedule_task
+#define flush_scheduled_work flush_scheduled_tasks
+
+#endif /* 2.5.28 => 2.4.17 */
+
+/*****************************************************************************/
+/* 2.6.0 => 2.5.28 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+#define MODULE_INFO(version, _version)
+#ifndef CONFIG_E1000_DISABLE_PACKET_SPLIT
+#define CONFIG_E1000_DISABLE_PACKET_SPLIT 1
+#endif
+
+#define pci_set_consistent_dma_mask(dev,mask) 1
+
+#undef dev_put
+#define dev_put(dev) __dev_put(dev)
+
+#ifndef skb_fill_page_desc
+#define skb_fill_page_desc _kc_skb_fill_page_desc
+extern void _kc_skb_fill_page_desc(struct sk_buff *skb, int i, struct page *page, int off, int size);
+#endif
+
+#ifndef pci_dma_mapping_error
+#define pci_dma_mapping_error _kc_pci_dma_mapping_error
+static inline int _kc_pci_dma_mapping_error(dma_addr_t dma_addr)
+{
+	return dma_addr == 0;
+}
+#endif
+
+#undef ALIGN
+#define ALIGN(x,a) (((x)+(a)-1)&~((a)-1))
+
+#endif /* 2.6.0 => 2.5.28 */
+
+/*****************************************************************************/
+/* 2.6.4 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) )
+#define MODULE_VERSION(_version) MODULE_INFO(version, _version)
+#endif /* 2.6.4 => 2.6.0 */
+
+/*****************************************************************************/
+/* 2.6.5 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,5) )
+#define pci_dma_sync_single_for_cpu	pci_dma_sync_single
+#define pci_dma_sync_single_for_device	pci_dma_sync_single_for_cpu
+#endif /* 2.6.5 => 2.6.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,7) )
+#undef if_mii
+#define if_mii _kc_if_mii
+static inline struct mii_ioctl_data *_kc_if_mii(struct ifreq *rq)
+{
+	return (struct mii_ioctl_data *) &rq->ifr_ifru;
+}
+#endif /* < 2.6.7 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,8) )
+#define msleep(x)	do { set_current_state(TASK_UNINTERRUPTIBLE); \
+				schedule_timeout((x * HZ)/1000 + 2); \
+			} while (0)
+#endif
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,9))
+#define __iomem
+
+#ifndef kcalloc
+#define kcalloc(n, size, flags) _kc_kzalloc(((n) * (size)), flags)
+extern void *_kc_kzalloc(size_t size, int flags);
+#endif
+#define MSEC_PER_SEC    1000L
+static inline unsigned int _kc_jiffies_to_msecs(const unsigned long j)
+{
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+	return (MSEC_PER_SEC / HZ) * j;
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+	return (j + (HZ / MSEC_PER_SEC) - 1)/(HZ / MSEC_PER_SEC);
+#else
+	return (j * MSEC_PER_SEC) / HZ;
+#endif
+}
+static inline unsigned long _kc_msecs_to_jiffies(const unsigned int m)
+{
+	if (m > _kc_jiffies_to_msecs(MAX_JIFFY_OFFSET))
+		return MAX_JIFFY_OFFSET;
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+	return (m + (MSEC_PER_SEC / HZ) - 1) / (MSEC_PER_SEC / HZ);
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+	return m * (HZ / MSEC_PER_SEC);
+#else
+	return (m * HZ + MSEC_PER_SEC - 1) / MSEC_PER_SEC;
+#endif
+}
+
+#define msleep_interruptible _kc_msleep_interruptible
+static inline unsigned long _kc_msleep_interruptible(unsigned int msecs)
+{
+	unsigned long timeout = _kc_msecs_to_jiffies(msecs) + 1;
+
+	while (timeout && !signal_pending(current)) {
+		__set_current_state(TASK_INTERRUPTIBLE);
+		timeout = schedule_timeout(timeout);
+	}
+	return _kc_jiffies_to_msecs(timeout);
+}
+#endif /* < 2.6.9 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,6) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(2,6,10) )
+#ifdef pci_save_state
+#undef pci_save_state
+#endif
+#define pci_save_state(X) { \
+        int i; \
+        if (adapter->pci_state) { \
+                for (i = 0; i < 16; i++) { \
+                        pci_read_config_dword((X), \
+                                              i * 4, \
+                                              &adapter->pci_state[i]); \
+                } \
+        } \
+}
+
+#ifdef pci_restore_state
+#undef pci_restore_state
+#endif
+#define pci_restore_state(X) { \
+        int i; \
+        if (adapter->pci_state) { \
+                for (i = 0; i < 16; i++) { \
+                        pci_write_config_dword((X), \
+                                               i * 4, \
+                                               adapter->pci_state[i]); \
+                } \
+        } else { \
+                for (i = 0; i < 6; i++) { \
+                        pci_write_config_dword((X), \
+                                               PCI_BASE_ADDRESS_0 + (i * 4), \
+                                               (X)->resource[i].start); \
+                } \
+        } \
+}
+#endif /* 2.4.6 <= x < 2.6.10 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,10) )
+#ifdef module_param_array_named
+#undef module_param_array_named
+#define module_param_array_named(name, array, type, nump, perm)          \
+	static struct kparam_array __param_arr_##name                    \
+	= { ARRAY_SIZE(array), nump, param_set_##type, param_get_##type, \
+	    sizeof(array[0]), array };                                   \
+	module_param_call(name, param_array_set, param_array_get,        \
+			  &__param_arr_##name, perm)
+#endif /* module_param_array_named */
+#endif /* < 2.6.10 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11) )
+#define PCI_D0      0
+#define PCI_D1      1
+#define PCI_D2      2
+#define PCI_D3hot   3
+#define PCI_D3cold  4
+#define pci_choose_state(pdev,state) state
+#define PMSG_SUSPEND 3
+
+#undef NETIF_F_LLTX
+
+#ifndef ARCH_HAS_PREFETCH
+#define prefetch(X)
+#endif
+
+#ifndef NET_IP_ALIGN
+#define NET_IP_ALIGN 2
+#endif
+
+#endif /* < 2.6.11 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,12) )
+#include <linux/reboot.h>
+#define USE_REBOOT_NOTIFIER
+#endif
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,14) )
+#define pm_message_t u32
+#ifndef kzalloc
+#define kzalloc _kc_kzalloc
+extern void *_kc_kzalloc(size_t size, int flags);
+#endif
+#endif
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,16) )
+#undef CONFIG_E1000_PCI_ERS
+#undef CONFIG_IXGB_PCI_ERS
+#else
+#define CONFIG_E1000_PCI_ERS
+#define CONFIG_IXGB_PCI_ERS
+#endif
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18) )
+
+#ifndef IRQF_PROBE_SHARED
+#ifdef SA_PROBEIRQ
+#define IRQF_PROBE_SHARED SA_PROBEIRQ
+#else
+#define IRQF_PROBE_SHARED 0
+#endif
+#endif
+
+#ifndef IRQF_SHARED
+#define IRQF_SHARED SA_SHIRQ
+#endif
+
+#ifndef ARRAY_SIZE
+#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
+#endif
+
+#ifndef netdev_alloc_skb
+#define netdev_alloc_skb _kc_netdev_alloc_skb
+extern struct sk_buff *_kc_netdev_alloc_skb(struct net_device *dev,
+                                            unsigned int length);
+#endif
+
+#ifndef skb_is_gso
+#ifdef NETIF_F_TSO
+#define skb_is_gso _kc_skb_is_gso
+static inline int _kc_skb_is_gso(const struct sk_buff *skb)
+{
+	return skb_shinfo(skb)->gso_size;
+}
+#endif
+#endif
+
+#endif /* < 2.6.18 */
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19) )
+typedef _Bool bool;
+
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0) )
+#ifndef RHEL_VERSION
+#define RHEL_VERSION 0
+#endif
+#if (!(( RHEL_VERSION == 4 ) && ( RHEL_UPDATE >= 5 )))
+typedef irqreturn_t (*irq_handler_t)(int, void*, struct pt_regs *);
+#endif
+typedef irqreturn_t (*new_handler_t)(int, void*);
+static inline irqreturn_t _kc_request_irq(unsigned int irq, new_handler_t handler, unsigned long flags, const char *devname, void *dev_id)
+#else /* 2.4.x */
+typedef void (*irq_handler_t)(int, void*, struct pt_regs *);
+typedef void (*new_handler_t)(int, void*);
+static inline int _kc_request_irq(unsigned int irq, new_handler_t handler, unsigned long flags, const char *devname, void *dev_id)
+#endif
+{
+	irq_handler_t new_handler = (irq_handler_t) handler;
+	return request_irq(irq, new_handler, flags, devname, dev_id);
+}
+
+#undef request_irq
+#define request_irq(irq, handler, flags, devname, dev_id) _kc_request_irq((irq), (handler), (flags), (devname), (dev_id))
+
+/* pci_restore_state and pci_save_state handles MSI/PCIE from 2.6.19 */
+#define PCIE_CONFIG_SPACE_LEN 256
+#define PCI_CONFIG_SPACE_LEN 64
+#define PCIE_LINK_STATUS 0x12
+#undef pci_save_state
+#define pci_save_state(pdev) _kc_pci_save_state(adapter)
+#define _kc_pci_save_state(adapter) 0; { \
+	int size, i; \
+	u16 pcie_link_status; \
+	\
+	u16 cap_offset = pci_find_capability(pdev, PCI_CAP_ID_EXP); \
+	if (cap_offset) { \
+	if (pci_read_config_word(pdev, cap_offset + PCIE_LINK_STATUS, &pcie_link_status)) \
+		size = PCI_CONFIG_SPACE_LEN; \
+	else \
+		size = PCIE_CONFIG_SPACE_LEN; \
+	WARN_ON(adapter->config_space != NULL); \
+	adapter->config_space = kmalloc(size, GFP_KERNEL); \
+	if (!adapter->config_space) { \
+		printk(KERN_ERR "Out of memory in pci_save_msi_state\n"); \
+		return -ENOMEM; \
+	} \
+	for (i = 0; i < (size / 4); i++) \
+		pci_read_config_dword(pdev, i * 4, &adapter->config_space[i]); \
+	} \
+}
+#undef pci_restore_state
+#define pci_restore_state(pdev) _kc_pci_restore_state(adapter)
+#define _kc_pci_restore_state(adapter) { \
+	int size, i; \
+	u16 pcie_link_status; \
+	\
+	u16 cap_offset = pci_find_capability(pdev, PCI_CAP_ID_EXP); \
+	if (cap_offset) { \
+	if (adapter->config_space != NULL) { \
+	if (pci_read_config_word(pdev, cap_offset + PCIE_LINK_STATUS, &pcie_link_status)) \
+		size = PCI_CONFIG_SPACE_LEN; \
+	else \
+		size = PCIE_CONFIG_SPACE_LEN; \
+	\
+	for (i = 0; i < (size / 4); i++) \
+		pci_write_config_dword(pdev, i * 4, adapter->config_space[i]); \
+	kfree(adapter->config_space); \
+	adapter->config_space = NULL; \
+	} \
+	} \
+}
+
+#endif /* < 2.6.19 */
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20) )
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,28) )
+#undef INIT_WORK
+#define INIT_WORK(_work, _func) \
+do { \
+	INIT_LIST_HEAD(&(_work)->entry); \
+	(_work)->pending = 0; \
+	(_work)->func = (void (*)(void *))_func; \
+	(_work)->data = _work; \
+	init_timer(&(_work)->timer); \
+} while (0)
+#endif
+
+#ifndef round_jiffies
+#define round_jiffies(x) x
+#endif
+
+#define csum_offset csum
+
+#endif /* < 2.6.20 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21) )
+#define vlan_group_get_device(vg, id) (vg->vlan_devices[id])
+#define vlan_group_set_device(vg, id, dev) if (vg) vg->vlan_devices[id] = dev;
+#define pci_channel_offline(pdev) (pdev->error_state && \
+	pdev->error_state != pci_channel_io_normal)
+#endif /* < 2.6.21 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) )
+#define tcp_hdr(skb) (skb->h.th)
+#define tcp_hdrlen(skb) (skb->h.th->doff << 2)
+#define skb_transport_offset(skb) (skb->h.raw - skb->data)
+#define skb_transport_header(skb) (skb->h.raw)
+#define ipv6_hdr(skb) (skb->nh.ipv6h)
+#define ip_hdr(skb) (skb->nh.iph)
+#define skb_network_offset(skb) (skb->nh.raw - skb->data)
+#define skb_network_header(skb) (skb->nh.raw)
+#define skb_tail_pointer(skb) skb->tail
+#define skb_copy_to_linear_data_offset(skb, offset, from, len) \
+                                 memcpy(skb->data + offset, from, len)
+#define skb_network_header_len(skb) (skb->h.raw - skb->nh.raw)
+#define pci_register_driver pci_module_init
+
+#ifndef alloc_etherdev_mq
+#define alloc_etherdev_mq(_a, _b) alloc_etherdev(_a)
+#endif
+#endif /* < 2.6.22 */
+
+#endif /* _KCOMPAT_H_ */
+
diff -Naur linux-2.6.9.src/drivers/net/e1000/Makefile linux-2.6.9/drivers/net/e1000/Makefile
--- linux-2.6.9.src/drivers/net/e1000/Makefile	2004-10-18 23:53:07.000000000 +0200
+++ linux-2.6.9/drivers/net/e1000/Makefile	2007-07-16 13:33:15.000000000 +0200
@@ -1,35 +1,338 @@
 ################################################################################
 #
-# 
-# Copyright(c) 1999 - 2003 Intel Corporation. All rights reserved.
-# 
-# This program is free software; you can redistribute it and/or modify it 
-# under the terms of the GNU General Public License as published by the Free 
-# Software Foundation; either version 2 of the License, or (at your option) 
-# any later version.
-# 
-# This program is distributed in the hope that it will be useful, but WITHOUT 
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or 
-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for 
+# Intel PRO/1000 Linux driver
+# Copyright(c) 1999 - 2007 Intel Corporation.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms and conditions of the GNU General Public License,
+# version 2, as published by the Free Software Foundation.
+#
+# This program is distributed in the hope it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 # more details.
-# 
+#
 # You should have received a copy of the GNU General Public License along with
-# this program; if not, write to the Free Software Foundation, Inc., 59 
-# Temple Place - Suite 330, Boston, MA  02111-1307, USA.
-# 
-# The full GNU General Public License is included in this distribution in the
-# file called LICENSE.
-# 
+# this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+#
+# The full GNU General Public License is included in this distribution in
+# the file called "COPYING".
+#
 # Contact Information:
 # Linux NICS <linux.nics@intel.com>
+# e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
 # Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 #
 ################################################################################
 
-#
-# Makefile for the Intel(R) PRO/1000 ethernet driver
-#
+###########################################################################
+# Driver files
+FAMILYC = e1000_82540.c e1000_82542.c e1000_82571.c e1000_82541.c \
+	  e1000_82543.c e1000_ich8lan.c e1000_80003es2lan.c
+FAMILYH = e1000_82571.h e1000_82541.h e1000_82543.h e1000_ich8lan.h \
+	  e1000_80003es2lan.h
+
+# core driver files
+CFILES = e1000_main.c $(FAMILYC) e1000_mac.c e1000_nvm.c e1000_phy.c \
+	 e1000_manage.c e1000_param.c e1000_ethtool.c kcompat.c e1000_api.c
+HFILES = e1000.h e1000_hw.h e1000_osdep.h e1000_defines.h e1000_mac.h \
+	 e1000_nvm.h e1000_manage.h $(FAMILYH) kcompat.h e1000_regs.h \
+	 e1000_api.h
+ifeq (,$(BUILD_KERNEL))
+BUILD_KERNEL=$(shell uname -r)
+endif
+
+###########################################################################
+# Environment tests
+
+# Kernel Search Path
+# All the places we look for kernel source
+KSP :=  /lib/modules/$(BUILD_KERNEL)/build \
+        /lib/modules/$(BUILD_KERNEL)/source \
+        /usr/src/linux-$(BUILD_KERNEL) \
+        /usr/src/linux-$($(BUILD_KERNEL) | sed 's/-.*//') \
+        /usr/src/kernel-headers-$(BUILD_KERNEL) \
+        /usr/src/kernel-source-$(BUILD_KERNEL) \
+        /usr/src/linux-$($(BUILD_KERNEL) | sed 's/\([0-9]*\.[0-9]*\)\..*/\1/') \
+        /usr/src/linux
+
+# prune the list down to only values that exist
+# and have an include/linux sub-directory
+test_dir = $(shell [ -e $(dir)/include/linux ] && echo $(dir))
+KSP := $(foreach dir, $(KSP), $(test_dir))
+
+# we will use this first valid entry in the search path
+ifeq (,$(KSRC))
+  KSRC := $(firstword $(KSP))
+endif
+
+ifeq (,$(KSRC))
+  $(error Linux kernel source not found)
+else
+ifeq (/lib/modules/$(shell uname -r)/source, $(KSRC))
+  KOBJ :=  /lib/modules/$(shell uname -r)/build
+else
+  KOBJ :=  $(KSRC)
+endif
+endif
+
+# check for version.h and autoconf.h for running kernel in /boot (SUSE)
+ifneq (,$(wildcard /boot/vmlinuz.version.h))
+  VERSION_FILE := /boot/vmlinuz.version.h
+  CONFIG_FILE  := /boot/vmlinuz.autoconf.h
+  KVER := $(shell $(CC) $(CFLAGS) -E -dM $(VERSION_FILE) | \
+          grep UTS_RELEASE | awk '{ print $$3 }' | sed 's/\"//g')
+  ifeq ($(KVER),$(shell uname -r))
+    # set up include path to override headers from kernel source
+    x:=$(shell rm -rf include)
+    x:=$(shell mkdir -p include/linux)
+    x:=$(shell cp /boot/vmlinuz.version.h include/linux/version.h)
+    x:=$(shell cp /boot/vmlinuz.autoconf.h include/linux/autoconf.h)
+    CFLAGS += -I./include
+  else
+    ifneq (,$(wildcard $(KOBJ)/include/linux/utsrelease.h))
+      VERSION_FILE := $(KOBJ)/include/linux/utsrelease.h
+    else
+      VERSION_FILE := $(KOBJ)/include/linux/version.h
+    endif
+    CONFIG_FILE  := $(KSRC)/include/linux/autoconf.h
+  endif
+else
+  ifneq (,$(wildcard $(KOBJ)/include/linux/utsrelease.h))
+    VERSION_FILE := $(KOBJ)/include/linux/utsrelease.h
+  else
+    VERSION_FILE := $(KOBJ)/include/linux/version.h
+  endif
+  CONFIG_FILE  := $(KSRC)/include/linux/autoconf.h
+endif
+
+ifeq (,$(wildcard $(VERSION_FILE)))
+  $(error Linux kernel source not configured - missing version.h)
+endif
+
+ifeq (,$(wildcard $(CONFIG_FILE)))
+  $(error Linux kernel source not configured - missing autoconf.h)
+endif
+
+# pick a compiler
+ifneq (,$(findstring egcs-2.91.66, $(shell cat /proc/version)))
+  CC := kgcc gcc cc
+else
+  CC := gcc cc
+endif
+test_cc = $(shell $(cc) --version > /dev/null 2>&1 && echo $(cc))
+CC := $(foreach cc, $(CC), $(test_cc))
+CC := $(firstword $(CC))
+ifeq (,$(CC))
+  $(error Compiler not found)
+endif
+
+# we need to know what platform the driver is being built on
+# some additional features are only built on Intel platforms
+ARCH := $(shell uname -m | sed 's/i.86/i386/')
+ifeq ($(ARCH),alpha)
+  CFLAGS += -ffixed-8 -mno-fp-regs
+endif
+ifeq ($(ARCH),x86_64)
+  CFLAGS += -mcmodel=kernel -mno-red-zone
+endif
+ifeq ($(ARCH),ppc)
+  CFLAGS += -msoft-float
+endif
+ifeq ($(ARCH),ppc64)
+  CFLAGS += -m64 -msoft-float
+  LDFLAGS += -melf64ppc
+endif
+ifneq (,$(findstring NO_82542_SUPPORT, $(CFLAGS_EXTRA)))
+  CFILES := $(filter-out e1000_82542.c, $(CFILES))
+endif
+
+# standard flags for module builds
+CFLAGS += -DLINUX -D__KERNEL__ -DMODULE -O2 -pipe -Wall
+CFLAGS += -I$(KSRC)/include -I.
+CFLAGS += $(shell [ -f $(KSRC)/include/linux/modversions.h ] && \
+            echo "-DMODVERSIONS -DEXPORT_SYMTAB \
+                  -include $(KSRC)/include/linux/modversions.h")
+
+CFLAGS += $(CFLAGS_EXTRA)
+
+RHC := $(KSRC)/include/linux/rhconfig.h
+ifneq (,$(wildcard $(RHC)))
+  # 7.3 typo in rhconfig.h
+  ifneq (,$(shell $(CC) $(CFLAGS) -E -dM $(RHC) | grep __module__bigmem))
+	CFLAGS += -D__module_bigmem
+  endif
+endif
+
+# get the kernel version - we use this to find the correct install path
+KVER := $(shell $(CC) $(CFLAGS) -E -dM $(VERSION_FILE) | grep UTS_RELEASE | \
+        awk '{ print $$3 }' | sed 's/\"//g')
+
+# assume source symlink is the same as build, otherwise adjust KOBJ
+ifneq (,$(wildcard /lib/modules/$(KVER)/build))
+ifneq ($(KSRC),$(shell cd /lib/modules/$(KVER)/build ; pwd -P))
+  KOBJ=/lib/modules/$(KVER)/build
+endif
+endif
+
+KKVER := $(shell echo $(KVER) | \
+         awk '{ if ($$0 ~ /2\.[4-9]\./) print "1"; else print "0"}')
+ifeq ($(KKVER), 0)
+  $(error *** Aborting the build. \
+          *** This driver is not supported on kernel versions older than 2.4.0)
+endif
+
+# set the install path
+INSTDIR := /lib/modules/$(KVER)/kernel/drivers/net/e1000
+
+# look for SMP in config.h
+SMP := $(shell $(CC) $(CFLAGS) -E -dM $(CONFIG_FILE) | \
+         grep -w CONFIG_SMP | awk '{ print $$3 }')
+ifneq ($(SMP),1)
+  SMP := 0
+endif
+
+ifneq ($(SMP),$(shell uname -a | grep SMP > /dev/null 2>&1 && echo 1 || echo 0))
+  $(warning ***)
+  ifeq ($(SMP),1)
+    $(warning *** Warning: kernel source configuration (SMP))
+    $(warning *** does not match running kernel (UP))
+  else
+    $(warning *** Warning: kernel source configuration (UP))
+    $(warning *** does not match running kernel (SMP))
+  endif
+  $(warning *** Continuing with build,)
+  $(warning *** resulting driver may not be what you want)
+  $(warning ***)
+endif
+
+ifeq ($(SMP),1)
+  CFLAGS += -D__SMP__
+endif
+
+###########################################################################
+# 2.4.x & 2.6.x Specific rules
+
+K_VERSION:=$(shell uname -r | cut -c1-3 | sed 's/2\.[56]/2\.6/')
+
+ifeq ($(K_VERSION), 2.6)
+
+# Makefile for 2.6.x kernel
+TARGET = e1000.ko
+
+# man page
+MANSECTION = 7
+MANFILE = $(TARGET:.ko=.$(MANSECTION))
+
+ifneq ($(PATCHLEVEL),)
+EXTRA_CFLAGS += $(CFLAGS_EXTRA)
+obj-m += e1000.o
+e1000-objs := $(CFILES:.c=.o)
+else
+default:
+ifeq ($(KOBJ),$(KSRC))
+	$(MAKE) -C $(KSRC) SUBDIRS=$(shell pwd) modules
+else
+	$(MAKE) -C $(KSRC) O=$(KOBJ) SUBDIRS=$(shell pwd) modules
+endif
+endif
+
+else # ifeq ($(K_VERSION),2.6)
+
+# Makefile for 2.4.x kernel
+TARGET = e1000.o
+
+# man page
+MANSECTION = 7
+MANFILE = $(TARGET:.o=.$(MANSECTION))
+
+# Get rid of compile warnings in kernel header files from SuSE
+ifneq (,$(wildcard /etc/SuSE-release))
+  CFLAGS += -Wno-sign-compare -fno-strict-aliasing
+endif
+
+# Get rid of compile warnings in kernel header files from fedora
+ifneq (,$(wildcard /etc/fedora-release))
+  CFLAGS += -fno-strict-aliasing
+endif
+
+.SILENT: $(TARGET)
+$(TARGET): $(filter-out $(TARGET), $(CFILES:.c=.o))
+	$(LD) $(LDFLAGS) -r $^ -o $@
+	echo; echo
+	echo "**************************************************"
+	echo "** $(TARGET) built for $(KVER)"
+	echo -n "** SMP               "
+	if [ "$(SMP)" = "1" ]; \
+		then echo "Enabled"; else echo "Disabled"; fi
+	echo "**************************************************"
+	echo
+
+$(CFILES:.c=.o): $(HFILES) Makefile
+default:
+	$(MAKE)
+
+endif # ifeq ($(K_VERSION),2.6)
+
+ifeq (,$(MANDIR))
+  # find the best place to install the man page
+  MANPATH := $(shell (manpath 2>/dev/null || echo $MANPATH) | sed 's/:/ /g')
+  ifneq (,$(MANPATH))
+    # test based on inclusion in MANPATH
+    test_dir = $(findstring $(dir), $(MANPATH))
+  else
+    # no MANPATH, test based on directory existence
+    test_dir = $(shell [ -e $(dir) ] && echo $(dir))
+  endif
+  # our preferred install path
+  # should /usr/local/man be in here ?
+  MANDIR := /usr/share/man /usr/man
+  MANDIR := $(foreach dir, $(MANDIR), $(test_dir))
+  MANDIR := $(firstword $(MANDIR))
+endif
+ifeq (,$(MANDIR))
+  # fallback to /usr/man
+  MANDIR := /usr/man
+endif
+
+# depmod version for rpm builds
+DEPVER := $(shell /sbin/depmod -V 2>/dev/null | \
+          awk 'BEGIN {FS="."} NR==1 {print $$2}')
+
+###########################################################################
+# Build rules
+
+$(MANFILE).gz: ../$(MANFILE)
+	gzip -c $< > $@
+
+install: default $(MANFILE).gz
+	# remove all old versions of the driver
+	find $(INSTALL_MOD_PATH)/lib/modules/$(KVER) -name $(TARGET) -exec rm -f {} \; || true
+	find $(INSTALL_MOD_PATH)/lib/modules/$(KVER) -name $(TARGET).gz -exec rm -f {} \; || true
+	install -D -m 644 $(TARGET) $(INSTALL_MOD_PATH)$(INSTDIR)/$(TARGET)
+ifeq (,$(INSTALL_MOD_PATH))
+	/sbin/depmod -a || true
+else
+  ifeq ($(DEPVER),1 )
+	/sbin/depmod -r $(INSTALL_MOD_PATH) -a || true
+  else
+	/sbin/depmod -b $(INSTALL_MOD_PATH) -a -n $(KVERSION) > /dev/null || true
+  endif
+endif
+	install -D -m 644 $(MANFILE).gz $(INSTALL_MOD_PATH)$(MANDIR)/man$(MANSECTION)/$(MANFILE).gz
+	man -c -P'cat > /dev/null' $(MANFILE:.$(MANSECTION)=) || true
+
+uninstall:
+	if [ -e $(INSTDIR)/$(TARGET) ] ; then \
+	    rm -f $(INSTDIR)/$(TARGET) ; \
+	fi
+	/sbin/depmod -a
+	if [ -e $(MANDIR)/man$(MANSECTION)/$(MANFILE).gz ] ; then \
+		rm -f $(MANDIR)/man$(MANSECTION)/$(MANFILE).gz ; \
+	fi
 
-obj-$(CONFIG_E1000) += e1000.o
+.PHONY: clean install
 
-e1000-objs := e1000_main.o e1000_hw.o e1000_ethtool.o e1000_param.o
+clean:
+	rm -rf $(TARGET) $(TARGET:.ko=.o) $(TARGET:.ko=.mod.c) $(TARGET:.ko=.mod.o) $(CFILES:.c=.o) $(MANFILE).gz .*cmd .tmp_versions
diff -Naur linux-2.6.9.src/drivers/net/ns83820.c linux-2.6.9/drivers/net/ns83820.c
--- linux-2.6.9.src/drivers/net/ns83820.c	2004-10-18 23:55:28.000000000 +0200
+++ linux-2.6.9/drivers/net/ns83820.c	2007-07-03 11:07:40.000000000 +0200
@@ -92,9 +92,17 @@
 //#define dprintk		printk
 #define dprintk(x...)		do { } while (0)
 
+
 #include <linux/module.h>
 #include <linux/types.h>
 #include <linux/pci.h>
+
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+#include "../../afterburn/annotate.h"
+#define readl dp83820_read_annotate
+#define writel dp83820_write_annotate
+#endif
+
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
 #include <linux/delay.h>
@@ -573,6 +581,12 @@
 		struct sk_buff *skb;
 		long res;
 		/* extra 16 bytes for alignment */
+#if 0
+		skb = __dev_alloc_skb(REAL_RX_BUF_SIZE+NET_IP_ALIGN, gfp);
+		if (unlikely(!skb))
+			break;
+		skb_reserve(skb,NET_IP_ALIGN);
+#else
 		skb = __dev_alloc_skb(REAL_RX_BUF_SIZE+16, gfp);
 		if (unlikely(!skb))
 			break;
@@ -581,6 +595,7 @@
 		res = 0x10 - res;
 		res &= 0xf;
 		skb_reserve(skb, res);
+#endif
 
 		skb->dev = ndev;
 		if (gfp != GFP_ATOMIC)
diff -Naur linux-2.6.9.src/fs/mpage.c linux-2.6.9/fs/mpage.c
--- linux-2.6.9.src/fs/mpage.c	2004-10-18 23:53:43.000000000 +0200
+++ linux-2.6.9/fs/mpage.c	2007-07-03 11:07:40.000000000 +0200
@@ -638,8 +638,11 @@
 	}
 
 	writepage = NULL;
-	if (get_block == NULL)
+	if (get_block == NULL) {
 		writepage = mapping->a_ops->writepage;
+		if (writepage == NULL)
+			return ret;
+	}
 
 	pagevec_init(&pvec, 0);
 	if (wbc->sync_mode == WB_SYNC_NONE) {
diff -Naur linux-2.6.9.src/include/asm-i386/apic.h linux-2.6.9/include/asm-i386/apic.h
--- linux-2.6.9.src/include/asm-i386/apic.h	2004-10-18 23:55:29.000000000 +0200
+++ linux-2.6.9/include/asm-i386/apic.h	2007-07-03 11:07:40.000000000 +0200
@@ -29,6 +29,17 @@
 			printk(s, ##a);    \
 	} while (0)
 
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+
+#include "../../afterburn/annotate.h"
+#define __apic_read			lapic_read_annotate
+#define __apic_write			lapic_write_annotate
+#define __apic_write_atomic		lapic_xchg_annotate
+#else
+#define __apic_read(a)		*(a)
+#define __apic_write(v,a)		*(a) = v
+#define __apic_write_atomic(v,a)	xchg(a, v)
+#endif
 
 #ifdef CONFIG_X86_LOCAL_APIC
 
@@ -38,17 +49,18 @@
 
 static __inline void apic_write(unsigned long reg, unsigned long v)
 {
-	*((volatile unsigned long *)(APIC_BASE+reg)) = v;
+	__apic_write(v, (volatile unsigned long *)(APIC_BASE+reg));
 }
 
 static __inline void apic_write_atomic(unsigned long reg, unsigned long v)
 {
-	xchg((volatile unsigned long *)(APIC_BASE+reg), v);
+	__apic_write_atomic(v, (volatile unsigned long *)(APIC_BASE+reg));
 }
 
 static __inline unsigned long apic_read(unsigned long reg)
 {
-	return *((volatile unsigned long *)(APIC_BASE+reg));
+	
+	return __apic_read((volatile unsigned long *)(APIC_BASE+reg));
 }
 
 static __inline__ void apic_wait_icr_idle(void)
diff -Naur linux-2.6.9.src/include/asm-i386/dma-mapping.h linux-2.6.9/include/asm-i386/dma-mapping.h
--- linux-2.6.9.src/include/asm-i386/dma-mapping.h	2004-10-18 23:55:36.000000000 +0200
+++ linux-2.6.9/include/asm-i386/dma-mapping.h	2007-07-03 11:07:40.000000000 +0200
@@ -7,6 +7,10 @@
 #include <asm/io.h>
 #include <asm/scatterlist.h>
 
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+extern unsigned long (*afterburn_phys_to_dma_hook)(unsigned long phys, unsigned long size);
+#endif
+
 #define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 #define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 
@@ -22,6 +26,10 @@
 {
 	BUG_ON(direction == DMA_NONE);
 	flush_write_buffers();
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+	if (afterburn_phys_to_dma_hook)
+		return afterburn_phys_to_dma_hook(virt_to_phys(ptr), size);
+#endif
 	return virt_to_phys(ptr);
 }
 
@@ -43,6 +51,11 @@
 	for (i = 0; i < nents; i++ ) {
 		BUG_ON(!sg[i].page);
 
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+		if (afterburn_phys_to_dma_hook)
+			sg[i].dma_address = afterburn_phys_to_dma_hook(page_to_phys(sg[i].page), sg[i].length) + sg[i].offset;
+		else
+#endif
 		sg[i].dma_address = page_to_phys(sg[i].page) + sg[i].offset;
 	}
 
@@ -55,6 +68,10 @@
 	     size_t size, enum dma_data_direction direction)
 {
 	BUG_ON(direction == DMA_NONE);
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+	if (afterburn_phys_to_dma_hook)
+		return afterburn_phys_to_dma_hook(page_to_phys(page), PAGE_SIZE) + offset;
+#endif
 	return page_to_phys(page) + offset;
 }
 
diff -Naur linux-2.6.9.src/include/asm-i386/fixmap.h linux-2.6.9/include/asm-i386/fixmap.h
--- linux-2.6.9.src/include/asm-i386/fixmap.h	2004-10-18 23:53:08.000000000 +0200
+++ linux-2.6.9/include/asm-i386/fixmap.h	2007-07-03 11:07:40.000000000 +0200
@@ -20,7 +20,11 @@
  * Leave one empty page between vmalloc'ed areas and
  * the start of the fixmap.
  */
-#define __FIXADDR_TOP	0xfffff000
+#if defined(CONFIG_AFTERBURN_RELINK)
+#define __FIXADDR_TOP ((unsigned long)CONFIG_AFTERBURN_VADDR_END-0x1000)
+#else
+#define __FIXADDR_TOP 0xfffff000
+#endif
 
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
diff -Naur linux-2.6.9.src/include/asm-i386/io_apic.h linux-2.6.9/include/asm-i386/io_apic.h
--- linux-2.6.9.src/include/asm-i386/io_apic.h	2004-10-18 23:54:38.000000000 +0200
+++ linux-2.6.9/include/asm-i386/io_apic.h	2007-07-03 11:07:40.000000000 +0200
@@ -162,16 +162,26 @@
 /* non-0 if default (table-less) MP configuration */
 extern int mpc_default_type;
 
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+#include "../../afterburn/annotate.h"
+#define __io_apic_read			ioapic_read_annotate
+#define __io_apic_write			ioapic_write_annotate
+#else
+#define __io_apic_read(a)		*(a)
+#define __io_apic_write(v,a)            *(a) = v
+
+#endif
+
 static inline unsigned int io_apic_read(unsigned int apic, unsigned int reg)
 {
-	*IO_APIC_BASE(apic) = reg;
-	return *(IO_APIC_BASE(apic)+4);
+	__io_apic_write(reg, IO_APIC_BASE(apic));
+	return __io_apic_read(IO_APIC_BASE(apic)+4);
 }
 
 static inline void io_apic_write(unsigned int apic, unsigned int reg, unsigned int value)
 {
-	*IO_APIC_BASE(apic) = reg;
-	*(IO_APIC_BASE(apic)+4) = value;
+	__io_apic_write(reg, IO_APIC_BASE(apic));
+	__io_apic_write(value, IO_APIC_BASE(apic)+4);
 }
 
 /*
@@ -184,8 +194,9 @@
 static inline void io_apic_modify(unsigned int apic, unsigned int reg, unsigned int value)
 {
 	if (sis_apic_bug)
-		*IO_APIC_BASE(apic) = reg;
-	*(IO_APIC_BASE(apic)+4) = value;
+	    __io_apic_write(reg, IO_APIC_BASE(apic));
+
+	__io_apic_write(value, IO_APIC_BASE(apic)+4);
 }
 
 /* 1 if "noapic" boot option passed */
diff -Naur linux-2.6.9.src/include/asm-i386/io.h linux-2.6.9/include/asm-i386/io.h
--- linux-2.6.9.src/include/asm-i386/io.h	2004-10-18 23:55:29.000000000 +0200
+++ linux-2.6.9/include/asm-i386/io.h	2007-07-03 11:07:40.000000000 +0200
@@ -45,6 +45,12 @@
 
 #ifdef __KERNEL__
 
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+extern unsigned long (*afterburn_phys_to_dma_hook)(unsigned long phys, unsigned long size);
+extern unsigned long (*afterburn_dma_to_phys_hook)(unsigned long dma);
+#include <linux/mm.h>
+#endif
+
 #include <asm-generic/iomap.h>
 
 #include <linux/vmalloc.h>
@@ -123,6 +129,36 @@
 /*
  * ISA I/O bus memory addresses are 1:1 with the physical address.
  */
+#if defined(CONFIG_AFTERBURN_HOOK_DMA)
+static inline unsigned long virt_to_bus( void * virt )
+{
+	if (afterburn_phys_to_dma_hook)
+		return afterburn_phys_to_dma_hook(virt_to_phys(virt),PAGE_SIZE);
+	else
+		return virt_to_phys(virt);
+}
+
+static inline void * bus_to_virt( unsigned long bus )
+{
+	if (afterburn_dma_to_phys_hook)
+		return phys_to_virt(afterburn_dma_to_phys_hook(bus));
+	else
+		return phys_to_virt(bus);
+}
+
+static inline unsigned long page_to_bus( struct page *page )
+{
+	if (afterburn_phys_to_dma_hook)
+		return afterburn_phys_to_dma_hook(page_to_phys(page),PAGE_SIZE);
+	else
+		return page_to_phys(page);
+}
+
+#define isa_virt_to_bus virt_to_bus
+#define isa_page_to_bus page_to_bus
+#define isa_bus_to_virt bus_to_virt
+
+#else
 #define isa_virt_to_bus virt_to_phys
 #define isa_page_to_bus page_to_phys
 #define isa_bus_to_virt phys_to_virt
@@ -135,6 +171,7 @@
  */
 #define virt_to_bus virt_to_phys
 #define bus_to_virt phys_to_virt
+#endif
 
 /*
  * readX/writeX() are used to access memory mapped devices. On some
diff -Naur linux-2.6.9.src/include/asm-i386/page.h linux-2.6.9/include/asm-i386/page.h
--- linux-2.6.9.src/include/asm-i386/page.h	2004-10-18 23:53:22.000000000 +0200
+++ linux-2.6.9/include/asm-i386/page.h	2007-07-03 11:07:40.000000000 +0200
@@ -67,8 +67,14 @@
 #endif
 
 
+#if defined(CONFIG_AFTERBURN_XEN_HOOKS)
+#define pmd_val(x) pgd_read_annotate( (x).pmd )
+#define pgd_val(x) pgd_read_annotate( (x).pgd )
+#else
 #define pmd_val(x)	((x).pmd)
 #define pgd_val(x)	((x).pgd)
+#endif
+
 #define pgprot_val(x)	((x).pgprot)
 
 #define __pte(x) ((pte_t) { (x) } )
@@ -120,16 +126,25 @@
 
 #endif /* __ASSEMBLY__ */
 
-#ifdef __ASSEMBLY__
-#define __PAGE_OFFSET		(0xC0000000)
+#if defined(CONFIG_AFTERBURN_RELINK)
+# ifdef __ASSEMBLY__
+# define __PAGE_OFFSET		(CONFIG_AFTERBURN_LINK_BASE)
+# else
+# define __PAGE_OFFSET		((unsigned long)CONFIG_AFTERBURN_LINK_BASE)
+# endif
+# define MAXMEM			(CONFIG_AFTERBURN_VADDR_END-__PAGE_OFFSET-__VMALLOC_RESERVE)
 #else
-#define __PAGE_OFFSET		(0xC0000000UL)
+# ifdef __ASSEMBLY__
+# define __PAGE_OFFSET		(0xC0000000)
+# else
+# define __PAGE_OFFSET		(0xC0000000UL)
+# endif
+# define MAXMEM			(-__PAGE_OFFSET-__VMALLOC_RESERVE)
 #endif
 
 
 #define PAGE_OFFSET		((unsigned long)__PAGE_OFFSET)
 #define VMALLOC_RESERVE		((unsigned long)__VMALLOC_RESERVE)
-#define MAXMEM			(-__PAGE_OFFSET-__VMALLOC_RESERVE)
 #define __pa(x)			((unsigned long)(x)-PAGE_OFFSET)
 #define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
 #define pfn_to_kaddr(pfn)      __va((pfn) << PAGE_SHIFT)
diff -Naur linux-2.6.9.src/include/asm-i386/param.h linux-2.6.9/include/asm-i386/param.h
--- linux-2.6.9.src/include/asm-i386/param.h	2004-10-18 23:53:24.000000000 +0200
+++ linux-2.6.9/include/asm-i386/param.h	2007-07-03 11:07:40.000000000 +0200
@@ -1,8 +1,14 @@
 #ifndef _ASMi386_PARAM_H
 #define _ASMi386_PARAM_H
 
+#include <linux/config.h>
+
 #ifdef __KERNEL__
-# define HZ		1000		/* Internal kernel timer frequency */
+#if defined(CONFIG_AFTERBURN)
+# define HZ		100		/* Internal kernel timer frequency */
+#else
+# define HZ		100		/* Internal kernel timer frequency */
+#endif
 # define USER_HZ	100		/* .. some user interfaces are in "ticks" */
 # define CLOCKS_PER_SEC		(USER_HZ)	/* like times() */
 #endif
diff -Naur linux-2.6.9.src/include/asm-i386/pgtable-2level.h linux-2.6.9/include/asm-i386/pgtable-2level.h
--- linux-2.6.9.src/include/asm-i386/pgtable-2level.h	2004-10-18 23:54:31.000000000 +0200
+++ linux-2.6.9/include/asm-i386/pgtable-2level.h	2007-07-03 11:07:40.000000000 +0200
@@ -23,6 +23,15 @@
  * within a page table are directly modified.  Thus, the following
  * hook is made available.
  */
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+#include "../../afterburn/annotate.h"
+#define set_pte(pteptr, pteval) pte_set_annotate( (pte_t *)(pteptr), (pteval).pte_low )
+#define set_pgd(pgdptr, pgdval) pgd_set_annotate( (pgd_t *)(pgdptr), (pgdval).pgd )
+#define set_pmd(pmdptr, pmdval) pmd_set_annotate( (pmd_t *)(pmdptr), (pmdval).pmd )
+#define set_pte_atomic(pteptr, pteval) set_pte(pteptr,pteval)
+
+#else
+
 #define set_pte(pteptr, pteval) (*(pteptr) = pteval)
 #define set_pte_atomic(pteptr, pteval) set_pte(pteptr,pteval)
 /*
@@ -31,6 +40,7 @@
  */
 #define set_pmd(pmdptr, pmdval) (*(pmdptr) = pmdval)
 #define set_pgd(pgdptr, pgdval) (*(pgdptr) = pgdval)
+#endif
 
 #define pgd_page(pgd) \
 ((unsigned long) __va(pgd_val(pgd) & PAGE_MASK))
@@ -39,11 +49,29 @@
 {
 	return (pmd_t *) dir;
 }
-#define ptep_get_and_clear(xp)	__pte(xchg(&(xp)->pte_low, 0))
-#define pte_same(a, b)		((a).pte_low == (b).pte_low)
+static inline pte_t ptep_get_and_clear(pte_t *ptep)
+{
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+	return pte_read_clear_annotate(ptep);
+#endif
+	return __pte(xchg(&ptep->pte_low, 0));
+}
 #define pte_page(x)		pfn_to_page(pte_pfn(x))
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+#define pte_same(a, b)		(pte_read_annotate(a) == pte_read_annotate(b))
+#define pte_none(x)		(!(pte_read_annotate(x)))
+#else
+#define pte_same(a, b)		((a).pte_low == (b).pte_low)
 #define pte_none(x)		(!(x).pte_low)
-#define pte_pfn(x)		((unsigned long)(((x).pte_low >> PAGE_SHIFT)))
+#endif
+
+
+#if defined(CONFIG_AFTERBURN_XEN_HOOKS)
+#include "../../afterburn/annotate.h"
+#define pte_pfn(x)		((unsigned long)(((pte_read_annotate(pte_val(x))) >> PAGE_SHIFT)))
+#else
+#define pte_pfn(x)		((unsigned long)(((pte_val(x)) >> PAGE_SHIFT)))
+#endif
 #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #define pfn_pmd(pfn, prot)	__pmd(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
 
diff -Naur linux-2.6.9.src/include/asm-i386/pgtable.h linux-2.6.9/include/asm-i386/pgtable.h
--- linux-2.6.9.src/include/asm-i386/pgtable.h	2004-10-18 23:54:40.000000000 +0200
+++ linux-2.6.9/include/asm-i386/pgtable.h	2007-07-03 11:07:40.000000000 +0200
@@ -208,7 +208,6 @@
 #define pmd_clear(xp)	do { set_pmd(xp, __pmd(0)); } while (0)
 #define	pmd_bad(x)	((pmd_val(x) & (~PAGE_MASK & ~_PAGE_USER)) != _KERNPG_TABLE)
 
-
 #define pages_to_mb(x) ((x) >> (20-PAGE_SHIFT))
 
 /*
@@ -243,8 +242,15 @@
 # include <asm/pgtable-2level.h>
 #endif
 
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+#include "../../afterburn/annotate.h"
+#endif
+
 static inline int ptep_test_and_clear_dirty(pte_t *ptep)
 {
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+	return pte_test_and_clear_bit_annotate( _PAGE_BIT_DIRTY, ptep );
+#endif
 	if (!pte_dirty(*ptep))
 		return 0;
 	return test_and_clear_bit(_PAGE_BIT_DIRTY, &ptep->pte_low);
@@ -252,12 +258,22 @@
 
 static inline int ptep_test_and_clear_young(pte_t *ptep)
 {
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+	return pte_test_and_clear_bit_annotate( _PAGE_BIT_ACCESSED, ptep );
+#endif
 	if (!pte_young(*ptep))
 		return 0;
 	return test_and_clear_bit(_PAGE_BIT_ACCESSED, &ptep->pte_low);
 }
 
-static inline void ptep_set_wrprotect(pte_t *ptep)		{ clear_bit(_PAGE_BIT_RW, &ptep->pte_low); }
+static inline void ptep_set_wrprotect(pte_t *ptep)		
+{
+#if defined(CONFIG_AFTERBURN_ANNOTATIONS)
+	pte_test_and_clear_bit_annotate( _PAGE_BIT_RW, ptep );
+	return;
+#endif
+	clear_bit(_PAGE_BIT_RW, &ptep->pte_low); 
+}
 static inline void ptep_mkdirty(pte_t *ptep)			{ set_bit(_PAGE_BIT_DIRTY, &ptep->pte_low); }
 
 /*
@@ -393,7 +409,7 @@
 #define ptep_set_access_flags(__vma, __address, __ptep, __entry, __dirty) \
 	do {								  \
 		if (__dirty) {						  \
-			(__ptep)->pte_low = (__entry).pte_low;	  	  \
+			set_pte(__ptep, __entry);                         \
 			flush_tlb_page(__vma, __address);		  \
 		}							  \
 	} while (0)
diff -Naur linux-2.6.9.src/include/asm-i386/processor.h linux-2.6.9/include/asm-i386/processor.h
--- linux-2.6.9.src/include/asm-i386/processor.h	2004-10-18 23:53:07.000000000 +0200
+++ linux-2.6.9/include/asm-i386/processor.h	2007-07-03 11:07:40.000000000 +0200
@@ -290,7 +290,11 @@
 /*
  * User space process size: 3GB (default).
  */
+#if defined(CONFIG_AFTERBURN)
+#define TASK_SIZE	(CONFIG_AFTERBURN_USER_VADDR_END)
+#else
 #define TASK_SIZE	(PAGE_OFFSET)
+#endif
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
@@ -461,6 +465,13 @@
 static inline void load_esp0(struct tss_struct *tss, struct thread_struct *thread)
 {
 	tss->esp0 = thread->esp0;
+#if defined(CONFIG_AFTERBURN_XEN_HOOKS)
+	{
+	    extern void (*afterburn_sync_esp0)( void );
+	    if( afterburn_sync_esp0 )
+		afterburn_sync_esp0();
+	}
+#endif
 	/* This can only happen when SEP is enabled, no need to test "SEP"arately */
 	if (unlikely(tss->ss1 != thread->sysenter_cs)) {
 		tss->ss1 = thread->sysenter_cs;
diff -Naur linux-2.6.9.src/include/asm-i386/system.h linux-2.6.9/include/asm-i386/system.h
--- linux-2.6.9.src/include/asm-i386/system.h	2004-10-18 23:53:06.000000000 +0200
+++ linux-2.6.9/include/asm-i386/system.h	2007-07-03 11:07:40.000000000 +0200
@@ -81,7 +81,7 @@
 #define loadsegment(seg,value)			\
 	asm volatile("\n"			\
 		"1:\t"				\
-		"movl %0,%%" #seg "\n"		\
+		"mov %0,%%" #seg "\n"		\
 		"2:\n"				\
 		".section .fixup,\"ax\"\n"	\
 		"3:\t"				\
@@ -93,13 +93,13 @@
 		".align 4\n\t"			\
 		".long 1b,3b\n"			\
 		".previous"			\
-		: :"m" (*(unsigned int *)&(value)))
+		: :"m" (value))
 
 /*
  * Save a segment register away
  */
 #define savesegment(seg, value) \
-	asm volatile("movl %%" #seg ",%0":"=m" (*(int *)&(value)))
+	asm volatile("mov %%" #seg ",%0":"=m" (value))
 
 /*
  * Clear and set 'TS' bit respectively
diff -Naur linux-2.6.9.src/include/asm-i386/uaccess.h linux-2.6.9/include/asm-i386/uaccess.h
--- linux-2.6.9.src/include/asm-i386/uaccess.h	2004-10-18 23:53:22.000000000 +0200
+++ linux-2.6.9/include/asm-i386/uaccess.h	2007-07-03 11:07:40.000000000 +0200
@@ -24,6 +24,15 @@
 
 #define MAKE_MM_SEG(s)	((mm_segment_t) { (s) })
 
+#if defined(CONFIG_AFTERBURN_HOOK_UACCESS)
+#define ON_AFTERBURN_HOOK_UACCESS(a) a
+extern unsigned long (*afterburn_get_user_hook)(void *to, const void *from, unsigned long n);
+extern unsigned long (*afterburn_put_user_hook)(void *to, const void *from, unsigned long n);
+extern unsigned long (*afterburn_copy_from_user_hook)(void *to, const void *from, unsigned long n);
+extern unsigned long (*afterburn_copy_to_user_hook)(void *to, const void *from, unsigned long n);
+#else
+#define ON_AFTERBURN_HOOK_UACCESS(a)
+#endif
 
 #define KERNEL_DS	MAKE_MM_SEG(0xFFFFFFFFUL)
 #define USER_DS		MAKE_MM_SEG(PAGE_OFFSET)
@@ -172,12 +181,23 @@
 #define get_user(x,ptr)							\
 ({	int __ret_gu,__val_gu;						\
 	__chk_user_ptr(ptr);						\
+ON_AFTERBURN_HOOK_UACCESS(						\
+	if(afterburn_get_user_hook) { 					\
+		if(unlikely(!__addr_ok(ptr)))				\
+			__ret_gu = 0;					\
+		else							\
+			__ret_gu = afterburn_get_user_hook(&__val_gu, ptr, sizeof(*(ptr)));\
+		if(likely(__ret_gu == sizeof(*(ptr)))) __ret_gu = 0;	\
+		else { __ret_gu = -EFAULT; __val_gu = 0; }		\
+	} else {							\
+)									\
 	switch(sizeof (*(ptr))) {					\
 	case 1:  __get_user_x(1,__ret_gu,__val_gu,ptr); break;		\
 	case 2:  __get_user_x(2,__ret_gu,__val_gu,ptr); break;		\
 	case 4:  __get_user_x(4,__ret_gu,__val_gu,ptr); break;		\
 	default: __get_user_x(X,__ret_gu,__val_gu,ptr); break;		\
 	}								\
+ON_AFTERBURN_HOOK_UACCESS( } )						\
 	(x) = (__typeof__(*(ptr)))__val_gu;				\
 	__ret_gu;							\
 })
@@ -291,6 +311,13 @@
 do {									\
 	retval = 0;							\
 	__chk_user_ptr(ptr);						\
+ON_AFTERBURN_HOOK_UACCESS(						\
+	if(afterburn_put_user_hook) {					\
+		__typeof__(*(ptr)) __pus_tmp = x; 			\
+		if(unlikely(!afterburn_put_user_hook(ptr, &__pus_tmp, size))) \
+			retval = errret;				\
+	} else {							\
+)									\
 	switch (size) {							\
 	case 1: __put_user_asm(x,ptr,retval,"b","b","iq",errret);break;	\
 	case 2: __put_user_asm(x,ptr,retval,"w","w","ir",errret);break; \
@@ -298,6 +325,7 @@
 	case 8: __put_user_u64((__typeof__(*ptr))(x),ptr,retval); break;\
 	  default: __put_user_bad();					\
 	}								\
+ON_AFTERBURN_HOOK_UACCESS( } )						\
 } while (0)
 
 #else
@@ -350,12 +378,20 @@
 do {									\
 	retval = 0;							\
 	__chk_user_ptr(ptr);						\
+ON_AFTERBURN_HOOK_UACCESS(						\
+	if(afterburn_get_user_hook) {					\
+		if(unlikely(!afterburn_get_user_hook(&(x), ptr, size)))	\
+			{ (x) = 0; retval = errret; }			\
+	}								\
+	else {								\
+)									\
 	switch (size) {							\
 	case 1: __get_user_asm(x,ptr,retval,"b","b","=q",errret);break;	\
 	case 2: __get_user_asm(x,ptr,retval,"w","w","=r",errret);break;	\
 	case 4: __get_user_asm(x,ptr,retval,"l","","=r",errret);break;	\
 	default: (x) = __get_user_bad();				\
 	}								\
+ON_AFTERBURN_HOOK_UACCESS( } )						\
 } while (0)
 
 #define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)	\
diff -Naur linux-2.6.9.src/include/linux/sched.h linux-2.6.9/include/linux/sched.h
--- linux-2.6.9.src/include/linux/sched.h	2004-10-18 23:53:13.000000000 +0200
+++ linux-2.6.9/include/linux/sched.h	2007-07-03 11:07:40.000000000 +0200
@@ -459,6 +459,9 @@
 #ifdef CONFIG_SCHEDSTATS
 	struct sched_info sched_info;
 #endif
+#ifdef CONFIG_AFTERBURN_THREAD_HOOKS
+	void * afterburn_handle;
+#endif
 
 	struct list_head tasks;
 	/*
diff -Naur linux-2.6.9.src/kernel/exit.c linux-2.6.9/kernel/exit.c
--- linux-2.6.9.src/kernel/exit.c	2004-10-18 23:55:06.000000000 +0200
+++ linux-2.6.9/kernel/exit.c	2007-07-03 11:07:40.000000000 +0200
@@ -815,6 +815,12 @@
 	__exit_fs(tsk);
 	exit_namespace(tsk);
 	exit_thread();
+#if defined(CONFIG_AFTERBURN_THREAD_HOOKS)
+	{
+		extern void (*afterburn_exit_hook)( void * handle );
+		afterburn_exit_hook(tsk->afterburn_handle);
+	}
+#endif
 
 	if (tsk->signal->leader)
 		disassociate_ctty(1);
diff -Naur linux-2.6.9.src/kernel/fork.c linux-2.6.9/kernel/fork.c
--- linux-2.6.9.src/kernel/fork.c	2004-10-18 23:53:13.000000000 +0200
+++ linux-2.6.9/kernel/fork.c	2007-07-03 11:07:40.000000000 +0200
@@ -975,6 +975,10 @@
 
 	p->proc_dentry = NULL;
 
+#if defined(CONFIG_AFTERBURN_THREAD_HOOKS)
+	p->afterburn_handle = NULL;
+#endif
+
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	init_waitqueue_head(&p->wait_chldexit);
diff -Naur linux-2.6.9.src/kernel/module.c linux-2.6.9/kernel/module.c
--- linux-2.6.9.src/kernel/module.c	2004-10-18 23:54:40.000000000 +0200
+++ linux-2.6.9/kernel/module.c	2007-07-03 11:07:40.000000000 +0200
@@ -1688,6 +1688,17 @@
 		DEBUGP("\t0x%lx %s\n", sechdrs[i].sh_addr, secstrings + sechdrs[i].sh_name);
 	}
 	/* Module has been moved. */
+#if defined(CONFIG_AFTERBURN_MODULE_HOOKS)
+	{
+	    extern int (*afterburn_rewrite_module_hook)(Elf_Ehdr *);
+	    if( afterburn_rewrite_module_hook && 
+		    !afterburn_rewrite_module_hook(hdr) ) {
+	       	err = -ENOEXEC;
+    		goto cleanup;
+	    }
+	}
+#endif
+
 	mod = (void *)sechdrs[modindex].sh_addr;
 
 	/* Now we've moved module, initialize linked lists, etc. */
diff -Naur linux-2.6.9.src/kernel/sched.c linux-2.6.9/kernel/sched.c
--- linux-2.6.9.src/kernel/sched.c	2004-10-18 23:54:55.000000000 +0200
+++ linux-2.6.9/kernel/sched.c	2007-07-03 11:07:40.000000000 +0200
@@ -3879,6 +3879,7 @@
 {
 	runqueue_t *rq_dest, *rq_src;
 
+	//printk("%x %d -> %d\n", p->afterburn_handle, src_cpu, dest_cpu);
 	if (unlikely(cpu_is_offline(dest_cpu)))
 		return;
 
diff -Naur linux-2.6.9.src/Makefile linux-2.6.9/Makefile
--- linux-2.6.9.src/Makefile	2004-10-18 23:54:38.000000000 +0200
+++ linux-2.6.9/Makefile	2007-07-03 11:07:40.000000000 +0200
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 6
 SUBLEVEL = 9
-EXTRAVERSION =
+EXTRAVERSION = -afterburn
 NAME=Zonked Quokka
 
 # *DOCUMENTATION*
@@ -525,6 +525,7 @@
 
 ifeq ($(KBUILD_EXTMOD),)
 core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/
+core-$(CONFIG_AFTERBURN) += afterburn/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff -Naur linux-2.6.9.src/mm/mmap.c linux-2.6.9/mm/mmap.c
--- linux-2.6.9.src/mm/mmap.c	2004-10-18 23:54:37.000000000 +0200
+++ linux-2.6.9/mm/mmap.c	2007-07-03 11:07:40.000000000 +0200
@@ -1832,6 +1832,13 @@
 
 	lru_add_drain();
 
+
+#if defined(CONFIG_AFTERBURN)
+	{
+		extern unsigned long (*afterburn_free_pgd_hook)(unsigned long pgd);
+		afterburn_free_pgd_hook(__pa((void*)mm->pgd));
+	}
+#endif
 	spin_lock(&mm->page_table_lock);
 
 	tlb = tlb_gather_mmu(mm, 1);
diff -Naur linux-2.6.9.src/scripts/Makefile.build linux-2.6.9/scripts/Makefile.build
--- linux-2.6.9.src/scripts/Makefile.build	2004-10-18 23:54:55.000000000 +0200
+++ linux-2.6.9/scripts/Makefile.build	2007-07-03 11:07:40.000000000 +0200
@@ -143,7 +143,15 @@
 quiet_cmd_cc_o_c = CC $(quiet_modtag)  $@
 
 ifndef CONFIG_MODVERSIONS
+ifndef CONFIG_AFTERBURN
 cmd_cc_o_c = $(CC) $(c_flags) -c -o $@ $<
+else
+params_afterburner-$(CONFIG_AFTERBURN_KERNEL_PROFILING) = -p
+params_cc_o_c-$(CONFIG_AFTERBURN_KERNEL_PROFILING) = -pg
+cmd_cc_o_c  = $(CC) $(c_flags) -c -S $(params_cc_o_c-y) -o $(@D)/.preburn_$(@F:.o=.s) $< 
+cmd_cc_o_c += && afterburner $(params_afterburner-y) $(@D)/.preburn_$(@F:.o=.s) > $(@D)/.afterburnt_$(@F:.o=.s)
+cmd_cc_o_c += && $(CC) $(filter-out -g,$(c_flags)) -c -o $@ $(@D)/.afterburnt_$(@F:.o=.s)
+endif
 
 else
 # When module versioning is enabled the following steps are executed:
@@ -220,7 +228,13 @@
 	$(call if_changed_dep,as_s_S)
 
 quiet_cmd_as_o_S = AS $(quiet_modtag)  $@
+ifndef CONFIG_AFTERBURN
 cmd_as_o_S       = $(CC) $(a_flags) -c -o $@ $<
+else
+cmd_as_o_S        = $(CC) $(a_flags) -c -E -o $(@D)/.preburn_$(@F:.o=.s) $< 
+cmd_as_o_S       += && afterburner $(@D)/.preburn_$(@F:.o=.s) > $(@D)/.afterburnt_$(@F:.o=.s)
+cmd_as_o_S       += && $(CC) $(filter-out -g,$(a_flags)) -c -o $@ $(@D)/.afterburnt_$(@F:.o=.s)
+endif
 
 %.o: %.S FORCE
 	$(call if_changed_dep,as_o_S)
